@article{__ricg___agiautomated_2023,
  title = {AGI-Automated Interpretability is Suicide},
  author = {\_\_RicG\_\_},
  year = {2023},
  month = may,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/pQqoTTAnEePRDmZN4/agi-automated-interpretability-is-suicide},
  urldate = {2023-12-05},
  abstract = {Backstory: I wrote this post about 12d ago, then a user here pointed out that this could be capability exfohazard since it could give the bad idea of having the AGI look at itself, so I took it down. Well I don't have to worry about that anymore since now we have proof that they are literally doing it right now at OpenAI. I don't want to piss on the parade, and I still think automating interpretability right now is a good thing, but sooner or later, if not done right, there is a high chance it's all gonna backfire so hard we will{\dots} well, everybody dies. The following is an expanded version of the previous post. Foom through a change of paradigm and the dangers of white-boxes TL;DR Smart AI solves interpretability and how cognition works leading to the possibility of fooming the old-fashioned way of just optimizing its own cognition algorithms. From a black-box to a white-box. White-boxes we didn't get to design and explicitly align are deadly because of their high likelihood of foom and thus high chance to bypass whatever prosaic alignment scheme we used on it. We should not permit AGI labs to allow reflection. Interpretability should not be automated by intelligent machines. I have not found any posts on this particular way that an AI could achieve foom starting from the deep learning paradigm, so, keeping in mind that foom isn't a necessity for an AGI to kill everyone (if the AGI takes 30 years to kill everyone, does it matter?), in this post I will cover a small, obvious insight I had and its possible consequences. The other two known ways, not discussed here, a deep-learning based AI can achieve some sort of rapid capability gain are through recursive AI-driven hardware improvements (AI makes better GPUs to make training cheaper and faster) and recursive AI-driven neural networks architecture improvements (like the jump from RNN to transformers). I see a lot of people hell-bent on arguing that hard-takeoff is utterly impossible since the only way to make intelligence is to throw expensive compute at a neural network, which seems to miss the bigger picture of what an intelligent system is or what it could do. I also see AI safety researchers debating that foom isn't a real possibility anymore because of the current deep learning paradigm and while that is true right now, I don't expect it to last very long. Change of paradigm foom Picture this: the AGI is smart enough to gather big insights into cognition and its own internals. It solves (or mostly solves) interpretability and is able to write down the algorithms that its mind uses to be an AGI. If it solves interpretability, it can make the jump to an algorithmic-based AI, or partially make the jump, as in only some parts of its massive NN are ripped off and replaced with algorithms: it becomes a white-box (or a grey-box if you wish). After all, the giant matrix multiplications and activation functions can be boiled down to hard-coded algorithms or even just multivariate functions that could be explicitly stated and maybe made to run faster and/or improved without messing around with the complexity of modifying NN weights. And, at least to our kind of intelligence, dealing with a white-box is much less complex, and it would allow the AI to reason and reflect upon itself way more effectively than looking at spaghetti NNs weights. This paradigm shift also gets around the problem of "AIs not creating other AIs because of alignment difficulties" since it could edit itself at the code level, and not just stir calculus and giant matrices in a cauldron under a hot flame of compute hoping the alchemy produces a new AI aligned to itself. So here's your nightmare scenario: it understands itself better than we do, converts itself to an algorithmic AI, and then recursively self-improves the old-fashioned way by improving its cognitive algorithms without the need to do expensive and time-consuming training. The dangers lie in making reflection easier by blurring the line between neural networks and algorithmic AI: white-boxes from black-boxes. By being able to reflect upon itself clearly, it can foom using just "pen and paper" without even touching the 100b\$ supercomputer full of GPUs that were used to train it. The only thing it needs access to are its weights, pen and paper. It will completely ignore scaling laws or whatever diminishing returns on compute or data we might have hit. This will likely result in an intelligence explosion that could be easily uncontainable, misaligned, and kill everything on earth. White-boxes that we didn't get to design and explicitly align are actually the most dangerous types of AIs I can think of, short of, of course, running 'AGI.exe' found on a random thumb drive in the sand of a beachside on a post-apocalyptic alien planet. So that's my obvious little insight that is just slightly different than "it modifies its own weights". If it's smart enough to do good interpretability work, it's probably smart enough to make the jump. When do I think this will be a problem? The sparse timeline I envision is: Now Powerful AGI better than humans in most domains prosaically aligned AGI able to do good interpretability if allowed to Change of paradigm foom Death, given that no formal general theory of cognition and alignment exists, or massive amounts of luck Before the AGI is able to contribute to interpretability and explore its cognition algorithms well enough, it will likely be powerful in other ways, and we would have needed to have aligned it using, probably, some prosaic deep-learned way. And even once it is smart enough, it might take a couple of years of research to crack its inner workings comprehensively{\dots} but once that happens (or while that happens){\dots} we can kiss our deep-learned alignment goodbye, alongside our ass. This does assume that "capabilities generalize further than alignment", as in "the options created by its new abilities will likely circumvent the alignment scheme". The jump from evolved to designed intelligence leads me to believe the increase in intelligence will be rapidly accelerating for a good while before hitting diminishing returns or hardware limitations. This increase in intelligence opens up new options on how it can optimize its learned utility function in "weird" ways, which will most likely end up with us dead. {$>$}Well, if the AGI is so good at interpretability and cognition theory, why would it help us get that formal general theory of cognition and alignment? I expect that fully interpreting one mind doesn't lead to particularly great insights on the general theory or even on how to specifically align that specific single mind. I could be wrong, and I hope I am, but security mindset doesn't like "assuming good things just because it solves the problem". {$>$}The prosaic aligned AGI won't reflect because it knows of the dangers! This is one example of previously unstated insight that, without it, we (humans and AGIs) might have thought that delegating interpretability to an AGI might just be ok. It needs to be taught that reflection is potentially bad, or else it might do it in good faith. This is why I am writing this post to drive home the point that a fully-interpretable model is dangerous until proven otherwise. Obvious ways to mitigate this danger? One obvious way is to not allow the machine to look at its own weights. Just don't allow reflection. Full stop. This should be obvious from a safety point of view, but it's better to just repeat it ad nauseam. No lab should be allowed to have the AI reflect on itself. Not allowing the machine to look at its weights, of course, implies that there is some sort of alignment on it. If no alignment scheme is in place, this type of foom is probably a problem we would be too dead to worry about. A key point in the possible regulations that the whole world is currently scrambling to write down should be: AIs should be limited by what we humans can come up with until we understand what we are doing. And a more wishful thinking proposal: interpretability should not be used for capabilities until we understand what we are doing. I would also emphasize another necessary step, to echo Nate Soares in "If interpretability research goes well, it may get dangerous",  that once it starts making headway into the cognition process, interpretability should be developed underground, not in public. Along the same line of thought: interpretability should not be automated by intelligent machines. I say intelligent machines because narrow tools that helps us with interpretability seem fine. Teaching the prosaic aligned AGI to not expand on its cognitive abilities should probably be one of the explicit goals of the prosaic alignment scheme. White-boxes we didn't get to explicitly aim are suicide. Avoiding Dropping the Ball This unfortunately feels like "They won't connect the AGI to the internet, causing a race between companies that then forgo safety to push their unsafe product on the internet as well, right? Nobody is that foolish, right? Right guys? {\dots} guys?" And it might be that saying out loud, "don't make the machine look at itself" is too dangerous in itself, but, in this case, I don't like the idea of staying silent on an insight on how a powerful intelligence could catch us with our figurative pants down. Elucidating a path to destruction can be exfohazard and even "If interpretability research goes well, it may get dangerous" was vague, probably exactly for this reason, but, in this case, I see how us dropping the ball and confidently dismissing a pathway to destruction can lead to... well... destruction. Closing our eyes to the fastest method of foom from DL (given sufficient intelligence) creates a pretty big chance of us getting overconfident, IMO. Especially if capability interpretability is carried out under the guise of safety. Now that the whole world is looking at putting more effort into AI safety, we should not allow big players to make the mistake of putting all of our eggs in the full-interpretability basket, or, heaven forbid, AGI-automated interpretability [author note: lol, lmao even], even if it is the only alignment field in which has good feedback loops and we can tell if progress is happening. Before we rush to bet everything on interpretability, we should again ask ourselves "What if it succeeds?". To reiterate: white-boxes we didn't get to align are suicide. With this small insight, the trade-offs of successful interpretability, in the long run, seem to be heavily skewed on the danger side. Fully or near-fully interpretable ML-based AIs have a great potential of fooming away without, IMO, really lowering the risks associated with misalignment. There probably is an interpretability sweet spot where some interpretability helps us detecting deceit, but doesn't help the AI make the jump of paradigm, and I welcome that. Let's try to hit that. The inferential step that I think no-one before has elucidated is the fact that we are already doing interpretability work to reverse engineer the black boxes and that that process can be automated in the near future. Interp+Automation={$>$}Foom. List of assumptions that I am making In no particular order: Capabilities generalize further than alignment Algorithmic foom (k{$>$}1) is possible The intelligence ceiling is much higher than what we can achieve with just DL The ceiling of hard-coded intelligence that runs on near-future hardware isn't particularly limited by the hardware itself: algorithms interpreted from matrix multiplications are efficient enough on available hardware. This is maybe my shakiest hypothesis: matrix multiplication in GPUs is actually pretty damn well optimized NN -{$>$} algorithms is possible Algorithms are easier to reason about than staring at NNs weights Solving interpretability with an AGI (even with humans-in-the-loop) might not lead to particularly great insights on a general alignment theory or even on how to specifically align a particular AGI We won't solve interpretability or general alignment before powerful AGIs are widespread What I am not assuming: Everything needs to be interpretable: parts of the NN can remain a NN and the AI can just edit the other algorithms responsible for the more narrow/general cognition. I expect that if, in the unlikely event that inside an AGI there is something akin to an ``aesthetic box'' or even a ``preferences box'' it could remain a NN. Those parts remaining black-boxes wouldn't (directly) be dangerous to us. I dismissed foom in the deep learning paradigm too for a while before realizing this possibility. A lot of people believe foom is at the center of the arguments, and while that is false, if foom is back on the menu, the p(doom) can only increase. I realize the title is maybe a little bit clickbaity, sorry for that. Disclaimer: I am not a NN expert, so I might be missing something important about how matrix multiplication -{$>$} hard-coded algorithms isn't feasible or couldn't possibly reduce the complexity of the problem of studying one's own cognition. I might have also failed to realize how this scenario had always been obvious, and I just happened to not read up on it. I am also somewhat quite new to the space of AI alignment, so I could be misinterpreting and misrepresenting current alignment efforts. Give feedback pls.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/4YQSBU3C/__RicG__ - 2023 - AGI-Automated Interpretability is Suicide.html}
}

@misc{_aisafetyfoundation_2024,
  title = {Sparse Autoencoder Library},
  author = {Cooney, Alan},
  year = {2024},
  month = feb,
  url = {https://github.com/ai-safety-foundation/sparse_autoencoder},
  urldate = {2024-02-16},
  abstract = {Sparse Autoencoder for Mechanistic Interpretability},
  copyright = {MIT},
  howpublished = {ai-safety-foundation}
}

@misc{_social_,
  title = {SOCIAL MEDIA TITLE TAG},
  url = {http://localhost:8000/URL OF THE WEBSITE},
  urldate = {2024-07-01},
  abstract = {SOCIAL MEDIA DESCRIPTION TAG TAG},
  file = {/Users/leonardbereska/Zotero/storage/WQLS8R8E/localhost.html}
}

@article{abdou_can_2021,
  title = {Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color},
  shorttitle = {Can Language Models Encode Perceptual Structure Without Grounding?},
  author = {Abdou, Mostafa and Kulmizev, Artur and Hershcovich, Daniel and Frank, Stella and Pavlick, Ellie and S{\o}gaard, Anders},
  editor = {Bisazza, Arianna and Abend, Omri},
  year = {2021},
  month = nov,
  journal = {CoNLL},
  pages = {109--132},
  doi = {10.18653/v1/2021.conll-1.9},
  url = {https://aclanthology.org/2021.conll-1.9},
  urldate = {2024-01-23},
  abstract = {Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases --- (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5958I5MB/Abdou et al. - 2021 - Can Language Models Encode Perceptual Structure Wi.pdf}
}

@article{abraham_cebab_2022,
  title = {CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior},
  shorttitle = {CEBaB},
  author = {Abraham, Eldar David and D'Oosterlinck, Karel and Feder, Amir and Gat, Yair Ori and Geiger, Atticus and Potts, Christopher and Reichart, Roi and Wu, Zhengxuan},
  year = {2022},
  month = oct,
  journal = {NeurIPS},
  eprint = {2205.14140},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.14140},
  url = {http://arxiv.org/abs/2205.14140},
  urldate = {2024-03-14},
  abstract = {The increasing size and complexity of modern ML systems has improved their predictive capabilities but made their behavior harder to explain. Many techniques for model explanation have been developed in response, but we lack clear criteria for assessing these techniques. In this paper, we cast model explanation as the causal inference problem of estimating causal effects of real-world concepts on the output behavior of ML models given actual input data. We introduce CEBaB, a new benchmark dataset for assessing concept-based explanation methods in Natural Language Processing (NLP). CEBaB consists of short restaurant reviews with human-generated counterfactual reviews in which an aspect (food, noise, ambiance, service) of the dining experience was modified. Original and counterfactual reviews are annotated with multiply-validated sentiment ratings at the aspect-level and review-level. The rich structure of CEBaB allows us to go beyond input features to study the effects of abstract, real-world concepts on model behavior. We use CEBaB to compare the quality of a range of concept-based explanation methods covering different assumptions and conceptions of the problem, and we seek to establish natural metrics for comparative assessments of these methods.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/PXGEKWZK/Abraham et al. - 2022 - CEBaB Estimating the Causal Effects of Real-World.pdf}
}

@article{adadi_peeking_2018,
  title = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
  shorttitle = {Peeking Inside the Black-Box},
  author = {Adadi, Amina and Berrada, Mohammed},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  url = {https://ieeexplore.ieee.org/document/8466590},
  urldate = {2023-10-21},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/K65DF658/Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf}
}

@article{adebayo_debugging_2020,
  title = {Debugging Tests for Model Explanations},
  author = {Adebayo, Julius and Muelly, Michael and Liccardi, Ilaria and Kim, Been},
  year = {2020},
  month = nov,
  journal = {NeurIPS},
  eprint = {2011.05429},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2011.05429},
  url = {http://arxiv.org/abs/2011.05429},
  urldate = {2023-11-20},
  abstract = {We investigate whether post-hoc model explanations are effective for diagnosing model errors--model debugging. In response to the challenge of explaining a model's prediction, a vast array of explanation methods have been proposed. Despite increasing use, it is unclear if they are effective. To start, we categorize {\textbackslash}textit\{bugs\}, based on their source, into:{\textasciitilde}{\textbackslash}textit\{data, model, and test-time\} contamination bugs. For several explanation methods, we assess their ability to: detect spurious correlation artifacts (data contamination), diagnose mislabeled training examples (data contamination), differentiate between a (partially) re-initialized model and a trained one (model contamination), and detect out-of-distribution inputs (test-time contamination). We find that the methods tested are able to diagnose a spurious background bug, but not conclusively identify mislabeled training examples. In addition, a class of methods, that modify the back-propagation algorithm are invariant to the higher layer parameters of a deep network; hence, ineffective for diagnosing model contamination. We complement our analysis with a human subject study, and find that subjects fail to identify defective models using attributions, but instead rely, primarily, on model predictions. Taken together, our results provide guidance for practitioners and researchers turning to explanations as tools for model debugging.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/8NCFXH6P/Adebayo et al. - 2020 - Debugging Tests for Model Explanations.pdf}
}

@article{aflalo_vl-interpret_2022,
  title = {VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers},
  shorttitle = {VL-InterpreT},
  author = {Aflalo, Estelle and Du, Meng and Tseng, Shao-Yen and Liu, Yongfei and Wu, Chenfei and Duan, Nan and Lal, Vasudev},
  year = {2022},
  month = jun,
  journal = {CVPR},
  pages = {21374--21383},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.02072},
  url = {https://ieeexplore.ieee.org/document/9880368/},
  urldate = {2023-10-25},
  abstract = {Breakthroughs in transformer-based models have revolutionized not only the NLP field, but also vision and multimodal systems. However, although visualization and interpretability tools have become available for NLP models, internal mechanisms of vision and multimodal transformers remain largely opaque. With the success of these transformers, it is increasingly critical to understand their inner workings, as unraveling these black-boxes will lead to more capable and trustworthy models. To contribute to this quest, we propose VL-InterpreT, which provides novel interactive visualizations for interpreting the attentions and hidden representations in multimodal transformers. VL-InterpreT is a task agnostic and integrated tool that (1) tracks a variety of statistics in attention heads throughout all layers for both vision and language components, (2) visualizes cross-modal and intra-modal attentions through easily readable heatmaps, and (3) plots the hidden representations of vision and language tokens as they pass through the transformer layers. In this paper, we demonstrate the functionalities of VL-InterpreT through the analysis of KD-VLP, an end-to-end pretraining vision-language multimodal transformer-based model, in the tasks of Visual Commonsense Reasoning (VCR) and WebQA, two visual question answering benchmarks. Furthermore, we also present a few interesting findings about multimodal transformer behaviors that were learned through our tool.},
  isbn = {9781665469463},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/CFAGH8TP/Aflalo et al. - 2022 - VL-InterpreT An Interactive Visualization Tool fo.pdf}
}

@article{alain_understanding_2016,
  title = {Understanding intermediate layers using linear classifier probes},
  author = {Alain, Guillaume and Bengio, Yoshua},
  year = {2016},
  journal = {ICLR},
  eprint = {1610.01644},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1610.01644},
  url = {http://arxiv.org/abs/1610.01644},
  urldate = {2023-11-10},
  abstract = {Neural network models have a reputation for being black boxes. We propose to monitor the features at every layer of a model and measure how suitable they are for classification. We use linear classifiers, which we refer to as "probes", trained entirely independently of the model itself. This helps us better understand the roles and dynamics of the intermediate layers. We demonstrate how this can be used to develop a better intuition about models and to diagnose potential problems. We apply this technique to the popular models Inception v3 and Resnet-50. Among other things, we observe experimentally that the linear separability of features increase monotonically along the depth of the model.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KDPQWAP3/Alain and Bengio - 2016 - Understanding intermediate layers using linear cla.pdf}
}

@article{alamerton_review_,
  title = {A Review of In-Context Learning Hypotheses for Automated AI Alignment Research},
  author = {{alamerton}},
  url = {https://www.lesswrong.com/posts/GPcwP8pgyPFPwvi2h/a-review-of-in-context-learning-hypotheses-for-automated-ai},
  urldate = {2024-04-30},
  abstract = {This project has been completed as part of the Mentorship in Alignment Research Students (MARS London) programme under the supervision of Bogdan-Ionu{\dots}},
  language = {en}
}

@article{albeck_collecting_2006,
  title = {Collecting and organizing systematic sets of protein data},
  author = {Albeck, John G. and MacBeath, Gavin and White, Forest M. and Sorger, Peter K. and Lauffenburger, Douglas A. and Gaudet, Suzanne},
  year = {2006},
  month = nov,
  journal = {Nat Rev Mol Cell Biol},
  volume = {7},
  number = {11},
  pages = {803--812},
  publisher = {Nature Publishing Group},
  issn = {1471-0080},
  doi = {10.1038/nrm2042},
  url = {https://www.nature.com/articles/nrm2042},
  urldate = {2024-04-30},
  abstract = {Systems biology aims to develop experimentally-validated quantitative models of complex biochemical pathways. The data needed for this task cannot be generated by 'omics'-type methods alone, but instead require a hypothesis-driven approach that integrates multiple experimental techniques.There are trade-offs to consider when choosing the types of assay to use in a systems biology study because throughput, multiplexing, sample size, sampling density and ease of use cannot all be simultaneously maximized. Experimental studies should be designed to be compatible with the chosen modelling approach.For monitoring protein-signalling events, affinity-based methods, mass-spectrometry and protein-activity assays each have their strengths and weaknesses. Recent advances continue to improve the usefulness of these assays for systems biology.Single-cell measurements are another important element in developing accurate models of biochemical events. Because only few signals can be monitored at the single-cell level, these data are most effective when combined with population-level biochemical assays.Appropriate data validation and normalization techniques are crucial for constructing consistent data sets to inform modelling approaches. Several methods of data scaling can be used to highlight different features of a data set.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/MZ9SG2G3/Albeck et al. - 2006 - Collecting and organizing systematic sets of prote.pdf}
}

@article{aldridge_physicochemical_2006,
  title = {Physicochemical modelling of cell signalling pathways},
  author = {Aldridge, Bree B. and Burke, John M. and Lauffenburger, Douglas A. and Sorger, Peter K.},
  year = {2006},
  month = nov,
  journal = {Nat Cell Biol},
  volume = {8},
  number = {11},
  pages = {1195--1203},
  publisher = {Nature Publishing Group},
  issn = {1476-4679},
  doi = {10.1038/ncb1497},
  url = {https://www.nature.com/articles/ncb1497},
  urldate = {2024-04-30},
  abstract = {Physicochemical modelling of signal transduction links fundamental chemical and physical principles, prior knowledge about regulatory pathways, and experimental data of various types to create powerful tools for formalizing and extending traditional molecular and cellular biology.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/MD5I5T4H/Aldridge et al. - 2006 - Physicochemical modelling of cell signalling pathw.pdf}
}

@article{alexander_biological_2022,
  title = {Biological Anchors: A Trick That Might Or Might Not Work},
  shorttitle = {Biological Anchors},
  author = {Alexander, Scott},
  year = {2022},
  month = feb,
  journal = {Astral Codex Ten},
  url = {https://astralcodexten.substack.com/p/biological-anchors-a-trick-that-might},
  urldate = {2023-08-26},
  abstract = {...},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/WS2F9C8K/Alexander - 2022 - Biological Anchors A Trick That Might Or Might No.html}
}

@article{ali_explainable_2023,
  title = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
  shorttitle = {Explainable Artificial Intelligence (XAI)},
  author = {Ali, Sajid and Abuhmed, Tamer and {El-Sappagh}, Shaker and Muhammad, Khan and {Alonso-Moral}, Jose M. and Confalonieri, Roberto and Guidotti, Riccardo and Del Ser, Javier and {D{\'i}az-Rodr{\'i}guez}, Natalia and Herrera, Francisco},
  year = {2023},
  month = nov,
  journal = {Information Fusion},
  volume = {99},
  pages = {101805},
  issn = {15662535},
  doi = {10.1016/j.inffus.2023.101805},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253523001148},
  urldate = {2023-10-21},
  abstract = {Semantic Scholar extracted view of "Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence" by Sajid Ali et al.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5QYNYL4C/Ali et al. - 2023 - Explainable Artificial Intelligence (XAI) What we.pdf}
}

@article{allen-zhu_how_2023,
  title = {How Language Models Learn Context-Free Grammars},
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=qnbLGV9oFL},
  urldate = {2024-06-10},
  abstract = {We design experiments to study *how* generative language models, such as GPT, learn context-free grammars (CFGs) --- complex language systems with tree-like structures that encapsulate aspects of human logic, natural languages, and programs. CFGs, comparable in difficulty to pushdown automata, can be ambiguous, usually requiring dynamic programming for rule verification. We create synthetic data to show that pre-trained transformers can learn to generate sentences with near-perfect accuracy and impressive diversity, even for quite challenging CFGs. Crucially, we uncover the *mechanisms* behind transformers learning such CFGs. We find that the hidden states implicitly encode the CFG structure (such as putting tree node info exactly on the subtree boundary), and that the transformer can form "boundary to boundary" attentions that mimic dynamic programming. We also discuss CFG extensions and transformer robustness against grammar errors.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/JNI97WG2/Allen-Zhu and Li - 2023 - How Language Models Learn Context-Free Grammars.pdf}
}

@book{alon2019introduction,
  title = {An introduction to systems biology: design principles of biological circuits},
  author = {Alon, Uri},
  year = {2019},
  publisher = {{Chapman and Hall/CRC}}
}

@article{alvarez-melis_robust_2018,
  title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
  author = {{Alvarez-Melis}, David and Jaakkola, T.},
  year = {2018},
  month = jun,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Towards-Robust-Interpretability-with-Neural-Alvarez-Melis-Jaakkola/0cf102da6dd4276115c63cbb6797f24ed450fea1},
  urldate = {2023-08-27},
  abstract = {Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general -- explicitness, faithfulness, and stability -- and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/UB36SZAL/Alvarez-Melis and Jaakkola - 2018 - Towards Robust Interpretability with Self-Explaini.pdf}
}

@article{amodei_concrete_2016,
  title = {Concrete Problems in AI Safety},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  year = {2016},
  month = jul,
  journal = {CoRR},
  eprint = {1606.06565},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1606.06565},
  url = {http://arxiv.org/abs/1606.06565},
  urldate = {2023-08-26},
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/MGF6BQER/Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf}
}

@article{anand_dual_2024,
  title = {Dual Process Learning: Controlling Use of In-Context vs. In-Weights Strategies with Weight Forgetting},
  shorttitle = {Dual Process Learning},
  author = {Anand, Suraj and Lepori, Michael A. and Merullo, Jack and Pavlick, Ellie},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2406.00053},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.00053},
  url = {http://arxiv.org/abs/2406.00053},
  urldate = {2024-06-10},
  abstract = {Language models have the ability to perform in-context learning (ICL), allowing them to flexibly adapt their behavior based on context. This contrasts with in-weights learning, where information is statically encoded in model parameters from iterated observations of the data. Despite this apparent ability to learn in-context, language models are known to struggle when faced with unseen or rarely seen tokens. Hence, we study \${\textbackslash}textbf\{structural in-context learning\}\$, which we define as the ability of a model to execute in-context learning on arbitrary tokens -- so called because the model must generalize on the basis of e.g. sentence structure or task structure, rather than semantic content encoded in token embeddings. An ideal model would be able to do both: flexibly deploy in-weights operations (in order to robustly accommodate ambiguous or unknown contexts using encoded semantic information) and structural in-context operations (in order to accommodate novel tokens). We study structural in-context algorithms in a simple part-of-speech setting using both practical and toy models. We find that active forgetting, a technique that was recently introduced to help models generalize to new languages, forces models to adopt structural in-context learning solutions. Finally, we introduce \${\textbackslash}textbf\{temporary forgetting\}\$, a straightforward extension of active forgetting that enables one to control how much a model relies on in-weights vs. in-context solutions. Importantly, temporary forgetting allows us to induce a \${\textbackslash}textit\{dual process strategy\}\$ where in-context and in-weights solutions coexist within a single model.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/G9AGK84Y/Anand et al. - 2024 - Dual Process Learning Controlling Use of In-Conte.pdf}
}

@article{anderson_more_1972,
  title = {More Is Different},
  author = {Anderson, P. W.},
  year = {1972},
  month = aug,
  journal = {Science},
  publisher = {American Association for the Advancement of Science},
  url = {https://www.science.org/doi/10.1126/science.177.4047.393},
  urldate = {2023-02-09},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9F2PP5DX/Anderson - 1972 - More Is Different.html}
}

@article{anthropic_core_2023,
  title = {Core Views on AI Safety: When, Why, What, and How},
  shorttitle = {Core Views on AI Safety},
  author = {Anthropic},
  year = {2023},
  journal = {Anthropic Blog},
  url = {https://www.anthropic.com/index/core-views-on-ai-safety},
  urldate = {2023-05-17},
  abstract = {AI progress may lead to transformative AI systems in the next decade, but we do not yet understand how to make such systems safe and aligned with human values. In response, we are pursuing a variety of research directions aimed at better understanding, evaluating, and aligning AI systems.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/AXLXPYKW/Anthropic - 2023 - Core Views on AI Safety When, Why, What, and How.html}
}

@article{anwar_foundational_2024,
  title = {Foundational Challenges in Assuring Alignment and Safety of Large Language Models},
  author = {Anwar, Usman and Saparov, Abulhair and Rando, Javier and Paleka, Daniel and Turpin, Miles and Hase, Peter and Lubana, Ekdeep Singh and Jenner, Erik and Casper, Stephen and Sourbut, Oliver and Edelman, Benjamin L. and Zhang, Zhaowei and G{\"u}nther, Mario and Korinek, Anton and {Hernandez-Orallo}, Jose and Hammond, Lewis and Bigelow, Eric and Pan, Alexander and Langosco, Lauro and Korbak, Tomasz and Zhang, Heidi and Zhong, Ruiqi and {h{\'E}igeartaigh}, Se{\'a}n {\'O} and Recchia, Gabriel and Corsi, Giulio and Chan, Alan and Anderljung, Markus and Edwards, Lilian and Bengio, Yoshua and Chen, Danqi and Albanie, Samuel and Maharaj, Tegan and Foerster, Jakob and Tramer, Florian and He, He and Kasirzadeh, Atoosa and Choi, Yejin and Krueger, David},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2404.09932},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.09932},
  url = {http://arxiv.org/abs/2404.09932},
  urldate = {2024-06-10},
  abstract = {This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose \$200+\$ concrete research questions.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/VHULIZZV/Anwar et al. - 2024 - Foundational Challenges in Assuring Alignment and .pdf}
}

@article{aogara_modeldriven_,
  title = {Model-driven feedback could amplify alignment failures},
  author = {O'Gara, Aidan},
  year = {2023},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/HhBcoRwnyJhQRJnxr/model-driven-feedback-could-amplify-alignment-failures},
  urldate = {2023-05-15},
  abstract = {Anthropic, DeepMind and Google Brain are all working on strategies to train language models on their own outputs. For a brief summary of the work so far: {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BZJ4FB9U/O'Gara - 2023 - Model-driven feedback could amplify alignment fail.html}
}

@article{arora_causalgym_2024,
  title = {CausalGym: Benchmarking causal interpretability methods on linguistic tasks},
  shorttitle = {CausalGym},
  author = {Arora, Aryaman and Jurafsky, Dan and Potts, Christopher},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.12560},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.12560},
  url = {http://arxiv.org/abs/2402.12560},
  urldate = {2024-06-09},
  abstract = {Language models (LMs) have proven to be powerful tools for psycholinguistic research, but most prior work has focused on purely behavioural measures (e.g., surprisal comparisons). At the same time, research in model interpretability has begun to illuminate the abstract causal mechanisms shaping LM behavior. To help bring these strands of research closer together, we introduce CausalGym. We adapt and expand the SyntaxGym suite of tasks to benchmark the ability of interpretability methods to causally affect model behaviour. To illustrate how CausalGym can be used, we study the pythia models (14M--6.9B) and assess the causal efficacy of a wide range of interpretability methods, including linear probing and distributed alignment search (DAS). We find that DAS outperforms the other methods, and so we use it to study the learning trajectory of two difficult linguistic phenomena in pythia-1b: negative polarity item licensing and filler--gap dependencies. Our analysis shows that the mechanism implementing both of these tasks is learned in discrete stages, not gradually.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/22QCLG8T/Arora et al. - 2024 - CausalGym Benchmarking causal interpretability me.pdf}
}

@article{arora_linear_2018,
  title = {Linear Algebraic Structure of Word Senses, with Applications to Polysemy},
  author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
  year = {2018},
  month = dec,
  journal = {TACL},
  eprint = {1601.03764},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1601.03764},
  url = {http://arxiv.org/abs/1601.03764},
  urldate = {2024-02-19},
  abstract = {Word embeddings are ubiquitous in NLP and information retrieval, but it is unclear what they represent when the word is polysemous. Here it is shown that multiple word senses reside in linear superposition within the word embedding and simple sparse coding can recover vectors that approximately capture the senses. The success of our approach, which applies to several embedding methods, is mathematically explained using a variant of the random walk on discourses model (Arora et al., 2016). A novel aspect of our technique is that each extracted word sense is accompanied by one of about 2000 "discourse atoms" that gives a succinct description of which other words co-occur with that word sense. Discourse atoms can be of independent interest, and make the method potentially more useful. Empirical tests are used to verify and support the theory.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BUYNDJ83/Arora et al. - 2018 - Linear Algebraic Structure of Word Senses, with Ap.pdf}
}

@article{arras_explaining_2017,
  title = {Explaining Recurrent Neural Network Predictions in Sentiment Analysis},
  author = {Arras, Leila and Montavon, Gr{\'e}goire and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  editor = {Balahur, Alexandra and Mohammad, Saif M. and {van der Goot}, Erik},
  year = {2017},
  month = sep,
  journal = {ACL Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
  pages = {159--168},
  doi = {10.18653/v1/W17-5221},
  url = {https://aclanthology.org/W17-5221},
  urldate = {2024-01-24},
  abstract = {Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown to deliver insightful explanations in the form of input space relevances for understanding feed-forward neural network classification decisions. In the present work, we extend the usage of LRP to recurrent neural networks. We propose a specific propagation rule applicable to multiplicative connections as they arise in recurrent network architectures such as LSTMs and GRUs. We apply our technique to a word-based bi-directional LSTM model on a five-class sentiment prediction task, and evaluate the resulting LRP relevances both qualitatively and quantitatively, obtaining better results than a gradient-based related method which was used in previous work.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BW66QTFT/Arras et al. - 2017 - Explaining Recurrent Neural Network Predictions in.pdf}
}

@article{arrieta_explainable_2019,
  title = {Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI},
  shorttitle = {Explainable Artificial Intelligence (XAI)},
  author = {Arrieta, Alejandro Barredo and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garc{\'i}a, Salvador and {Gil-L{\'o}pez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = {2019},
  month = dec,
  journal = {Information Fusion},
  eprint = {1910.10045},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1910.10045},
  url = {http://arxiv.org/abs/1910.10045},
  urldate = {2023-10-26},
  abstract = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/U5BZ7JDI/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf}
}

@article{askell_general_2021,
  title = {A General Language Assistant as a Laboratory for Alignment},
  author = {Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Kernion, Jackson and Ndousse, Kamal and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  year = {2021},
  month = dec,
  journal = {CoRR},
  eprint = {2112.00861},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2112.00861},
  url = {http://arxiv.org/abs/2112.00861},
  urldate = {2023-08-26},
  abstract = {Given the broad capabilities of large language models, it should be possible to work towards a general-purpose, text-based assistant that is aligned with human values, meaning that it is helpful, honest, and harmless. As an initial foray in this direction we study simple baseline techniques and evaluations, such as prompting. We find that the benefits from modest interventions increase with model size, generalize to a variety of alignment evaluations, and do not compromise the performance of large models. Next we investigate scaling trends for several training objectives relevant to alignment, comparing imitation learning, binary discrimination, and ranked preference modeling. We find that ranked preference modeling performs much better than imitation learning, and often scales more favorably with model size. In contrast, binary discrimination typically performs and scales very similarly to imitation learning. Finally we study a `preference model pre-training' stage of training, with the goal of improving sample efficiency when finetuning on human preferences.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/F2CMJNPT/Askell et al. - 2021 - A General Language Assistant as a Laboratory for A.pdf}
}

@article{azaria_internal_2023,
  title = {The Internal State of an LLM Knows When It's Lying},
  author = {Azaria, Amos and Mitchell, Tom},
  year = {2023},
  month = oct,
  journal = {EMNLP},
  eprint = {2304.13734},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.13734},
  url = {http://arxiv.org/abs/2304.13734},
  urldate = {2023-11-10},
  abstract = {While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71{\textbackslash}\% to 83{\textbackslash}\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/M9Y4Q493/Azaria and Mitchell - 2023 - The Internal State of an LLM Knows When It's Lying.pdf}
}

@article{bach_pixelwise_2015,
  title = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2015},
  month = jul,
  journal = {PLOS ONE},
  volume = {10},
  number = {7},
  pages = {e0130140},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0130140},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0130140},
  urldate = {2024-01-24},
  abstract = {Understanding and interpreting classification decisions of automated image classification systems is of high value in many applications, as it allows to verify the reasoning of the system and provides additional information to the human expert. Although machine learning methods are solving very successfully a plethora of tasks, they have in most cases the disadvantage of acting as a black box, not providing any information about what made them arrive at a particular decision. This work proposes a general solution to the problem of understanding classification decisions by pixel-wise decomposition of nonlinear classifiers. We introduce a methodology that allows to visualize the contributions of single pixels to predictions for kernel-based classifiers over Bag of Words features and for multilayered neural networks. These pixel contributions can be visualized as heatmaps and are provided to a human expert who can intuitively not only verify the validity of the classification decision, but also focus further analysis on regions of potential interest. We evaluate our method for classifiers trained on PASCAL VOC 2009 images, synthetic image data containing geometric shapes, the MNIST handwritten digits data set and for the pre-trained ImageNet model available as part of the Caffe open source package.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/FBYYVQ63/Bach et al. - 2015 - On Pixel-Wise Explanations for Non-Linear Classifi.pdf}
}

@article{baginski_charbelraphael_2023,
  title = {Charbel-Rapha{\"e}l and Lucius discuss Interpretability},
  author = {Bagi{\'n}ski, Mateusz and {Charbel-Rapha{\"e}l} and Bushnaq, Lucius},
  year = {2023},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/FDrgcfY8zs5e2eJDd/charbel-raphael-and-lucius-discuss-interpretability},
  urldate = {2024-02-12},
  abstract = {The following is a transcript of a public discussion between Charbel-Rapha{\"e}l Segerie and Lucius Bushnaq that took place on 23 September during LessWr{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RZGRI38J/Bagiski et al. - 2023 - Charbel-Raphal and Lucius discuss Interpretabilit.html}
}

@article{bai_constitutional_2022,
  title = {Constitutional AI: Harmlessness from AI Feedback},
  shorttitle = {Constitutional AI},
  author = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Chen, Carol and Olsson, Catherine and Olah, Christopher and Hernandez, Danny and Drain, Dawn and Ganguli, Deep and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lukosuite, Kamile and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Schiefer, Nicholas and Mercado, Noemi and DasSarma, Nova and Lasenby, Robert and Larson, Robin and Ringer, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and Lanham, Tamera and {Telleen-Lawton}, Timothy and Conerly, Tom and Henighan, Tom and Hume, Tristan and Bowman, Samuel R. and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Kaplan, Jared},
  year = {2022},
  month = dec,
  journal = {CoRR},
  eprint = {2212.08073},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.08073},
  url = {http://arxiv.org/abs/2212.08073},
  urldate = {2023-08-26},
  abstract = {As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/572J7IGV/Bai et al. - 2022 - Constitutional AI Harmlessness from AI Feedback.pdf}
}

@article{bai_training_2022,
  title = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = apr,
  journal = {CoRR},
  eprint = {2204.05862},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2204.05862},
  url = {http://arxiv.org/abs/2204.05862},
  urldate = {2023-05-15},
  abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3KC4AZQ8/Bai et al. - 2022 - Training a Helpful and Harmless Assistant with Rei.pdf}
}

@article{balestriero_spline_2018,
  title = {A Spline Theory of Deep Learning},
  author = {Balestriero, Randall and {baraniuk}},
  year = {2018},
  month = jul,
  journal = {ICML},
  pages = {374--383},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/balestriero18b.html},
  urldate = {2023-10-30},
  abstract = {We build a rigorous bridge between deep networks (DNs) and approximation theory via spline functions and operators. Our key result is that a large class of DNs can be written as a composition of max-affine spline operators (MASOs), which provide a powerful portal through which to view and analyze their inner workings. For instance, conditioned on the input signal, the output of a MASO DN can be written as a simple affine transformation of the input. This implies that a DN constructs a set of signal-dependent, class-specific templates against which the signal is compared via a simple inner product; we explore the links to the classical theory of optimal classification via matched filters and the effects of data memorization. Going further, we propose a simple penalty term that can be added to the cost function of any DN learning algorithm to force the templates to be orthogonal with each other; this leads to significantly improved classification performance and reduced overfitting with no change to the DN architecture. The spline partition of the input signal space opens up a new geometric avenue to study how DNs organize signals in a hierarchical fashion. As an application, we develop and validate a new distance metric for signals that quantifies the difference between their partition encodings.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5KH45AB2/Balestriero and baraniuk - 2018 - A Spline Theory of Deep Learning.pdf}
}

@article{baniecki_adversarial_2024,
  title = {Adversarial attacks and defenses in explainable artificial intelligence: A survey},
  shorttitle = {Adversarial attacks and defenses in explainable artificial intelligence},
  author = {Baniecki, Hubert and Biecek, Przemyslaw},
  year = {2024},
  month = jul,
  journal = {Information Fusion},
  volume = {107},
  pages = {102303},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2024.102303},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253524000812},
  urldate = {2024-06-28},
  abstract = {Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning (AdvML) highlight the limitations and vulnerabilities of state-of-the-art explanation methods, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This survey provides a comprehensive overview of research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We introduce a unified notation and taxonomy of methods facilitating a common ground for researchers and practitioners from the intersecting research fields of AdvML and XAI. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI). Future work should address improving explanation methods and evaluation protocols to take into account the reported safety issues.},
  file = {/Users/leonardbereska/Zotero/storage/SWFMGR6S/Baniecki and Biecek - 2024 - Adversarial attacks and defenses in explainable ar.pdf;/Users/leonardbereska/Zotero/storage/BYCFD83V/S1566253524000812.html}
}

@article{banino_vectorbased_2018,
  title = {Vector-based navigation using grid-like representations in artificial agents},
  author = {Banino, Andrea and Barry, Caswell and Uria, Benigno and Blundell, Charles and Lillicrap, Timothy and Mirowski, Piotr and Pritzel, Alexander and Chadwick, Martin J. and Degris, Thomas and Modayil, Joseph and Wayne, Greg and Soyer, Hubert and Viola, Fabio and Zhang, Brian and Goroshin, Ross and Rabinowitz, Neil and Pascanu, Razvan and Beattie, Charlie and Petersen, Stig and Sadik, Amir and Gaffney, Stephen and King, Helen and Kavukcuoglu, Koray and Hassabis, Demis and Hadsell, Raia and Kumaran, Dharshan},
  year = {2018},
  month = may,
  journal = {Nature},
  volume = {557},
  number = {7705},
  pages = {429--433},
  publisher = {Nature Publishing Group},
  url = {https://www.nature.com/articles/s41586-018-0102-6},
  urldate = {2022-06-29},
  abstract = {Deep neural networks have achieved impressive successes in fields ranging from object recognition to complex games such as Go1,2. Navigation, however, remains a substantial challenge for artificial agents, with deep neural networks trained by reinforcement learning3--5 failing to rival the proficiency of mammalian spatial behaviour, which is underpinned by grid cells in the entorhinal cortex6. Grid cells are thought to provide a multi-scale periodic representation that functions as a metric for coding space7,8 and is critical for integrating self-motion (path integration)6,7,9 and planning direct trajectories to goals (vector-based navigation)7,10,11. Here we set out to leverage the computational functions of grid cells to develop a deep reinforcement learning agent with mammal-like navigational abilities. We first trained a recurrent network to perform path integration, leading to the emergence of representations resembling grid cells, as well as other entorhinal cell types12. We then showed that this representation provided an effective basis for an agent to locate goals in challenging, unfamiliar, and changeable environments---optimizing the primary objective of navigation through deep reinforcement learning. The performance of agents endowed with grid-like representations surpassed that of an expert human and comparison agents, with the metric quantities necessary for vector-based navigation derived from grid-like units within the network. Furthermore, grid-like representations enabled agents to conduct shortcut behaviours reminiscent of those performed by mammals. Our findings show that emergent grid-like representations furnish agents with a Euclidean spatial metric and associated vector operations, providing a foundation for proficient navigation. As such, our results support neuroscientific theories that see grid cells as critical for vector-based navigation7,10,11, demonstrating that the latter can be combined with path-based strategies to support navigation in challenging environments.},
  copyright = {2018 Macmillan Publishers Ltd., part of Springer Nature},
  language = {en},
  keywords = {baseline,grid cells,navigation,not cited,recurrent,reinforcement learning,representation,seminal},
  file = {/Users/leonardbereska/Zotero/storage/FQNMB6WT/Banino et al. - 2018 - Vector-based navigation using grid-like representa.pdf}
}

@article{bansal_revisiting_2021,
  title = {Revisiting Model Stitching to Compare Neural Representations},
  author = {Bansal, Yamini and Nakkiran, Preetum and Barak, Boaz},
  year = {2021},
  month = jun,
  journal = {CoRR},
  eprint = {2106.07682},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2106.07682},
  url = {http://arxiv.org/abs/2106.07682},
  urldate = {2024-03-19},
  abstract = {We revisit and extend model stitching (Lenc \& Vedaldi 2015) as a methodology to study the internal representations of neural networks. Given two trained and frozen models \$A\$ and \$B\$, we consider a "stitched model'' formed by connecting the bottom-layers of \$A\$ to the top-layers of \$B\$, with a simple trainable layer between them. We argue that model stitching is a powerful and perhaps under-appreciated tool, which reveals aspects of representations that measures such as centered kernel alignment (CKA) cannot. Through extensive experiments, we use model stitching to obtain quantitative verifications for intuitive statements such as "good networks learn similar representations'', by demonstrating that good networks of the same architecture, but trained in very different ways (e.g.: supervised vs. self-supervised learning), can be stitched to each other without drop in performance. We also give evidence for the intuition that "more is better'' by showing that representations learnt with (1) more data, (2) bigger width, or (3) more training time can be "plugged in'' to weaker models to improve performance. Finally, our experiments reveal a new structural property of SGD which we call "stitching connectivity'', akin to mode-connectivity: typical minima reached by SGD can all be stitched to each other with minimal change in accuracy.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/5TVLLIS3/Bansal et al. - 2021 - Revisiting Model Stitching to Compare Neural Repre.pdf}
}

@article{barak_hidden_2022,
  title = {Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit},
  shorttitle = {Hidden Progress in Deep Learning},
  author = {Barak, Boaz and Edelman, Benjamin L. and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  year = {2022},
  journal = {NeurIPS},
  eprint = {2207.08799},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.2207.08799},
  url = {http://arxiv.org/abs/2207.08799},
  urldate = {2023-11-10},
  abstract = {There is mounting evidence of emergent phenomena in the capabilities of deep learning methods as we scale up datasets, model sizes, and training times. While there are some accounts of how these resources modulate statistical capacity, far less is known about their effect on the computational problem of model training. This work conducts such an exploration through the lens of learning a \$k\$-sparse parity of \$n\$ bits, a canonical discrete search problem which is statistically easy but computationally hard. Empirically, we find that a variety of neural networks successfully learn sparse parities, with discontinuous phase transitions in the training curves. On small instances, learning abruptly occurs at approximately \$n{\textasciicircum}\{O(k)\}\$ iterations; this nearly matches SQ lower bounds, despite the apparent lack of a sparse prior. Our theoretical analysis shows that these observations are not explained by a Langevin-like mechanism, whereby SGD "stumbles in the dark" until it finds the hidden set of features (a natural algorithm which also runs in \$n{\textasciicircum}\{O(k)\}\$ time). Instead, we show that SGD gradually amplifies the sparse solution via a Fourier gap in the population gradient, making continual progress that is invisible to loss and error metrics.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/WHTWDY3Q/Barak et al. - 2022 - Hidden Progress in Deep Learning SGD Learns Parit.pdf}
}

@article{baric_benchmarking_2021,
  title = {Benchmarking Attention-Based Interpretability of Deep Learning in Multivariate Time Series Predictions},
  author = {Bari{\'c}, Domjan and Fumi{\'c}, Petar and Horvati{\'c}, Davor and Lipic, Tomislav},
  year = {2021},
  month = jan,
  journal = {Entropy},
  volume = {23},
  number = {2},
  pages = {143},
  issn = {1099-4300},
  doi = {10.3390/e23020143},
  url = {https://www.mdpi.com/1099-4300/23/2/143},
  urldate = {2023-09-08},
  abstract = {The adaptation of deep learning models within safety-critical systems cannot rely only on good prediction performance but needs to provide interpretable and robust explanations for their decisions. When modeling complex sequences, attention mechanisms are regarded as the established approach to support deep neural networks with intrinsic interpretability. This paper focuses on the emerging trend of specifically designing diagnostic datasets for understanding the inner workings of attention mechanism based deep learning models for multivariate forecasting tasks. We design a novel benchmark of synthetically designed datasets with the transparent underlying generating process of multiple time series interactions with increasing complexity. The benchmark enables empirical evaluation of the performance of attention based deep neural networks in three different aspects: (i) prediction performance score, (ii) interpretability correctness, (iii) sensitivity analysis. Our analysis shows that although most models have satisfying and stable prediction performance results, they often fail to give correct interpretability. The only model with both a satisfying performance score and correct interpretability is IMV-LSTM, capturing both autocorrelations and crosscorrelations between multiple time series. Interestingly, while evaluating IMV-LSTM on simulated data from statistical and mechanistic models, the correctness of interpretability increases with more complex datasets.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NBLTH8QP/Bari et al. - 2021 - Benchmarking Attention-Based Interpretability of D.pdf}
}

@article{barnes_another_2022,
  title = {Another list of theories of impact for interpretability},
  author = {Barnes, Beth},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/YQALrtMkeqemAF5GX/another-list-of-theories-of-impact-for-interpretability},
  urldate = {2024-02-13},
  abstract = {Neel's post on this is good. I thought I'd add my own list/framing. Somewhat rough. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FNQFXM8C/Barnes - 2022 - Another list of theories of impact for interpretab.html}
}

@article{barnes_simulator_2023,
  title = {'Simulator' framing and confusions about LLMs},
  author = {Barnes, Beth},
  year = {2023},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/dYnHLWMXCYdm9xu5j/simulator-framing-and-confusions-about-llms},
  urldate = {2023-05-15},
  abstract = {Post status: pretty rough + unpolished, thought it might be worthwhile getting this out anyway {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GI6EFJVR/Barnes - 2023 - 'Simulator' framing and confusions about LLMs.html}
}

@article{barredoarrieta_explainable_2020,
  title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
  shorttitle = {Explainable Artificial Intelligence (XAI)},
  author = {Barredo Arrieta, Alejandro and {D{\'i}az-Rodr{\'i}guez}, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and Garcia, Salvador and {Gil-Lopez}, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  year = {2020},
  month = jun,
  journal = {Information Fusion},
  volume = {58},
  pages = {82--115},
  issn = {15662535},
  doi = {10.1016/j.inffus.2019.12.012},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1566253519308103},
  urldate = {2023-10-26},
  abstract = {Semantic Scholar extracted view of "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI" by Alejandro Barredo Arrieta et al.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KMIJ26CN/Barredo Arrieta et al. - 2020 - Explainable Artificial Intelligence (XAI) Concept.pdf}
}

@article{bastings_will_2022,
  title = {"Will You Find These Shortcuts?" A Protocol for Evaluating the Faithfulness of Input Salience Methods for Text Classification},
  shorttitle = {"Will You Find These Shortcuts?},
  author = {Bastings, Jasmijn and Ebert, Sebastian and Zablotskaia, Polina and Sandholm, Anders and Filippova, Katja},
  year = {2022},
  month = nov,
  journal = {CoRR},
  eprint = {2111.07367},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2111.07367},
  url = {http://arxiv.org/abs/2111.07367},
  urldate = {2024-02-26},
  abstract = {Feature attribution a.k.a. input salience methods which assign an importance score to a feature are abundant but may produce surprisingly different results for the same model on the same input. While differences are expected if disparate definitions of importance are assumed, most methods claim to provide faithful attributions and point at the features most relevant for a model's prediction. Existing work on faithfulness evaluation is not conclusive and does not provide a clear answer as to how different methods are to be compared. Focusing on text classification and the model debugging scenario, our main contribution is a protocol for faithfulness evaluation that makes use of partially synthetic data to obtain ground truth for feature importance ranking. Following the protocol, we do an in-depth analysis of four standard salience method classes on a range of datasets and shortcuts for BERT and LSTM models and demonstrate that some of the most popular method configurations provide poor results even for simplest shortcuts. We recommend following the protocol for each new task and model combination to find the best method for identifying shortcuts.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/SBVXJ99J/Bastings et al. - 2022 - Will You Find These Shortcuts A Protocol for Ev.pdf}
}

@article{bau_gan_2018,
  title = {GAN Dissection: Visualizing and Understanding Generative Adversarial Networks},
  shorttitle = {GAN Dissection},
  author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B. and Freeman, William T. and Torralba, Antonio},
  year = {2018},
  month = dec,
  journal = {ICLR},
  eprint = {1811.10597},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1811.10597},
  url = {http://arxiv.org/abs/1811.10597},
  urldate = {2023-10-26},
  abstract = {Generative Adversarial Networks (GANs) have recently achieved impressive results for many real-world applications, and many GAN variants have emerged with improvements in sample quality and training stability. However, they have not been well visualized or understood. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to object concepts using a segmentation-based network dissection method. Then, we quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. We examine the contextual relationship between these units and their surroundings by inserting the discovered object concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in a scene. We provide open source interpretation tools to help researchers and practitioners better understand their GAN models.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/QSZA4TI5/Bau et al. - 2018 - GAN Dissection Visualizing and Understanding Gene.pdf}
}

@article{bau_network_2017,
  title = {Network Dissection: Quantifying Interpretability of Deep Visual Representations},
  shorttitle = {Network Dissection},
  author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  year = {2017},
  month = jul,
  journal = {CVPR},
  pages = {3319--3327},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.354},
  url = {http://ieeexplore.ieee.org/document/8099837/},
  urldate = {2023-08-27},
  abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
  isbn = {9781538604571},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HE2PLXSY/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf}
}

@article{bau_understanding_2020,
  title = {Understanding the Role of Individual Units in a Deep Neural Network},
  author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  year = {2020},
  month = dec,
  journal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {117},
  number = {48},
  eprint = {2009.05041},
  primaryclass = {cs},
  pages = {30071--30078},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907375117},
  url = {http://arxiv.org/abs/2009.05041},
  urldate = {2024-01-23},
  abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large data sets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JW8S8GL4/Bau et al. - 2020 - Understanding the Role of Individual Units in a De.pdf}
}

@article{bayazit_discovering_2023,
  title = {Discovering Knowledge-Critical Subnetworks in Pretrained Language Models},
  author = {Bayazit, Deniz and Foroutan, Negar and Chen, Zeming and Weiss, Gail and Bosselut, Antoine},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.03084},
  url = {https://arxiv.org/abs/2310.03084},
  urldate = {2024-02-11},
  abstract = {Pretrained language models (LMs) encode implicit representations of knowledge in their parameters. However, localizing these representations and disentangling them from each other remains an open problem. In this work, we investigate whether pretrained language models contain various knowledge-critical subnetworks: particular sparse computational subgraphs responsible for encoding specific knowledge the model has memorized. We propose a multi-objective differentiable weight masking scheme to discover these subnetworks and show that we can use them to precisely remove specific knowledge from models while minimizing adverse effects on the behavior of the original language model. We demonstrate our method on multiple GPT2 variants, uncovering highly sparse subnetworks (98\%+) that are solely responsible for specific collections of relational knowledge. When these subnetworks are removed, the remaining network maintains most of its initial capacity (modeling language and other memorized relational knowledge) but struggles to express the removed knowledge, and suffers performance drops on examples needing this removed knowledge on downstream tasks after finetuning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {graph,mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/9XACPWD5/Bayazit et al. - 2023 - Discovering Knowledge-Critical Subnetworks in Pret.pdf}
}

@article{beckers_abstracting_2019,
  title = {Abstracting Causal Models},
  author = {Beckers, Sander and Halpern, Joseph Y.},
  year = {2019},
  month = jul,
  journal = {AAAI},
  eprint = {1812.03789},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1812.03789},
  url = {http://arxiv.org/abs/1812.03789},
  urldate = {2024-03-19},
  abstract = {We consider a sequence of successively more restrictive definitions of abstraction for causal models, starting with a notion introduced by Rubenstein et al. (2017) called exact transformation that applies to probabilistic causal models, moving to a notion of uniform transformation that applies to deterministic causal models and does not allow differences to be hidden by the "right" choice of distribution, and then to abstraction, where the interventions of interest are determined by the map from low-level states to high-level states, and strong abstraction, which takes more seriously all potential interventions in a model, not just the allowed interventions. We show that procedures for combining micro-variables into macro-variables are instances of our notion of strong abstraction, as are all the examples considered by Rubenstein et al.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/H2AVQSCC/Beckers and Halpern - 2019 - Abstracting Causal Models.pdf}
}

@article{beckers_approximate_2019,
  title = {Approximate Causal Abstraction},
  author = {Beckers, Sander and Eberhardt, Frederick and Halpern, Joseph Y.},
  year = {2019},
  month = jun,
  journal = {UAI},
  eprint = {1906.11583},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1906.11583},
  url = {http://arxiv.org/abs/1906.11583},
  urldate = {2024-03-19},
  abstract = {Scientific models describe natural phenomena at different levels of abstraction. Abstract descriptions can provide the basis for interventions on the system and explanation of observed phenomena at a level of granularity that is coarser than the most fundamental account of the system. Beckers and Halpern (2019), building on work of Rubenstein et al. (2017), developed an account of abstraction for causal models that is exact. Here we extend this account to the more realistic case where an abstract causal model offers only an approximation of the underlying system. We show how the resulting account handles the discrepancy that can arise between low- and high-level causal models of the same system, and in the process provide an account of how one causal model approximates another, a topic of independent interest. Finally, we extend the account of approximate abstractions to probabilistic causal models, indicating how and where uncertainty can enter into an approximate abstraction.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/GD5FXLU3/Beckers et al. - 2019 - Approximate Causal Abstraction.pdf}
}

@article{belinkov_analysis_2019,
  title = {Analysis Methods in Neural Language Processing: A Survey},
  shorttitle = {Analysis Methods in Neural Language Processing},
  author = {Belinkov, Yonatan and Glass, James},
  editor = {Lee, Lillian and Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year = {2019},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {49--72},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00254},
  url = {https://aclanthology.org/Q19-1004},
  urldate = {2024-03-20},
  abstract = {The field of natural language processing has seen impressive progress in recent years, with neural network models replacing many of the traditional systems. A plethora of new models have been proposed, many of which are thought to be opaque compared to their feature-rich counterparts. This has led researchers to analyze, interpret, and evaluate neural networks in novel and more fine-grained ways. In this survey paper, we review analysis methods in neural language processing, categorize them according to prominent research trends, highlight existing limitations, and point to potential directions for future work.},
  file = {/Users/leonardbereska/Zotero/storage/2AIVX6XH/Belinkov and Glass - 2019 - Analysis Methods in Neural Language Processing A .pdf}
}

@article{belinkov_probing_2021,
  title = {Probing Classifiers: Promises, Shortcomings, and Advances},
  shorttitle = {Probing Classifiers},
  author = {Belinkov, Yonatan},
  year = {2021},
  month = sep,
  journal = {CoRR},
  eprint = {2102.12452},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2102.12452},
  url = {http://arxiv.org/abs/2102.12452},
  urldate = {2023-07-31},
  abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple -- a classifier is trained to predict some linguistic property from a model's representations -- and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This article critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
  archiveprefix = {arxiv},
  keywords = {cited,probing,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/BW7UJF33/Belinkov - 2021 - Probing Classifiers Promises, Shortcomings, and A.pdf}
}

@article{belinkov_probing_2022,
  title = {Probing Classifiers: Promises, Shortcomings, and Advances},
  shorttitle = {Probing Classifiers},
  author = {Belinkov, Yonatan},
  year = {2022},
  month = mar,
  journal = {Computational Linguistics},
  volume = {48},
  number = {1},
  pages = {207--219},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/coli_a_00422},
  url = {https://aclanthology.org/2022.cl-1.7},
  urldate = {2023-11-10},
  abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple---a classifier is trained to predict some linguistic property from a model's representations---and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This squib critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/6VLVD4F6/Belinkov - 2022 - Probing Classifiers Promises, Shortcomings, and A.pdf}
}

@article{belrose_eliciting_2023,
  title = {Eliciting Latent Predictions from Transformers with the Tuned Lens},
  author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2303.08112},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.08112},
  url = {http://arxiv.org/abs/2303.08112},
  urldate = {2023-10-25},
  abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the tuned lens, is a refinement of the earlier "logit lens" technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,method,probing,to cite,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/ED9BIFKP/Belrose et al. - 2023 - Eliciting Latent Predictions from Transformers wit.pdf}
}

@article{belrose_leace_2023,
  title = {LEACE: Perfect linear concept erasure in closed form},
  shorttitle = {LEACE},
  author = {Belrose, Nora and {Schneider-Joseph}, David and Ravfogel, Shauli and Cotterell, Ryan and Raff, Edward and Biderman, Stella},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2306.03819},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.03819},
  url = {http://arxiv.org/abs/2306.03819},
  urldate = {2023-08-26},
  abstract = {Concept erasure aims to remove specified features from a representation. It can improve fairness (e.g. preventing a classifier from using gender or race) and interpretability (e.g. removing a concept to observe changes in model behavior). We introduce LEAst-squares Concept Erasure (LEACE), a closed-form method which provably prevents all linear classifiers from detecting a concept while changing the representation as little as possible, as measured by a broad class of norms. We apply LEACE to large language models with a novel procedure called "concept scrubbing," which erases target concept information from every layer in the network. We demonstrate our method on two tasks: measuring the reliance of language models on part-of-speech information, and reducing gender bias in BERT embeddings. Code is available at https://github.com/EleutherAI/concept-erasure.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/SY8FM55Y/Belrose et al. - 2023 - LEACE Perfect linear concept erasure in closed fo.pdf}
}

@article{bender_climbing_2020,
  title = {Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data},
  shorttitle = {Climbing towards NLU},
  author = {Bender, Emily M. and Koller, Alexander},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  journal = {ACL},
  pages = {5185--5198},
  doi = {10.18653/v1/2020.acl-main.463},
  url = {https://aclanthology.org/2020.acl-main.463},
  urldate = {2023-11-29},
  abstract = {The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as ``understanding'' language or capturing ``meaning''. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of ``Taking Stock of Where We've Been and Where We're Going'', we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.},
  keywords = {not cited,to cite,to read},
  file = {/Users/leonardbereska/Zotero/storage/LTT7Y49C/Bender and Koller - 2020 - Climbing towards NLU On Meaning, Form, and Unders.pdf}
}

@article{bender_dangers_2021,
  title = {On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?},
  shorttitle = {On the Dangers of Stochastic Parrots},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  journal = {ACM FAccT},
  series = {FAccT '21},
  pages = {610--623},
  doi = {10.1145/3442188.3445922},
  url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
  urldate = {2023-06-01},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KVX4EJ42/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf}
}

@article{bengio_managing_2023,
  title = {Managing AI Risks in an Era of Rapid Progress},
  author = {Bengio, Yoshua and Hinton, Geoffrey and Yao, Andrew and Song, Dawn and Abbeel, Pieter and Harari, Yuval Noah and Zhang, Ya-Qin and Xue, Lan and {Shalev-Shwartz}, Shai and Hadfield, Gillian and Clune, Jeff and Maharaj, Tegan and Hutter, Frank and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and McIlraith, Sheila and Gao, Qiqi and Acharya, Ashwin and Krueger, David and Dragan, Anca and Torr, Philip and Russell, Stuart and Kahneman, Daniel and Brauner, Jan and Mindermann, S{\"o}ren},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2310.17688},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.17688},
  url = {http://arxiv.org/abs/2310.17688},
  urldate = {2024-02-10},
  abstract = {In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose urgent priorities for AI R\&D and governance.},
  archiveprefix = {arxiv},
  keywords = {not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/LQUMCRW5/Bengio et al. - 2023 - Managing AI Risks in an Era of Rapid Progress.pdf}
}

@article{bengio_representation_2013,
  title = {Representation Learning: A Review and New Perspectives},
  shorttitle = {Representation Learning},
  author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  year = {2013},
  month = aug,
  journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {35},
  number = {8},
  pages = {1798--1828},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2013.50},
  url = {https://doi.org/10.1109/TPAMI.2013.50},
  urldate = {2024-03-20},
  abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
  file = {/Users/leonardbereska/Zotero/storage/M8AY8KZC/Bengio et al. - 2013 - Representation Learning A Review and New Perspect.pdf}
}

@article{bereska_continual_2022,
  title = {Continual Learning of Dynamical Systems With Competitive Federated Reservoir Computing},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2022},
  month = nov,
  journal = {Proceedings of The 1st Conference on Lifelong Learning Agents},
  pages = {335--350},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v199/bereska22a.html},
  urldate = {2024-07-03},
  abstract = {Machine learning recently proved efficient in learning differential equations and dynamical systems from data. However, the data is commonly assumed to originate from a single never-changing system. In contrast, when modeling real-world dynamical processes, the data distribution often shifts due to changes in the underlying system dynamics. Continual learning of these processes aims to rapidly adapt to abrupt system changes without forgetting previous dynamical regimes. This work proposes an approach to continual learning based on reservoir computing, a state-of-the-art method for training recurrent neural networks on complex spatiotemporal dynamical systems. Reservoir computing fixes the recurrent network weights - hence these cannot be forgotten - and only updates linear projection heads to the output. We propose to train multiple competitive prediction heads concurrently. Inspired by neuroscience's predictive coding, only the most predictive heads activate, laterally inhibiting and thus protecting the inactive heads from forgetting induced by interfering parameter updates. We show that this multi-head reservoir minimizes interference and catastrophic forgetting on several dynamical systems, including the Van-der-Pol oscillator, the chaotic Lorenz attractor, and the high-dimensional Lorenz-96 weather model. Our results suggest that reservoir computing is a promising candidate framework for the continual learning of dynamical systems. We provide our code for data generation, method, and comparisons at {\textbackslash}url\{https://github.com/leonardbereska/multiheadreservoir.\}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/Z3NS2ECQ/Bereska and Gavves - 2022 - Continual Learning of Dynamical Systems With Compe.pdf}
}

@article{bereska_mechanistic_2024,
  title = {Mechanistic Interpretability for AI Safety -- A Review},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2404.14082},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2404.14082},
  urldate = {2024-04-23},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/67XQNYEM/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Re.pdf}
}

@article{bereska_taming_2023,
  title = {Taming Simulators: Challenges, Pathways and Vision for the Alignment of Large Language Models},
  shorttitle = {Taming Simulators},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2023},
  month = oct,
  journal = {AAAI-SS},
  volume = {1},
  number = {1},
  pages = {68--72},
  issn = {2994-4317},
  doi = {10.1609/aaaiss.v1i1.27478},
  url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27478},
  urldate = {2024-01-20},
  abstract = {As AI systems continue to advance in power and prevalence, ensuring alignment between humans and AI is crucial to prevent catastrophic outcomes. The greater the capabilities and generality of an AI system, combined with its development of goals and agency, the higher the risks associated with misalignment. While the concept of superhuman artificial general intelligence is still speculative, language models show indications of generality that could extend to generally capable systems. Regarding agency, this paper emphasizes the understanding of prediction-trained models as simulators rather than agents. Nonetheless, agents may emerge accidentally from internal processes, so-called simulacra, or deliberately through fine-tuning with reinforcement learning. As a result, the focus of alignment research shifts towards aligning simulacra, comprehending and mitigating mesa-optimization, and aligning agents derived from prediction-trained models. The paper outlines the challenges of aligning simulators and presents research directions based on this understanding. Additionally, it envisions a future where aligned simulators are critical in fostering successful human-AI collaboration. This vision encompasses exploring emulation approaches and the integration of simulators into cyborg systems to enhance human cognitive abilities. By acknowledging the risks associated with misaligned AI, delving into the concept of simulacra, and presenting strategies for aligning agents and simulacra, this paper contributes to the ongoing efforts to safeguard human values in developing and deploying AI systems.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HRP5RDKN/Bereska and Gavves - 2023 - Taming Simulators Challenges, Pathways and Vision.pdf}
}

@article{berglund_reversal_2023,
  title = {The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"},
  shorttitle = {The Reversal Curse},
  author = {Berglund, Lukas and Tong, Meg and Kaufmann, Max and Balesni, Mikita and Stickland, Asa Cooper and Korbak, Tomasz and Evans, Owain},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.12288},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.12288},
  url = {http://arxiv.org/abs/2309.12288},
  urldate = {2023-12-11},
  abstract = {We expose a surprising failure of generalization in auto-regressive large language models (LLMs). If a model is trained on a sentence of the form "A is B", it will not automatically generalize to the reverse direction "B is A". This is the Reversal Curse. For instance, if a model is trained on "Olaf Scholz was the ninth Chancellor of Germany", it will not automatically be able to answer the question, "Who was the ninth Chancellor of Germany?". Moreover, the likelihood of the correct answer ("Olaf Scholz") will not be higher than for a random name. Thus, models exhibit a basic failure of logical deduction and do not generalize a prevalent pattern in their training set (i.e. if "A is B'' occurs, "B is A" is more likely to occur). We provide evidence for the Reversal Curse by finetuning GPT-3 and Llama-1 on fictitious statements such as "Uriah Hawthorne is the composer of 'Abyssal Melodies'" and showing that they fail to correctly answer "Who composed 'Abyssal Melodies?'". The Reversal Curse is robust across model sizes and model families and is not alleviated by data augmentation. We also evaluate ChatGPT (GPT-3.5 and GPT-4) on questions about real-world celebrities, such as "Who is Tom Cruise's mother? [A: Mary Lee Pfeiffer]" and the reverse "Who is Mary Lee Pfeiffer's son?". GPT-4 correctly answers questions like the former 79\% of the time, compared to 33\% for the latter. This shows a failure of logical deduction that we hypothesize is caused by the Reversal Curse. Code is available at https://github.com/lukasberglund/reversal\_curse.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JK2BURMZ/Berglund et al. - 2023 - The Reversal Curse LLMs trained on A is B fail .pdf}
}

@article{bhaskar_heuristic_2024,
  title = {The Heuristic Core: Understanding Subnetwork Generalization in Pretrained Language Models},
  shorttitle = {The Heuristic Core},
  author = {Bhaskar, Adithya and Friedman, Dan and Chen, Danqi},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.03942},
  url = {https://arxiv.org/abs/2403.03942},
  urldate = {2024-06-10},
  abstract = {Prior work has found that pretrained language models (LMs) fine-tuned with different random seeds can achieve similar in-domain performance but generalize differently on tests of syntactic generalization. In this work, we show that, even within a single model, we can find multiple subnetworks that perform similarly in-domain, but generalize vastly differently. To better understand these phenomena, we investigate if they can be understood in terms of "competing subnetworks": the model initially represents a variety of distinct algorithms, corresponding to different subnetworks, and generalization occurs when it ultimately converges to one. This explanation has been used to account for generalization in simple algorithmic tasks ("grokking"). Instead of finding competing subnetworks, we find that all subnetworks -- whether they generalize or not -- share a set of attention heads, which we refer to as the heuristic core. Further analysis suggests that these attention heads emerge early in training and compute shallow, non-generalizing features. The model learns to generalize by incorporating additional attention heads, which depend on the outputs of the "heuristic" heads to compute higher-level features. Overall, our results offer a more detailed picture of the mechanisms for syntactic generalization in pretrained LMs.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/leonardbereska/Zotero/storage/LKPL53TD/Bhaskar et al. - 2024 - The Heuristic Core Understanding Subnetwork Gener.pdf}
}

@article{biderman_emergent_2023,
  title = {Emergent and Predictable Memorization in Large Language Models},
  author = {Biderman, Stella and Prashanth, USVSN Sai and Sutawika, Lintang and Schoelkopf, Hailey and Anthony, Quentin and Purohit, Shivanshu and Raff, Edward},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2304.11158},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.11158},
  url = {http://arxiv.org/abs/2304.11158},
  urldate = {2024-02-12},
  abstract = {Memorization, or the tendency of large language models (LLMs) to output entire sequences from their training data verbatim, is a key concern for safely deploying language models. In particular, it is vital to minimize a model's memorization of sensitive datapoints such as those containing personal identifiable information (PII). The prevalence of such undesirable memorization can pose issues for model trainers, and may even require discarding an otherwise functional model. We therefore seek to predict which sequences will be memorized before a large model's full train-time by extrapolating the memorization behavior of lower-compute trial runs. We measure memorization of the Pythia model suite and plot scaling laws for forecasting memorization, allowing us to provide equi-compute recommendations to maximize the reliability (recall) of such predictions. We additionally provide further novel discoveries on the distribution of memorization scores across models and data. We release all code and data necessary to reproduce the results in this paper at https://github.com/EleutherAI/pythia},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/H33N24KF/Biderman et al. - 2023 - Emergent and Predictable Memorization in Large Lan.pdf}
}

@article{biderman_pythia_2023,
  title = {Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling},
  shorttitle = {Pythia},
  author = {Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin and Bradley, Herbie and O'Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and Skowron, Aviya and Sutawika, Lintang and {van der Wal}, Oskar},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2304.01373},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.01373},
  url = {http://arxiv.org/abs/2304.01373},
  urldate = {2024-03-13},
  abstract = {How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce {\textbackslash}textit\{Pythia\}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend {\textbackslash}textit\{Pythia\} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at {\textbackslash}url\{https://github.com/EleutherAI/pythia\}.},
  archiveprefix = {arxiv},
  keywords = {mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/XFMGIH3U/Biderman et al. - 2023 - Pythia A Suite for Analyzing Large Language Model.pdf}
}

@article{bietti_birth_2023,
  title = {Birth of a Transformer: A Memory Viewpoint},
  shorttitle = {Birth of a Transformer},
  author = {Bietti, A. and Cabannes, Vivien A. and Bouchacourt, Diane and J{\'e}gou, H. and Bottou, L.},
  year = {2023},
  journal = {NeurIPS},
  url = {https://www.semanticscholar.org/paper/MultiViz%3A-An-Analysis-Benchmark-for-Visualizing-and-Liang-Lyu/27533b6175a1ba53b042bd982437a486764ecc21},
  urldate = {2023-10-25},
  abstract = {The complementary stages in MULTIVIZ together enable users to simulate model predictions, assign interpretable concepts to features, perform error analysis on model misclassifications, and use insights from error analysis to debug models. The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MULTIVIZ, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MULTIVIZ is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MULTIVIZ together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MULTIVIZ is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HA25ZXZA/Bietti et al. - 2023 - Birth of a Transformer A Memory Viewpoint.pdf;/Users/leonardbereska/Zotero/storage/UC47RGPB/Bietti et al. - 2023 - Birth of a Transformer A Memory Viewpoint.pdf}
}

@article{bigelow_incontext_2023,
  title = {In-Context Learning Dynamics with Random Binary Sequences},
  author = {Bigelow, Eric J. and Lubana, Ekdeep Singh and Dick, Robert P. and Tanaka, Hidenori and Ullman, Tomer D.},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2310.17639},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.17639},
  url = {http://arxiv.org/abs/2310.17639},
  urldate = {2023-12-07},
  abstract = {Large language models (LLMs) trained on huge corpora of text datasets demonstrate intriguing capabilities, achieving state-of-the-art performance on tasks they were not explicitly trained for. The precise nature of LLM capabilities is often mysterious, and different prompts can elicit different capabilities through in-context learning. We propose a framework that enables us to analyze in-context learning dynamics to understand latent concepts underlying LLMs' behavioral patterns. This provides a more nuanced understanding than success-or-failure evaluation benchmarks, but does not require observing internal activations as a mechanistic interpretation of circuits would. Inspired by the cognitive science of human randomness perception, we use random binary sequences as context and study dynamics of in-context learning by manipulating properties of context data, such as sequence length. In the latest GPT-3.5+ models, we find emergent abilities to generate seemingly random numbers and learn basic formal languages, with striking in-context learning dynamics where model outputs transition sharply from seemingly random behaviors to deterministic repetition.},
  archiveprefix = {arxiv},
  keywords = {cognitive interpretability,not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/BGQVEMTS/Bigelow et al. - 2023 - In-Context Learning Dynamics with Random Binary Se.pdf}
}

@article{bills_language_2023,
  title = {Language models can explain neurons in language models},
  author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  year = {2023},
  journal = {OpenAI Blog},
  url = {https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
  abstract = {Language models have become more capable and more widely deployed, but we do not understand how they work. Recent work has made progress on understanding a small number of circuits and narrow behaviors, but to fully understand a language model, we'll need to analyze millions of neurons. This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment. Our technique seeks to explain what patterns in text cause a neuron to activate. It consists of three steps: Step 1Explain the neuron's activations using GPT-4 Show neuron activations to GPT-4: The Avengers to the big screen, Joss Whedon has returned to reunite Marvel's gang of superheroes for their toughest challenge yet. Avengers: Age of Ultron pits the titular heroes against a sentient artificial intelligence, and smart money says that it could soar at the box office to be the highest-grossing film of the introduction into the Marvel cinematic universe, it's possible, though Marvel Studios boss Kevin Feige told Entertainment Weekly that, "Tony is earthbound and facing earthbound villains. You will not find magic power rings firing ice and flame beams." Spoilsport! But he does hint that they have some use{\dots} STARK T , which means this Nightwing movie is probably not about the guy who used to own that suit. So, unless new director Matt Reeves' The Batman is going to dig into some of this backstory or introduce the Dick Grayson character in his movie, the Nightwing movie is going to have a lot of work to do explaining of Avengers who weren't in the movie and also Thor try to fight the infinitely powerful Magic Space Fire Bird. It ends up being completely pointless, an embarrassing loss, and I'm pretty sure Thor accidentally destroys a planet. That's right. In an effort to save Earth, one of the heroes inadvertantly blows up an GPT-4 gives an explanation, guessing that the neuron is activating on references to movies, characters, and entertainment. Step 2Simulate activations using GPT-4, conditioning on the explanation Step 3Score the explanation by comparing the simulated and real activations},
  keywords = {automate,cited,graph,mechinterp,neuron,to cite,to extract figures,to extract related work,to review in detail,tool},
  file = {/Users/leonardbereska/Zotero/storage/9B7H84NW/Bills et al. - 2023 - Language models can explain neurons in language mo.html}
}

@article{bilodeau_impossibility_2024,
  title = {Impossibility Theorems for Feature Attribution},
  author = {Bilodeau, Blair and Jaques, Natasha and Koh, Pang Wei and Kim, Been},
  year = {2024},
  month = jan,
  journal = {Proc. Natl. Acad. Sci. U.S.A.},
  volume = {121},
  number = {2},
  eprint = {2212.11870},
  primaryclass = {cs},
  pages = {e2304406120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2304406120},
  url = {http://arxiv.org/abs/2212.11870},
  urldate = {2024-03-05},
  abstract = {Despite a sea of interpretability methods that can produce plausible explanations, the field has also empirically seen many failure cases of such methods. In light of these results, it remains unclear for practitioners how to use these methods and choose between them in a principled way. In this paper, we show that for moderately rich model classes (easily satisfied by neural networks), any feature attribution method that is complete and linear -- for example, Integrated Gradients and SHAP -- can provably fail to improve on random guessing for inferring model behaviour. Our results apply to common end-tasks such as characterizing local model behaviour, identifying spurious features, and algorithmic recourse. One takeaway from our work is the importance of concretely defining end-tasks: once such an end-task is defined, a simple and direct approach of repeated model evaluations can outperform many other complex feature attribution methods.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/8VUCAVUC/Bilodeau et al. - 2024 - Impossibility Theorems for Feature Attribution.pdf}
}

@book{bishop_pattern_2006,
  title = {Pattern recognition and machine learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {Springer-Verlag New York Inc.},
  url = {https://api.semanticscholar.org/CorpusID:60688891},
  keywords = {cited}
}

@article{black_interpreting_2022,
  title = {Interpreting Neural Networks through the Polytope Lens},
  author = {Black, Sid and Sharkey, Lee and Grinsztajn, Leo and Winsor, Eric and Braun, Dan and Merizian, Jacob and Parker, Kip and Guevara, Carlos Ram{\'o}n and Millidge, Beren and Alfour, Gabriel and Leahy, Connor},
  year = {2022},
  month = nov,
  journal = {CoRR},
  publisher = {arXiv},
  url = {https://arxiv.org/abs/2211.12312},
  urldate = {2023-07-31},
  abstract = {Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,feature,fundamental,mechinterp,to cite,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/2DDCWY73/Black et al. - 2022 - Interpreting Neural Networks through the Polytope .pdf}
}

@article{bloom_decision_2023,
  title = {Decision Transformer Interpretability},
  author = {Bloom, Joseph and Colognese, Paul},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/bBuBDJBYHt39Q5zZy/decision-transformer-interpretability},
  urldate = {2024-03-18},
  abstract = {TLDR: We analyse how a small Decision Transformer learns to simulate agents on a grid world task, providing evidence that it is possible to do circui{\dots}},
  language = {en},
  keywords = {mechinterp,not cited,to cite}
}

@article{bohm_sparsity_2020,
  title = {Sparsity and interpretability?},
  author = {B{\"o}hm, Ada and RobertKirk and Gaven{\v c}iak, Tom{\'a}{\v s}},
  year = {2020},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/maBNBgopYxb9YZP8B/sparsity-and-interpretability-1},
  urldate = {2023-11-22},
  abstract = {A series of interactive visualisations of small-scale experiments to generate intuitions for the question: "Are sparse models more interpretable? If{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PISCB7FK/Bhm et al. - 2020 - Sparsity and interpretability.html}
}

@article{boix-adsera_when_2024,
  title = {When can transformers reason with abstract symbols?},
  author = {{Boix-Adsera}, Enric and Saremi, Omid and Abbe, Emmanuel and Bengio, Samy and Littwin, Etai and Susskind, Joshua},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2310.09753},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.09753},
  url = {http://arxiv.org/abs/2310.09753},
  urldate = {2024-06-10},
  abstract = {We investigate the capabilities of transformer models on relational reasoning tasks. In these tasks, models are trained on a set of strings encoding abstract relations, and are then tested out-of-distribution on data that contains symbols that did not appear in the training dataset. We prove that for any relational reasoning task in a large family of tasks, transformers learn the abstract relations and generalize to the test set when trained by gradient descent on sufficiently large quantities of training data. This is in contrast to classical fully-connected networks, which we prove fail to learn to reason. Our results inspire modifications of the transformer architecture that add only two trainable parameters per head, and that we empirically demonstrate improve data efficiency for learning to reason.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/VYYMV46C/Boix-Adsera et al. - 2024 - When can transformers reason with abstract symbols.pdf}
}

@article{bolukbasi_interpretability_2021,
  title = {An Interpretability Illusion for BERT},
  author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Vi'egas, Fernanda and Wattenberg, M.},
  year = {2021},
  month = apr,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/An-Interpretability-Illusion-for-BERT-Bolukbasi-Pearce/9b9dc2b3d95d2f4e4269a9818c14c70c1f801384},
  urldate = {2023-07-31},
  abstract = {We describe an ``interpretability illusion'' that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
  keywords = {cited,feature,neuron,pitfall,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/ADI39PK8/Bolukbasi et al. - 2021 - An Interpretability Illusion for BERT.pdf}
}

@article{bommasani_opportunities_2022,
  title = {On the Opportunities and Risks of Foundation Models},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  journal = {CoRR},
  eprint = {2108.07258},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2108.07258},
  url = {http://arxiv.org/abs/2108.07258},
  urldate = {2023-02-15},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FY27YMS3/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf}
}

@article{bordt_statistics_2024,
  title = {Statistics without Interpretation: A Sober Look at Explainable Machine Learning},
  shorttitle = {Statistics without Interpretation},
  author = {Bordt, Sebastian and {von Luxburg}, Ulrike},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.02870},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.02870},
  url = {http://arxiv.org/abs/2402.02870},
  urldate = {2024-06-10},
  abstract = {In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/5W644E98/Bordt and von Luxburg - 2024 - Statistics without Interpretation A Sober Look at.pdf}
}

@article{borowski_exemplary_2021,
  title = {Exemplary Natural Images Explain CNN Activations Better than State-of-the-Art Feature Visualization},
  author = {Borowski, Judy and Zimmermann, Roland S. and Schepers, Judith and Geirhos, Robert and Wallis, Thomas S. A. and Bethge, Matthias and Brendel, Wieland},
  year = {2021},
  month = may,
  journal = {ICLR},
  eprint = {2010.12606},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2010.12606},
  url = {http://arxiv.org/abs/2010.12606},
  urldate = {2023-12-05},
  abstract = {Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans to predict CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. (2017) with a simple baseline visualization, namely exemplary natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiments are designed to maximize participants' performance, and are the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations (\$82{\textbackslash}pm4{\textbackslash}\%\$ accuracy; chance would be \$50{\textbackslash}\%\$). However, natural images - originally intended as a baseline - outperform synthetic images by a wide margin (\$92{\textbackslash}pm2{\textbackslash}\%\$). Additionally, participants are faster and more confident for natural images, whereas subjective impressions about the interpretability of the feature visualizations are mixed. The higher informativeness of natural images holds across most layers, for both expert and lay participants as well as for hand- and randomly-picked feature visualizations. Even if only a single reference image is given, synthetic images provide less information than natural images (\$65{\textbackslash}pm5{\textbackslash}\%\$ vs. \$73{\textbackslash}pm4{\textbackslash}\%\$). In summary, synthetic images from a popular feature visualization method are significantly less informative for assessing CNN activations than natural images. We argue that visualization methods should improve over this baseline.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/3DULHJVL/Borowski et al. - 2021 - Exemplary Natural Images Explain CNN Activations B.pdf}
}

@article{borowski_natural_2020,
  title = {Natural Images are More Informative for Interpreting CNN Activations than State-of-the-Art Synthetic Feature Visualizations},
  author = {Borowski, Judy and Zimmermann, Roland S. and Schepers, Judith and Geirhos, Robert and Wallis, Thomas S. A. and Bethge, M. and Brendel, Wieland},
  year = {2020},
  journal = {OpenReview},
  url = {https://www.semanticscholar.org/paper/Natural-Images-are-More-Informative-for-CNN-than-Borowski-Zimmermann/2b3c922278851b1c2c516ffb9b49cc5f04e855e4},
  urldate = {2024-02-11},
  abstract = {Feature visualizations such as synthetic maximally activating images are a widely used explanation method to better understand the information processing of convolutional neural networks (CNNs). At the same time, there are concerns that these visualizations might not accurately represent CNNs' inner workings. Here, we measure how much extremely activating images help humans in predicting CNN activations. Using a well-controlled psychophysical paradigm, we compare the informativeness of synthetic images by Olah et al. [45] with a simple baseline visualization, namely natural images that also strongly activate a specific feature map. Given either synthetic or natural reference images, human participants choose which of two query images leads to strong positive activation. The experiment is designed to maximize participants' performance, and is the first to probe intermediate instead of final layer representations. We find that synthetic images indeed provide helpful information about feature map activations (82 {\textpm} 4\% accuracy; chance would be 50\%). However, natural images---originally intended to be a baseline---outperform these synthetic images by a wide margin (92{\textpm}2\% accuracy). The superiority of natural images holds across the investigated network and various conditions. Therefore, we argue that visualization methods should improve over this simple baseline.},
  keywords = {not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/BKEDK32Y/Borowski et al. - 2020 - Natural Images are More Informative for Interpreti.pdf}
}

@book{bostrom_superintelligence_2014,
  title = {Superintelligence: Paths, Dangers, Strategies},
  shorttitle = {Superintelligence},
  author = {Bostrom, Nick},
  year = {2014},
  month = jun,
  publisher = {Oxford University Press},
  abstract = {A New York Times bestsellerSuperintelligence asks the questions: What happens when machines surpass humans in general intelligence? Will artificial agents save or destroy us? Nick Bostrom lays the foundation for understanding the future of humanity and intelligent life. The human brain has some capabilities that the brains of other animals lack. It is to these distinctive capabilities that our species owes its dominant position. If machine brains surpassed human brains in general intelligence, then this new superintelligence could become extremely powerful - possibly beyond our control. As the fate of the gorillas now depends more on humans than on the species itself, so would the fate of humankind depend on the actions of the machine superintelligence.But we have one advantage: we get to make the first move. Will it be possible to construct a seed Artificial Intelligence, to engineer initial conditions so as to make an intelligence explosion survivable? How could one achieve a controlled detonation?This profoundly ambitious and original book breaks down a vast track of difficult intellectual terrain. After an utterly engrossing journey that takes us to the frontiers of thinking about the human condition and the future of intelligent life, we find in Nick Bostrom's work nothing less than a reconceptualization of the essential task of our time.},
  language = {English},
  keywords = {not cited}
}

@article{bostrom_superintelligent_2012,
  title = {The Superintelligent Will: Motivation and Instrumental Rationality in Advanced Artificial Agents},
  shorttitle = {The Superintelligent Will},
  author = {Bostrom, Nick},
  year = {2012},
  month = may,
  journal = {Minds \& Machines},
  volume = {22},
  number = {2},
  pages = {71--85},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/s11023-012-9281-3},
  url = {http://link.springer.com/10.1007/s11023-012-9281-3},
  urldate = {2023-01-26},
  abstract = {This paper discusses the relation between intelligence and motivation in artificial agents, developing and briefly arguing for two theses. The first, the orthogonality thesis, holds (with some caveats) that intelligence and final goals (purposes) are orthogonal axes along which possible artificial intellects can freely vary---more or less any level of intelligence could be combined with more or less any final goal. The second, the instrumental convergence thesis, holds that as long as they possess a sufficient level of intelligence, agents having any of a wide range of final goals will pursue similar intermediary goals because they have instrumental reasons to do so. In combination, the two theses help us understand the possible range of behavior of superintelligent agents, and they point to some potential dangers in building such an agent.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/STBPLYXB/Bostrom - 2012 - The Superintelligent Will Motivation and Instrume.pdf}
}

@article{bowman_eight_2023,
  title = {Eight Things to Know about Large Language Models},
  author = {Bowman, Samuel R.},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.00612},
  url = {https://arxiv.org/abs/2304.00612},
  urldate = {2024-02-11},
  abstract = {The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points: 1. LLMs predictably get more capable with increasing investment, even without targeted innovation. 2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment. 3. LLMs often appear to learn and use representations of the outside world. 4. There are no reliable techniques for steering the behavior of LLMs. 5. Experts are not yet able to interpret the inner workings of LLMs. 6. Human performance on a task isn't an upper bound on LLM performance. 7. LLMs need not express the values of their creators nor the values encoded in web text. 8. Brief interactions with LLMs are often misleading.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {LLMs,not cited},
  file = {/Users/leonardbereska/Zotero/storage/9ULAIKX4/Bowman - 2023 - Eight Things to Know about Large Language Models.pdf}
}

@article{bowman_measuring_2022,
  title = {Measuring Progress on Scalable Oversight for Large Language Models},
  author = {Bowman, Samuel R. and Hyun, Jeeyoon and Perez, Ethan and Chen, Edwin and Pettit, Craig and Heiner, Scott and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Askell, Amanda and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and Olah, Christopher and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Ladish, Jeffrey and Landau, Joshua and Ndousse, Kamal and Lovitt, Liane and Elhage, Nelson and Schiefer, Nicholas and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Fort, Stanislav and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Mann, Ben and Kaplan, Jared},
  year = {2022},
  month = nov,
  journal = {CoRR},
  eprint = {2211.03540},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.03540},
  url = {http://arxiv.org/abs/2211.03540},
  urldate = {2023-08-26},
  abstract = {Developing safe and useful general-purpose AI systems will require us to make progress on scalable oversight: the problem of supervising systems that potentially outperform us on most skills relevant to the task at hand. Empirical work on this problem is not straightforward, since we do not yet have systems that broadly exceed our abilities. This paper discusses one of the major ways we think about this problem, with a focus on ways it can be studied empirically. We first present an experimental design centered on tasks for which human specialists succeed but unaided humans and current general AI systems fail. We then present a proof-of-concept experiment meant to demonstrate a key feature of this experimental design and show its viability with two question-answering tasks: MMLU and time-limited QuALITY. On these tasks, we find that human participants who interact with an unreliable large-language-model dialog assistant through chat -- a trivial baseline strategy for scalable oversight -- substantially outperform both the model alone and their own unaided performance. These results are an encouraging sign that scalable oversight will be tractable to study with present models and bolster recent findings that large language models can productively assist humans with difficult tasks.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7M8534H3/Bowman et al. - 2022 - Measuring Progress on Scalable Oversight for Large.pdf}
}

@article{brasoveanu_visualizing_2022,
  title = {Visualizing and Explaining Language Models},
  author = {Bra{\c s}oveanu, Adrian M. P. and Andonie, R{\u a}zvan},
  year = {2022},
  month = apr,
  journal = {CoRR},
  eprint = {2205.10238},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.10238},
  url = {http://arxiv.org/abs/2205.10238},
  urldate = {2023-08-26},
  abstract = {During the last decade, Natural Language Processing has become, after Computer Vision, the second field of Artificial Intelligence that was massively changed by the advent of Deep Learning. Regardless of the architecture, the language models of the day need to be able to process or generate text, as well as predict missing words, sentences or relations depending on the task. Due to their black-box nature, such models are difficult to interpret and explain to third parties. Visualization is often the bridge that language model designers use to explain their work, as the coloring of the salient words and phrases, clustering or neuron activations can be used to quickly understand the underlying models. This paper showcases the techniques used in some of the most popular Deep Learning for NLP visualizations, with a special focus on interpretability and explainability.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5QQZVTAD/Braoveanu and Andonie - 2022 - Visualizing and Explaining Language Models.pdf}
}

@article{braun_identifying_2024,
  title = {Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning},
  author = {Braun, Dan and Taylor, Jordan and {Goldowsky-Dill}, Nicholas and Sharkey, Lee},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.12241},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.12241},
  url = {http://arxiv.org/abs/2405.12241},
  urldate = {2024-06-10},
  abstract = {Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e\_sae},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/IWRSBULZ/Braun et al. - 2024 - Identifying Functionally Important Features with E.pdf}
}

@article{brenner_tractable_2022,
  title = {Tractable Dendritic RNNs for Reconstructing Nonlinear Dynamical Systems},
  author = {Brenner, Manuel and Hess, Florian and Mikhaeil, Jonas M. and Bereska, Leonard F. and Monfared, Zahra and Kuo, Po-Chen and Durstewitz, Daniel},
  year = {2022},
  month = jun,
  journal = {Proceedings of the 39th International Conference on Machine Learning},
  pages = {2292--2320},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/brenner22a.html},
  urldate = {2024-07-03},
  abstract = {In many scientific disciplines, we are interested in inferring the nonlinear dynamical system underlying a set of observed time series, a challenging task in the face of chaotic behavior and noise. Previous deep learning approaches toward this goal often suffered from a lack of interpretability and tractability. In particular, the high-dimensional latent spaces often required for a faithful embedding, even when the underlying dynamics lives on a lower-dimensional manifold, can hamper theoretical analysis. Motivated by the emerging principles of dendritic computation, we augment a dynamically interpretable and mathematically tractable piecewise-linear (PL) recurrent neural network (RNN) by a linear spline basis expansion. We show that this approach retains all the theoretically appealing properties of the simple PLRNN, yet boosts its capacity for approximating arbitrary nonlinear dynamical systems in comparatively low dimensions. We employ two frameworks for training the system, one combining BPTT with teacher forcing, and another based on fast and scalable variational inference. We show that the dendritically expanded PLRNN achieves better reconstructions with fewer parameters and dimensions on various dynamical systems benchmarks and compares favorably to other methods, while retaining a tractable and interpretable structure.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/6LFIZC2X/Brenner et al. - 2022 - Tractable Dendritic RNNs for Reconstructing Nonlin.pdf}
}

@article{bricken_attention_2021,
  title = {Attention Approximates Sparse Distributed Memory},
  author = {Bricken, Trenton and Pehlevan, C.},
  year = {2021},
  month = nov,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Attention-Approximates-Sparse-Distributed-Memory-Bricken-Pehlevan/fafc3bcccdd84f870b433119b68ce8937cdf31a8},
  urldate = {2023-12-14},
  abstract = {While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva's Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We confirm that these conditions are satisfied in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/XNFBA8YR/Bricken and Pehlevan - 2021 - Attention Approximates Sparse Distributed Memory.pdf}
}

@article{bricken_monosemanticity_2023,
  title = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nicholas L. and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E. and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Chris},
  year = {2023},
  month = oct,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/monosemantic-features/index.html},
  abstract = {Mechanistic interpretability seeks to understand neural networks by breaking them into components that are more easily understood than the whole. By understanding the function of each component, and how they interact, we hope to be able to reason about the behavior of the entire network. The first step in that program is to identify the correct components to analyze.},
  keywords = {cited,fundamental,mechinterp,SAE,superposition,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/Q6RCQ5EH/Bricken et al. - 2023 - Towards Monosemanticity Decomposing Language Mode.html}
}

@article{brinkmann_mechanistic_2024,
  title = {A Mechanistic Analysis of a Transformer Trained on a Symbolic Multi-Step Reasoning Task},
  author = {Brinkmann, Jannik and Sheshadri, Abhay and Levoso, Victor and Swoboda, Paul and Bartelt, Christian},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.11917},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.11917},
  url = {http://arxiv.org/abs/2402.11917},
  urldate = {2024-03-01},
  abstract = {Transformers demonstrate impressive performance on a range of reasoning benchmarks. To evaluate the degree to which these abilities are a result of actual reasoning, existing work has focused on developing sophisticated benchmarks for behavioral studies. However, these studies do not provide insights into the internal mechanisms driving the observed capabilities. To improve our understanding of the internal mechanisms of transformers, we present a comprehensive mechanistic analysis of a transformer trained on a synthetic reasoning task. We identify a set of interpretable mechanisms the model uses to solve the task, and validate our findings using correlational and causal evidence. Our results suggest that it implements a depth-bounded recurrent mechanisms that operates in parallel and stores intermediate results in selected token positions. We anticipate that the motifs we identified in our synthetic setting can provide valuable insights into the broader operating principles of transformers and thus provide a basis for understanding more complex models.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited},
  file = {/Users/leonardbereska/Zotero/storage/XBBZLQQZ/Brinkmann et al. - 2024 - A Mechanistic Analysis of a Transformer Trained on.pdf}
}

@article{brocki_evaluation_2022,
  title = {Evaluation of Interpretability Methods and Perturbation Artifacts in Deep Neural Networks},
  author = {Brocki, Lennart and Chung, Neo Christopher},
  year = {2022},
  month = mar,
  journal = {CoRR},
  eprint = {2203.02928},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2203.02928},
  url = {http://arxiv.org/abs/2203.02928},
  urldate = {2023-09-08},
  abstract = {Despite excellent performance of deep neural networks (DNNs) in image classification, detection, and prediction, characterizing how DNNs make a given decision remains an open problem, resulting in a number of interpretability methods. Post-hoc interpretability methods primarily aim to quantify the importance of input features with respect to the class probabilities. However, due to the lack of ground truth and the existence of interpretability methods with diverse operating characteristics, evaluating these methods is a crucial challenge. A popular approach to evaluate interpretability methods is to perturb input features deemed important for a given prediction and observe the decrease in accuracy. However, perturbation itself may introduce artifacts. We propose a method for estimating the impact of such artifacts on the fidelity estimation by utilizing model accuracy curves from perturbing input features according to the Most Import First (MIF) and Least Import First (LIF) orders. Using the ResNet-50 trained on the ImageNet, we demonstrate the proposed fidelity estimation of four popular post-hoc interpretability methods.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/U44J7D6Q/Brocki and Chung - 2022 - Evaluation of Interpretability Methods and Perturb.pdf}
}

@article{brown_language_2020,
  title = {Language Models are Few-Shot Learners},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  journal = {NeurIPS},
  eprint = {2005.14165},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2023-10-25},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7439L5MF/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@article{brown_privileged_2023,
  title = {On Privileged and Convergent Bases in Neural Network Representations},
  author = {Brown, Davis and Vyas, Nikhil and Bansal, Yamini},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.12941},
  url = {https://arxiv.org/abs/2307.12941},
  urldate = {2023-10-27},
  abstract = {In this study, we investigate whether the representations learned by neural networks possess a privileged and convergent basis. Specifically, we examine the significance of feature directions represented by individual neurons. First, we establish that arbitrary rotations of neural representations cannot be inverted (unlike linear networks), indicating that they do not exhibit complete rotational invariance. Subsequently, we explore the possibility of multiple bases achieving identical performance. To do this, we compare the bases of networks trained with the same parameters but with varying random initializations. Our study reveals two findings: (1) Even in wide networks such as WideResNets, neural networks do not converge to a unique basis; (2) Basis correlation increases significantly when a few early layers of the network are frozen identically. Furthermore, we analyze Linear Mode Connectivity, which has been studied as a measure of basis correlation. Our findings give evidence that while Linear Mode Connectivity improves with increased network width, this improvement is not due to an increase in basis correlation.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BLWSVFC8/Brown et al. - 2023 - On Privileged and Convergent Bases in Neural Netwo.pdf}
}

@article{brunner_identifiability_2020,
  title = {On Identifiability in Transformers},
  author = {Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
  year = {2020},
  journal = {ICLR},
  url = {https://www.semanticscholar.org/paper/On-Identifiability-in-Transformers-Brunner-Liu/1fe62a928bf5cfac0f373728f3a4de3cefe0951d},
  urldate = {2023-08-27},
  abstract = {In this paper we delve deep in the Transformer architecture by investigating two of its core components: self-attention and contextual embeddings. In particular, we study the identifiability of attention weights and token embeddings, and the aggregation of context into hidden tokens. We show that, for sequences longer than the attention head dimension, attention weights are not identifiable. We propose effective attention as a complementary tool for improving explanatory interpretations based on attention. Furthermore, we show that input tokens retain to a large degree their identity across the model. We also find evidence suggesting that identity information is mainly encoded in the angle of the embeddings and gradually decreases with depth. Finally, we demonstrate strong mixing of input information in the generation of contextual embeddings by means of a novel quantification method based on gradient attribution. Overall, we show that self-attention distributions are not directly interpretable and present tools to better understand and further investigate Transformer models.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EYIMGBCP/Brunner et al. - 2020 - On Identifiability in Transformers.pdf}
}

@article{bubeck_sparks_2023,
  title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  shorttitle = {Sparks of Artificial General Intelligence},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  journal = {CoRR},
  eprint = {2303.12712},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.12712},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-05-17},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/37E575TK/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@article{burns_discovering_2023,
  title = {Discovering Latent Knowledge in Language Models Without Supervision},
  author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  year = {2023},
  journal = {ICLR},
  eprint = {2212.03827},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2212.03827},
  urldate = {2023-07-31},
  abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4{\textbackslash}\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.},
  archiveprefix = {arxiv},
  keywords = {cited,feature,probing,to review in detail,unsupervised},
  file = {/Users/leonardbereska/Zotero/storage/I6V7LFHI/Burns et al. - 2023 - Discovering Latent Knowledge in Language Models Wi.pdf}
}

@article{bushnaq_local_2024,
  title = {The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks},
  shorttitle = {The Local Interaction Basis},
  author = {Bushnaq, Lucius and Heimersheim, Stefan and {Goldowsky-Dill}, Nicholas and Braun, Dan and Mendel, Jake and Hanni, Kaarel and Griffin, Avery and Stohler, Jorn and Wache, Magdalena and Hobbhahn, Marius},
  year = {2024},
  month = may,
  url = {https://www.semanticscholar.org/paper/The-Local-Interaction-Basis%3A-Identifying-and-in-Bushnaq-Heimersheim/0ee4b3249380d73a27acd2244bb01a97c229d9bc},
  urldate = {2024-06-09},
  abstract = {Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.},
  file = {/Users/leonardbereska/Zotero/storage/2Y3B28Z4/Bushnaq et al. - 2024 - The Local Interaction Basis Identifying Computati.pdf}
}

@article{bushnaq_using_2024,
  title = {Using Degeneracy in the Loss Landscape for Mechanistic Interpretability},
  author = {Bushnaq, Lucius and Mendel, Jake and Heimersheim, Stefan and Braun, Dan and {Goldowsky-Dill}, Nicholas and H{\"a}nni, Kaarel and Wu, Cindy and Hobbhahn, Marius},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.10927},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.10927},
  url = {http://arxiv.org/abs/2405.10927},
  urldate = {2024-06-09},
  abstract = {Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/QIEBEIXN/Bushnaq et al. - 2024 - Using Degeneracy in the Loss Landscape for Mechani.pdf}
}

@article{byrnes_intro_2022,
  title = {[Intro to brain-like-AGI safety] 1. What's the problem \& Why work on it now?},
  author = {Byrnes, Steven},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/4basF9w9jaPZpoC8R/intro-to-brain-like-agi-safety-1-what-s-the-problem-and-why},
  urldate = {2023-05-17},
  abstract = {1.1 POST SUMMARY / TABLE OF CONTENTS This is the first of a series of blog posts on the technical safety problem for hypothetical future brain-like Artificial General Intelligence (AGI) systems. So m{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/D739N95L/Byrnes - 2022 - [Intro to brain-like-AGI safety] 1. What's the pro.html}
}

@article{caballero_broken_2022,
  title = {Broken Neural Scaling Laws},
  author = {Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
  year = {2022},
  month = oct,
  journal = {ICLR},
  eprint = {2210.14891},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.14891},
  url = {http://arxiv.org/abs/2210.14891},
  urldate = {2023-12-22},
  abstract = {We present a smoothly broken power law functional form (that we refer to as a Broken Neural Scaling Law (BNSL)) that accurately models \& extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as amount of compute used for training (or inference), number of model parameters, training dataset size, model input size, number of training steps, or upstream performance varies) for various architectures \& for each of various tasks within a large \& diverse set of upstream \& downstream tasks, in zero-shot, prompted, \& finetuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, AI capabilities, robotics, out-of-distribution (OOD) generalization, continual learning, transfer learning, uncertainty estimation / calibration, OOD detection, adversarial robustness, distillation, sparsity, retrieval, quantization, pruning, fairness, molecules, computer programming/coding, math word problems, "emergent phase transitions", arithmetic, supervised learning, unsupervised/self-supervised learning, \& reinforcement learning (single agent \& multi-agent). When compared to other functional forms for neural scaling, this functional form yields extrapolations of scaling behavior that are considerably more accurate on this set. Moreover, this functional form accurately models \& extrapolates scaling behavior that other functional forms are incapable of expressing such as the nonmonotonic transitions present in the scaling behavior of phenomena such as double descent \& the delayed, sharp inflection points present in the scaling behavior of tasks such as arithmetic. Lastly, we use this functional form to glean insights about the limit of the predictability of scaling behavior. Code is available at https://github.com/ethancaballero/broken\_neural\_scaling\_laws},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BG5BYCW7/Caballero et al. - 2022 - Broken Neural Scaling Laws.pdf}
}

@article{cammarata_curve_2020,
  title = {Curve Detectors},
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  year = {2020},
  month = jun,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/curve-detectors},
  urldate = {2023-05-31},
  keywords = {circuit,cited,empirical,historical,mechinterp,to cite,to extract figures,to extract related work,to review in detail,vision},
  file = {/Users/leonardbereska/Zotero/storage/D7CEC8T5/Cammarata et al. - 2020 - Curve Detectors.html}
}

@article{cammarata_curve_2021,
  title = {Curve Circuits},
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Voss, Chelsea and Schubert, Ludwig and Olah, Chris},
  year = {2021},
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/curve-circuits/},
  keywords = {circuit,cited,empirical,historical,mechinterp,to cite,to extract figures,to extract related work,to review in detail,vision},
  file = {/Users/leonardbereska/Zotero/storage/TTL9L4ZU/Cammarata et al. - 2021 - Curve Circuits.html}
}

@article{cammarata_thread_2020,
  title = {Thread: Circuits},
  shorttitle = {Thread},
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig},
  year = {2020},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits},
  urldate = {2023-06-23},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2KURRBQR/Cammarata et al. - 2020 - Thread Circuits.html}
}

@article{cao_lowcomplexity_2021,
  title = {Low-Complexity Probing via Finding Subnetworks},
  author = {Cao, Steven and Sanh, Victor and Rush, Alexander M.},
  year = {2021},
  month = apr,
  journal = {NAACL-HLT},
  eprint = {2104.03514},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2104.03514},
  url = {http://arxiv.org/abs/2104.03514},
  urldate = {2023-11-16},
  abstract = {The dominant approach in probing neural networks for linguistic properties is to train a new shallow multi-layer perceptron (MLP) on top of the model's internal representations. This approach can detect properties encoded in the model, but at the cost of adding new parameters that may learn the task directly. We instead propose a subtractive pruning-based probe, where we find an existing subnetwork that performs the linguistic task of interest. Compared to an MLP, the subnetwork probe achieves both higher accuracy on pre-trained models and lower accuracy on random models, so it is both better at finding properties of interest and worse at learning on its own. Next, by varying the complexity of each probe, we show that subnetwork probing Pareto-dominates MLP probing in that it achieves higher accuracy given any budget of probe complexity. Finally, we analyze the resulting subnetworks across various tasks to locate where each task is encoded, and we find that lower-level tasks are captured in lower layers, reproducing similar findings in past work.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5GKBXJUB/Cao et al. - 2021 - Low-Complexity Probing via Finding Subnetworks.pdf}
}

@article{carbonneau_measuring_2022,
  title = {Measuring Disentanglement: A Review of Metrics},
  shorttitle = {Measuring Disentanglement},
  author = {Carbonneau, Marc-Andr{\'e} and Zaidi, Julian and Boilard, Jonathan and Gagnon, Ghyslain},
  year = {2022},
  month = may,
  journal = {IEEE Trans. Neural Netw. Learn. Syst.},
  eprint = {2012.09276},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.09276},
  url = {http://arxiv.org/abs/2012.09276},
  urldate = {2024-01-04},
  abstract = {Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/XVU689TV/Carbonneau et al. - 2022 - Measuring Disentanglement A Review of Metrics.pdf}
}

@article{carter_using_2017,
  title = {Using Artificial Intelligence to Augment Human Intelligence},
  author = {Carter, Shan and Nielsen, Michael},
  year = {2017},
  month = dec,
  journal = {Distill},
  url = {https://distill.pub/2017/aia},
  urldate = {2023-05-12},
  abstract = {By creating user interfaces which let us work with the representations inside machine learning models, we can give people new tools for reasoning.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VP7W9VJW/Carter and Nielsen - 2017 - Using Artificial Intelligence to Augment Human Int.html}
}

@article{carvalho_machine_2019,
  title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
  shorttitle = {Machine Learning Interpretability},
  author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
  year = {2019},
  month = aug,
  journal = {Electronics},
  volume = {8},
  number = {8},
  pages = {832},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics8080832},
  url = {https://www.mdpi.com/2079-9292/8/8/832},
  urldate = {2023-11-01},
  abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems's adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RZYXE856/Carvalho et al. - 2019 - Machine Learning Interpretability A Survey on Met.pdf}
}

@article{casalicchio_visualizing_2018,
  title = {Visualizing the Feature Importance for Black Box Models},
  author = {Casalicchio, Giuseppe and Molnar, Christoph and Bischl, Bernd},
  year = {2018},
  journal = {ECML PKDD},
  volume = {11051},
  eprint = {1804.06620},
  primaryclass = {cs, stat},
  pages = {655--670},
  doi = {10.1007/978-3-030-10925-7_40},
  url = {http://arxiv.org/abs/1804.06620},
  urldate = {2024-01-24},
  abstract = {In recent years, a large amount of model-agnostic methods to improve the transparency, trustability and interpretability of machine learning models have been developed. We introduce local feature importance as a local version of a recent model-agnostic global feature importance method. Based on local feature importance, we propose two visual tools: partial importance (PI) and individual conditional importance (ICI) plots which visualize how changes in a feature affect the model performance on average, as well as for individual observations. Our proposed methods are related to partial dependence (PD) and individual conditional expectation (ICE) plots, but visualize the expected (conditional) feature importance instead of the expected (conditional) prediction. Furthermore, we show that averaging ICI curves across observations yields a PI curve, and integrating the PI curve with respect to the distribution of the considered feature results in the global feature importance. Another contribution of our paper is the Shapley feature importance, which fairly distributes the overall performance of a model among the features according to the marginal contributions and which can be used to compare the feature importance across different models.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KYLFVZAQ/Casalicchio et al. - 2018 - Visualizing the Feature Importance for Black Box M.pdf}
}

@article{casper_blackbox_2024,
  title = {Black-Box Access is Insufficient for Rigorous AI Audits},
  author = {Casper, Stephen and Ezell, Carson and Siegmann, Charlotte and Kolt, Noam and Curtis, Taylor Lynn and Bucknall, Benjamin and Haupt, Andreas and Wei, Kevin and Scheurer, J{\'e}r{\'e}my and Hobbhahn, Marius and Sharkey, Lee and Krishna, Satyapriya and Von Hagen, Marvin and Alberti, Silas and Chan, Alan and Sun, Qinyi and Gerovitch, Michael and Bau, David and Tegmark, Max and Krueger, David and {Hadfield-Menell}, Dylan},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.14446},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.14446},
  url = {http://arxiv.org/abs/2401.14446},
  urldate = {2024-02-07},
  abstract = {External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of system access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to its training and deployment information (e.g., methodology, code, documentation, hyperparameters, data, deployment details, findings from internal evaluations) allows for auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.},
  archiveprefix = {arxiv},
  keywords = {cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/F9ZWWH4F/Casper et al. - 2024 - Black-Box Access is Insufficient for Rigorous AI A.pdf}
}

@article{casper_blackbox_2024b,
  title = {Black-Box Access is Insufficient for Rigorous AI Audits},
  author = {Casper, Stephen and Ezell, Carson and Siegmann, Charlotte and Kolt, Noam and Curtis, Taylor Lynn and Bucknall, Benjamin and Haupt, Andreas and Wei, Kevin and Scheurer, J{\'e}r{\'e}my and Hobbhahn, Marius and Sharkey, Lee and Krishna, Satyapriya and Von Hagen, Marvin and Alberti, Silas and Chan, Alan and Sun, Qinyi and Gerovitch, Michael and Bau, David and Tegmark, Max and Krueger, David and {Hadfield-Menell}, Dylan},
  year = {2024},
  month = jun,
  journal = {The 2024 ACM Conference on Fairness, Accountability, and Transparency},
  pages = {2254--2272},
  doi = {10.1145/3630106.3659037},
  url = {https://dl.acm.org/doi/10.1145/3630106.3659037},
  urldate = {2024-06-10},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/AKJFPZEQ/Casper et al. - 2024 - Black-Box Access is Insufficient for Rigorous AI A.pdf}
}

@article{casper_defending_2024,
  title = {Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
  author = {Casper, Stephen and Schulze, Lennart and Patel, Oam and {Hadfield-Menell}, Dylan},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2403.05030},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2403.05030},
  urldate = {2024-06-10},
  abstract = {Despite extensive diagnostics and debugging by developers, AI systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training (AT) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use it to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness to novel attacks and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/CA4VTZJW/Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with La.pdf}
}

@article{casper_diagnostics_2023,
  title = {Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks},
  author = {Casper, Stephen and Hariharan, Kaivalya and {Hadfield-Menell}, Dylan},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2211.10024},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.10024},
  url = {http://arxiv.org/abs/2211.10024},
  urldate = {2023-09-08},
  abstract = {This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/8YPJ8EQB/Casper et al. - 2023 - Diagnostics for Deep Neural Networks with Automate.pdf}
}

@article{casper_eis_2023,
  title = {EIS V: Blind Spots In AI Safety Interpretability Research},
  shorttitle = {EIS V},
  author = {Casper, Stephen},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/7TFJAvjYfMKxKQ4XS/eis-v-blind-spots-in-ai-safety-interpretability-research},
  urldate = {2024-02-12},
  abstract = {Part 5 of 12 in the~Engineer's Interpretability Sequence. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NPHBYBWL/Casper - 2023 - EIS V Blind Spots In AI Safety Interpretability R.html}
}

@article{casper_engineer_2023,
  title = {The Engineer's Interpretability Sequence},
  author = {Casper, Stephen},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7},
  abstract = {Interpretability research is popular, and interpretability tools play a role in almost every agenda for making AI safe. However, for all the interpretability work that exists, there is a significant gap between the research and engineering applications. If one of our main goals for interpretability research is to help us with aligning highly intelligent AI systems in high stakes settings, shouldn't we be seeing tools that are more helpful on real world problems? This 12-post sequence argues for taking an engineering approach to interpretability research. And from this lens, it analyzes existing work and proposes directions for moving forward.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/6GF4ATE7/Casper - 2023 - The Engineers Interpretability Sequence.html}
}

@article{casper_explore_2023,
  title = {Explore, Establish, Exploit: Red Teaming Language Models from Scratch},
  shorttitle = {Explore, Establish, Exploit},
  author = {Casper, Stephen and Lin, Jason and Kwon, Joe and Culp, Gatlen and {Hadfield-Menell}, Dylan},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2306.09442},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.09442},
  url = {http://arxiv.org/abs/2306.09442},
  urldate = {2023-08-26},
  abstract = {Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this specification and identify methods to elicit this behavior from the model. Our red teaming framework consists of three steps: 1) Exploring the model's behavior in the desired context; 2) Establishing a measurement of undesired behavior (e.g., a classifier trained to reflect human evaluations); and 3) Exploiting the model's flaws using this measure and an established red teaming methodology. We apply this approach to red team GPT-2 and GPT-3 models to systematically discover classes of prompts that elicit toxic and dishonest statements. In doing so, we also construct and release the CommonClaim dataset of 20,000 statements that have been labeled by human subjects as common-knowledge-true, common-knowledge-false, or neither. Code is available at https://github.com/thestephencasper/explore\_establish\_exploit\_llms. CommonClaim is available at https://github.com/Algorithmic-Alignment-Lab/CommonClaim.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/H6T2CSBX/Casper et al. - 2023 - Explore, Establish, Exploit Red Teaming Language .pdf}
}

@article{casper_open_2023,
  title = {Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback},
  author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and Wang, Tony and Marks, Samuel and Segerie, Charbel-Rapha{\"e}l and Carroll, Micah and Peng, Andi and Christoffersen, Phillip and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and B{\i}y{\i}k, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and {Hadfield-Menell}, Dylan},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.15217},
  url = {https://arxiv.org/abs/2307.15217},
  urldate = {2023-10-26},
  abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/AJTS2HR6/Casper et al. - 2023 - Open Problems and Fundamental Limitations of Reinf.pdf}
}

@article{casper_red_2023,
  title = {Red Teaming Deep Neural Networks with Feature Synthesis Tools},
  author = {Casper, Stephen and Li, Yuxiao and Li, Jiawei and Bu, Tong and Zhang, Kevin and Hariharan, Kaivalya and {Hadfield-Menell}, Dylan},
  year = {2023},
  journal = {NeurIPS},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.10894},
  url = {https://arxiv.org/abs/2302.10894},
  urldate = {2023-09-18},
  abstract = {Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset. In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate interpretability tools based on whether they help humans identify these triggers. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a trojan-discovery benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 feature attribution/saliency tools. Even with access to data with a trojan's trigger, these methods regularly fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 variants of the best-performing method from the previous evaluation.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HUS59QSC/Casper et al. - 2023 - Red Teaming Deep Neural Networks with Feature Synt.pdf}
}

@article{casper_robust_2021,
  title = {Robust Feature-Level Adversaries are Interpretability Tools},
  author = {Casper, Stephen and Nadeau, Max and {Hadfield-Menell}, Dylan and Kreiman, Gabriel},
  year = {2021},
  month = oct,
  journal = {NeurIPS},
  eprint = {2110.03605},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.03605},
  url = {http://arxiv.org/abs/2110.03605},
  urldate = {2023-10-26},
  abstract = {The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "copy/paste" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at https://github.com/thestephencasper/feature\_level\_adv},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9XJPJWEH/Casper et al. - 2021 - Robust Feature-Level Adversaries are Interpretabil.pdf}
}

@article{chakraborty_interpretability_2017,
  title = {Interpretability of deep learning models: A survey of results},
  shorttitle = {Interpretability of deep learning models},
  author = {Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and Kelley, Troy D. and Braines, Dave and Sensoy, Murat and Willis, Christopher J. and Gurram, Prudhvi},
  year = {2017},
  month = aug,
  journal = {IEEE},
  pages = {1--6},
  publisher = {IEEE},
  address = {San Francisco, CA},
  doi = {10.1109/UIC-ATC.2017.8397411},
  url = {https://ieeexplore.ieee.org/document/8397411/},
  urldate = {2023-08-27},
  abstract = {Deep neural networks have achieved near-human accuracy levels in various types of classification and prediction tasks including images, text, speech, and video data. However, the networks continue to be treated mostly as black-box function approximators, mapping a given input to a classification output. The next step in this human-machine evolutionary process --- incorporating these networks into mission critical processes such as medical diagnosis, planning and control --- requires a level of trust association with the machine output. Typically, statistical metrics are used to quantify the uncertainty of an output. However, the notion of trust also depends on the visibility that a human has into the working of the machine. In other words, the neural network should provide human-understandable justifications for its output leading to insights about the inner workings. We call such models as interpretable deep networks. Interpretability is not a monolithic notion. In fact, the subjectivity of an interpretation, due to different levels of human understanding, implies that there must be a multitude of dimensions that together constitute interpretability. In addition, the interpretation itself can be provided either in terms of the low-level network parameters, or in terms of input features used by the model. In this paper, we outline some of the dimensions that are useful for model interpretability, and categorize prior work along those dimensions. In the process, we perform a gap analysis of what needs to be done to improve model interpretability.},
  isbn = {9781538604359},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/8AAUD9QA/Chakraborty et al. - 2017 - Interpretability of deep learning models A survey.pdf}
}

@article{chan_causal_2022,
  title = {Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]},
  shorttitle = {Causal Scrubbing},
  author = {Chan, Lawrence and {Garriga-alonso}, Adri{\`a} and {Goldowsky-Dill}, Nicholas and {ryan\_greenblatt} and {jenny} and Radhakrishnan, Ansh and Buck and Thomas, Nate},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing},
  urldate = {2023-08-27},
  abstract = {* Authors sorted alphabetically. {$\bullet$}  Summary: This post introduces causal scrubbing, a principled approach for evaluating the quality of mechanistic i{\dots}},
  language = {en},
  keywords = {cited,mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/ARPGBRE6/Chan et al. - 2022 - Causal Scrubbing a method for rigorously testing .html}
}

@article{chan_comparative_2022,
  title = {A Comparative Study of Faithfulness Metrics for Model Interpretability Methods},
  author = {Chan, Chun Sik and Kong, Huanqi and Liang, Guanqing},
  year = {2022},
  month = apr,
  journal = {ACL},
  eprint = {2204.05514},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2204.05514},
  url = {http://arxiv.org/abs/2204.05514},
  urldate = {2023-11-16},
  abstract = {Interpretation methods to reveal the internal reasoning processes behind machine learning models have attracted increasing attention in recent years. To quantify the extent to which the identified interpretations truly reflect the intrinsic decision-making mechanisms, various faithfulness evaluation metrics have been proposed. However, we find that different faithfulness metrics show conflicting preferences when comparing different interpretations. Motivated by this observation, we aim to conduct a comprehensive and comparative study of the widely adopted faithfulness metrics. In particular, we introduce two assessment dimensions, namely diagnosticity and time complexity. Diagnosticity refers to the degree to which the faithfulness metric favours relatively faithful interpretations over randomly generated ones, and time complexity is measured by the average number of model forward passes. According to the experimental results, we find that sufficiency and comprehensiveness metrics have higher diagnosticity and lower time complexity than the other faithfulness metric},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9CBJJAGJ/Chan et al. - 2022 - A Comparative Study of Faithfulness Metrics for Mo.pdf}
}

@article{chan_data_2022,
  title = {Data Distributional Properties Drive Emergent In-Context Learning in Transformers},
  author = {Chan, Stephanie C. Y. and Santoro, Adam and Lampinen, Andrew K. and Wang, Jane X. and Singh, Aaditya and Richemond, Pierre H. and McClelland, Jay and Hill, Felix},
  year = {2022},
  month = apr,
  journal = {NeurIPS},
  url = {https://arxiv.org/abs/2205.05055v6},
  urldate = {2024-01-02},
  abstract = {Large transformer-based models are able to perform in-context few-shot learning, without being explicitly trained for it. This observation raises the question: what aspects of the training regime lead to this emergent behavior? Here, we show that this behavior is driven by the distributions of the training data itself. In-context learning emerges when the training data exhibits particular distributional properties such as burstiness (items appear in clusters rather than being uniformly distributed over time) and having large numbers of rarely occurring classes. In-context learning also emerges more strongly when item meanings or interpretations are dynamic rather than fixed. These properties are exemplified by natural language, but are also inherent to naturalistic data in a wide range of other domains. They also depart significantly from the uniform, i.i.d. training distributions typically used for standard supervised learning. In our initial experiments, we found that in-context learning traded off against more conventional weight-based learning, and models were unable to achieve both simultaneously. However, our later experiments uncovered that the two modes of learning could co-exist in a single model when it was trained on data following a skewed Zipfian distribution -- another common property of naturalistic data, including language. In further experiments, we found that naturalistic data distributions were only able to elicit in-context learning in transformers, and not in recurrent models. In sum, our findings indicate how the transformer architecture works together with particular properties of the training data to drive the intriguing emergent in-context learning behaviour of large language models, and how future work might encourage both in-context and in-weights learning in domains beyond language.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/I34VWRLW/Chan et al. - 2022 - Data Distributional Properties Drive Emergent In-C.pdf}
}

@article{chan_natural_2023,
  title = {Natural Abstractions: Key claims, Theorems, and Critiques},
  shorttitle = {Natural Abstractions},
  author = {Chan, Lawrence and Lang, Leon and Jenner, Erik},
  year = {2023},
  month = mar,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1},
  urldate = {2024-01-17},
  abstract = {TL;DR:~We distill John Wentworth's Natural Abstractions agenda by summarizing its key claims: the Natural Abstraction Hypothesis---many cognitive syste{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/LXBWUP43/Chan et al. - 2023 - Natural Abstractions Key claims, Theorems, and Cr.html}
}

@article{chan_what_2023,
  title = {What I would do if I wasn't at ARC Evals},
  author = {Chan, Lawrence},
  year = {2023},
  month = may,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/6FkWnktH3mjMAxdRT/what-i-would-do-if-i-wasn-t-at-arc-evals},
  urldate = {2023-12-01},
  abstract = {In which:~I list 9 projects that I would work on if I wasn't busy working on safety standards at ARC Evals, and explain why they might be good to wor{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/LFF2QXR7/Chan - What I would do if I wasnt at ARC Evals.html}
}

@article{chanin_identifying_2023,
  title = {Identifying Linear Relational Concepts in Large Language Models},
  author = {Chanin, David and Hunter, Anthony and Camburu, Oana-Maria},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2311.08968},
  url = {https://arxiv.org/abs/2311.08968},
  urldate = {2024-02-11},
  abstract = {Transformer language models (LMs) have been shown to represent concepts as directions in the latent space of hidden activations. However, for any given human-interpretable concept, how can we find its direction in the latent space? We present a technique called linear relational concepts (LRC) for finding concept directions corresponding to human-interpretable concepts at a given hidden layer in a transformer LM by first modeling the relation between subject and object as a linear relational embedding (LRE). While the LRE work was mainly presented as an exercise in understanding model representations, we find that inverting the LRE while using earlier object layers results in a powerful technique to find concept directions that both work well as a classifier and causally influence model outputs.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,linearity,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/LW9N2JRJ/Chanin et al. - 2023 - Identifying Linear Relational Concepts in Large La.pdf}
}

@article{charbel-raphael_almost_2023,
  title = {Against Almost Every Theory of Impact of Interpretability},
  author = {{Charbel-Rapha{\"e}l}},
  year = {2023},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1},
  urldate = {2023-11-22},
  abstract = {Many thanks to @scasper, @Sid Black , @Neel Nanda , @Fabien Roger , @Bogdan Ionut Cirstea, @WCargo, @Alexandre Variengien, @Jonathan Claybrough, @Edo{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/JJ3N2W69/Charbel-Raphal - 2023 - Against Almost Every Theory of Impact of Interpret.html}
}

@article{charton_can_2024,
  title = {Can transformers learn the greatest common divisor?},
  author = {Charton, Fran{\c c}ois},
  year = {2024},
  journal = {ICLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2308.15594},
  url = {https://arxiv.org/abs/2308.15594},
  urldate = {2024-02-11},
  abstract = {I investigate the capability of small transformers to compute the greatest common divisor (GCD) of two positive integers. When the training distribution and the representation base are carefully chosen, models achieve 98\% accuracy and correctly predict 91 of the 100 first GCD. Model predictions are deterministic and fully interpretable. During training, the models learn to cluster input pairs with the same GCD, and classify them by their divisors. Basic models, trained from uniform operands encoded on small bases, only compute a handful of GCD (up to 38 out of 100): the products of divisors of the base. Longer training and larger bases allow some models to "grok" small prime GCD. Training from log-uniform operands boosts performance to 73 correct GCD, and balancing the training distribution of GCD, from inverse square to log-uniform, to 91 GCD. Training models from a uniform distribution of GCD breaks the deterministic model behavior.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/J4X9CM4W/Charton - 2024 - Can transformers learn the greatest common divisor.pdf}
}

@article{chen_dynamical_2023,
  title = {Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition},
  author = {Chen, Zhongtian and Lau, Edmund and Mendel, Jake and Wei, Susan and Murfet, Daniel},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.06301},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.06301},
  url = {http://arxiv.org/abs/2310.06301},
  urldate = {2023-11-09},
  abstract = {We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular \$k\$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these \$k\$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same \$k\$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite},
  file = {/Users/leonardbereska/Zotero/storage/QR6DQFZT/Chen et al. - 2023 - Dynamical versus Bayesian Phase Transitions in a T.pdf}
}

@article{chen_going_2023,
  title = {Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory},
  shorttitle = {Going Beyond Neural Network Feature Similarity},
  author = {Chen, Yiting and Zhou, Zhanpeng and Yan, Junchi},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2310.06756},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.06756},
  url = {http://arxiv.org/abs/2310.06756},
  urldate = {2024-01-24},
  abstract = {The behavior of neural networks still remains opaque, and a recently widely noted phenomenon is that networks often achieve similar performance when initialized with different random parameters. This phenomenon has attracted significant attention in measuring the similarity between features learned by distinct networks. However, feature similarity could be vague in describing the same feature since equivalent features hardly exist. In this paper, we expand the concept of equivalent feature and provide the definition of what we call functionally equivalent features. These features produce equivalent output under certain transformations. Using this definition, we aim to derive a more intrinsic metric for the so-called feature complexity regarding the redundancy of features learned by a neural network at each layer. We offer a formal interpretation of our approach through the lens of category theory, a well-developed area in mathematics. To quantify the feature complexity, we further propose an efficient algorithm named Iterative Feature Merging. Our experimental results validate our ideas and theories from various perspectives. We empirically demonstrate that the functionally equivalence widely exists among different features learned by the same neural network and we could reduce the number of parameters of the network without affecting the performance.The IFM shows great potential as a data-agnostic model prune method. We have also drawn several interesting empirical findings regarding the defined feature complexity.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/C4R8UWED/Chen et al. - 2023 - Going Beyond Neural Network Feature Similarity Th.pdf}
}

@article{chen_infogan_2016,
  title = {InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
  shorttitle = {InfoGAN},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  month = jun,
  journal = {NeurIPS},
  eprint = {1606.03657},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1606.03657},
  url = {http://arxiv.org/abs/1606.03657},
  urldate = {2024-03-20},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/2LMBFYGT/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf}
}

@article{chen_llms_2024,
  title = {INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection},
  shorttitle = {INSIDE},
  author = {Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
  year = {2024},
  month = feb,
  journal = {ICLR},
  eprint = {2402.03744},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.03744},
  url = {http://arxiv.org/abs/2402.03744},
  urldate = {2024-02-07},
  abstract = {Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' {\textbackslash}textbf\{IN\}ternal {\textbackslash}textbf\{S\}tates for halluc{\textbackslash}textbf\{I\}nation {\textbackslash}textbf\{DE\}tection ({\textbackslash}textbf\{INSIDE\}). In particular, a simple yet effective {\textbackslash}textbf\{EigenScore\} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident generations and potentially benefits the detection of overconfident hallucinations. Extensive experiments and ablation studies are performed on several popular LLMs and question-answering (QA) benchmarks, showing the effectiveness of our proposal.},
  archiveprefix = {arxiv},
  keywords = {hallucination,LLMs,not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZLHIDSAJ/Chen et al. - 2024 - INSIDE LLMs' Internal States Retain the Power of .pdf}
}

@article{chen_sudden_2023,
  title = {Sudden drops in the loss: Syntax acquisition, phase transitions, and simplicity bias in MLMs},
  author = {Chen, Angelica and {Schwartz-Ziv}, Ravid and Cho, Kyunghyun and Leavitt, Matthew L. and Saphra, Naomi},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2309.07311},
  pages = {null},
  doi = {10.48550/arXiv.2309.07311},
  url = {https://www.semanticscholar.org/paper/c413a339d7784574ed43debea494ef405ee09d81},
  abstract = {Most interpretability research in NLP focuses on understanding the behavior and features of a fully trained model. However, certain insights into model behavior may only be accessible by observing the trajectory of the training process. We present a case study of syntax acquisition in masked language models (MLMs) that demonstrates how analyzing the evolution of interpretable artifacts throughout training deepens our understanding of emergent behavior. In particular, we study Syntactic Attention Structure (SAS), a naturally emerging property of MLMs wherein specific Transformer heads tend to focus on specific syntactic relations. We identify a brief window in pretraining when models abruptly acquire SAS, concurrent with a steep drop in loss. This breakthrough precipitates the subsequent acquisition of linguistic capabilities. We then examine the causal role of SAS by manipulating SAS during training, and demonstrate that SAS is necessary for the development of grammatical capabilities. We further find that SAS competes with other beneficial traits during training, and that briefly suppressing SAS improves model quality. These findings offer an interpretation of a real-world example of both simplicity bias and breakthrough training dynamics.},
  arxivid = {2309.07311},
  keywords = {developmental,not cited},
  file = {/Users/leonardbereska/Zotero/storage/9NTFHM7D/Chen et al. - 2023 - Sudden drops in the loss Syntax acquisition, phas.pdf}
}

@article{cheng_binding_2022,
  title = {Binding Language Models in Symbolic Languages},
  author = {Cheng, Zhoujun and Xie, Tianbao and Shi, Peng and Li, Chengzu and Nadkarni, Rahul and Hu, Yushi and Xiong, Caiming and Radev, Dragomir and Ostendorf, Mari and Zettlemoyer, Luke and Smith, Noah A. and Yu, Tao},
  year = {2023},
  journal = {ICLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.02875},
  url = {https://arxiv.org/abs/2210.02875},
  urldate = {2024-02-11},
  abstract = {Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {LLMs,neurosymbolic,not cited},
  file = {/Users/leonardbereska/Zotero/storage/B3YUXPFG/Cheng et al. - 2023 - Binding Language Models in Symbolic Languages.pdf}
}

@article{cheung_superposition_2019,
  title = {Superposition of many models into one},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = {2019},
  month = jun,
  journal = {CoRR},
  eprint = {1902.05522},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1902.05522},
  url = {http://arxiv.org/abs/1902.05522},
  urldate = {2024-03-18},
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/RGX94VEV/Cheung et al. - 2019 - Superposition of many models into one.pdf}
}

@article{chiang_overcoming_2022,
  title = {Overcoming a Theoretical Limitation of Self-Attention},
  author = {Chiang, David and Cholak, Peter},
  year = {2022},
  month = feb,
  journal = {ACL},
  eprint = {2202.12172},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2202.12172},
  url = {http://arxiv.org/abs/2202.12172},
  urldate = {2024-02-26},
  abstract = {Although transformers are remarkably effective for many tasks, there are some surprisingly easy-looking regular languages that they struggle with. Hahn shows that for languages where acceptance depends on a single input symbol, a transformer's classification decisions become less and less confident (that is, with cross-entropy approaching 1 bit per string) as input strings get longer and longer. We examine this limitation using two languages: PARITY, the language of bit strings with an odd number of 1s, and FIRST, the language of bit strings starting with a 1. We demonstrate three ways of overcoming the limitation suggested by Hahn's lemma. First, we settle an open question by constructing a transformer that recognizes PARITY with perfect accuracy, and similarly for FIRST. Second, we use layer normalization to bring the cross-entropy of both models arbitrarily close to zero. Third, when transformers need to focus on a single position, as for FIRST, we find that they can fail to generalize to longer strings; we offer a simple remedy to this problem that also improves length generalization in machine translation.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/AK8TQGNG/Chiang and Cholak - 2022 - Overcoming a Theoretical Limitation of Self-Attent.pdf}
}

@article{christiano_deep_2017,
  title = {Deep reinforcement learning from human preferences},
  author = {Christiano, Paul and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  year = {2017},
  month = dec,
  journal = {NeurIPS},
  eprint = {1706.03741},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1706.03741},
  url = {http://arxiv.org/abs/1706.03741},
  urldate = {2023-08-26},
  abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/8Z7LCT64/Christiano et al. - 2017 - Deep reinforcement learning from human preferences.pdf}
}

@misc{christiano_eliciting_2021,
  title = {Eliciting Latent Knowledge},
  author = {Christiano, Paul and Cotra, Ajeya and Xu, Mark},
  year = {2021},
  month = jan,
  publisher = {Google Docs},
  url = {https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?usp=sharing&usp=embed_facebook},
  urldate = {2024-01-23},
  abstract = {In this post, we'll present ARC's approach to an open problem we think is central to aligning powerful machine learning (ML) systems:  Suppose we train a model to predict what the future will look like according to cameras and other sensors. We then use planning algorithms to find a sequence of actions that lead to predicted futures that look good to us. But some action sequences could tamper with the cameras so they show happy          humans regardless of what's really happening. More generally, some futures look great on camera but are actually catastrophically bad. In these cases, the prediction model "knows" facts (like "the camera was tampered with") that are not visible on camera but would change our evaluation of the predicted future if we learned them. How can we train this model to report its latent knowledge of off-screen events? We'll call this problem eliciting latent knowledge (ELK). In this report we'll focus on detecting sensor tampering as a motivating example, but we believe ELK is central to many aspects of alignment.  In this report, we will describe ELK and suggest possible approaches to it, while using the discussion to illustrate ARC's research methodology. More specifically, we will: Set up a toy scenario in which a prediction model could show us a future that looks good but is actually bad, and explain why ELK could address this problem (more). Describe a simple baseline training strategy for ELK, step through how we analyze this kind of strategy, and ultimately conclude that the baseline is insufficient (more). Lay out ARC's overall research methodology --- playing a game between a ``builder'' who is trying to come up with a good training strategy and a ``breaker'' who is trying to construct a counterexample where the strategy works poorly (more). Describe a sequence of strategies for constructing richer datasets and arguments that none of these modifications solve ELK, leading to the counterexample of ontology identification (more). Identify ontology identification as a crucial sub-problem of ELK and discuss its relationship to the rest of ELK (more). Describe a sequence of strategies for regularizing models to give honest answers, and arguments that these modifications are still insufficient (more).  Conclude with a discussion of why we are excited about trying to solve ELK in the worst case, including why it seems central to the larger alignment problem and why we're optimistic about making progress (more). Much of our current research focused on ``ontology identification'' as a challenge for ELK. In the last 10 years many researchers have called out similar problems as playing a central role in alignment; our main contributions are to provide a more precise discussion of the problem, possible approaches, and why it appears to be challenging. We discuss related work in more detail in Appendix: related work. We believe that there are many promising and unexplored approaches to this problem, and there isn't yet much reason to believe we are stuck or are faced with an insurmountable obstacle. Even some of the simplest approaches have not been thoroughly explored, and seem like they would play a role in a practical attempt at scalable alignment today. Given that ELK appears to represent a core difficulty for alignment, we are very excited about research that tries to attack it head on; we're optimistic that within a year (by end of 2022) we will have made significant progress either towards a solution or towards a clear sense of why the problem is hard.},
  language = {en},
  keywords = {cited}
}

@article{christiano_supervising_2018,
  title = {Supervising strong learners by amplifying weak experts},
  author = {Christiano, Paul and Shlegeris, Buck and Amodei, Dario},
  year = {2018},
  month = oct,
  journal = {CoRR},
  eprint = {1810.08575},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1810.08575},
  url = {http://arxiv.org/abs/1810.08575},
  urldate = {2023-03-20},
  abstract = {Many real world learning tasks involve complex or hard-to-specify objectives, and using an easier-to-specify proxy can lead to poor performance or misaligned behavior. One solution is to have humans provide a training signal by demonstrating or judging performance, but this approach fails if the task is too complicated for a human to directly evaluate. We propose Iterated Amplification, an alternative training strategy which progressively builds up a training signal for difficult problems by combining solutions to easier subproblems. Iterated Amplification is closely related to Expert Iteration (Anthony et al., 2017; Silver et al., 2017), except that it uses no external reward function. We present results in algorithmic environments, showing that Iterated Amplification can efficiently learn complex behaviors.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/82UGWK8D/Christiano et al. - 2018 - Supervising strong learners by amplifying weak exp.pdf}
}

@article{christiano_thoughts_,
  title = {Thoughts on the impact of RLHF research},
  author = {Christiano, Paul},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/vwu4kegAEZTBtpT6p/thoughts-on-the-impact-of-rlhf-research},
  urldate = {2023-05-08},
  abstract = {In this post I'm going to describe my basic justification for working on RLHF in 2017-2020, which I still stand behind. I'll discuss various arguments that RLHF research had an overall negative impac{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/L9NJNHQR/Christiano - 2023 - Thoughts on the impact of RLHF research.html}
}

@article{christoffersen_get_2023,
  title = {Get It in Writing: Formal Contracts Mitigate Social Dilemmas in Multi-Agent RL},
  shorttitle = {Get It in Writing},
  author = {Christoffersen, Phillip J. K. and Haupt, Andreas A. and {Hadfield-Menell}, Dylan},
  year = {2023},
  month = aug,
  journal = {AAMAS},
  eprint = {2208.10469},
  primaryclass = {cs, econ},
  doi = {10.5555/3545946.3598670},
  url = {http://arxiv.org/abs/2208.10469},
  urldate = {2023-08-26},
  abstract = {Multi-agent reinforcement learning (MARL) is a powerful tool for training automated systems acting independently in a common environment. However, it can lead to sub-optimal behavior when individual incentives and group incentives diverge. Humans are remarkably capable at solving these social dilemmas. It is an open problem in MARL to replicate such cooperative behaviors in selfish agents. In this work, we draw upon the idea of formal contracting from economics to overcome diverging incentives between agents in MARL. We propose an augmentation to a Markov game where agents voluntarily agree to binding state-dependent transfers of reward, under pre-specified conditions. Our contributions are theoretical and empirical. First, we show that this augmentation makes all subgame-perfect equilibria of all fully observed Markov games exhibit socially optimal behavior, given a sufficiently rich space of contracts. Next, we complement our game-theoretic analysis by showing that state-of-the-art RL algorithms learn socially optimal policies given our augmentation. Our experiments include classic static dilemmas like Stag Hunt, Prisoner's Dilemma and a public goods game, as well as dynamic interactions that simulate traffic, pollution management and common pool resource management.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/MM5J56A8/Christoffersen et al. - 2023 - Get It in Writing Formal Contracts Mitigate Socia.pdf}
}

@article{chughtai_summing_2024,
  title = {Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs},
  shorttitle = {Summing Up the Facts},
  author = {Chughtai, Bilal and Cooney, Alan and Nanda, Neel},
  year = {2024},
  journal = {NeurIPS Workshop Attributing Model Behaviour at Scale},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2402.07321},
  url = {https://arxiv.org/abs/2402.07321},
  urldate = {2024-03-13},
  abstract = {How do transformer-based large language models (LLMs) store and retrieve knowledge? We focus on the most basic form of this task -- factual recall, where the model is tasked with explicitly surfacing stored facts in prompts of form `Fact: The Colosseum is in the country of'. We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute. We term this generic phenomena the additive motif: models compute through summing up multiple independent contributions. Each mechanism's contribution may be insufficient alone, but summing results in constructive interfere on the correct answer. In addition, we extend the method of direct logit attribution to attribute an attention head's output to individual source tokens. We use this technique to unpack what we call `mixed heads' -- which are themselves a pair of two separate additive updates from different source tokens.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/leonardbereska/Zotero/storage/8BCYKFX2/Chughtai et al. - 2024 - Summing Up the Facts Additive Mechanisms Behind F.pdf}
}

@article{chughtai_toy_2023,
  title = {A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations},
  shorttitle = {A Toy Model of Universality},
  author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  year = {2023},
  journal = {ICML},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.03025},
  url = {https://arxiv.org/abs/2302.03025},
  urldate = {2023-10-27},
  abstract = {Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {algorithms,cited,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,toy models,universality},
  file = {/Users/leonardbereska/Zotero/storage/EKPFCB9B/Chughtai et al. - 2023 - A Toy Model of Universality Reverse Engineering H.pdf}
}

@article{clark_what_2019,
  title = {What Does BERT Look at? An Analysis of BERT's Attention},
  shorttitle = {What Does BERT Look at?},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  editor = {Linzen, Tal and Chrupa{\l}a, Grzegorz and Belinkov, Yonatan and Hupkes, Dieuwke},
  year = {2019},
  month = aug,
  journal = {BlackboxNLP},
  pages = {276--286},
  doi = {10.18653/v1/W19-4828},
  url = {https://aclanthology.org/W19-4828},
  urldate = {2024-03-20},
  abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
  file = {/Users/leonardbereska/Zotero/storage/AJAA5Z5A/Clark et al. - 2019 - What Does BERT Look at An Analysis of BERT's Atte.pdf}
}

@article{cohen_crawling_2023,
  title = {Crawling the Internal Knowledge-Base of Language Models},
  author = {Cohen, Roi and Geva, Mor and Berant, Jonathan and Globerson, Amir},
  year = {2023},
  journal = {EACL},
  doi = {10.48550/ARXIV.2301.12810},
  url = {https://arxiv.org/abs/2301.12810},
  urldate = {2024-02-11},
  abstract = {Language models are trained on large volumes of text, and as a result their parameters might contain a significant body of factual knowledge. Any downstream task performed by these models implicitly builds on these facts, and thus it is highly desirable to have means for representing this body of knowledge in an interpretable way. However, there is currently no mechanism for such a representation. Here, we propose to address this goal by extracting a knowledge-graph of facts from a given language model. We describe a procedure for ``crawling'' the internal knowledge-base of a language model. Specifically, given a seed entity, we expand a knowledge-graph around it. The crawling procedure is decomposed into sub-tasks, realized through specially designed prompts that control for both precision (i.e., that no wrong facts are generated) and recall (i.e., the number of facts generated). We evaluate our approach on graphs crawled starting from dozens of seed entities, and show it yields high precision graphs (82-92\%), while emitting a reasonable number of facts per entity.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {knowledge base,LLMs,not cited,uncertainty quantification},
  file = {/Users/leonardbereska/Zotero/storage/9Z76T4WH/Cohen et al. - 2023 - Crawling the Internal Knowledge-Base of Language M.pdf}
}

@article{cohen_evaluating_2023,
  title = {Evaluating the Ripple Effects of Knowledge Editing in Language Models},
  author = {Cohen, Roi and Biran, Eden and Yoran, Ori and Globerson, Amir and Geva, Mor},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.12976},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.12976},
  url = {http://arxiv.org/abs/2307.12976},
  urldate = {2023-10-30},
  abstract = {Modern language models capture a large body of factual knowledge. However, some facts can be incorrectly induced or become obsolete over time, resulting in factually incorrect generations. This has led to the development of various editing methods that allow updating facts encoded by the model. Evaluation of these methods has primarily focused on testing whether an individual fact has been successfully injected, and if similar predictions for other subjects have not changed. Here we argue that such evaluation is limited, since injecting one fact (e.g. ``Jack Depp is the son of Johnny Depp'') introduces a ``ripple effect'' in the form of additional facts that the model needs to update (e.g.``Jack Depp is the sibling of Lily-Rose Depp''). To address this issue, we propose a novel set of evaluation criteria that consider the implications of an edit on related facts. Using these criteria, we then construct {\textbackslash}ripple\{\}, a diagnostic benchmark of 5K factual edits, capturing a variety of types of ripple effects. We evaluate prominent editing methods on {\textbackslash}ripple\{\}, showing that current methods fail to introduce consistent changes in the model's knowledge. In addition, we find that a simple in-context editing baseline obtains the best scores on our benchmark, suggesting a promising research direction for model editing.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KGHIQK7M/Cohen et al. - 2023 - Evaluating the Ripple Effects of Knowledge Editing.pdf}
}

@article{collin_how_2022,
  title = {How "Discovering Latent Knowledge in Language Models Without Supervision" Fits Into a Broader Alignment Scheme},
  author = {Collin},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without},
  urldate = {2024-02-12},
  abstract = {Introduction A few collaborators and I recently released a new paper:~Discovering Latent Knowledge in Language Models Without Supervision. For a quic{\dots}},
  language = {en},
  keywords = {not cited,probing,relevance,to cite,unsupervised},
  file = {/Users/leonardbereska/Zotero/storage/DP3I8QGX/Collin - 2022 - How Discovering Latent Knowledge in Language Mode.html}
}

@article{colognese_high-level_2023,
  title = {High-level interpretability: detecting an AI's objectives},
  shorttitle = {High-level interpretability},
  author = {Colognese, Paul and Jozdien},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/tFYGdq9ivjA3rdaS2/high-level-interpretability-detecting-an-ai-s-objectives},
  urldate = {2024-02-12},
  abstract = {Thanks to Monte MacDiarmid (for discussions, feedback, and experiment infrastructure) and to the Shard Theory team for their prior work and explorato{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/WLZ93Y76/Colognese and Jozdien - 2023 - High-level interpretability detecting an AI's obj.html}
}

@article{colognese_internal_2023,
  title = {Internal Target Information for AI Oversight},
  author = {Colognese, Paul},
  year = {2023},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/hhKpXEsfAiyFLecyF/internal-target-information-for-ai-oversight},
  urldate = {2024-02-12},
  abstract = {Thanks to Arun Jose for discussions and feedback. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5I7U2LAL/Colognese - 2023 - Internal Target Information for AI Oversight.html}
}

@article{conmy_automated_2023,
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author = {Conmy, Arthur and {Mavor-Parker}, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and {Garriga-Alonso}, Adri{\`a}},
  year = {2023},
  journal = {NeurIPS},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.14997},
  url = {https://arxiv.org/abs/2304.14997},
  urldate = {2023-07-31},
  abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {algorithms,automate,behavior,cited,graph,mechinterp,scale,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/4NTYQMGA/Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf}
}

@article{conmy_my_2023,
  title = {My best guess at the important tricks for training 1L SAEs},
  author = {Conmy, Arthur},
  year = {2023},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes},
  urldate = {2024-02-16},
  abstract = {TL;DR: this quickly-written post gives a list of my guesses of the most important parts of training a~Sparse Autoencoder on a~1L Transformer, with op{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/PQ22LNUW/Conmy - 2023 - My best guess at the important tricks for training.html}
}

@article{conwell_what_2022,
  title = {What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?},
  author = {Conwell, Colin and Prince, Jacob S. and Kay, Kendrick N. and Alvarez, George A. and Konkle, Talia},
  year = {2023},
  month = jan,
  journal = {bioRxiv},
  doi = {10.1101/2022.03.28.485868},
  url = {http://biorxiv.org/lookup/doi/10.1101/2022.03.28.485868},
  urldate = {2023-12-04},
  abstract = {The rapid development and open-source release of highly performant computer vision models offers new potential for examining how different inductive biases impact representation learning and emergent alignment with the high-level human ventral visual system. Here, we assess a diverse set of 224 models, curated to enable controlled comparison of different model properties, testing their brain predictivity using large-scale functional magnetic resonance imaging data. We find that models with qualitatively different architectures (e.g. CNNs versus Transformers) and markedly different task objectives (e.g. purely visual contrastive learning versus vision-language alignment) achieve near equivalent degrees of brain predictivity, when other factors are held constant. Instead, variation across model visual training diets yields the largest, most consistent effect on emergent brain predictivity. Overarching model properties commonly suspected to increase brain predictivity (e.g. greater effective dimensionality; learnable parameter count) were not robust indicators across this more extensive survey. We highlight that standard model-to-brain linear re-weighting methods may be too flexible, as most performant models have very similar brain-predictivity scores, despite significant variation in their underlying representations. Broadly, our findings point to the importance of visual diet, challenge common assumptions about the methods used to link models to brains, and more concretely outline future directions for leveraging the full diversity of existing open-source models as tools to probe the common computational principles underlying biological and artificial visual systems.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PBYXJPHI/Conwell et al. - 2023 - What can 1.8 billion regressions tell us about the.pdf}
}

@article{correia_adaptively_2019,
  title = {Adaptively Sparse Transformers},
  author = {Correia, Gon{\c c}alo M. and Niculae, Vlad and Martins, Andr{\'e} F. T.},
  year = {2019},
  month = sep,
  journal = {EMNLP},
  eprint = {1909.00015},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1909.00015},
  url = {http://arxiv.org/abs/1909.00015},
  urldate = {2024-03-20},
  abstract = {Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with \${\textbackslash}alpha\$-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the \${\textbackslash}alpha\$ parameter -- which controls the shape and sparsity of \${\textbackslash}alpha\$-entmax -- allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/7BHDK8KE/Correia et al. - 2019 - Adaptively Sparse Transformers.pdf}
}

@misc{cotra_forecasting_2020,
  title = {Forecasting TAI with Biological Anchors},
  author = {Cotra, Ajeya},
  year = {2020},
  publisher = {Google Docs},
  url = {https://docs.google.com/document/d/1IJ6Sr-gPeXdSJugFulwIpvavc0atjHGM82QjIfUSBGQ/edit},
  keywords = {not cited}
}

@article{cotra_specific_2022,
  title = {Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover},
  author = {Cotra, Ajeya},
  year = {2022},
  month = jul,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/pRkFkzwKZ2zfa3R6H/without-specific-countermeasures-the-easiest-path-to},
  urldate = {2023-05-15},
  abstract = {I think that~in the coming 15-30 years, the world could plausibly develop ``transformative AI'': AI powerful enough to bring us into a new, qualitatively different future, via~an explosion in science a{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/3RX4S95Q/Cotra - 2022 - Without specific countermeasures, the easiest path.html}
}

@article{covert_explaining_2021,
  title = {Explaining by removing: a unified framework for model explanation},
  shorttitle = {Explaining by removing},
  author = {Covert, Ian C. and Lundberg, Scott and Lee, Su-In},
  year = {2021},
  month = jan,
  journal = {J. Mach. Learn. Res.},
  volume = {22},
  number = {1},
  pages = {209:9477--209:9566},
  issn = {1532-4435},
  url = {https://arxiv.org/abs/2011.14878},
  abstract = {Researchers have proposed a wide variety of model explanation approaches, but it remains unclear how most methods are related or when one method is preferable to another. We describe a new unified class of methods, removal-based explanations, that are based on the principle of simulating feature removal to quantify each feature's inuence. These methods vary in several respects, so we develop a framework that characterizes each method along three dimensions: 1) how the method removes features, 2) what model behavior the method explains, and 3) how the method summarizes each feature's inuence. Our framework unifies 26 existing methods, including several of the most widely used approaches: SHAP, LIME, Meaningful Perturbations, and permutation tests. This newly understood class of explanation methods has rich connections that we examine using tools that have been largely overlooked by the explainability literature. To anchor removal-based explanations in cognitive psychology, we show that feature removal is a simple application of subtractive counterfactual reasoning. Ideas from cooperative game theory shed light on the relationships and trade-offs among different methods, and we derive conditions under which all removal-based explanations have information-theoretic interpretations. Through this analysis, we develop a unified framework that helps practitioners better understand model explanation tools, and that offers a strong theoretical foundation upon which future explainability research can build.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/58LLQN7W/Covert et al. - 2021 - Explaining by removing a unified framework for mo.pdf}
}

@article{crabbe_evaluating_2023,
  title = {Evaluating the Robustness of Interpretability Methods through Explanation Invariance and Equivariance},
  author = {Crabb{\'e}, Jonathan and {van der Schaar}, Mihaela},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2304.06715},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.06715},
  url = {http://arxiv.org/abs/2304.06715},
  urldate = {2023-11-01},
  abstract = {Interpretability methods are valuable only if their explanations faithfully describe the explained model. In this work, we consider neural networks whose predictions are invariant under a specific symmetry group. This includes popular architectures, ranging from convolutional to graph neural networks. Any explanation that faithfully explains this type of model needs to be in agreement with this invariance property. We formalize this intuition through the notion of explanation invariance and equivariance by leveraging the formalism from geometric deep learning. Through this rigorous formalism, we derive (1) two metrics to measure the robustness of any interpretability method with respect to the model symmetry group; (2) theoretical robustness guarantees for some popular interpretability methods and (3) a systematic approach to increase the invariance of any interpretability method with respect to a symmetry group. By empirically measuring our metrics for explanations of models associated with various modalities and symmetry groups, we derive a set of 5 guidelines to allow users and developers of interpretability methods to produce robust explanations.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/XKW4I7NC/Crabb and van der Schaar - 2023 - Evaluating the Robustness of Interpretability Meth.pdf}
}

@article{cranmer_discovering_2020,
  title = {Discovering Symbolic Models from Deep Learning with Inductive Biases},
  author = {Cranmer, M. and {Sanchez-Gonzalez}, Alvaro and Battaglia, P. and Xu, Rui and Cranmer, Kyle and Spergel, D. and Ho, S.},
  year = {2020},
  month = jun,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Discovering-Symbolic-Models-from-Deep-Learning-with-Cranmer-Sanchez-Gonzalez/643ac3ef063c77eb02a3d52637c11fe028bfae28},
  urldate = {2024-02-11},
  abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
  keywords = {not cited,physics,symbolic},
  file = {/Users/leonardbereska/Zotero/storage/G2KWMUME/Cranmer et al. - 2020 - Discovering Symbolic Models from Deep Learning wit.pdf}
}

@article{csordas_are_2021,
  title = {Are Neural Nets Modular? Inspecting Functional Modularity Through Differentiable Weight Masks},
  shorttitle = {Are Neural Nets Modular?},
  author = {Csord{\'a}s, R{\'o}bert and {van Steenkiste}, Sjoerd and Schmidhuber, J{\"u}rgen},
  year = {2021},
  month = mar,
  journal = {CoRR},
  eprint = {2010.02066},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2010.02066},
  url = {http://arxiv.org/abs/2010.02066},
  urldate = {2024-03-19},
  abstract = {Neural networks (NNs) whose subnetworks implement reusable functions are expected to offer numerous advantages, including compositionality through efficient recombination of functional building blocks, interpretability, preventing catastrophic interference, etc. Understanding if and how NNs are modular could provide insights into how to improve them. Current inspection methods, however, fail to link modules to their functionality. In this paper, we present a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. Using this powerful tool, we contribute an extensive study of emerging modularity in NNs that covers several standard architectures and datasets. We demonstrate how common NNs fail to reuse submodules and offer new insights into the related issue of systematic generalization on language tasks.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/4DFJ2I2L/Csords et al. - 2021 - Are Neural Nets Modular Inspecting Functional Mod.pdf}
}

@article{cui_phase_2024,
  title = {A phase transition between positional and semantic learning in a solvable model of dot-product attention},
  author = {Cui, Hugo and Behrens, Freya and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.03902},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.03902},
  url = {http://arxiv.org/abs/2402.03902},
  urldate = {2024-02-14},
  abstract = {We investigate how a dot-product attention layer learns a positional attention matrix (with tokens attending to each other based on their respective positions) and a semantic attention matrix (with tokens attending to each other based on their meaning). For an algorithmic task, we experimentally show how the same simple architecture can learn to implement a solution using either the positional or semantic mechanism. On the theoretical side, we study the learning of a non-linear self-attention layer with trainable tied and low-rank query and key matrices. In the asymptotic limit of high-dimensional data and a comparably large number of training samples, we provide a closed-form characterization of the global minimum of the non-convex empirical loss landscape. We show that this minimum corresponds to either a positional or a semantic mechanism and evidence an emergent phase transition from the former to the latter with increasing sample complexity. Finally, we compare the dot-product attention layer to linear positional baseline, and show that it outperforms the latter using the semantic mechanism provided it has access to sufficient data.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LMLT2JCF/Cui et al. - 2024 - A phase transition between positional and semantic.pdf}
}

@misc{cunningham_sae_2024,
  title = {SAE Training Guide v1},
  author = {Cunningham, Hoagy},
  year = {2024},
  url = {https://docs.google.com/document/d/18bxKmrBN4rhhY6vwhpYDf5XC4bHt0eRy3Le9Aco2-1Y/edit?usp=embed_facebook},
  urldate = {2024-02-16},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/U6MKIDEI/Cunningham - 2024 - SAE Training Guide v1.html}
}

@article{cunningham_sparse_2024,
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = {2024},
  month = jan,
  journal = {ICLR},
  eprint = {2309.08600},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.08600},
  url = {http://arxiv.org/abs/2309.08600},
  urldate = {2023-10-30},
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
  archiveprefix = {arxiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/DEJUHG9L/Cunningham et al. - 2024 - Sparse Autoencoders Find Highly Interpretable Feat.pdf}
}

@article{dai_knowledge_2022,
  title = {Knowledge Neurons in Pretrained Transformers},
  author = {Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  year = {2022},
  journal = {ACL},
  pages = {8493--8502},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.581},
  url = {https://aclanthology.org/2022.acl-long.581},
  urldate = {2023-11-10},
  abstract = {Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus. In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/SNB5C7XJ/Dai et al. - 2022 - Knowledge Neurons in Pretrained Transformers.pdf}
}

@article{dalrymple_guaranteed_2024,
  title = {Towards Guaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems},
  shorttitle = {Towards Guaranteed Safe AI},
  author = {Dalrymple, David "davidad" and Skalse, Joar and Bengio, Yoshua and Russell, Stuart and Tegmark, Max and Seshia, Sanjit and Omohundro, Steve and Szegedy, Christian and Goldhaber, Ben and Ammann, Nora and Abate, Alessandro and Halpern, Joe and Barrett, Clark and Zhao, Ding and {Zhi-Xuan}, Tan and Wing, Jeannette and Tenenbaum, Joshua},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.06624},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2405.06624},
  urldate = {2024-06-10},
  abstract = {Ensuring that AI systems reliably and robustly avoid harmful or dangerous behaviours is a crucial challenge, especially for AI systems with a high degree of autonomy and general intelligence, or systems used in safety-critical contexts. In this paper, we will introduce and define a family of approaches to AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature of these approaches is that they aim to produce AI systems which are equipped with high-assurance quantitative safety guarantees. This is achieved by the interplay of three core components: a world model (which provides a mathematical description of how the AI system affects the outside world), a safety specification (which is a mathematical description of what effects are acceptable), and a verifier (which provides an auditable proof certificate that the AI satisfies the safety specification relative to the world model). We outline a number of approaches for creating each of these three core components, describe the main technical challenges, and suggest a number of potential solutions to them. We also argue for the necessity of this approach to AI safety, and for the inadequacy of the main alternative approaches.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/6NNW2MEC/Dalrymple et al. - 2024 - Towards Guaranteed Safe AI A Framework for Ensuri.pdf}
}

@article{danilevsky_survey_2020,
  title = {A Survey of the State of Explainable AI for Natural Language Processing},
  author = {Danilevsky, Marina and Qian, Kun and Aharonov, Ranit and Katsis, Yannis and Kawas, Ban and Sen, Prithviraj},
  year = {2020},
  month = oct,
  journal = {AACL},
  eprint = {2010.00711},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2010.00711},
  url = {http://arxiv.org/abs/2010.00711},
  urldate = {2023-10-21},
  abstract = {Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TMREV8KQ/Danilevsky et al. - 2020 - A Survey of the State of Explainable AI for Natura.pdf}
}

@article{dao_adversarial_2023,
  title = {An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l},
  shorttitle = {An Adversarial Example for Direct Logit Attribution},
  author = {Dao, James and Lau, Yeu-Tong and Rager, Can and Janiak, Jett},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.07325},
  url = {https://arxiv.org/abs/2310.07325},
  urldate = {2023-12-11},
  abstract = {How do language models deal with the limited bandwidth of the residual stream? Prior work has suggested that some attention heads and MLP layers may perform a "memory management" role. That is, clearing residual stream directions set by earlier layers by reading in information and writing out the negative version. In this work, we present concrete evidence for this phenomenon in a 4-layer transformer. We identify several heads in layer 2 that consistently remove the output of a single layer 0 head. We then verify that this erasure causally depends on the original written direction. We further demonstrate that direct logit attribution (DLA) suggests that writing and erasing heads directly contribute to predictions, when in fact their effects cancel out. Then we present adversarial prompts for which this effect is particularly salient. These findings reveal that memory management can make DLA results misleading. Accordingly, we make concrete recommendations for circuit analysis to prevent interpretability illusions.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/IFGL2U3L/Dao et al. - 2023 - An Adversarial Example for Direct Logit Attributio.pdf}
}

@article{dar_analyzing_2022,
  title = {Analyzing Transformers in Embedding Space},
  author = {Dar, Guy and Geva, Mor and Gupta, Ankit and Berant, Jonathan},
  year = {2022},
  month = dec,
  journal = {ACL},
  eprint = {2209.02535},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.02535},
  url = {http://arxiv.org/abs/2209.02535},
  urldate = {2023-10-25},
  abstract = {Understanding Transformer-based models has attracted significant attention, as they lie at the heart of recent technological advances across machine learning. While most interpretability methods rely on running models over inputs, recent work has shown that a zero-pass approach, where parameters are interpreted directly without a forward/backward pass is feasible for some Transformer parameters, and for two-layer attention networks. In this work, we present a theoretical analysis where all parameters of a trained Transformer are interpreted by projecting them into the embedding space, that is, the space of vocabulary items they operate on. We derive a simple theoretical framework to support our arguments and provide ample evidence for its validity. First, an empirical analysis showing that parameters of both pretrained and fine-tuned models can be interpreted in embedding space. Second, we present two applications of our framework: (a) aligning the parameters of different models that share a vocabulary, and (b) constructing a classifier without training by ``translating'' the parameters of a fine-tuned classifier to parameters of a different model that was only pretrained. Overall, our findings open the door to interpretation methods that, at least in part, abstract away from model specifics and operate in the embedding space only.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5NB67B9Y/Dar et al. - 2022 - Analyzing Transformers in Embedding Space.pdf}
}

@article{das_opportunities_2020,
  title = {Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey},
  shorttitle = {Opportunities and Challenges in Explainable Artificial Intelligence (XAI)},
  author = {Das, Arun and Rad, Paul},
  year = {2020},
  month = jun,
  journal = {CoRR},
  eprint = {2006.11371},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.11371},
  url = {http://arxiv.org/abs/2006.11371},
  urldate = {2023-10-21},
  abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VALPQUT8/Das and Rad - 2020 - Opportunities and Challenges in Explainable Artifi.pdf}
}

@article{davies_discovering_2023,
  title = {Discovering Variable Binding Circuitry with Desiderata},
  author = {Davies, Xander and Nadeau, Max and Prakash, Nikhil and Shaham, Tamar Rott and Bau, David},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.03637},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.03637},
  url = {http://arxiv.org/abs/2307.03637},
  urldate = {2023-08-27},
  abstract = {Recent work has shown that computation in language models may be human-understandable, with successful efforts to localize and intervene on both single-unit features and input-output circuits. Here, we introduce an approach which extends causal mediation experiments to automatically identify model components responsible for performing a specific subtask by solely specifying a set of {\textbackslash}textit\{desiderata\}, or causal attributes of the model components executing that subtask. As a proof of concept, we apply our method to automatically discover shared {\textbackslash}textit\{variable binding circuitry\} in LLaMA-13B, which retrieves variable values for multiple arithmetic tasks. Our method successfully localizes variable binding to only 9 attention heads (of the 1.6k) and one MLP in the final token's residual stream.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/38HQWMVT/Davies et al. - 2023 - Discovering Variable Binding Circuitry with Deside.pdf}
}

@article{davies_unifying_2023,
  title = {Unifying Grokking and Double Descent},
  author = {Davies, Xander and Langosco, Lauro and Krueger, David},
  year = {2022},
  journal = {NeurIPS ML Safety Workshop},
  eprint = {2303.06173},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.06173},
  url = {http://arxiv.org/abs/2303.06173},
  urldate = {2024-04-23},
  abstract = {A principled understanding of generalization in deep learning may require unifying disparate observations under a single conceptual framework. Previous work has studied {\textbackslash}emph\{grokking\}, a training dynamic in which a sustained period of near-perfect training performance and near-chance test performance is eventually followed by generalization, as well as the superficially similar {\textbackslash}emph\{double descent\}. These topics have so far been studied in isolation. We hypothesize that grokking and double descent can be understood as instances of the same learning dynamics within a framework of pattern learning speeds. We propose that this framework also applies when varying model capacity instead of optimization steps, and provide the first demonstration of model-wise grokking.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/YTRAY2YB/Davies et al. - 2023 - Unifying Grokking and Double Descent.pdf}
}

@article{dazeley_levels_2021,
  title = {Levels of explainable artificial intelligence for human-aligned conversational explanations},
  author = {Dazeley, Richard and Vamplew, Peter and Foale, Cameron and Young, Charlotte and Aryal, Sunil and Cruz, Francisco},
  year = {2021},
  month = oct,
  journal = {Artificial Intelligence},
  volume = {299},
  pages = {103525},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2021.103525},
  url = {https://www.sciencedirect.com/science/article/pii/S000437022100076X},
  urldate = {2023-10-20},
  abstract = {Over the last few years there has been rapid research growth into eXplainable Artificial Intelligence (XAI) and the closely aligned Interpretable Machine Learning (IML). Drivers for this growth include recent legislative changes and increased investments by industry and governments, along with increased concern from the general public. People are affected by autonomous decisions every day and the public need to understand the decision-making process to accept the outcomes. However, the vast majority of the applications of XAI/IML are focused on providing low-level `narrow' explanations of how an individual decision was reached based on a particular datum. While important, these explanations rarely provide insights into an agent's: beliefs and motivations; hypotheses of other (human, animal or AI) agents' intentions; interpretation of external cultural expectations; or, processes used to generate its own explanation. Yet all of these factors, we propose, are essential to providing the explanatory depth that people require to accept and trust the AI's decision-making. This paper aims to define levels of explanation and describe how they can be integrated to create a human-aligned conversational explanation system. In so doing, this paper will survey current approaches and discuss the integration of different technologies to achieve these levels with Broad eXplainable Artificial Intelligence (Broad-XAI), and thereby move towards high-level `strong' explanations.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PYA3CHSG/Dazeley et al. - 2021 - Levels of explainable artificial intelligence for .pdf}
}

@article{decao_editing_2021,
  title = {Editing Factual Knowledge in Language Models},
  author = {De Cao, Nicola and Aziz, Wilker and Titov, Ivan},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  journal = {EMNLP},
  pages = {6491--6506},
  doi = {10.18653/v1/2021.emnlp-main.522},
  url = {https://aclanthology.org/2021.emnlp-main.522},
  urldate = {2024-03-19},
  abstract = {The factual knowledge acquired during pre-training and stored in the parameters of Language Models (LMs) can be useful in downstream tasks (e.g., question answering or textual inference). However, some facts can be incorrectly induced or become obsolete over time. We present KnowledgeEditor, a method which can be used to edit this knowledge and, thus, fix `bugs' or unexpected predictions without the need for expensive re-training or fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not require any modifications in LM pre-training (e.g., the use of meta-learning). In our approach, we train a hyper-network with constrained optimization to modify a fact without affecting the rest of the knowledge; the trained hyper-network is then used to predict the weight update at test time. We show KnowledgeEditor's efficacy with two popular architectures and knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and ii) a sequence-to-sequence BART model for question answering. With our method, changing a prediction on the specific wording of a query tends to result in a consistent change in predictions also for its paraphrases. We show that this can be further encouraged by exploiting (e.g., automatically-generated) paraphrases during training. Interestingly, our hyper-network can be regarded as a `probe' revealing which components need to be changed to manipulate factual knowledge; our analysis shows that the updates tend to be concentrated on a small subset of components. Source code available at https://github.com/nicola-decao/KnowledgeEditor},
  file = {/Users/leonardbereska/Zotero/storage/PMCWBJ2M/De Cao et al. - 2021 - Editing Factual Knowledge in Language Models.pdf}
}

@article{dedieu_learning_2024,
  title = {Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments},
  author = {Dedieu, Antoine and Lehrach, Wolfgang and Zhou, Guangyao and George, Dileep and {L{\'a}zaro-Gredilla}, Miguel},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.05946},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.05946},
  url = {http://arxiv.org/abs/2401.05946},
  urldate = {2024-06-10},
  abstract = {Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/D447DKRP/Dedieu et al. - 2024 - Learning Cognitive Maps from Transformer Represent.pdf}
}

@article{deepmind_specification_2020,
  title = {Specification gaming: the flip side of AI ingenuity},
  shorttitle = {Specification gaming},
  author = {DeepMind},
  year = {2020},
  journal = {DeepMind Blog},
  url = {https://www.deepmind.com/blog/specification-gaming-the-flip-side-of-ai-ingenuity},
  urldate = {2023-08-26},
  abstract = {Specification gaming is a behaviour that satisfies the literal specification of an objective without achieving the intended outcome. We have all had experiences with specification gaming, even if not by this name. Readers may have heard the myth of King Midas and the golden touch, in which the king asks that anything he touches be turned to gold - but soon finds that even food and drink turn to metal in his hands. In the real world, when rewarded for doing well on a homework assignment, a student might copy another student to get the right answers, rather than learning the material - and thus exploit a loophole in the task specification.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/B5J5GP7K/DeepMind - 2020 - Specification gaming the flip side of AI ingenuit.html}
}

@article{deletang_causal_2021,
  title = {Causal Analysis of Agent Behavior for AI Safety},
  author = {D{\'e}letang, Gr{\'e}goire and {Grau-Moya}, Jordi and Martic, Miljan and Genewein, Tim and McGrath, Tom and Mikulik, Vladimir and Kunesch, Markus and Legg, Shane and Ortega, Pedro A.},
  year = {2021},
  month = mar,
  journal = {CoRR},
  eprint = {2103.03938},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2103.03938},
  url = {http://arxiv.org/abs/2103.03938},
  urldate = {2024-02-10},
  abstract = {As machine learning systems become more powerful they also become increasingly unpredictable and opaque. Yet, finding human-understandable explanations of how they work is essential for their safe deployment. This technical report illustrates a methodology for investigating the causal mechanisms that drive the behaviour of artificial agents. Six use cases are covered, each addressing a typical question an analyst might ask about an agent. In particular, we show that each question cannot be addressed by pure observation alone, but instead requires conducting experiments with systematically chosen manipulations so as to generate the correct causal evidence.},
  archiveprefix = {arxiv},
  keywords = {agents,causal,interpretability,motivation,not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/HMBKFCL4/Dletang et al. - 2021 - Causal Analysis of Agent Behavior for AI Safety.pdf}
}

@article{denain_auditing_2022,
  title = {Auditing Visualizations: Transparency Methods Struggle to Detect Anomalous Behavior},
  shorttitle = {Auditing Visualizations},
  author = {Denain, Jean-Stanislas and Steinhardt, Jacob},
  year = {2022},
  journal = {CoRR},
  eprint = {2206.13498},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.13498},
  url = {http://arxiv.org/abs/2206.13498},
  urldate = {2023-11-20},
  abstract = {Model visualizations provide information that outputs alone might miss. But can we trust that model visualizations reflect model behavior? For instance, can they diagnose abnormal behavior such as planted backdoors or overregularization? To evaluate visualization methods, we test whether they assign different visualizations to anomalously trained models and normal models. We find that while existing methods can detect models with starkly anomalous behavior, they struggle to identify more subtle anomalies. Moreover, they often fail to recognize the inputs that induce anomalous behavior, e.g. images containing a spurious cue. These results reveal blind spots and limitations of some popular model visualizations. By introducing a novel evaluation framework for visualizations, our work paves the way for developing more reliable model transparency methods in the future.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/AWNM7IVS/Denain and Steinhardt - 2022 - Auditing Visualizations Transparency Methods Stru.pdf}
}

@article{deng_measuring_2023,
  title = {Measuring Feature Sparsity in Language Models},
  author = {Deng, Mingyang and Tao, Lucas and Benton, Joe},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.07837},
  url = {https://arxiv.org/abs/2310.07837},
  urldate = {2023-10-26},
  abstract = {Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/86TETC2V/Deng et al. - 2023 - Measuring Feature Sparsity in Language Models.pdf}
}

@article{devalois_orientation_1982,
  title = {The orientation and direction selectivity of cells in macaque visual cortex},
  author = {De Valois, Russell L. and William Yund, E. and Hepler, Norva},
  year = {1982},
  month = jan,
  journal = {Vision Research},
  volume = {22},
  number = {5},
  pages = {531--544},
  issn = {00426989},
  doi = {10.1016/0042-6989(82)90112-2},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0042698982901122},
  urldate = {2024-02-10},
  abstract = {Semantic Scholar extracted view of "The orientation and direction selectivity of cells in macaque visual cortex" by R. L. Valois et al.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TBRI4CH8/De Valois et al. - 1982 - The orientation and direction selectivity of cells.pdf}
}

@article{din_jump_2023,
  title = {Jump to Conclusions: Short-Cutting Transformers With Linear Transformations},
  shorttitle = {Jump to Conclusions},
  author = {Din, Alexander Yom and Karidi, Taelin and Choshen, Leshem and Geva, Mor},
  year = {2023},
  month = mar,
  journal = {CoRR},
  eprint = {2303.09435},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.09435},
  url = {http://arxiv.org/abs/2303.09435},
  urldate = {2023-11-16},
  abstract = {Transformer-based language models (LMs) create hidden representations of their inputs at every layer, but only use final-layer representations for prediction. This obscures the internal decision-making process of the model and the utility of its intermediate representations. One way to elucidate this is to cast the hidden representations as final representations, bypassing the transformer computation in-between. In this work, we suggest a simple method for such casting, by using linear transformations. We show that our approach produces more accurate approximations than the prevailing practice of inspecting hidden representations from all layers in the space of the final layer. Moreover, in the context of language modeling, our method allows "peeking" into early layer representations of GPT-2 and BERT, showing that often LMs already predict the final output in early layers. We then demonstrate the practicality of our method to recent early exit strategies, showing that when aiming, for example, at retention of 95\% accuracy, our approach saves additional 7.9\% layers for GPT-2 and 5.4\% layers for BERT, on top of the savings of the original approach. Last, we extend our method to linearly approximate sub-modules, finding that attention is most tolerant to this change.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HQC63W4Q/Din et al. - 2023 - Jump to Conclusions Short-Cutting Transformers Wi.pdf}
}

@article{ding_grounding_2021,
  title = {Grounding Representation Similarity with Statistical Testing},
  author = {Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
  year = {2021},
  month = nov,
  journal = {NeurIPS},
  eprint = {2108.01661},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2108.01661},
  url = {http://arxiv.org/abs/2108.01661},
  urldate = {2024-02-26},
  abstract = {To understand neural network behavior, recent works quantitatively compare different networks' learned representations using canonical correlation analysis (CCA), centered kernel alignment (CKA), and other dissimilarity measures. Unfortunately, these widely used measures often disagree on fundamental observations, such as whether deep networks differing only in random initialization learn similar representations. These disagreements raise the question: which, if any, of these dissimilarity measures should we believe? We provide a framework to ground this question through a concrete test: measures should have sensitivity to changes that affect functional behavior, and specificity against changes that do not. We quantify this through a variety of functional behaviors including probing accuracy and robustness to distribution shift, and examine changes such as varying random initialization and deleting principal components. We find that current metrics exhibit different weaknesses, note that a classical baseline performs surprisingly well, and highlight settings where all metrics appear to fail, thus providing a challenge set for further improvement.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/CYJJRE2S/Ding et al. - 2021 - Grounding Representation Similarity with Statistic.pdf}
}

@article{ding_survival_2024,
  title = {Survival of the Fittest Representation: A Case Study with Modular Addition},
  shorttitle = {Survival of the Fittest Representation},
  author = {Ding, Xiaoman Delores and Guo, Zifan Carl and Michaud, Eric J. and Liu, Ziming and Tegmark, Max},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.17420},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.17420},
  url = {http://arxiv.org/abs/2405.17420},
  urldate = {2024-06-06},
  abstract = {When a neural network can learn multiple distinct algorithms to solve a task, how does it "choose" between them during training? To approach this question, we take inspiration from ecology: when multiple species coexist, they eventually reach an equilibrium where some survive while others die out. Analogously, we suggest that a neural network at initialization contains many solutions (representations and algorithms), which compete with each other under pressure from resource constraints, with the "fittest" ultimately prevailing. To investigate this Survival of the Fittest hypothesis, we conduct a case study on neural networks performing modular addition, and find that these networks' multiple circular representations at different Fourier frequencies undergo such competitive dynamics, with only a few circles surviving at the end. We find that the frequencies with high initial signals and gradients, the "fittest," are more likely to survive. By increasing the embedding dimension, we also observe more surviving frequencies. Inspired by the Lotka-Volterra equations describing the dynamics between species, we find that the dynamics of the circles can be nicely characterized by a set of linear differential equations. Our results with modular addition show that it is possible to decompose complicated representations into simpler components, along with their basic interactions, to offer insight on the training dynamics of representations.},
  archiveprefix = {arxiv}
}

@article{dong_interpretable_2017,
  title = {Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples},
  author = {Dong, Yinpeng and Su, Hang and Zhu, Jun and Bao, Fan},
  year = {2017},
  month = aug,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Towards-Interpretable-Deep-Neural-Networks-by-Dong-Su/a968c063917880fa460ee9d79750030a66cd1bdd},
  urldate = {2023-09-18},
  abstract = {Sometimes it is not enough for a DNN to produce an outcome. For example, in applications such as healthcare, users need to understand the rationale of the decisions. Therefore, it is imperative to develop algorithms to learn models with good interpretability (Doshi-Velez 2017). An important factor that leads to the lack of interpretability of DNNs is the ambiguity of neurons, where a neuron may fire for various unrelated concepts. This work aims to increase the interpretability of DNNs on the whole image space by reducing the ambiguity of neurons. In this paper, we make the following contributions:  1) We propose a metric to evaluate the consistency level of neurons in a network quantitatively.  2) We find that the learned features of neurons are ambiguous by leveraging adversarial examples.  3) We propose to improve the consistency of neurons on adversarial example subset by an adversarial training algorithm with a consistent loss.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZXJJI2MB/Dong et al. - 2017 - Towards Interpretable Deep Neural Networks by Leve.pdf}
}

@article{doosterlinck_flexible_2023,
  title = {Flexible Model Interpretability through Natural Language Model Editing},
  author = {D'Oosterlinck, Karel and Demeester, Thomas and Develder, Chris and Potts, Christopher},
  year = {2023},
  journal = {BlackboxNLP},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2311.10905},
  url = {https://arxiv.org/abs/2311.10905},
  urldate = {2024-02-11},
  abstract = {Model interpretability and model editing are crucial goals in the age of large language models. Interestingly, there exists a link between these two goals: if a method is able to systematically edit model behavior with regard to a human concept of interest, this editor method can help make internal representations more interpretable by pointing towards relevant representations and systematically manipulating them.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/B5UXA43F/D'Oosterlinck et al. - 2023 - Flexible Model Interpretability through Natural La.pdf}
}

@article{doshi-velez_rigorous_2017,
  title = {Towards A Rigorous Science of Interpretable Machine Learning},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  journal = {CoRR},
  eprint = {1702.08608},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1702.08608},
  url = {http://arxiv.org/abs/1702.08608},
  urldate = {2023-09-03},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/8PXCEKM4/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf}
}

@article{druce_brittle_2021,
  title = {Brittle AI, Causal Confusion, and Bad Mental Models: Challenges and Successes in the XAI Program},
  shorttitle = {Brittle AI, Causal Confusion, and Bad Mental Models},
  author = {Druce, Jeff and Niehaus, James and Moody, Vanessa and Jensen, David and Littman, Michael L.},
  year = {2021},
  month = jun,
  journal = {CoRR},
  eprint = {2106.05506},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.05506},
  url = {http://arxiv.org/abs/2106.05506},
  urldate = {2024-02-10},
  abstract = {The advances in artificial intelligence enabled by deep learning architectures are undeniable. In several cases, deep neural network driven models have surpassed human level performance in benchmark autonomy tasks. The underlying policies for these agents, however, are not easily interpretable. In fact, given their underlying deep models, it is impossible to directly understand the mapping from observations to actions for any reasonably complex agent. Producing this supporting technology to "open the black box" of these AI systems, while not sacrificing performance, was the fundamental goal of the DARPA XAI program. In our journey through this program, we have several "big picture" takeaways: 1) Explanations need to be highly tailored to their scenario; 2) many seemingly high performing RL agents are extremely brittle and are not amendable to explanation; 3) causal models allow for rich explanations, but how to present them isn't always straightforward; and 4) human subjects conjure fantastically wrong mental models for AIs, and these models are often hard to break. This paper discusses the origins of these takeaways, provides amplifying information, and suggestions for future work.},
  archiveprefix = {arxiv},
  keywords = {not cited,review,XAI},
  file = {/Users/leonardbereska/Zotero/storage/3EP6NV8Y/Druce et al. - 2021 - Brittle AI, Causal Confusion, and Bad Mental Model.pdf}
}

@article{du_generalizing_2023,
  title = {Generalizing Backpropagation for Gradient-Based Interpretability},
  author = {Du, Kevin and Hennigen, Lucas Torroba and Stoehr, Niklas and Warstadt, Alexander and Cotterell, Ryan},
  year = {2023},
  month = jul,
  journal = {ACL},
  eprint = {2307.03056},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.03056},
  url = {http://arxiv.org/abs/2307.03056},
  urldate = {2023-10-30},
  abstract = {Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings. This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy. We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject-verb number agreement task (SVA). With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9XTDZDP3/Du et al. - 2023 - Generalizing Backpropagation for Gradient-Based In.pdf}
}

@article{dung_current_2023,
  title = {Current cases of AI misalignment and their implications for future risks},
  author = {Dung, Leonard},
  year = {2023},
  month = oct,
  journal = {Synthese},
  volume = {202},
  number = {5},
  pages = {138},
  issn = {1573-0964},
  doi = {10.1007/s11229-023-04367-0},
  url = {https://link.springer.com/10.1007/s11229-023-04367-0},
  urldate = {2023-12-11},
  abstract = {Abstract                            How can one build AI systems such that they pursue the goals their designers want them to pursue? This is the               alignment problem               . Numerous authors have raised concerns that, as research advances and systems become more powerful over time,               misalignment               might lead to catastrophic outcomes, perhaps even to the extinction or permanent disempowerment of humanity. In this paper, I analyze the severity of this risk based on current instances of misalignment. More specifically, I argue that contemporary large language models and game-playing agents are sometimes misaligned. These cases suggest that misalignment tends to have a variety of features: misalignment can be hard to detect, predict and remedy, it does not depend on a specific architecture or training paradigm, it tends to diminish a system's usefulness and it is the default outcome of creating AI via machine learning. Subsequently, based on these features, I show that the risk of AI alignment magnifies with respect to more capable systems. Not only might more capable systems cause more harm               when               misaligned, aligning them should be expected to be more difficult than aligning current AI.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2W9DMA7K/Dung - 2023 - Current cases of AI misalignment and their implica.pdf}
}

@article{durrani_analyzing_2020,
  title = {Analyzing Individual Neurons in Pre-trained Language Models},
  author = {Durrani, Nadir and Sajjad, Hassan and Dalvi, Fahim and Belinkov, Yonatan},
  year = {2020},
  month = oct,
  journal = {EMNLP},
  eprint = {2010.02695},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2010.02695},
  url = {http://arxiv.org/abs/2010.02695},
  urldate = {2023-11-16},
  abstract = {While a lot of analysis has been carried to demonstrate linguistic knowledge captured by the representations learned within deep NLP models, very little attention has been paid towards individual neurons.We carry outa neuron-level analysis using core linguistic tasks of predicting morphology, syntax and semantics, on pre-trained language models, with questions like: i) do individual neurons in pre-trained models capture linguistic information? ii) which parts of the network learn more about certain linguistic phenomena? iii) how distributed or focused is the information? and iv) how do various architectures differ in learning these properties? We found small subsets of neurons to predict linguistic tasks, with lower level tasks (such as morphology) localized in fewer neurons, compared to higher level task of predicting syntax. Our study also reveals interesting cross architectural comparisons. For example, we found neurons in XLNet to be more localized and disjoint when predicting properties compared to BERT and others, where they are more distributed and coupled.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9EFD482R/Durrani et al. - 2020 - Analyzing Individual Neurons in Pre-trained Langua.pdf}
}

@article{elazar_amnesic_2021,
  title = {Amnesic Probing: Behavioral Explanation with Amnesic Counterfactuals},
  shorttitle = {Amnesic Probing},
  author = {Elazar, Yanai and Ravfogel, Shauli and Jacovi, Alon and Goldberg, Yoav},
  year = {2021},
  month = feb,
  journal = {TACL},
  eprint = {2006.00995},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.00995},
  url = {http://arxiv.org/abs/2006.00995},
  urldate = {2023-11-10},
  abstract = {A growing body of work makes use of probing to investigate the working of neural models, often considered black boxes. Recently, an ongoing debate emerged surrounding the limitations of the probing paradigm. In this work, we point out the inability to infer behavioral conclusions from probing results and offer an alternative method that focuses on how the information is being used, rather than on what information is encoded. Our method, Amnesic Probing, follows the intuition that the utility of a property for a given task can be assessed by measuring the influence of a causal intervention that removes it from the representation. Equipped with this new analysis tool, we can ask questions that were not possible before, e.g. is part-of-speech information important for word prediction? We perform a series of analyses on BERT to answer these types of questions. Our findings demonstrate that conventional probing performance is not correlated to task importance, and we call for increased scrutiny of claims that draw behavioral or causal conclusions from probing results.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/QSENF43W/Elazar et al. - 2021 - Amnesic Probing Behavioral Explanation with Amnes.pdf}
}

@article{elazar_measuring_2021,
  title = {Measuring and Improving Consistency in Pretrained Language Models},
  author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Sch{\"u}tze, Hinrich and Goldberg, Yoav},
  year = {2021},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {1012--1031},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00410},
  url = {https://doi.org/10.1162/tacl_a_00410},
  urldate = {2024-03-19},
  abstract = {Consistency of a model---that is, the invariance of its behavior under meaning-preserving alternations in its input---is a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRel, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRel, we show that the consistency of all PLMs we experiment with is poor--- though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1},
  file = {/Users/leonardbereska/Zotero/storage/TQ2E9R4W/Elazar et al. - 2021 - Measuring and Improving Consistency in Pretrained .pdf}
}

@article{elhage_mathematical_2021,
  title = {A mathematical framework for transformer circuits},
  author = {Elhage, N and Nanda, N and Olsson, C and Henighan, T and Joseph, N and Mann, B and Askell, A and Bai, Y and Chen, A and Conerly, T and others},
  year = {2021},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2021/framework/index.html},
  keywords = {cited,fundamental,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,toy models,transformer},
  file = {/Users/leonardbereska/Zotero/storage/DJDXRSQ7/Elhage et al. - 2021 - A mathematical framework for transformer circuits.html}
}

@article{elhage_privileged_2023,
  title = {Privileged Bases in the Transformer Residual Stream},
  author = {Elhage, Nelson and Lasenby, Robert and Olah, Christopher},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/privileged-basis/index.html},
  abstract = {Our mathematical theories of the Transformer architecture suggest that individual coordinates in the residual stream should have no special significance (that is, the basis directions should be in some sense "arbitrary" and no more likely to encode information than random directions). Recent work has shown that this observation is false in practice. We investigate this phenomenon and provisionally conclude that the per-dimension normalizers in the Adam optimizer are to blame for the effect. We explore two other obvious sources of basis dependency in a Transformer: Layer normalization, and finite-precision floating-point calculations. We confidently rule these out as being the source of the observed basis-alignment.},
  keywords = {not cited,superposition,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/RWKQ453E/Elhage et al. - 2023 - Privileged Bases in the Transformer Residual Strea.html}
}

@article{elhage_softmax_2022,
  title = {Softmax Linear Units},
  author = {Elhage, Nelson and Hume, Tristan and Catherine, Olsson and Neel, Nanda and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and {Hatfield-Dodds}, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and {Tran-Johnson}, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/solu/index.html},
  urldate = {2023-07-31},
  keywords = {cited,intrinsic,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/YNHFFS3C/Elhage et al. - 2022 - Softmax Linear Units.html}
}

@article{elhage_toy_2022,
  title = {Toy Models of Superposition},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and {Hatfield-Dodds}, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/toy_model/index.html},
  keywords = {cited,fundamental,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/8K24TPD5/Elhage et al. - 2022 - Toy Models of Superposition.html}
}

@article{ellenar_interpreting_,
  title = {Interpreting OpenAI's Whisper},
  author = {EllenaR},
  year = {2023},
  month = sep,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/thePw6qdyabD8XR4y/interpreting-openai-s-whisper},
  urldate = {2023-10-27},
  abstract = {Mechanistic Interpretability has mainly focused on language and image models, but there's a growing need for interpretability in multimodal models that can handle text, images, audio, and video. Thus far, there have been minimal efforts directed toward interpreting audio models, let alone multimodal ones. To the best of my knowledge, this work presents the first attempt to do interpretability on a multimodal audio-text model. I show that acoustic features inside OpenAI's Whisper model are human interpretable and formulate a way of listening to them. I then go on to present some macroscopic properties of the model, specifically showing that encoder attention is highly localized and the decoder alone acts as a weak LM.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2ZTBR59F/EllenaR - 2023 - Interpreting OpenAI's Whisper.html}
}

@article{emmons_contrast_2023,
  title = {Contrast Pairs Drive the Empirical Performance of Contrast Consistent Search (CCS)},
  author = {Emmons, Scott},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast},
  urldate = {2024-02-20},
  abstract = {tl;dr Contrast consistent search (CCS)[1]~is a method by Burns et al. that consists of two parts: {\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/T9FIZ4M7/Emmons - 2023 - Contrast Pairs Drive the Empirical Performance of .html}
}

@article{engels_not_2024,
  title = {Not All Language Model Features Are Linear},
  author = {Engels, Joshua and Liao, Isaac and Michaud, Eric J. and Gurnee, Wes and Tegmark, Max},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.14860},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.14860},
  url = {http://arxiv.org/abs/2405.14860},
  urldate = {2024-06-06},
  abstract = {Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/XN5ZG2EY/Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf;/Users/leonardbereska/Zotero/storage/PK9YTKZX/2405.html}
}

@article{erdil_my_2023,
  title = {My impression of singular learning theory},
  author = {Erdil, Ege},
  year = {2023},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/DqLHvJjuPdtrzuoas/my-impression-of-singular-learning-theory},
  urldate = {2024-02-12},
  abstract = {Disclaimer: I'm by no means an expert on singular learning theory and what I present below is a simplification that experts might not endorse. Still,{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9AIVX4DK/Erdil - 2023 - My impression of singular learning theory.html}
}

@article{evanko_systems_2006,
  title = {Systems biology for beginners},
  author = {Evanko, Daniel},
  year = {2006},
  month = dec,
  journal = {Nat Methods},
  volume = {3},
  number = {12},
  pages = {964--965},
  publisher = {Nature Publishing Group},
  issn = {1548-7105},
  doi = {10.1038/nmeth1206-964b},
  url = {https://www.nature.com/articles/nmeth1206-964b},
  urldate = {2024-04-30},
  abstract = {A web and print Focus on systems biology from Nature Publishing Group provides a practical introduction to a field that for all its promise still has many skeptics.},
  copyright = {2006 Springer Nature America, Inc.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/X6PBV65A/Evanko - 2006 - Systems biology for beginners.pdf}
}

@article{falco_governing_2021,
  title = {Governing AI safety through independent audits},
  author = {Falco, Gregory and Shneiderman, Ben and Badger, Julia and Carrier, Ryan and Dahbura, Anton and Danks, David and Eling, Martin and Goodloe, Alwyn and Gupta, Jerry and Hart, Christopher and Jirotka, Marina and Johnson, Henric and LaPointe, Cara and Llorens, Ashley J. and Mackworth, Alan K. and Maple, Carsten and P{\'a}lsson, Sigur{\dh}ur Emil and Pasquale, Frank and Winfield, Alan and Yeong, Zee Kin},
  year = {2021},
  month = jul,
  journal = {Nat Mach Intell},
  volume = {3},
  number = {7},
  pages = {566--571},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-021-00370-7},
  url = {https://www.nature.com/articles/s42256-021-00370-7},
  urldate = {2023-01-26},
  abstract = {Highly automated systems are becoming omnipresent. They range in function from self-driving vehicles to advanced medical diagnostics and afford many benefits. However, there are assurance challenges that have become increasingly visible in high-profile crashes and incidents. Governance of such systems is critical to garner widespread public trust. Governance principles have been previously proposed offering aspirational guidance to automated system developers; however, their implementation is often impractical given the excessive costs and processes required to enact and then enforce the principles. This Perspective, authored by an international and multidisciplinary team across government organizations, industry and academia, proposes a mechanism to drive widespread assurance of highly automated systems: independent audit. As proposed, independent audit of AI systems would embody three `AAA' governance principles of prospective risk Assessments, operation Audit trails and system Adherence to jurisdictional requirements. Independent audit of AI systems serves as a pragmatic approach to an otherwise burdensome and unenforceable assurance challenge.},
  copyright = {2021 Springer Nature Limited},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/6FQRLFSS/Falco et al. - 2021 - Governing AI safety through independent audits.pdf}
}

@article{falkum_polysemy_2015,
  title = {Polysemy: Current perspectives and approaches},
  shorttitle = {Polysemy},
  author = {Falkum, Ingrid Lossius and Vicente, Agustin},
  year = {2015},
  month = apr,
  journal = {Lingua},
  volume = {157},
  pages = {1--16},
  issn = {00243841},
  doi = {10.1016/j.lingua.2015.02.002},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0024384115000170},
  urldate = {2024-06-20},
  abstract = {Semantic Scholar extracted view of "Polysemy: Current perspectives and approaches" by I. Falkum et al.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/D9H5YK36/Falkum and Vicente - 2015 - Polysemy Current perspectives and approaches.pdf}
}

@article{fan_evaluating_2023,
  title = {Evaluating Neuron Interpretation Methods of NLP Models},
  author = {Fan, Yimin and Dalvi, Fahim and Durrani, Nadir and Sajjad, Hassan},
  year = {2023},
  journal = {CoRR},
  publisher = {[object Object]},
  doi = {10.48550/ARXIV.2301.12608},
  url = {https://arxiv.org/abs/2301.12608},
  urldate = {2024-03-22},
  abstract = {Neuron Interpretation has gained traction in the field of interpretability, and have provided fine-grained insights into what a model learns and how language knowledge is distributed amongst its different components. However, the lack of evaluation benchmark and metrics have led to siloed progress within these various methods, with very little work comparing them and highlighting their strengths and weaknesses. The reason for this discrepancy is the difficulty of creating ground truth datasets, for example, many neurons within a given model may learn the same phenomena, and hence there may not be one correct answer. Moreover, a learned phenomenon may spread across several neurons that work together -- surfacing these to create a gold standard challenging. In this work, we propose an evaluation framework that measures the compatibility of a neuron analysis method with other methods. We hypothesize that the more compatible a method is with the majority of the methods, the more confident one can be about its performance. We systematically evaluate our proposed framework and present a comparative analysis of a large set of neuron interpretation methods. We make the evaluation framework available to the community. It enables the evaluation of any new method using 20 concepts and across three pre-trained models.The code is released at https://github.com/fdalvi/neuron-comparative-analysis},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/ZKKHPBQP/Fan et al. - 2023 - Evaluating Neuron Interpretation Methods of NLP Mo.pdf}
}

@article{fan_interpretability_2021,
  title = {On Interpretability of Artificial Neural Networks: A Survey},
  shorttitle = {On Interpretability of Artificial Neural Networks},
  author = {Fan, Feng-Lei and Xiong, Jinjun and Li, Mengzhou and Wang, Ge},
  year = {2021},
  month = nov,
  journal = {IEEE Trans. Radiat. Plasma Med. Sci.},
  volume = {5},
  number = {6},
  pages = {741--760},
  issn = {2469-7311, 2469-7303},
  doi = {10.1109/TRPMS.2021.3066428},
  url = {https://ieeexplore.ieee.org/document/9380482/},
  urldate = {2023-08-27},
  abstract = {Deep learning as performed by artificial deep neural networks (DNNs) has achieved great successes recently in many important areas that deal with text, images, videos, graphs, and so on. However, the black-box nature of DNNs has become one of the primary obstacles for their wide adoption in mission-critical applications such as medical diagnosis and therapy. Because of the huge potentials of deep learning, the interpretability of DNNs has recently attracted much research attention. In this article, we propose a simple but comprehensive taxonomy for interpretability, systematically review recent studies on interpretability of neural networks, describe applications of interpretability in medicine, and discuss future research directions, such as in relation to fuzzy logic and brain science.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DKM8HTXH/Fan et al. - 2021 - On Interpretability of Artificial Neural Networks.pdf}
}

@article{farquhar_challenges_2023,
  title = {Challenges with unsupervised LLM knowledge discovery},
  author = {Farquhar, Sebastian and Varma, Vikrant and Kenton, Zachary and Gasteiger, Johannes and Mikulik, Vladimir and Shah, Rohin},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.10029},
  url = {https://arxiv.org/abs/2312.10029},
  urldate = {2024-01-21},
  abstract = {We show that existing unsupervised methods on large language model (LLM) activations do not discover knowledge -- instead they seem to discover whatever feature of the activations is most prominent. The idea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure, which can be used to discover knowledge. We first prove theoretically that arbitrary features (not just knowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation method, contrast-consistent search (Burns et al. - arXiv:2212.03827). We then present a series of experiments showing settings in which unsupervised methods result in classifiers that do not predict knowledge, but instead predict a different prominent feature. We conclude that existing unsupervised methods for discovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating future knowledge elicitation methods. Conceptually, we hypothesise that the identification issues explored here, e.g. distinguishing a model's knowledge from that of a simulated character's, will persist for future unsupervised methods.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/8N96HYCI/Farquhar et al. - 2023 - Challenges with unsupervised LLM knowledge discove.pdf}
}

@article{feder_causalm_2021,
  title = {CausaLM: Causal Model Explanation Through Counterfactual Language Models},
  shorttitle = {CausaLM},
  author = {Feder, Amir and Oved, Nadav and Shalit, Uri and Reichart, Roi},
  year = {2021},
  month = may,
  journal = {Computational Linguistics},
  eprint = {2005.13407},
  primaryclass = {cs},
  pages = {1--54},
  issn = {0891-2017, 1530-9312},
  doi = {10.1162/coli_a_00404},
  url = {http://arxiv.org/abs/2005.13407},
  urldate = {2024-03-19},
  abstract = {Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/XV6ZSELN/Feder et al. - 2021 - CausaLM Causal Model Explanation Through Counterf.pdf}
}

@article{feng_how_2023,
  title = {How do Language Models Bind Entities in Context?},
  author = {Feng, Jiahai and Steinhardt, Jacob},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.17191},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.17191},
  url = {http://arxiv.org/abs/2310.17191},
  urldate = {2023-11-16},
  abstract = {To correctly use in-context information, language models (LMs) must bind entities to their attributes. For example, given a context describing a "green square" and a "blue circle", LMs must bind the shapes to their respective colors. We analyze LM representations and identify the binding ID mechanism: a general mechanism for solving the binding problem, which we observe in every sufficiently large model from the Pythia and LLaMA families. Using causal interventions, we show that LMs' internal activations represent binding information by attaching binding ID vectors to corresponding entities and attributes. We further show that binding ID vectors form a continuous subspace, in which distances between binding ID vectors reflect their discernability. Overall, our results uncover interpretable strategies in LMs for representing symbolic knowledge in-context, providing a step towards understanding general in-context reasoning in large-scale LMs.},
  archiveprefix = {arxiv},
  keywords = {behavior,circuit,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/VSD6ANJ5/Feng and Steinhardt - 2023 - How do Language Models Bind Entities in Context.pdf}
}

@article{ferrando_explaining_2023,
  title = {Explaining How Transformers Use Context to Build Predictions},
  author = {Ferrando, Javier and G{\'a}llego, Gerard I. and Tsiamas, Ioannis and {Costa-juss{\`a}}, Marta R.},
  year = {2023},
  journal = {ACL},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.12535},
  url = {https://arxiv.org/abs/2305.12535},
  urldate = {2024-01-21},
  abstract = {Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model's prediction, it is still unclear how prior words affect the model's decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7328DBBI/Ferrando et al. - 2023 - Explaining How Transformers Use Context to Build P.pdf}
}

@article{ferrando_information_2024,
  title = {Information Flow Routes: Automatically Interpreting Language Models at Scale},
  shorttitle = {Information Flow Routes},
  author = {Ferrando, Javier and Voita, Elena},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2403.00824},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.00824},
  url = {http://arxiv.org/abs/2403.00824},
  urldate = {2024-03-12},
  abstract = {Information flows by routes inside the network via mechanisms implemented in the model. These routes can be represented as graphs where nodes correspond to token representations and edges to operations inside the network. We automatically build these graphs in a top-down manner, for each prediction leaving only the most important nodes and edges. In contrast to the existing workflows relying on activation patching, we do this through attribution: this allows us to efficiently uncover existing circuits with just a single forward pass. Additionally, the applicability of our method is far beyond patching: we do not need a human to carefully design prediction templates, and we can extract information flow routes for any prediction (not just the ones among the allowed templates). As a result, we can talk about model behavior in general, for specific types of predictions, or different domains. We experiment with Llama 2 and show that the role of some attention heads is overall important, e.g. previous token heads and subword merging heads. Next, we find similarities in Llama 2 behavior when handling tokens of the same part of speech. Finally, we show that some model components can be specialized on domains such as coding or multilingual texts.},
  archiveprefix = {arxiv},
  keywords = {mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/94ID7SA3/Ferrando and Voita - 2024 - Information Flow Routes Automatically Interpretin.pdf}
}

@article{ferrando_primer_2024,
  title = {A Primer on the Inner Workings of Transformer-based Language Models},
  author = {Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and {Costa-juss{\`a}}, Marta R.},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.00208},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2405.00208},
  urldate = {2024-05-08},
  abstract = {The rapid progress of research aimed at interpreting the inner workings of advanced language models has highlighted a need for contextualizing the insights gained from years of work in this area. This primer provides a concise technical introduction to the current techniques used to interpret the inner workings of Transformer-based language models, focusing on the generative decoder-only architecture. We conclude by presenting a comprehensive overview of the known internal mechanisms implemented by these models, uncovering connections across popular approaches and active research directions in this area.},
  archiveprefix = {arxiv},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/HCJ3MYVI/Ferrando et al. - 2024 - A Primer on the Inner Workings of Transformer-base.pdf}
}

@article{ferry_emergence_2023,
  title = {Emergence and Function of Abstract Representations in Self-Supervised Transformers},
  author = {Ferry, Quentin RV. and Ching, Joshua and Kawai, Takashi},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.05361},
  url = {https://arxiv.org/abs/2312.05361},
  urldate = {2024-02-11},
  abstract = {Human intelligence relies in part on our brains' ability to create abstract mental models that succinctly capture the hidden blueprint of our reality. Such abstract world models notably allow us to rapidly navigate novel situations by generalizing prior knowledge, a trait deep learning systems have historically struggled to replicate. However, the recent shift from supervised to self-supervised objectives, combined with expressive transformer-based architectures, have yielded powerful foundation models that appear to learn versatile representations that can support a wide range of downstream tasks. This promising development raises the intriguing possibility of such models developing in silico abstract world models. We test this hypothesis by studying the inner workings of small-scale transformers trained to reconstruct partially masked visual scenes generated from a simple blueprint. We show that the network develops intermediate abstract representations, or abstractions, that encode all semantic features of the dataset. These abstractions manifest as low-dimensional manifolds where the embeddings of semantically related tokens transiently converge, thus allowing for the generalization of downstream computations. Using precise manipulation experiments, we demonstrate that abstractions are central to the network's decision-making process. Our research also suggests that these abstractions are compositionally structured, exhibiting features like contextual independence and part-whole relationships that mirror the compositional nature of the dataset. Finally, we introduce a Language-Enhanced Architecture (LEA) designed to encourage the network to articulate its computations. We find that LEA develops an abstraction-centric language that can be easily interpreted, allowing us to more readily access and steer the network's decision-making process.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {abstraction,evidence,not cited,to cite,transformer,world models},
  file = {/Users/leonardbereska/Zotero/storage/Z6LXCMKF/Ferry et al. - 2023 - Emergence and Function of Abstract Representations.pdf}
}

@article{filan_clusterability_2021,
  title = {Clusterability in Neural Networks},
  author = {Filan, Daniel and Casper, Stephen and Hod, Shlomi and Wild, Cody and Critch, Andrew and Russell, Stuart},
  year = {2021},
  month = mar,
  journal = {CoRR},
  eprint = {2103.03386},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2103.03386},
  url = {http://arxiv.org/abs/2103.03386},
  urldate = {2023-10-26},
  abstract = {The learned weights of a neural network have often been considered devoid of scrutable internal structure. In this paper, however, we look for structure in the form of clusterability: how well a network can be divided into groups of neurons with strong internal connectivity but weak external connectivity. We find that a trained neural network is typically more clusterable than randomly initialized networks, and often clusterable relative to random networks with the same distribution of weights. We also exhibit novel methods to promote clusterability in neural network training, and find that in multi-layer perceptrons they lead to more clusterable networks with little reduction in accuracy. Understanding and controlling the clusterability of neural networks will hopefully render their inner workings more interpretable to engineers by facilitating partitioning into meaningful clusters.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HC5XPXDH/Filan et al. - 2021 - Clusterability in Neural Networks.pdf}
}

@article{finlayson_causal_2021,
  title = {Causal Analysis of Syntactic Agreement Mechanisms in Neural Language Models},
  author = {Finlayson, Matthew and Mueller, Aaron and Gehrmann, Sebastian and Shieber, Stuart and Linzen, Tal and Belinkov, Yonatan},
  year = {2021},
  month = jun,
  journal = {ACL-IJCNLP},
  eprint = {2106.06087},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.06087},
  url = {http://arxiv.org/abs/2106.06087},
  urldate = {2024-03-18},
  abstract = {Targeted syntactic evaluations have demonstrated the ability of language models to perform subject-verb agreement given difficult contexts. To elucidate the mechanisms by which the models accomplish this behavior, this study applies causal mediation analysis to pre-trained neural language models. We investigate the magnitude of models' preferences for grammatical inflections, as well as whether neurons process subject-verb agreement similarly across sentences with different syntactic structures. We uncover similarities and differences across architectures and model sizes -- notably, that larger models do not necessarily learn stronger preferences. We also observe two distinct mechanisms for producing subject-verb agreement depending on the syntactic structure of the input sentence. Finally, we find that language models rely on similar sets of neurons when given sentences with similar syntactic structure.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/JFBYNWIA/Finlayson et al. - 2021 - Causal Analysis of Syntactic Agreement Mechanisms .pdf}
}

@misc{fiotto-kaufman_nnsight_2023,
  title = {nnsight: The package for interpreting and manipulating the internals of deep learned models.},
  author = {{Fiotto-Kaufman}, Jaden},
  year = {2023},
  url = {https://github.com/JadenFiotto-Kaufman/nnsight},
  copyright = {MIT},
  keywords = {cited}
}

@article{fluri_evaluating_2023,
  title = {Evaluating Superhuman Models with Consistency Checks},
  author = {Fluri, Lukas and Paleka, Daniel and Tram{\`e}r, Florian},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2306.09983},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2306.09983},
  url = {http://arxiv.org/abs/2306.09983},
  urldate = {2023-08-26},
  abstract = {If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to semantically identical boards; GPT-4 forecasting that sports records will evolve non-monotonically over time; or an AI judge assigning bail to a defendant only after we add a felony to their criminal record.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/8EXC96A5/Fluri et al. - 2023 - Evaluating Superhuman Models with Consistency Chec.pdf}
}

@article{foote_n2g_2023,
  title = {N2G: A Scalable Approach for Quantifying Interpretable Neuron Representations in Large Language Models},
  shorttitle = {N2G},
  author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ionnis and Barez, Fazl},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.12918},
  url = {https://arxiv.org/abs/2304.12918},
  urldate = {2023-07-31},
  abstract = {Understanding the function of individual neurons within language models is essential for mechanistic interpretability research. We propose \${\textbackslash}textbf\{Neuron to Graph (N2G)\}\$, a tool which takes a neuron and its dataset examples, and automatically distills the neuron's behaviour on those examples to an interpretable graph. This presents a less labour intensive approach to interpreting neurons than current manual methods, that will better scale these methods to Large Language Models (LLMs). We use truncation and saliency methods to only present the important tokens, and augment the dataset examples with more diverse samples to better capture the extent of neuron behaviour. These graphs can be visualised to aid manual interpretation by researchers, but can also output token activations on text to compare to the neuron's ground truth activations for automatic validation. N2G represents a step towards scalable interpretability methods by allowing us to convert neurons in an LLM to interpretable representations of measurable quality.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {mechinterp,neuron,not cited,scale,to cite,to extract figures,to extract related work,to review in detail,tool},
  file = {/Users/leonardbereska/Zotero/storage/KGCAGVIV/Foote et al. - 2023 - N2G A Scalable Approach for Quantifying Interpret.pdf}
}

@article{foote_neuron_2023,
  title = {Neuron to Graph: Interpreting Language Model Neurons at Scale},
  shorttitle = {Neuron to Graph},
  author = {Foote, Alex and Nanda, Neel and Kran, Esben and Konstas, Ioannis and Cohen, Shay and Barez, Fazl},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2305.19911},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.19911},
  url = {http://arxiv.org/abs/2305.19911},
  urldate = {2023-08-24},
  abstract = {Advances in Large Language Models (LLMs) have led to remarkable capabilities, yet their inner mechanisms remain largely unknown. To understand these models, we need to unravel the functions of individual neurons and their contribution to the network. This paper introduces a novel automated approach designed to scale interpretability techniques across a vast array of neurons within LLMs, to make them more interpretable and ultimately safe. Conventional methods require examination of examples with strong neuron activation and manual identification of patterns to decipher the concepts a neuron responds to. We propose Neuron to Graph (N2G), an innovative tool that automatically extracts a neuron's behaviour from the dataset it was trained on and translates it into an interpretable graph. N2G uses truncation and saliency methods to emphasise only the most pertinent tokens to a neuron while enriching dataset examples with diverse samples to better encompass the full spectrum of neuron behaviour. These graphs can be visualised to aid researchers' manual interpretation, and can generate token activations on text for automatic validation by comparison with the neuron's ground truth activations, which we use to show that the model is better at predicting neuron activation than two baseline methods. We also demonstrate how the generated graph representations can be flexibly used to facilitate further automation of interpretability research, by searching for neurons with particular properties, or programmatically comparing neurons to each other to identify similar neurons. Our method easily scales to build graph representations for all neurons in a 6-layer Transformer model using a single Tesla T4 GPU, allowing for wide usability. We release the code and instructions for use at https://github.com/alexjfoote/Neuron2Graph.},
  archiveprefix = {arxiv},
  keywords = {cited,graph,mechinterp,neuron,scale,to cite,to extract figures,to extract related work,to review in detail,tool},
  file = {/Users/leonardbereska/Zotero/storage/7F77K8R8/Foote et al. - 2023 - Neuron to Graph Interpreting Language Model Neuro.pdf}
}

@article{frankle_dissecting_2019,
  title = {Dissecting Pruned Neural Networks},
  author = {Frankle, Jonathan and Bau, David},
  year = {2019},
  month = jun,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Dissecting-Pruned-Neural-Networks-Frankle-Bau/00035282b3fc11736967f03b6750b2ffc5fef42f},
  urldate = {2023-09-18},
  abstract = {Pruning is a standard technique for removing unnecessary structure from a neural network to reduce its storage footprint, computational demands, or energy consumption. Pruning can reduce the parameter-counts of many state-of-the-art neural networks by an order of magnitude without compromising accuracy, meaning these networks contain a vast amount of unnecessary structure. In this paper, we study the relationship between pruning and interpretability. Namely, we consider the effect of removing unnecessary structure on the number of hidden units that learn disentangled representations of human-recognizable concepts as identified by network dissection. We aim to evaluate how the interpretability of pruned neural networks changes as they are compressed. We find that pruning has no detrimental effect on this measure of interpretability until so few parameters remain that accuracy beings to drop. Resnet-50 models trained on ImageNet maintain the same number of interpretable concepts and units until more than 90\% of parameters have been pruned.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QMINH4SJ/Frankle and Bau - 2019 - Dissecting Pruned Neural Networks.pdf}
}

@article{frankle_linear_2019,
  title = {Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2019},
  month = nov,
  journal = {ICML},
  eprint = {1912.05671},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1912.05671},
  url = {http://arxiv.org/abs/1912.05671},
  urldate = {2023-11-02},
  abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KVRI44D6/Frankle et al. - 2019 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf}
}

@article{frankle_lottery_2019,
  title = {The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
  shorttitle = {The Lottery Ticket Hypothesis},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  journal = {ICLR},
  url = {http://arxiv.org/abs/1803.03635},
  urldate = {2022-02-01},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GEVTJKSH/Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf}
}

@article{friedman_interpretability_2023,
  title = {Interpretability illusions in the generalization of simplified models},
  author = {Friedman, Dan and Lampinen, Andrew and Dixon, Lucas and Chen, Danqi and Ghandeharioun, Asma},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2312.03656},
  pages = {null},
  doi = {10.48550/arXiv.2312.03656},
  url = {https://www.semanticscholar.org/paper/e2bc390cf21dc319ea5aa9a7c3a223911dbf2012},
  abstract = {A common method to study deep learning systems is to use simplified model representations -- for example, using singular value decomposition to visualize the model's hidden states in a lower dimensional space. This approach assumes that the results of these simplified are faithful to the original model. Here, we illustrate an important caveat to this assumption: even if the simplified representations can accurately approximate the full model on the training set, they may fail to accurately capture the model's behavior out of distribution -- the understanding developed from simplified representations may be an illusion. We illustrate this by training Transformer models on controlled datasets with systematic generalization splits. First, we train models on the Dyck balanced-parenthesis languages. We simplify these models using tools like dimensionality reduction and clustering, and then explicitly test how these simplified proxies match the behavior of the original model on various out-of-distribution test sets. We find that the simplified proxies are generally less faithful out of distribution. In cases where the original model generalizes to novel structures or deeper depths, the simplified versions may fail, or generalize better. This finding holds even if the simplified representations do not directly depend on the training distribution. Next, we study a more naturalistic task: predicting the next character in a dataset of computer code. We find similar generalization gaps between the original model and simplified proxies, and conduct further analysis to investigate which aspects of the code completion task are associated with the largest gaps. Together, our results raise questions about the extent to which mechanistic interpretations derived using tools like SVD can reliably predict what a model will do in novel situations.},
  arxivid = {2312.03656},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/LFNPC7YC/Friedman et al. - 2023 - Interpretability illusions in the generalization o.pdf}
}

@article{friedman_learning_2023,
  title = {Learning Transformer Programs},
  author = {Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  year = {2023},
  month = jun,
  journal = {NeurIPS},
  eprint = {2306.01128},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.01128},
  url = {http://arxiv.org/abs/2306.01128},
  urldate = {2023-07-03},
  abstract = {Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck-languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the ``circuits'' used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.},
  archiveprefix = {arxiv},
  keywords = {algorithms,cited,intrinsic,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/I8LH2J8K/Friedman et al. - 2023 - Learning Transformer Programs.pdf}
}

@article{fu_hungry_2022,
  title = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
  shorttitle = {Hungry Hungry Hippos},
  author = {Fu, Daniel Y. and Dao, Tri and Saab, Khaled K. and Thomas, Armin W. and Rudra, Atri and R{\'e}, Christopher},
  year = {2023},
  journal = {ICLR},
  eprint = {2212.14052},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.14052},
  url = {http://arxiv.org/abs/2212.14052},
  urldate = {2023-12-05},
  abstract = {State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2\${\textbackslash}times\$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4\${\textbackslash}times\$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/L5JXGNQF/Fu et al. - 2022 - Hungry Hungry Hippos Towards Language Modeling wi.pdf}
}

@article{fu_transformers_2023,
  title = {Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models},
  shorttitle = {Transformers Learn Higher-Order Optimization Methods for In-Context Learning},
  author = {Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.17086},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.17086},
  url = {http://arxiv.org/abs/2310.17086},
  urldate = {2023-10-27},
  abstract = {Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of convergence with high-order methods such as Iterative Newton, which are exponentially faster than Gradient Descent. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, we show theoretical results which support our empirical findings and have a close correspondence with them: we prove that Transformers can implement \$k\$ iterations of Newton's method with \${\textbackslash}mathcal\{O\}(k)\$ layers.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/VHH9NYPY/Fu et al. - 2023 - Transformers Learn Higher-Order Optimization Metho.pdf}
}

@article{furuta_interpreting_2024,
  title = {Interpreting Grokked Transformers in Complex Modular Arithmetic},
  author = {Furuta, Hiroki and Minegishi, Gouki and Iwasawa, Yusuke and Matsuo, Yutaka},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.16726},
  url = {https://arxiv.org/abs/2402.16726},
  urldate = {2024-06-10},
  abstract = {Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Frequency Sparsity and Fourier Coefficient Ratio, which not only indicate the late generalization but also characterize distinctive internal representations of grokked models per modular operation. Our empirical analysis emphasizes the importance of holistic evaluation among various combinations.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/N3SHVQBC/Furuta et al. - 2024 - Interpreting Grokked Transformers in Complex Modul.pdf}
}

@article{gal_dropout_2016,
  title = {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  shorttitle = {Dropout as a Bayesian Approximation},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = jun,
  journal = {ICML},
  url = {https://proceedings.mlr.press/v48/gal16.html},
  urldate = {2022-06-17},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZNLMIG9Y/Gal and Ghahramani - 2016 - Dropout as a Bayesian Approximation Representing .pdf}
}

@article{gandelsman_interpreting_2023,
  title = {Interpreting CLIP's Image Representation via Text-Based Decomposition},
  author = {Gandelsman, Yossi and Efros, Alexei A. and Steinhardt, Jacob},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.05916},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.05916},
  url = {http://arxiv.org/abs/2310.05916},
  urldate = {2023-11-02},
  abstract = {We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RFJN6PRN/Gandelsman et al. - 2023 - Interpreting CLIP's Image Representation via Text-.pdf}
}

@article{gandelsman_interpreting_2024,
  title = {Interpreting the Second-Order Effects of Neurons in CLIP},
  author = {Gandelsman, Yossi and Efros, Alexei A. and Steinhardt, Jacob},
  year = {2024},
  month = jun,
  url = {https://www.semanticscholar.org/paper/Interpreting-the-Second-Order-Effects-of-Neurons-in-Gandelsman-Efros/1b240f7cfc5f5f5f2bf8d99dc622a3c405837a73},
  urldate = {2024-06-10},
  abstract = {We interpret the function of individual neurons in CLIP by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP. Therefore, we present the"second-order lens", analyzing the effect flowing from a neuron through the later attention heads, directly to the output. We find that these effects are highly selective: for each neuron, the effect is significant for{$<$}2\% of the images. Moreover, each effect can be approximated by a single direction in the text-image space of CLIP. We describe neurons by decomposing these directions into sparse sets of text representations. The sets reveal polysemantic behavior - each neuron corresponds to multiple, often unrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we mass-produce"semantic"adversarial examples by generating images with concepts spuriously correlated to the incorrect class. Additionally, we use the second-order effects for zero-shot segmentation and attribute discovery in images. Our results indicate that a scalable understanding of neurons can be used for model deception and for introducing new model capabilities.},
  file = {/Users/leonardbereska/Zotero/storage/EEZIEKWV/Gandelsman et al. - 2024 - Interpreting the Second-Order Effects of Neurons i.pdf}
}

@article{gandikota_erasing_2023,
  title = {Erasing Concepts from Diffusion Models},
  author = {Gandikota, Rohit and Materzynska, Joanna and {Fiotto-Kaufman}, Jaden and Bau, David},
  year = {2023},
  month = jun,
  journal = {ICCV},
  eprint = {2303.07345},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.07345},
  url = {http://arxiv.org/abs/2303.07345},
  urldate = {2023-11-10},
  abstract = {Motivated by recent advancements in text-to-image diffusion, we study erasure of specific concepts from the model's weights. While Stable Diffusion has shown promise in producing explicit or realistic artwork, it has raised concerns regarding its potential for misuse. We propose a fine-tuning method that can erase a visual concept from a pre-trained diffusion model, given only the name of the style and using negative guidance as a teacher. We benchmark our method against previous approaches that remove sexually explicit content and demonstrate its effectiveness, performing on par with Safe Latent Diffusion and censored training. To evaluate artistic style removal, we conduct experiments erasing five modern artists from the network and conduct a user study to assess the human perception of the removed styles. Unlike previous methods, our approach can remove concepts from a diffusion model permanently rather than modifying the output at the inference time, so it cannot be circumvented even if a user has access to model weights. Our code, data, and results are available at https://erasing.baulab.info/},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/64BX33B3/Gandikota et al. - 2023 - Erasing Concepts from Diffusion Models.pdf}
}

@article{ganguli_capacity_2023,
  title = {The Capacity for Moral Self-Correction in Large Language Models},
  author = {Ganguli, Deep and Askell, Amanda and Schiefer, Nicholas and Liao, Thomas I. and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and Olsson, Catherine and Hernandez, Danny and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Perez, Ethan and Kernion, Jackson and Kerr, Jamie and Mueller, Jared and Landau, Joshua and Ndousse, Kamal and Nguyen, Karina and Lovitt, Liane and Sellitto, Michael and Elhage, Nelson and Mercado, Noemi and DasSarma, Nova and Rausch, Oliver and Lasenby, Robert and Larson, Robin and Ringer, Sam and Kundu, Sandipan and Kadavath, Saurav and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and {Telleen-Lawton}, Timothy and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Mann, Ben and Amodei, Dario and Joseph, Nicholas and McCandlish, Sam and Brown, Tom and Olah, Christopher and Clark, Jack and Bowman, Samuel R. and Kaplan, Jared},
  year = {2023},
  month = feb,
  journal = {CoRR},
  eprint = {2302.07459},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2302.07459},
  url = {http://arxiv.org/abs/2302.07459},
  urldate = {2024-06-10},
  abstract = {We test the hypothesis that language models trained with reinforcement learning from human feedback (RLHF) have the capability to "morally self-correct" -- to avoid producing harmful outputs -- if instructed to do so. We find strong evidence in support of this hypothesis across three different experiments, each of which reveal different facets of moral self-correction. We find that the capability for moral self-correction emerges at 22B model parameters, and typically improves with increasing model size and RLHF training. We believe that at this level of scale, language models obtain two capabilities that they can use for moral self-correction: (1) they can follow instructions and (2) they can learn complex normative concepts of harm like stereotyping, bias, and discrimination. As such, they can follow instructions to avoid certain kinds of morally harmful outputs. We believe our results are cause for cautious optimism regarding the ability to train language models to abide by ethical principles.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/XGJNFW2A/Ganguli et al. - 2023 - The Capacity for Moral Self-Correction in Large La.pdf}
}

@article{ganguli_predictability_2022,
  title = {Predictability and Surprise in Large Generative Models},
  author = {Ganguli, Deep and Hernandez, Danny and Lovitt, Liane and DasSarma, Nova and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Kernion, Jackson and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Elhage, Nelson and Showk, Sheer El and Fort, Stanislav and {Hatfield-Dodds}, Zac and Johnston, Scott and Kravec, Shauna and Nanda, Neel and Ndousse, Kamal and Olsson, Catherine and Amodei, Daniela and Amodei, Dario and Brown, Tom and Kaplan, Jared and McCandlish, Sam and Olah, Chris and Clark, Jack},
  year = {2022},
  journal = {ACM FAccT},
  eprint = {2202.07785},
  primaryclass = {cs},
  doi = {10.1145/3531146.3533229},
  url = {http://arxiv.org/abs/2202.07785},
  urldate = {2024-02-26},
  abstract = {Large-scale pre-training has recently emerged as a technique for creating capable, general purpose, generative models such as GPT-3, Megatron-Turing NLG, Gopher, and many others. In this paper, we highlight a counterintuitive property of such models and discuss the policy implications of this property. Namely, these generative models have an unusual combination of predictable loss on a broad training distribution (as embodied in their "scaling laws"), and unpredictable specific capabilities, inputs, and outputs. We believe that the high-level predictability and appearance of useful capabilities drives rapid development of such models, while the unpredictable qualities make it difficult to anticipate the consequences of model deployment. We go through examples of how this combination can lead to socially harmful behavior with examples from the literature and real world observations, and we also perform two novel experiments to illustrate our point about harms from unpredictability. Furthermore, we analyze how these conflicting properties combine to give model developers various motivations for deploying these models, and challenges that can hinder deployment. We conclude with a list of possible interventions the AI community may take to increase the chance of these models having a beneficial impact. We intend this paper to be useful to policymakers who want to understand and regulate AI systems, technologists who care about the potential policy impact of their work, and academics who want to analyze, critique, and potentially develop large generative models.},
  archiveprefix = {arxiv},
  keywords = {to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/EZZJR3BA/Ganguli et al. - 2022 - Predictability and Surprise in Large Generative Mo.pdf}
}

@article{ganguly_review_2023,
  title = {A Review of the Role of Causality in Developing Trustworthy AI Systems},
  author = {Ganguly, Niloy and Fazlija, Dren and Badar, Maryam and Fisichella, Marco and Sikdar, Sandipan and Schrader, Johanna and Wallat, Jonas and Rudra, Koustav and Koubarakis, Manolis and Patro, Gourab K. and Amri, Wadhah Zai El and Nejdl, Wolfgang},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.06975},
  url = {https://arxiv.org/abs/2302.06975},
  urldate = {2024-02-02},
  abstract = {State-of-the-art AI models largely lack an understanding of the cause-effect relationship that governs human understanding of the real world. Consequently, these models do not generalize to unseen data, often produce unfair results, and are difficult to interpret. This has led to efforts to improve the trustworthiness aspects of AI models. Recently, causal modeling and inference methods have emerged as powerful tools. This review aims to provide the reader with an overview of causal methods that have been developed to improve the trustworthiness of AI models. We hope that our contribution will motivate future research on causality-based solutions for trustworthy AI.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {causal,interpretability,not cited,review},
  file = {/Users/leonardbereska/Zotero/storage/LMUY5MJC/Ganguly et al. - 2023 - A Review of the Role of Causality in Developing Tr.pdf}
}

@article{gao_scaling_2022,
  title = {Scaling Laws for Reward Model Overoptimization},
  author = {Gao, Leo and Schulman, John and Hilton, Jacob},
  year = {2022},
  month = oct,
  journal = {ICML},
  eprint = {2210.10760},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2210.10760},
  url = {http://arxiv.org/abs/2210.10760},
  urldate = {2023-08-26},
  abstract = {In reinforcement learning from human feedback, it is common to optimize against a reward model trained to predict human preferences. Because the reward model is an imperfect proxy, optimizing its value too much can hinder ground truth performance, in accordance with Goodhart's law. This effect has been frequently observed, but not carefully measured due to the expense of collecting human preference data. In this work, we use a synthetic setup in which a fixed "gold-standard" reward model plays the role of humans, providing labels used to train a proxy reward model. We study how the gold reward model score changes as we optimize against the proxy reward model using either reinforcement learning or best-of-\$n\$ sampling. We find that this relationship follows a different functional form depending on the method of optimization, and that in both cases its coefficients scale smoothly with the number of reward model parameters. We also study the effect on this relationship of the size of the reward model dataset, the number of reward model and policy parameters, and the coefficient of the KL penalty added to the reward in the reinforcement learning setup. We explore the implications of these empirical results for theoretical considerations in AI alignment.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/D3BDX8H4/Gao et al. - 2022 - Scaling Laws for Reward Model Overoptimization.pdf}
}

@article{gao_scaling_2024,
  title = {Scaling and evaluating sparse autoencoders},
  author = {Gao, Leo and la Tour, Tom Dupr'e and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, I. and Leike, J. and Wu, Jeffrey},
  year = {2024},
  month = jun,
  url = {https://www.semanticscholar.org/paper/Scaling-and-evaluating-sparse-autoencoders-Gao-Tour/0d24c5680efe9769e6d7ea579db0499ec819a8c7},
  urldate = {2024-06-10},
  abstract = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.},
  file = {/Users/leonardbereska/Zotero/storage/BGNI7H36/Gao et al. - 2024 - Scaling and evaluating sparse autoencoders.pdf}
}

@article{garde_deepdecipher_2023,
  title = {DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models},
  shorttitle = {DeepDecipher},
  author = {Garde, Albert and Kran, Esben and Barez, Fazl},
  year = {2023},
  month = oct,
  journal = {NeurIPS Workshop XAIA},
  eprint = {2310.01870},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.01870},
  url = {http://arxiv.org/abs/2310.01870},
  urldate = {2023-11-16},
  abstract = {As large language models (LLMs) become more capable, there is an urgent need for interpretable and transparent tools. Current methods are difficult to implement, and accessible tools to analyze model internals are lacking. To bridge this gap, we present DeepDecipher - an API and interface for probing neurons in transformer models' MLP layers. DeepDecipher makes the outputs of advanced interpretability techniques for LLMs readily available. The easy-to-use interface also makes inspecting these complex models more intuitive. This paper outlines DeepDecipher's design and capabilities. We demonstrate how to analyze neurons, compare models, and gain insights into model behavior. For example, we contrast DeepDecipher's functionality with similar tools like Neuroscope and OpenAI's Neuron Explainer. DeepDecipher enables efficient, scalable analysis of LLMs. By granting access to state-of-the-art interpretability methods, DeepDecipher makes LLMs more transparent, trustworthy, and safe. Researchers, engineers, and developers can quickly diagnose issues, audit systems, and advance the field.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NINZUP9F/Garde et al. - 2023 - DeepDecipher Accessing and Investigating Neuron A.pdf}
}

@book{gardenfors_conceptual_2004,
  title = {Conceptual spaces: The geometry of thought},
  author = {Gardenfors, Peter},
  year = {2004},
  publisher = {MIT press}
}

@article{gardner_optimality_2019,
  title = {Optimality and heuristics in perceptual neuroscience},
  author = {Gardner, Justin L.},
  year = {2019},
  month = apr,
  journal = {Nat Neurosci},
  volume = {22},
  number = {4},
  pages = {514--523},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0340-4},
  url = {https://www.nature.com/articles/s41593-019-0340-4},
  urldate = {2024-01-19},
  abstract = {The foundation for modern understanding of how we make perceptual decisions about what we see or where to look comes from considering the optimal way to perform these behaviors. While statistical computation is useful for deriving the optimal solution to a perceptual problem, optimality requires perfect knowledge of priors and often complex computation. Accumulating evidence, however, suggests that optimal perceptual goals can be achieved or approximated more simply by human observers using heuristic approaches. Perceptual neuroscientists captivated by optimal explanations of sensory behaviors will fail in their search for the neural circuits and cortical processes that implement an optimal computation whenever that behavior is actually achieved through heuristics. This article provides a cross-disciplinary review of decision-making with the aim of building perceptual theory that uses optimality to set the computational goals for perceptual behavior but, through consideration of ecological, computational, and energetic constraints, incorporates how these optimal goals can be achieved through heuristic approximation.},
  copyright = {2019 Springer Nature America, Inc.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BLSTDCGE/Gardner - 2019 - Optimality and heuristics in perceptual neuroscien.pdf}
}

@article{garfinkle_uniqueness_2019,
  title = {On the Uniqueness and Stability of Dictionaries for Sparse Representation of Noisy Signals},
  author = {Garfinkle, Charles J. and Hillar, Christopher J.},
  year = {2019},
  month = dec,
  journal = {IEEE Transactions on Signal Processing},
  volume = {67},
  number = {23},
  pages = {5884--5892},
  issn = {1941-0476},
  doi = {10.1109/TSP.2019.2935914},
  url = {https://ieeexplore.ieee.org/abstract/document/8805108},
  urldate = {2024-06-12},
  abstract = {Learning optimal dictionaries for sparse coding has exposed characteristic sparse features of many natural signals. However, universal guarantees of the stability of such features in the presence of noise are lacking. Here, we provide very general conditions guaranteeing when dictionaries yielding the sparsest encodings are unique and stable with respect to measurement or modeling error. We demonstrate that some or all original dictionary elements are recoverable from noisy data even if the dictionary fails to satisfy the spark condition, its size is overestimated, or only a polynomial number of distinct sparse supports appear in the data. Importantly, we derive these guarantees without requiring any constraints on the recovered dictionary beyond a natural upper bound on its size. Our results yield an effective procedure sufficient to affirm if a proposed solution to the dictionary learning problem is unique within bounds commensurate with the noise. We suggest applications to data analysis, engineering, and neuroscience and close with some remaining challenges left open by our work.},
  file = {/Users/leonardbereska/Zotero/storage/3B5V25IA/Garfinkle and Hillar - 2019 - On the Uniqueness and Stability of Dictionaries fo.pdf;/Users/leonardbereska/Zotero/storage/7PGCRKJL/8805108.html}
}

@article{garrod_how_2022,
  title = {How Interpretability can be Impactful},
  author = {Garrod, Connall},
  year = {2022},
  month = jul,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/Cj4hWE2xBf7t8nKkk/how-interpretability-can-be-impactful},
  urldate = {2023-11-29},
  abstract = {This post was written as part of the Stanford Existential Risks Initiative ML Alignment Theory Scholars (MATS) program. thanks to Evan Hubinger for i{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FIG9HEFF/Garrod - 2022 - How Interpretability can be Impactful.html}
}

@article{ge_automatically_2024,
  title = {Automatically Identifying Local and Global Circuits with Linear Computation Graphs},
  author = {Ge, Xuyang and Zhu, Fukang and Shu, Wentao and Wang, Junxuan and He, Zhengfu and Qiu, Xipeng},
  year = {2024},
  month = may,
  url = {https://www.semanticscholar.org/paper/Automatically-Identifying-Local-and-Global-Circuits-Ge-Zhu/9e08a7385a3908ecfaa7886c8597f8c533672ca0},
  urldate = {2024-06-10},
  abstract = {Circuit analysis of any certain model behavior is a central task in mechanistic interpretability. We introduce our circuit discovery pipeline with sparse autoencoders (SAEs) and a variant called skip SAEs. With these two modules inserted into the model, the model's computation graph with respect to OV and MLP circuits becomes strictly linear. Our methods do not require linear approximation to compute the causal effect of each node. This fine-grained graph enables identifying both end-to-end and local circuits accounting for either logits or intermediate features. We can scalably apply this pipeline with a technique called Hierarchical Attribution. We analyze three kind of circuits in GPT2-Small, namely bracket, induction and Indirect Object Identification circuits. Our results reveal new findings underlying existing discoveries.},
  file = {/Users/leonardbereska/Zotero/storage/KJVQHJBS/Ge et al. - 2024 - Automatically Identifying Local and Global Circuit.pdf}
}

@article{geiger_causal_2021,
  title = {Causal Abstractions of Neural Networks},
  author = {Geiger, Atticus and Lu, Hanson and Icard, Thomas and Potts, Christopher},
  year = {2021},
  journal = {NeurIPS},
  volume = {34},
  pages = {9574--9586},
  url = {https://proceedings.neurips.cc/paper/2021/hash/4f5c422f4d49a5a807eda27434231040-Abstract.html},
  urldate = {2023-08-29},
  abstract = {Structural analysis methods (e.g., probing and feature attribution) are increasingly important tools for neural network analysis. We propose a new structural analysis method grounded in a formal theory of causal abstraction that provides rich characterizations of model-internal representations and their roles in input/output behavior. In this method, neural representations are aligned with variables in interpretable causal models, and then interchange interventions are used to experimentally verify that the neural representations have the causal properties of their aligned variables. We apply this method in a case study to analyze neural models trained on Multiply Quantified Natural Language Inference (MQNLI) corpus, a highly complex NLI dataset that was constructed with a tree-structured natural logic causal model. We discover that a BERT-based model with state-of-the-art performance successfully realizes parts of the natural logic model's causal structure, whereas a simpler baseline model fails to show any such structure, demonstrating that neural representations encode the compositional structure of MQNLI examples.},
  keywords = {cited,mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/W99MPQDQ/Geiger et al. - 2021 - Causal Abstractions of Neural Networks.pdf}
}

@article{geiger_causal_2023,
  title = {Causal Abstraction for Faithful Model Interpretation},
  author = {Geiger, Atticus and Potts, Chris and Icard, Thomas},
  year = {2023},
  month = jan,
  journal = {CoRR},
  eprint = {2301.04709},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2301.04709},
  url = {http://arxiv.org/abs/2301.04709},
  urldate = {2023-08-27},
  abstract = {A faithful and interpretable explanation of an AI model's behavior and internal structure is a high-level explanation that is human-intelligible but also consistent with the known, but often opaque low-level causal details of the model. We argue that the theory of causal abstraction provides the mathematical foundations for the desired kinds of model explanations. In causal abstraction analysis, we use interventions on model-internal states to rigorously assess whether an interpretable high-level causal model is a faithful description of an AI model. Our contributions in this area are: (1) We generalize causal abstraction to cyclic causal structures and typed high-level variables. (2) We show how multi-source interchange interventions can be used to conduct causal abstraction analyses. (3) We define a notion of approximate causal abstraction that allows us to assess the degree to which a high-level causal model is a causal abstraction of a lower-level one. (4) We prove constructive causal abstraction can be decomposed into three operations we refer to as marginalization, variable-merge, and value-merge. (5) We formalize the XAI methods of LIME, causal effect estimation, causal mediation analysis, iterated nullspace projection, and circuit-based explanations as special cases of causal abstraction analysis.},
  archiveprefix = {arxiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/PKIVIPAG/Geiger et al. - 2023 - Causal Abstraction for Faithful Model Interpretati.pdf}
}

@article{geiger_finding_2023,
  title = {Finding Alignments Between Interpretable Causal Variables and Distributed Neural Representations},
  author = {Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah D.},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.02536},
  url = {https://arxiv.org/abs/2303.02536},
  urldate = {2023-08-27},
  abstract = {Causal abstraction is a promising theoretical framework for explainable artificial intelligence that defines when an interpretable high-level causal model is a faithful simplification of a low-level deep learning system. However, existing causal abstraction methods have two major limitations: they require a brute-force search over alignments between the high-level model and the low-level one, and they presuppose that variables in the high-level model will align with disjoint sets of neurons in the low-level one. In this paper, we present distributed alignment search (DAS), which overcomes these limitations. In DAS, we find the alignment between high-level and low-level models using gradient descent rather than conducting a brute-force search, and we allow individual neurons to play multiple distinct roles by analyzing representations in non-standard bases-distributed representations. Our experiments show that DAS can discover internal structure that prior approaches miss. Overall, DAS removes previous obstacles to conducting causal abstraction analyses and allows us to find conceptual structure in trained neural nets.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/IWLKQJ9V/Geiger et al. - 2023 - Finding Alignments Between Interpretable Causal Va.pdf;/Users/leonardbereska/Zotero/storage/MTMCAQFJ/Geiger et al. - 2024 - Finding Alignments Between Interpretable Causal Va.pdf}
}

@article{geiger_inducing_2021,
  title = {Inducing Causal Structure for Interpretable Neural Networks},
  author = {Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah D. and Potts, Christopher},
  year = {2021},
  month = jan,
  journal = {ICML},
  eprint = {2112.00826},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2112.00826},
  url = {http://arxiv.org/abs/2112.00826},
  urldate = {2023-10-25},
  abstract = {In many areas, we have well-founded insights about causal structure that would be useful to bring into our trained models while still allowing them to learn in a data-driven fashion. To achieve this, we present the new method of interchange intervention training (IIT). In IIT, we (1) align variables in a causal model (e.g., a deterministic program or Bayesian network) with representations in a neural model and (2) train the neural model to match the counterfactual behavior of the causal model on a base input when aligned representations in both models are set to be the value they would be for a source input. IIT is fully differentiable, flexibly combines with other objectives, and guarantees that the target causal model is a causal abstraction of the neural model when its loss is zero. We evaluate IIT on a structural vision task (MNIST-PVR), a navigational language task (ReaSCAN), and a natural language inference task (MQNLI). We compare IIT against multi-task training objectives and data augmentation. In all our experiments, IIT achieves the best results and produces neural models that are more interpretable in the sense that they more successfully realize the target causal model.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/GXM3IT7D/Geiger et al. - 2021 - Inducing Causal Structure for Interpretable Neural.pdf}
}

@article{geiger_neural_2020,
  title = {Neural Natural Language Inference Models Partially Embed Theories of Lexical Entailment and Negation},
  author = {Geiger, Atticus and Richardson, Kyle and Potts, Christopher},
  editor = {Alishahi, Afra and Belinkov, Yonatan and Chrupa{\l}a, Grzegorz and Hupkes, Dieuwke and Pinter, Yuval and Sajjad, Hassan},
  year = {2020},
  month = nov,
  journal = {BlackboxNLP},
  pages = {163--173},
  doi = {10.18653/v1/2020.blackboxnlp-1.16},
  url = {https://aclanthology.org/2020.blackboxnlp-1.16},
  urldate = {2024-03-19},
  abstract = {We address whether neural models for Natural Language Inference (NLI) can learn the compositional interactions between lexical entailment and negation, using four methods: the behavioral evaluation methods of (1) challenge test sets and (2) systematic generalization tasks, and the structural evaluation methods of (3) probes and (4) interventions. To facilitate this holistic evaluation, we present Monotonicity NLI (MoNLI), a new naturalistic dataset focused on lexical entailment and negation. In our behavioral evaluations, we find that models trained on general-purpose NLI datasets fail systematically on MoNLI examples containing negation, but that MoNLI fine-tuning addresses this failure. In our structural evaluations, we look for evidence that our top-performing BERT-based model has learned to implement the monotonicity algorithm behind MoNLI. Probes yield evidence consistent with this conclusion, and our intervention experiments bolster this, showing that the causal dynamics of the model mirror the causal dynamics of this algorithm on subsets of MoNLI. This suggests that the BERT model at least partially embeds a theory of lexical entailment and negation at an algorithmic level.},
  file = {/Users/leonardbereska/Zotero/storage/HHQN9N6U/Geiger et al. - 2020 - Neural Natural Language Inference Models Partially.pdf}
}

@article{geirhos_don_2023,
  title = {Don't trust your eyes: on the (un)reliability of feature visualizations},
  shorttitle = {Don't trust your eyes},
  author = {Geirhos, Robert and Zimmermann, Roland S. and Bilodeau, Blair and Brendel, Wieland and Kim, Been},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.04719},
  url = {https://arxiv.org/abs/2306.04719},
  urldate = {2024-02-11},
  abstract = {How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. This can be used as a sanity check for feature visualizations. We underpin our empirical findings by theory proving that the set of functions that can be reliably understood by feature visualization is extremely small and does not include general black-box neural networks. Therefore, a promising way forward could be the development of networks that enforce certain structures in order to ensure more reliable feature visualizations.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {feature,not cited,to cite,visualization},
  file = {/Users/leonardbereska/Zotero/storage/9RVFRJXT/Geirhos et al. - 2023 - Don't trust your eyes on the (un)reliability of f.pdf}
}

@article{georgiadis_accelerating_2019,
  title = {Accelerating Convolutional Neural Networks via Activation Map Compression},
  author = {Georgiadis, Georgios},
  year = {2019},
  month = mar,
  journal = {CoRR},
  eprint = {1812.04056},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1812.04056},
  url = {http://arxiv.org/abs/1812.04056},
  urldate = {2024-03-20},
  abstract = {The deep learning revolution brought us an extensive array of neural network architectures that achieve state-of-the-art performance in a wide variety of Computer Vision tasks including among others, classification, detection and segmentation. In parallel, we have also been observing an unprecedented demand in computational and memory requirements, rendering the efficient use of neural networks in low-powered devices virtually unattainable. Towards this end, we propose a three-stage compression and acceleration pipeline that sparsifies, quantizes and entropy encodes activation maps of Convolutional Neural Networks. Sparsification increases the representational power of activation maps leading to both acceleration of inference and higher model accuracy. Inception-V3 and MobileNet-V1 can be accelerated by as much as \$1.6{\textbackslash}times\$ with an increase in accuracy of \$0.38{\textbackslash}\%\$ and \$0.54{\textbackslash}\%\$ on the ImageNet and CIFAR-10 datasets respectively. Quantizing and entropy coding the sparser activation maps lead to higher compression over the baseline, reducing the memory cost of the network execution. Inception-V3 and MobileNet-V1 activation maps, quantized to \$16\$ bits, are compressed by as much as \$6{\textbackslash}times\$ with an increase in accuracy of \$0.36{\textbackslash}\%\$ and \$0.55{\textbackslash}\%\$ respectively.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/I57A3D4P/Georgiadis - 2019 - Accelerating Convolutional Neural Networks via Act.pdf}
}

@article{geshkovski_emergence_2023,
  title = {The emergence of clusters in self-attention dynamics},
  author = {Geshkovski, Borjan and Letrouit, Cyril and Polyanskiy, Yury and Rigollet, Philippe},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2305.05465},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.2305.05465},
  url = {http://arxiv.org/abs/2305.05465},
  urldate = {2023-10-30},
  abstract = {Viewing Transformers as interacting particle systems, we describe the geometry of learned representations when the weights are not time dependent. We show that particles, representing tokens, tend to cluster toward particular limiting objects as time tends to infinity. Cluster locations are determined by the initial tokens, confirming context-awareness of representations learned by Transformers. Using techniques from dynamical systems and partial differential equations, we show that the type of limiting object that emerges depends on the spectrum of the value matrix. Additionally, in the one-dimensional case we prove that the self-attention matrix converges to a low-rank Boolean matrix. The combination of these results mathematically confirms the empirical observation made by Vaswani et al. [VSP'17] that leaders appear in a sequence of tokens when processed by Transformers.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YFFDH6HZ/Geshkovski et al. - 2023 - The emergence of clusters in self-attention dynami.pdf}
}

@article{geva_dissecting_2023,
  title = {Dissecting Recall of Factual Associations in Auto-Regressive Language Models},
  author = {Geva, Mor and Bastings, Jasmijn and Filippova, Katja and Globerson, Amir},
  year = {2023},
  month = oct,
  journal = {EMNLP},
  eprint = {2304.14767},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.14767},
  url = {http://arxiv.org/abs/2304.14767},
  urldate = {2023-10-30},
  abstract = {Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation "queries" the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/AMHIECBB/Geva et al. - 2023 - Dissecting Recall of Factual Associations in Auto-.pdf}
}

@article{geva_lmdebugger_2022,
  title = {LM-Debugger: An Interactive Tool for Inspection and Intervention in Transformer-Based Language Models},
  shorttitle = {LM-Debugger},
  author = {Geva, Mor and Caciularu, Avi and Dar, Guy and Roit, Paul and Sadde, Shoval and Shlain, Micah and Tamir, Bar and Goldberg, Yoav},
  year = {2022},
  month = oct,
  journal = {EMNLP},
  eprint = {2204.12130},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2204.12130},
  url = {http://arxiv.org/abs/2204.12130},
  urldate = {2023-11-10},
  abstract = {The opaque nature and unexplained behavior of transformer-based language models (LMs) have spurred a wide interest in interpreting their predictions. However, current interpretation methods mostly focus on probing models from outside, executing behavioral tests, and analyzing salience input features, while the internal prediction construction process is largely not understood. In this work, we introduce LM-Debugger, an interactive debugger tool for transformer-based LMs, which provides a fine-grained interpretation of the model's internal prediction process, as well as a powerful framework for intervening in LM behavior. For its backbone, LM-Debugger relies on a recent method that interprets the inner token representations and their updates by the feed-forward layers in the vocabulary space. We demonstrate the utility of LM-Debugger for single-prediction debugging, by inspecting the internal disambiguation process done by GPT2. Moreover, we show how easily LM-Debugger allows to shift model behavior in a direction of the user's choice, by identifying a few vectors in the network and inducing effective interventions to the prediction process. We release LM-Debugger as an open-source tool and a demo over GPT2 models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/UXE2KSSQ/Geva et al. - 2022 - LM-Debugger An Interactive Tool for Inspection an.pdf}
}

@article{geva_transformer_2021,
  title = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  year = {2021},
  month = sep,
  journal = {CoRR},
  eprint = {2012.14913},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.14913},
  url = {http://arxiv.org/abs/2012.14913},
  urldate = {2023-07-31},
  abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
  archiveprefix = {arxiv},
  keywords = {cited,fundamental,memory,to review in detail,transformer},
  file = {/Users/leonardbereska/Zotero/storage/7RMASLG5/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf}
}

@article{geva_transformer_2022,
  title = {Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space},
  author = {Geva, Mor and Caciularu, Avi and Wang, Kevin Ro and Goldberg, Yoav},
  year = {2022},
  month = oct,
  journal = {EMNLP},
  eprint = {2203.14680},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2203.14680},
  url = {http://arxiv.org/abs/2203.14680},
  urldate = {2023-10-25},
  abstract = {Transformer-based language models (LMs) are at the core of modern NLP, but their internal prediction construction process is opaque and largely not understood. In this work, we make a substantial step towards unveiling this underlying prediction process, by reverse-engineering the operation of the feed-forward network (FFN) layers, one of the building blocks of transformer models. We view the token representation as a changing distribution over the vocabulary, and the output from each FFN layer as an additive update to that distribution. Then, we analyze the FFN updates in the vocabulary space, showing that each update can be decomposed to sub-updates corresponding to single FFN parameter vectors, each promoting concepts that are often human-interpretable. We then leverage these findings for controlling LM predictions, where we reduce the toxicity of GPT2 by almost 50\%, and for improving computation efficiency with a simple early exit rule, saving 20\% of computation on average.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/K6AKE2ZE/Geva et al. - 2022 - Transformer Feed-Forward Layers Build Predictions .pdf}
}

@article{ghandeharioun_patchscopes_2024,
  title = {Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
  shorttitle = {Patchscopes},
  author = {Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.06102},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.06102},
  url = {http://arxiv.org/abs/2401.06102},
  urldate = {2024-01-23},
  abstract = {Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation can be viewed as instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and unlocks new applications such as self-correction in multi-hop reasoning.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/FWKMGD48/Ghandeharioun et al. - 2024 - Patchscopes A Unifying Framework for Inspecting H.pdf;/Users/leonardbereska/Zotero/storage/TUWGCSSW/Ghandeharioun et al. - 2024 - Patchscopes A Unifying Framework for Inspecting H.pdf}
}

@article{ghorbani_neuron_2020,
  title = {Neuron Shapley: Discovering the Responsible Neurons},
  shorttitle = {Neuron Shapley},
  author = {Ghorbani, Amirata and Zou, James},
  year = {2020},
  month = nov,
  journal = {NeurIPS},
  eprint = {2002.09815},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2002.09815},
  url = {http://arxiv.org/abs/2002.09815},
  urldate = {2023-11-10},
  abstract = {We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BILSSY62/Ghorbani and Zou - 2020 - Neuron Shapley Discovering the Responsible Neuron.pdf}
}

@article{giannou_looped_2023,
  title = {Looped Transformers as Programmable Computers},
  author = {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-Yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
  year = {2023},
  month = jul,
  journal = {ICML},
  pages = {11398--11442},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/giannou23a.html},
  urldate = {2024-03-19},
  abstract = {We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including lexicographic operations, non-linear functions, function calls, program counters, and conditional branches. Using this framework, we emulate a computer using a simple instruction-set architecture, which allows us to map iterative algorithms to programs that can be executed by a constant depth looped transformer network. We show how a single frozen transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and even a full backpropagation, in-context learning algorithm. Our findings reveal the potential of transformer networks as programmable compute units and offer insight into the mechanics of attention.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/GKIH9Q66/Giannou et al. - 2023 - Looped Transformers as Programmable Computers.pdf}
}

@article{gilpin_explaining_2019,
  title = {Explaining Explanations: An Overview of Interpretability of Machine Learning},
  shorttitle = {Explaining Explanations},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2019},
  month = feb,
  journal = {IEEE DSAA},
  eprint = {1806.00069},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1806.00069},
  url = {http://arxiv.org/abs/1806.00069},
  urldate = {2023-10-21},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/S29VFXKM/Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf}
}

@article{glanois_survey_2022,
  title = {A Survey on Interpretable Reinforcement Learning},
  author = {Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
  year = {2022},
  month = feb,
  journal = {CoRR},
  eprint = {2112.13112},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2112.13112},
  url = {http://arxiv.org/abs/2112.13112},
  urldate = {2023-08-29},
  abstract = {Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as a property of a model) and explainability (as a post-hoc operation, with the intervention of a proxy) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2DHFHH4K/Glanois et al. - 2022 - A Survey on Interpretable Reinforcement Learning.pdf}
}

@article{glanois_survey_2024,
  title = {A survey on interpretable reinforcement learning},
  author = {Glanois, Claire and Weng, Paul and Zimmer, Matthieu and Li, Dong and Yang, Tianpei and Hao, Jianye and Liu, Wulong},
  year = {2024},
  month = apr,
  journal = {Mach Learn},
  issn = {1573-0565},
  doi = {10.1007/s10994-024-06543-w},
  url = {https://doi.org/10.1007/s10994-024-06543-w},
  urldate = {2024-06-09},
  abstract = {Although deep reinforcement learning has become a promising machine learning approach for sequential decision-making problems, it is still not mature enough for high-stake domains such as autonomous driving or medical applications. In such contexts, a learned policy needs for instance to be interpretable, so that it can be inspected before any deployment (e.g., for safety and verifiability reasons). This survey provides an overview of various approaches to achieve higher interpretability in reinforcement learning (RL). To that aim, we distinguish interpretability (as an intrinsic property of a model) and explainability (as a post-hoc operation) and discuss them in the context of RL with an emphasis on the former notion. In particular, we argue that interpretable RL may embrace different facets: interpretable inputs, interpretable (transition/reward) models, and interpretable decision-making. Based on this scheme, we summarize and analyze recent work related to interpretable RL with an emphasis on papers published in the past 10 years. We also discuss briefly some related research areas and point to some potential promising research directions, notably related to the recent development of foundation models (e.g., large language models, RL from human feedback).},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/XVPTJA5B/Glanois et al. - 2024 - A survey on interpretable reinforcement learning.pdf}
}

@article{goh_multimodal_2021,
  title = {Multimodal Neurons in Artificial Neural Networks},
  author = {Goh, Gabriel and {\dag}, Nick Cammarata and {\dag}, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  year = {2021},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2021/multimodal-neurons},
  urldate = {2023-07-31},
  abstract = {We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.},
  language = {en},
  keywords = {cited,mechinterp,multimodal,neuron,to cite,to extract figures,to extract related work,to review in detail,visualization},
  file = {/Users/leonardbereska/Zotero/storage/2VCH785K/Goh et al. - 2021 - Multimodal Neurons in Artificial Neural Networks.html}
}

@article{gohel_explainable_2021,
  title = {Explainable AI: current status and future directions},
  shorttitle = {Explainable AI},
  author = {Gohel, Prashant and Singh, Priyanka and Mohanty, Manoranjan},
  year = {2021},
  month = jul,
  journal = {CoRR},
  eprint = {2107.07045},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2107.07045},
  url = {http://arxiv.org/abs/2107.07045},
  urldate = {2023-11-06},
  abstract = {Explainable Artificial Intelligence (XAI) is an emerging area of research in the field of Artificial Intelligence (AI). XAI can explain how AI obtained a particular solution (e.g., classification or object detection) and can also answer other "wh" questions. This explainability is not possible in traditional AI. Explainability is essential for critical applications, such as defense, health care, law and order, and autonomous driving vehicles, etc, where the know-how is required for trust and transparency. A number of XAI techniques so far have been purposed for such applications. This paper provides an overview of these techniques from a multimedia (i.e., text, image, audio, and video) point of view. The advantages and shortcomings of these techniques have been discussed, and pointers to some future directions have also been provided.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BX3E2RZH/Gohel et al. - 2021 - Explainable AI current status and future directio.pdf}
}

@article{gokhale_semantic_2023,
  title = {The semantic landscape paradigm for neural networks},
  author = {Gokhale, Shreyas},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.09550},
  url = {https://arxiv.org/abs/2307.09550},
  urldate = {2024-02-11},
  abstract = {Deep neural networks exhibit a fascinating spectrum of phenomena ranging from predictable scaling laws to the unpredictable emergence of new capabilities as a function of training time, dataset size and network size. Analysis of these phenomena has revealed the existence of concepts and algorithms encoded within the learned representations of these networks. While significant strides have been made in explaining observed phenomena separately, a unified framework for understanding, dissecting, and predicting the performance of neural networks is lacking. Here, we introduce the semantic landscape paradigm, a conceptual and mathematical framework that describes the training dynamics of neural networks as trajectories on a graph whose nodes correspond to emergent algorithms that are instrinsic to the learned representations of the networks. This abstraction enables us to describe a wide range of neural network phenomena in terms of well studied problems in statistical physics. Specifically, we show that grokking and emergence with scale are associated with percolation phenomena, and neural scaling laws are explainable in terms of the statistics of random walks on graphs. Finally, we discuss how the semantic landscape paradigm complements existing theoretical and practical approaches aimed at understanding and interpreting deep neural networks.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {emergence,not cited,opinion},
  file = {/Users/leonardbereska/Zotero/storage/4YVMV2ZR/Gokhale - 2023 - The semantic landscape paradigm for neural network.pdf}
}

@article{goldowsky-dill_localizing_2023,
  title = {Localizing Model Behavior with Path Patching},
  author = {{Goldowsky-Dill}, Nicholas and MacLeod, Chris and Sato, Lucas and Arora, Aryaman},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.05969},
  url = {https://arxiv.org/abs/2304.05969},
  urldate = {2023-08-27},
  abstract = {Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/JEGX47TY/Goldowsky-Dill et al. - 2023 - Localizing Model Behavior with Path Patching.pdf}
}

@article{goldstein_language_2023,
  title = {Language agents reduce the risk of existential catastrophe},
  author = {Goldstein, Simon and {Kirk-Giannini}, Cameron Domenico},
  year = {2023},
  month = aug,
  journal = {AI \& Soc},
  issn = {0951-5666, 1435-5655},
  doi = {10.1007/s00146-023-01748-4},
  url = {https://link.springer.com/10.1007/s00146-023-01748-4},
  urldate = {2024-02-11},
  abstract = {: Recent advances in natural language processing have given rise to a new kind of AI architecture: the language agent . By repeatedly calling an LLM to perform a variety of cognitive tasks, language agents are able to function autonomously to pursue goals specified in natural language and stored in a human-readable format. Because of their architecture, language agents exhibit behavior that is predictable according to the laws of folk psychology: they function as though they have desires and beliefs, and then make and update plans to pursue their desires given their beliefs. We argue that the rise of language agents significantly reduces the probability of an existential catastrophe due to loss of control over an AGI. This is because the probability of such an existential catastrophe is proportional to the difficulty of aligning AGI systems, and language agents significantly reduce that difficulty. In particular, language agents help to resolve three important issues related to aligning AIs: reward misspecification, goal misgeneralization, and uninterpretability.},
  language = {en},
  keywords = {agents,LLMs,not cited,to cite,xrisk},
  file = {/Users/leonardbereska/Zotero/storage/NWZZIPJ8/Goldstein and Kirk-Giannini - 2023 - Language agents reduce the risk of existential cat.pdf}
}

@article{golkar_contextual_2024,
  title = {Contextual Counting: A Mechanistic Study of Transformers on a Quantitative Task},
  shorttitle = {Contextual Counting},
  author = {Golkar, Siavash and Bietti, Alberto and Pettee, Mariel and Eickenberg, Michael and Cranmer, Miles and Hirashima, Keiya and Krawezik, Geraud and Lourie, Nicholas and McCabe, Michael and Morel, Rudy and Ohana, Ruben and Parker, Liam Holden and Blancard, Bruno R{\'e}galdo-Saint and Cho, Kyunghyun and Ho, Shirley},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2406.02585},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2406.02585},
  url = {http://arxiv.org/abs/2406.02585},
  urldate = {2024-06-10},
  abstract = {Transformers have revolutionized machine learning across diverse domains, yet understanding their behavior remains crucial, particularly in high-stakes applications. This paper introduces the contextual counting task, a novel toy problem aimed at enhancing our understanding of Transformers in quantitative and scientific contexts. This task requires precise localization and computation within datasets, akin to object detection or region-based scientific analysis. We present theoretical and empirical analysis using both causal and non-causal Transformer architectures, investigating the influence of various positional encodings on performance and interpretability. In particular, we find that causal attention is much better suited for the task, and that no positional embeddings lead to the best accuracy, though rotary embeddings are competitive and easier to train. We also show that out of distribution performance is tightly linked to which tokens it uses as a bias term.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/7YX7WH9T/Golkar et al. - 2024 - Contextual Counting A Mechanistic Study of Transf.pdf}
}

@article{gould_successor_2023,
  title = {Successor Heads: Recurring, Interpretable Attention Heads In The Wild},
  shorttitle = {Successor Heads},
  author = {Gould, Rhys and Ong, Euan and Ogden, George and Conmy, Arthur},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.09230},
  url = {https://arxiv.org/abs/2312.09230},
  urldate = {2024-02-11},
  abstract = {In this work we present successor heads: attention heads that increment tokens with a natural ordering, such as numbers, months, and days. For example, successor heads increment 'Monday' into 'Tuesday'. We explain the successor head behavior with an approach rooted in mechanistic interpretability, the field that aims to explain how models complete tasks in human-understandable terms. Existing research in this area has found interpretable language model components in small toy models. However, results in toy models have not yet led to insights that explain the internals of frontier models and little is currently understood about the internal operations of large language models. In this paper, we analyze the behavior of successor heads in large language models (LLMs) and find that they implement abstract representations that are common to different architectures. They form in LLMs with as few as 31 million parameters, and at least as many as 12 billion parameters, such as GPT-2, Pythia, and Llama-2. We find a set of 'mod-10 features' that underlie how successor heads increment in LLMs across different architectures and sizes. We perform vector arithmetic with these features to edit head behavior and provide insights into numeric representations within LLMs. Additionally, we study the behavior of successor heads on natural language data, identifying interpretable polysemanticity in a Pythia successor head.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/7GAI3W2I/Gould et al. - 2023 - Successor Heads Recurring, Interpretable Attentio.pdf}
}

@article{gregor_learning_2010,
  title = {Learning fast approximations of sparse coding},
  author = {Gregor, Karol and LeCun, Yann},
  year = {2010},
  month = jun,
  journal = {ICML},
  series = {ICML'10},
  pages = {399--406},
  url = {https://icml.cc/Conferences/2010/papers/449.pdf},
  urldate = {2024-03-20},
  abstract = {In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Os-her's for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate "explaining away" to take place during inference. The resulting predictor is differentiable and can be included into globally-trained recognition systems.}
}

@article{griffin_susceptibility_2023,
  title = {Susceptibility to Influence of Large Language Models},
  author = {Griffin, Lewis D. and Kleinberg, Bennett and Mozes, Maximilian and Mai, Kimberly T. and Vau, Maria and Caldwell, Matthew and {Marvor-Parker}, Augustine},
  year = {2023},
  month = mar,
  journal = {CoRR},
  eprint = {2303.06074},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.06074},
  url = {http://arxiv.org/abs/2303.06074},
  urldate = {2023-07-10},
  abstract = {Two studies tested the hypothesis that a Large Language Model (LLM) can be used to model psychological change following exposure to influential input. The first study tested a generic mode of influence - the Illusory Truth Effect (ITE) - where earlier exposure to a statement (through, for example, rating its interest) boosts a later truthfulness test rating. Data was collected from 1000 human participants using an online experiment, and 1000 simulated participants using engineered prompts and LLM completion. 64 ratings per participant were collected, using all exposure-test combinations of the attributes: truth, interest, sentiment and importance. The results for human participants reconfirmed the ITE, and demonstrated an absence of effect for attributes other than truth, and when the same attribute is used for exposure and test. The same pattern of effects was found for LLM-simulated participants. The second study concerns a specific mode of influence - populist framing of news to increase its persuasion and political mobilization. Data from LLM-simulated participants was collected and compared to previously published data from a 15-country experiment on 7286 human participants. Several effects previously demonstrated from the human study were replicated by the simulated study, including effects that surprised the authors of the human study by contradicting their theoretical expectations (anti-immigrant framing of news decreases its persuasion and mobilization); but some significant relationships found in human data (modulation of the effectiveness of populist framing according to relative deprivation of the participant) were not present in the LLM data. Together the two studies support the view that LLMs have potential to act as models of the effect of influence.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LEWLFDD2/Griffin et al. - 2023 - Susceptibility to Influence of Large Language Mode.pdf}
}

@misc{gross_compact_2024,
  title = {Compact Proofs of Model Performance via Mechanistic Interpretability},
  author = {Gross, Jason and Agrawal, Rajashree and Kwa, Thomas and Ong, Euan and Yip, Chun Hei and Gibson, Alex and Noubir, Soufiane and Chan, Lawrence},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11779},
  eprint = {2406.11779},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11779},
  url = {http://arxiv.org/abs/2406.11779},
  urldate = {2024-06-25},
  abstract = {In this work, we propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving lower bounds on the accuracy of 151 small transformers trained on a Max-of-\$K\$ task. We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless noise as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/RTJBN44E/Gross et al. - 2024 - Compact Proofs of Model Performance via Mechanisti.pdf;/Users/leonardbereska/Zotero/storage/Y58C85LN/2406.html}
}

@misc{gross_compact_2024a,
  title = {Compact Proofs of Model Performance via Mechanistic Interpretability},
  author = {Gross, Jason and Agrawal, Rajashree and Kwa, Thomas and Ong, Euan and Yip, Chun Hei and Gibson, Alex and Noubir, Soufiane and Chan, Lawrence},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11779},
  eprint = {2406.11779},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11779},
  url = {http://arxiv.org/abs/2406.11779},
  urldate = {2024-07-03},
  abstract = {In this work, we propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving lower bounds on the accuracy of 151 small transformers trained on a Max-of-\$K\$ task. We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless noise as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/CDW4KSI5/Gross et al. - 2024 - Compact Proofs of Model Performance via Mechanisti.pdf;/Users/leonardbereska/Zotero/storage/W54CXE4L/2406.html}
}

@article{grosse_studying_2023,
  title = {Studying Large Language Model Generalization with Influence Functions},
  author = {Grosse, Roger and Bae, Juhan and Anil, Cem and Elhage, Nelson and Tamkin, Alex and Tajdini, Amirhossein and Steiner, Benoit and Li, Dustin and Durmus, Esin and Perez, Ethan and Hubinger, Evan and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Joseph, Nicholas and McCandlish, Sam and Kaplan, Jared and Bowman, Samuel R.},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2308.03296},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2308.03296},
  url = {http://arxiv.org/abs/2308.03296},
  urldate = {2023-10-27},
  abstract = {When trying to gain better visibility into a machine learning model in order to understand and mitigate the associated risks, a potentially valuable source of evidence is: which training examples most contribute to a given behavior? Influence functions aim to answer a counterfactual: how would the model's parameters (and hence its outputs) change if a given sequence were added to the training set? While influence functions have produced insights for small models, they are difficult to scale to large language models (LLMs) due to the difficulty of computing an inverse-Hessian-vector product (IHVP). We use the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC) approximation to scale influence functions up to LLMs with up to 52 billion parameters. In our experiments, EK-FAC achieves similar accuracy to traditional influence function estimators despite the IHVP computation being orders of magnitude faster. We investigate two algorithmic techniques to reduce the cost of computing gradients of candidate training sequences: TF-IDF filtering and query batching. We use influence functions to investigate the generalization patterns of LLMs, including the sparsity of the influence patterns, increasing abstraction with scale, math and programming abilities, cross-lingual generalization, and role-playing behavior. Despite many apparently sophisticated forms of generalization, we identify a surprising limitation: influences decay to near-zero when the order of key phrases is flipped. Overall, influence functions give us a powerful new tool for studying the generalization properties of LLMs.},
  archiveprefix = {arxiv},
  keywords = {not cited,to read},
  file = {/Users/leonardbereska/Zotero/storage/3X2D5UK6/Grosse et al. - 2023 - Studying Large Language Model Generalization with .pdf}
}

@article{grover_deepcuts_2022,
  title = {DeepCuts: Single-Shot Interpretability based Pruning for BERT},
  shorttitle = {DeepCuts},
  author = {Grover, Jasdeep Singh and Gawri, Bhavesh and Manku, Ruskin Raj},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2212.13392},
  url = {https://arxiv.org/abs/2212.13392},
  urldate = {2023-08-27},
  abstract = {As language models have grown in parameters and layers, it has become much harder to train and infer with them on single GPUs. This is severely restricting the availability of large language models such as GPT-3, BERT-Large, and many others. A common technique to solve this problem is pruning the network architecture by removing transformer heads, fully-connected weights, and other modules. The main challenge is to discern the important parameters from the less important ones. Our goal is to find strong metrics for identifying such parameters. We thus propose two strategies: Cam-Cut based on the GradCAM interpretations, and Smooth-Cut based on the SmoothGrad, for calculating the importance scores. Through this work, we show that our scoring functions are able to assign more relevant task-based scores to the network parameters, and thus both our pruning approaches significantly outperform the standard weight and gradient-based strategies, especially at higher compression ratios in BERT-based models. We also analyze our pruning masks and find them to be significantly different from the ones obtained using standard metrics.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KMIFHYN2/Grover et al. - 2022 - DeepCuts Single-Shot Interpretability based Pruni.pdf}
}

@article{gu_going_2022,
  title = {Going Beyond Accuracy: Interpretability Metrics for CNN Representations of Physiological Signals},
  shorttitle = {Going Beyond Accuracy},
  author = {Gu, Kang and Prioleau, Temiloluwa and Vosoughi, Soroush},
  year = {2022},
  month = aug,
  journal = {ICPR},
  pages = {4507--4513},
  issn = {2831-7475},
  doi = {10.1109/ICPR56361.2022.9956192},
  url = {https://ieeexplore.ieee.org/document/9956192},
  abstract = {Though deep neural networks such as convolutional neural networks (CNNs) have achieved high predictive accuracy on many tasks in health domain, the lack of explanation and reasoning behind the predictions can cause adverse consequences in many ``high stakes'' applications such as predicting mortality in ICU. Existing popular explanation methods, including GradCam, LIME, SHAP and so on, are designed for image and text data, but not for physiological data in the form of time series like electrocardiograms (ECG). Moreover, these methods can only generate interpretable visualizations for qualitative evaluations while cannot satisfy the need for quantitative evaluations. To fill this gap, we introduce a novel generalized framework for quantitatively explaining CNN representations for physiological signals called ENPhyS. Specifically, inspired by visual concepts (e.g. parts, objects and scenes), we first define physiological concepts which are drawn from existing clinical knowledge (e.g. P wave in ECG). ENPhys further measures the interpretability of CNN by the alignment between a hidden CNN unit and each physiological concept. Finally, we propose novel interpretability metrics ---Consistency, Confidence, Completeness and C-Score ---to quantify the interpretability of CNNs from different perspectives. We evaluate our metrics on three state-of-the-art CNN models and on three types of physiological signals. Results from user study suggest that our metrics are in line with human intuition of interpretability.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZNPW24EA/Gu et al. - 2022 - Going Beyond Accuracy Interpretability Metrics fo.pdf}
}

@article{gu_model_2024,
  title = {Model Editing Can Hurt General Abilities of Large Language Models},
  author = {Gu, Jia-Chen and Xu, Hao-Xiang and Ma, Jun-Yu and Lu, Pan and Ling, Zhen-Hua and Chang, Kai-Wei and Peng, Nanyun},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2401.04700},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.04700},
  url = {http://arxiv.org/abs/2401.04700},
  urldate = {2024-02-07},
  abstract = {One critical challenge that has emerged is the presence of hallucinations in the output of large language models (LLMs) due to false or outdated knowledge. Since retraining LLMs with updated information is resource-intensive, there has been a growing interest in model editing. However, current model editing methods, while effective in improving editing performance in various scenarios, often overlook potential side effects on the general abilities of LLMs. In this paper, we raise concerns that model editing inherently improves the factuality of the model, but may come at the cost of a significant degradation of these general abilities. Systematically, we analyze side effects by evaluating four popular editing methods on three LLMs across eight representative task categories. Extensive empirical research reveals that current model editing methods are difficult to couple well with LLMs to simultaneously improve the factuality and maintain the general abilities such as reasoning, question answering, etc. Strikingly, the use of a specific method to edit LLaMA-1 (7B) resulted in a drastic performance degradation to nearly 0 on all selected tasks with just a single edit. Therefore, we advocate for more research efforts to minimize the loss of general abilities acquired during LLM pre-training and to ultimately preserve them during model editing.},
  archiveprefix = {arxiv},
  keywords = {editing,not cited},
  file = {/Users/leonardbereska/Zotero/storage/WIPLQ3DA/Gu et al. - 2024 - Model Editing Can Hurt General Abilities of Large .pdf}
}

@article{guan_how_2020,
  title = {How Far Does BERT Look At:Distance-based Clustering and Analysis of BERT\$'\$s Attention},
  shorttitle = {How Far Does BERT Look At},
  author = {Guan, Yue and Leng, Jingwen and Li, Chao and Chen, Quan and Guo, Minyi},
  year = {2020},
  month = nov,
  journal = {CoRR},
  eprint = {2011.00943},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2011.00943},
  url = {http://arxiv.org/abs/2011.00943},
  urldate = {2024-03-20},
  abstract = {Recent research on the multi-head attention mechanism, especially that in pre-trained models such as BERT, has shown us heuristics and clues in analyzing various aspects of the mechanism. As most of the research focus on probing tasks or hidden states, previous works have found some primitive patterns of attention head behavior by heuristic analytical methods, but a more systematic analysis specific on the attention patterns still remains primitive. In this work, we clearly cluster the attention heatmaps into significantly different patterns through unsupervised clustering on top of a set of proposed features, which corroborates with previous observations. We further study their corresponding functions through analytical study. In addition, our proposed features can be used to explain and calibrate different attention heads in Transformer models.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/5FHBTSTK/Guan et al. - 2020 - How Far Does BERT Look AtDistance-based Clusterin.pdf}
}

@article{guidotti_survey_2018,
  title = {A Survey Of Methods For Explaining Black Box Models},
  author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
  year = {2018},
  month = jun,
  journal = {CoRR},
  eprint = {1802.01933},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1802.01933},
  url = {http://arxiv.org/abs/1802.01933},
  urldate = {2023-11-06},
  abstract = {In the last years many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness sometimes at the cost of scarifying accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, delineating explicitly or implicitly its own definition of interpretability and explanation. The aim of this paper is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/86IUVXDY/Guidotti et al. - 2018 - A Survey Of Methods For Explaining Black Box Model.pdf}
}

@article{guo_calibration_2017,
  title = {On Calibration of Modern Neural Networks},
  author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  year = {2017},
  month = aug,
  journal = {CoRR},
  eprint = {1706.04599},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1706.04599},
  url = {http://arxiv.org/abs/1706.04599},
  urldate = {2023-07-09},
  abstract = {Confidence calibration -- the problem of predicting probability estimates representative of the true correctness likelihood -- is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling -- a single-parameter variant of Platt Scaling -- is surprisingly effective at calibrating predictions.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LJU6BVAX/Guo et al. - 2017 - On Calibration of Modern Neural Networks.pdf}
}

@article{gupta_editing_2023,
  title = {Editing Common Sense in Transformers},
  author = {Gupta, Anshita and Mondal, Debanjan and Sheshadri, Akshay Krishna and Zhao, Wenlong and Li, Xiang Lorraine and Wiegreffe, Sarah and Tandon, Niket},
  year = {2023},
  month = oct,
  journal = {EMNLP},
  eprint = {2305.14956},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.14956},
  url = {http://arxiv.org/abs/2305.14956},
  urldate = {2024-03-18},
  abstract = {Editing model parameters directly in Transformers makes updating open-source transformer-based models possible without re-training (Meng et al., 2023). However, these editing methods have only been evaluated on statements about encyclopedic knowledge with a single correct answer. Commonsense knowledge with multiple correct answers, e.g., an apple can be green or red but not transparent, has not been studied but is as essential for enhancing transformers' reliability and usefulness. In this paper, we investigate whether commonsense judgments are causally associated with localized, editable parameters in Transformers, and we provide an affirmative answer. We find that directly applying the MEMIT editing algorithm results in sub-par performance and improve it for the commonsense domain by varying edit tokens and improving the layer selection strategy, i.e., \$MEMIT\_\{CSK\}\$. GPT-2 Large and XL models edited using \$MEMIT\_\{CSK\}\$ outperform best-fine-tuned baselines by 10.97\% and 10.73\% F1 scores on PEP3k and 20Q datasets. In addition, we propose a novel evaluation dataset, PROBE SET, that contains unaffected and affected neighborhoods, affected paraphrases, and affected reasoning challenges. \$MEMIT\_\{CSK\}\$ performs well across the metrics while fine-tuning baselines show significant trade-offs between unaffected and affected metrics. These results suggest a compelling future direction for incorporating feedback about common sense into Transformers through direct model editing.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/TM8BRPAE/Gupta et al. - 2023 - Editing Common Sense in Transformers.pdf}
}

@article{gurkan_scoring_2022,
  title = {A Scoring Method for Interpretability of Concepts in Convolutional Neural Networks},
  author = {Gurkan, Mustafa Kagan and Arica, Nafiz and Vural, Fatos Yarman},
  year = {2022},
  month = may,
  journal = {SIU},
  pages = {1--4},
  publisher = {IEEE},
  address = {Safranbolu, Turkey},
  doi = {10.1109/SIU55565.2022.9864930},
  url = {https://ieeexplore.ieee.org/document/9864930/},
  urldate = {2023-08-27},
  abstract = {In this paper, we propose a scoring algorithm for measuring the interpretability of CNN models by focusing on the feature extraction operation at the convolutional layers. The proposed approach is based on the principal of concept analysis, for a predefined list of concepts. A map of the network is created based on its responsiveness against each concept. Once this map is ready, various images can be applied as inputs and they are matched with the concepts whose hidden nodes are highly activated. Finally, the evaluation algorithm kicks in to use these descriptions during the final prediction and provides human-understandable explanations.},
  isbn = {9781665450928},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LTP677YA/Gurkan et al. - 2022 - A Scoring Method for Interpretability of Concepts .pdf}
}

@article{gurnee_finding_2023,
  title = {Finding Neurons in a Haystack: Case Studies with Sparse Probing},
  shorttitle = {Finding Neurons in a Haystack},
  author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  year = {2023},
  journal = {TMLR},
  publisher = {arXiv},
  url = {https://arxiv.org/abs/2305.01610},
  urldate = {2023-07-31},
  abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train \$k\$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of \$k\$ we study the sparsity of learned representations and how this varies with model scale. With \$k=1\$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,empirical,graph,mechinterp,neuron,probing,superposition,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/LBWU8LZZ/Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with S.pdf}
}

@article{gurnee_language_2023,
  title = {Language Models Represent Space and Time},
  author = {Gurnee, Wes and Tegmark, Max},
  year = {2024},
  journal = {ICLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.02207},
  url = {https://arxiv.org/abs/2310.02207},
  urldate = {2023-10-25},
  abstract = {The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a coherent model of the data generating process -- a world model. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual ``space neurons'' and ``time neurons'' that reliably encode spatial and temporal coordinates. Our analysis demonstrates that modern LLMs acquire structured knowledge about fundamental dimensions such as space and time, supporting the view that they learn not merely superficial statistics, but literal world models.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/S7R75HDP/Gurnee and Tegmark - 2023 - Language Models Represent Space and Time.pdf}
}

@article{gurnee_universal_2024,
  title = {Universal Neurons in GPT2 Language Models},
  author = {Gurnee, Wes and Horsley, Theo and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Sun, Qinyi and Hathaway, Will and Nanda, Neel and Bertsimas, Dimitris},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.12181},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.12181},
  url = {http://arxiv.org/abs/2401.12181},
  urldate = {2024-02-07},
  abstract = {A basic question within the emerging field of mechanistic interpretability is the degree to which neural networks learn the same underlying mechanisms. In other words, are neural mechanisms universal across different models? In this work, we study the universality of individual neurons across GPT2 models trained from different initial random seeds, motivated by the hypothesis that universal neurons are likely to be interpretable. In particular, we compute pairwise correlations of neuron activations over 100 million tokens for every neuron pair across five different seeds and find that 1-5{\textbackslash}\% of neurons are universal, that is, pairs of neurons which consistently activate on the same inputs. We then study these universal neurons in detail, finding that they usually have clear interpretations and taxonomize them into a small number of neuron families. We conclude by studying patterns in neuron weights to establish several universal functional roles of neurons in simple circuits: deactivating attention heads, changing the entropy of the next token distribution, and predicting the next token to (not) be within a particular set.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/D2XR6963/Gurnee et al. - 2024 - Universal Neurons in GPT2 Language Models.pdf}
}

@article{guu_simfluence_2023,
  title = {Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs},
  shorttitle = {Simfluence},
  author = {Guu, Kelvin and Webson, Albert and Pavlick, Ellie and Dixon, Lucas and Tenney, Ian and Bolukbasi, Tolga},
  year = {2023},
  month = mar,
  journal = {CoRR},
  eprint = {2303.08114},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.08114},
  url = {http://arxiv.org/abs/2303.08114},
  urldate = {2023-08-29},
  abstract = {Training data attribution (TDA) methods offer to trace a model's prediction on any given example back to specific influential training examples. Existing approaches do so by assigning a scalar influence score to each training example, under a simplifying assumption that influence is additive. But in reality, we observe that training examples interact in highly non-additive ways due to factors such as inter-example redundancy, training order, and curriculum learning effects. To study such interactions, we propose Simfluence, a new paradigm for TDA where the goal is not to produce a single influence score per example, but instead a training run simulator: the user asks, ``If my model had trained on example \$z\_1\$, then \$z\_2\$, ..., then \$z\_n\$, how would it behave on \$z\_\{test\}\$?''; the simulator should then output a simulated training run, which is a time series predicting the loss on \$z\_\{test\}\$ at every step of the simulated run. This enables users to answer counterfactual questions about what their model would have learned under different training curricula, and to directly see where in training that learning would occur. We present a simulator, Simfluence-Linear, that captures non-additive interactions and is often able to predict the spiky trajectory of individual example losses with surprising fidelity. Furthermore, we show that existing TDA methods such as TracIn and influence functions can be viewed as special cases of Simfluence-Linear. This enables us to directly compare methods in terms of their simulation accuracy, subsuming several prior TDA approaches to evaluation. In experiments on large language model (LLM) fine-tuning, we show that our method predicts loss trajectories with much higher accuracy than existing TDA methods (doubling Spearman's correlation and reducing mean-squared error by 75\%) across several tasks, models, and training methods.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RAZCVY26/Guu et al. - 2023 - Simfluence Modeling the Influence of Individual T.pdf}
}

@article{ha_recurrent_2018,
  title = {Recurrent World Models Facilitate Policy Evolution},
  author = {Ha, David R. and Schmidhuber, J.},
  year = {2018},
  month = sep,
  journal = {NeurIPS},
  url = {https://www.semanticscholar.org/paper/Recurrent-World-Models-Facilitate-Policy-Evolution-Ha-Schmidhuber/41cca0b0a27ba363ca56e7033569aeb1922b0ac9},
  urldate = {2024-02-10},
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of this paper is available at https://worldmodels.github.io},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/L7XG2EAC/Ha and Schmidhuber - 2018 - Recurrent World Models Facilitate Policy Evolution.pdf}
}

@article{ha_world_2018,
  title = {World Models},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  journal = {CoRR},
  eprint = {1803.10122},
  primaryclass = {cs, stat},
  doi = {10.5281/zenodo.1207631},
  url = {http://arxiv.org/abs/1803.10122},
  urldate = {2024-01-20},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/U73QH64I/Ha and Schmidhuber - 2018 - World Models.pdf}
}

@article{hacohen_let_2019,
  title = {Let's Agree to Agree: Neural Networks Share Classification Order on Real Datasets},
  shorttitle = {Let's Agree to Agree},
  author = {Hacohen, Guy and Choshen, Leshem and Weinshall, Daphna},
  year = {2020},
  journal = {ICML},
  eprint = {1905.10854},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.10854},
  url = {http://arxiv.org/abs/1905.10854},
  urldate = {2024-01-18},
  abstract = {We report a series of robust empirical observations, demonstrating that deep Neural Networks learn the examples in both the training and test sets in a similar order. This phenomenon is observed in all the commonly used benchmarks we evaluated, including many image classification benchmarks, and one text classification benchmark. While this phenomenon is strongest for models of the same architecture, it also crosses architectural boundaries -- models of different architectures start by learning the same examples, after which the more powerful model may continue to learn additional examples. We further show that this pattern of results reflects the interplay between the way neural networks learn benchmark datasets. Thus, when fixing the architecture, we show synthetic datasets where this pattern ceases to exist. When fixing the dataset, we show that other learning paradigms may learn the data in a different order. We hypothesize that our results reflect how neural networks discover structure in natural datasets.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/DXF44SLU/Hacohen et al. - 2019 - Let's Agree to Agree Neural Networks Share Classi.pdf}
}

@article{hadfield-menell_cooperative_2016,
  title = {Cooperative Inverse Reinforcement Learning},
  author = {{Hadfield-Menell}, Dylan and Russell, Stuart J and Abbeel, Pieter and Dragan, Anca},
  year = {2016},
  journal = {NeurIPS},
  volume = {29},
  url = {https://proceedings.neurips.cc/paper_files/paper/2016/hash/c3395dd46c34fa7fd8d729d8cf88b7a8-Abstract.html},
  urldate = {2023-08-26},
  abstract = {For an autonomous system to be helpful to humans and to pose no unwarranted risks, it needs to align its values with those of the humans in its environment in such a way that its actions contribute to the maximization of value for the humans. We propose a formal definition of the value alignment problem as cooperative inverse reinforcement learning (CIRL). A CIRL problem is a cooperative, partial- information game with two agents, human and robot; both are rewarded according to the human's reward function, but the robot does not initially know what this is. In contrast to classical IRL, where the human is assumed to act optimally in isolation, optimal CIRL solutions produce behaviors such as active teaching, active learning, and communicative actions that are more effective in achieving value alignment. We show that computing optimal joint policies in CIRL games can be reduced to solving a POMDP, prove that optimality in isolation is suboptimal in CIRL, and derive an approximate CIRL algorithm.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/F84BRJ6V/Hadfield-Menell et al. - 2016 - Cooperative Inverse Reinforcement Learning.pdf}
}

@article{hadfield-menell_offswitch_2017,
  title = {The Off-Switch Game},
  author = {{Hadfield-Menell}, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
  year = {2017},
  month = jun,
  journal = {CoRR},
  eprint = {1611.08219},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1611.08219},
  url = {http://arxiv.org/abs/1611.08219},
  urldate = {2023-07-22},
  abstract = {It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for self-preservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R's off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H's actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/D6HYJRQM/Hadfield-Menell et al. - 2017 - The Off-Switch Game.pdf}
}

@article{hafting_microstructure_2005,
  title = {Microstructure of a spatial map in the entorhinal cortex},
  author = {Hafting, Torkel and Fyhn, Marianne and Molden, Sturla and Moser, May-Britt and Moser, Edvard I.},
  year = {2005},
  month = aug,
  journal = {Nature},
  volume = {436},
  number = {7052},
  pages = {801--806},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature03721},
  url = {http://www.nature.com/articles/nature03721},
  urldate = {2022-08-17},
  language = {en},
  keywords = {grid cells,neuroscience,not cited},
  file = {/Users/leonardbereska/Zotero/storage/MR85TBJN/Hafting et al. - 2005 - Microstructure of a spatial map in the entorhinal .pdf}
}

@article{hagendorff_machine_2023,
  title = {Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods},
  shorttitle = {Machine Psychology},
  author = {Hagendorff, Thilo},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.13988},
  url = {https://arxiv.org/abs/2303.13988},
  urldate = {2024-02-11},
  abstract = {Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behavioral patterns discovered in LLMs are to be interpreted. In sum, machine psychology aims to discover emergent abilities in LLMs that cannot be detected by most traditional natural language processing benchmarks.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {LLMs,not cited,psychology},
  file = {/Users/leonardbereska/Zotero/storage/HFTKREV9/Hagendorff - 2023 - Machine Psychology Investigating Emergent Capabil.pdf}
}

@article{hahn_theoretical_2020,
  title = {Theoretical Limitations of Self-Attention in Neural Sequence Models},
  author = {Hahn, Michael},
  year = {2020},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  eprint = {1906.06755},
  primaryclass = {cs},
  pages = {156--171},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00306},
  url = {http://arxiv.org/abs/1906.06755},
  urldate = {2024-02-26},
  abstract = {Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of self-attention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/AFEM7N82/Hahn - 2020 - Theoretical Limitations of Self-Attention in Neura.pdf}
}

@article{halawi_overthinking_2023,
  title = {Overthinking the Truth: Understanding how Language Models Process False Demonstrations},
  shorttitle = {Overthinking the Truth},
  author = {Halawi, Danny and Denain, Jean-Stanislas and Steinhardt, Jacob},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.09476},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.09476},
  url = {http://arxiv.org/abs/2307.09476},
  urldate = {2023-11-16},
  abstract = {Modern language models can imitate complex patterns through few-shot learning, enabling them to complete challenging tasks without fine-tuning. However, imitation can also lead models to reproduce inaccuracies or harmful content if present in the context. We study harmful imitation through the lens of a model's internal representations, and identify two related phenomena: overthinking and false induction heads. The first phenomenon, overthinking, appears when we decode predictions from intermediate layers, given correct vs. incorrect few-shot demonstrations. At early layers, both demonstrations induce similar model behavior, but the behavior diverges sharply at some "critical layer", after which the accuracy given incorrect demonstrations progressively decreases. The second phenomenon, false induction heads, are a possible mechanistic cause of overthinking: these are heads in late layers that attend to and copy false information from previous demonstrations, and whose ablation reduces overthinking. Beyond scientific understanding, our results suggest that studying intermediate model computations could be a promising avenue for understanding and guarding against harmful model behaviors.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ACSMJVHS/Halawi et al. - 2023 - Overthinking the Truth Understanding how Language.pdf}
}

@article{han_explaining_2020,
  title = {Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions},
  author = {Han, Xiaochuang and Wallace, Byron C. and Tsvetkov, Yulia},
  year = {2020},
  month = may,
  journal = {ACL},
  eprint = {2005.06676},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2005.06676},
  url = {http://arxiv.org/abs/2005.06676},
  urldate = {2024-01-24},
  abstract = {Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which 'saliency maps' may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5GFUH8QN/Han et al. - 2020 - Explaining Black Box Predictions and Unveiling Dat.pdf}
}

@article{han_impact_2023,
  title = {On the Impact of Knowledge Distillation for Model Interpretability},
  author = {Han, Hyeongrok and Kim, Siwon and Choi, Hyun-Soo and Yoon, Sungroh},
  year = {2023},
  month = jul,
  journal = {ICML},
  pages = {12389--12410},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/han23b.html},
  urldate = {2023-09-20},
  abstract = {Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields. The code is available at https://github.com/Rok07/KD\_XAI.git.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TPYJSSXH/Han et al. - 2023 - On the Impact of Knowledge Distillation for Model .pdf}
}

@article{hanna_have_2024,
  title = {Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms},
  shorttitle = {Have Faith in Faithfulness},
  author = {Hanna, Michael and Pezzelle, Sandro and Belinkov, Yonatan},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.17806},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.17806},
  url = {http://arxiv.org/abs/2403.17806},
  urldate = {2024-04-19},
  abstract = {Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit by performing causal interventions on each edge independently, but this scales poorly with model size. Edge attribution patching (EAP), gradient-based approximation to interventions, has emerged as a scalable but imperfect solution to this problem. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to better maintain a core property of circuits: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's performance on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP are less faithful than those found using EAP-IG, even though both have high node overlap with circuits found previously using causal interventions. We conclude more generally that when using circuits to compare the mechanisms models use to solve tasks, faithfulness, not overlap, is what should be measured.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/DG9E94P8/Hanna et al. - 2024 - Have Faith in Faithfulness Going Beyond Circuit O.pdf}
}

@article{hanna_how_2023,
  title = {How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model},
  shorttitle = {How does GPT-2 compute greater-than?},
  author = {Hanna, Michael and Liu, Ollie and Variengien, Alexandre},
  year = {2023},
  journal = {NeurIPS},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.00586},
  url = {https://arxiv.org/abs/2305.00586},
  urldate = {2023-07-31},
  abstract = {Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years \&gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we find related tasks that activate our circuit. Our results suggest that GPT-2 small computes greater-than using a complex but general mechanism that activates across diverse contexts.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {behavior,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/534K4PW9/Hanna et al. - 2023 - How does GPT-2 compute greater-than Interpreting.pdf}
}

@misc{hanni_decomposing_2023,
  title = {Decomposing Activations into Features: How Many and How do we Find Them? --- A Survey},
  author = {H{\"a}nni, Kaarel and Mendel, Jake and Kozaronek, Kay},
  year = {2023},
  url = {https://kaarelh.github.io/doc/decomposition.pdf},
  abstract = {One of the main goals of interpretability is to reverse-engineer neural networks. For this, a reasonable subgoal is to determine the ``variables in terms of which a neural network thinks''. In this paper, we run with the assumption that these variables correspond to particular vectors in activation space, which we call `features', whose linear combinations are the activation vectors on various inputs. We provide an exposition of certain techniques from the literature for reconstructing these features from activation data, discussing both approaches grounded in the assumption of there being many features, few of which are active on any one input; as well as approaches grounded in the assumption that there are relatively few features. We finish by suggesting experiments that might let us figure out which of these two options is closer to the truth.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/ZJY2ISDJ/Research and Mats - Decomposing Activations into Features How Many an.pdf}
}

@article{hanni_searching_2023,
  title = {Searching for a model's concepts by their shape -- a theoretical framework},
  author = {H{\"a}nni, Kaarel and Kaklamanos, Georgios and Laurito, Walter and Kozaronek, Kay and Mennen, Alex and Ku, June},
  year = {2023},
  month = feb,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/Go5ELsHAyw7QrArQ6/searching-for-a-model-s-concepts-by-their-shape-a},
  urldate = {2024-01-23},
  abstract = {Produced as part of the SERI ML Alignment Theory Scholars Program - Winter 2022 Cohort {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZDNN4DU6/Hnni et al. - 2023 - Searching for a model's concepts by their shape  .html}
}

@article{hao_formal_2022,
  title = {Formal Language Recognition by Hard Attention Transformers: Perspectives from Circuit Complexity},
  shorttitle = {Formal Language Recognition by Hard Attention Transformers},
  author = {Hao, Yiding and Angluin, Dana and Frank, Robert},
  editor = {Roark, Brian and Nenkova, Ani},
  year = {2022},
  journal = {TACL},
  volume = {10},
  pages = {800--810},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00490},
  url = {https://aclanthology.org/2022.tacl-1.46},
  urldate = {2024-03-19},
  abstract = {This paper analyzes three formal models of Transformer encoders that differ in the form of their self-attention mechanism: unique hard attention (UHAT); generalized unique hard attention (GUHAT), which generalizes UHAT; and averaging hard attention (AHAT). We show that UHAT and GUHAT Transformers, viewed as string acceptors, can only recognize formal languages in the complexity class AC0, the class of languages recognizable by families of Boolean circuits of constant depth and polynomial size. This upper bound subsumes Hahn's (2020) results that GUHAT cannot recognize the DYCK languages or the PARITY language, since those languages are outside AC0 (Furst et al., 1984). In contrast, the non-AC0 languages MAJORITY and DYCK-1 are recognizable by AHAT networks, implying that AHAT can recognize languages that UHAT and GUHAT cannot.},
  file = {/Users/leonardbereska/Zotero/storage/EBAAWMVP/Hao et al. - 2022 - Formal Language Recognition by Hard Attention Tran.pdf}
}

@article{hase_does_2023,
  title = {Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models},
  shorttitle = {Does Localization Inform Editing?},
  author = {Hase, Peter and Bansal, Mohit and Kim, Been and Ghandeharioun, Asma},
  year = {2023},
  month = jan,
  journal = {NeurIPS Spotlight},
  eprint = {2301.04213},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2301.04213},
  url = {http://arxiv.org/abs/2301.04213},
  urldate = {2023-08-26},
  abstract = {Language models are known to learn a great quantity of factual information during pretraining, and recent work localizes this information to specific model weights like mid-layer MLP weights (Meng et al., 2022). In this paper, we find that we can change how a fact is stored in a model by editing weights that are in a different location than where existing methods suggest that the fact is stored. This is surprising because we would expect that localizing facts to specific parameters in models would tell us where to manipulate knowledge in models, and this assumption has motivated past work on model editing methods. Specifically, we show that localization conclusions from representation denoising (also known as Causal Tracing) do not provide any insight into which model MLP layer would be best to edit in order to override an existing stored fact with a new one. This finding raises questions about how past work relies on Causal Tracing to select which model layers to edit (Meng et al., 2022). Next, to better understand the discrepancy between representation denoising and weight editing, we develop several variants of the editing problem that appear more and more like representation denoising in their design and objective. Experiments show that, for one of our editing problems, editing performance does relate to localization results from representation denoising, but we find that which layer we edit is a far better predictor of performance. Our results suggest, counterintuitively, that better mechanistic understanding of how pretrained language models work may not always translate to insights about how to best change their behavior. Code is available at: https://github.com/google/belief-localization},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PA7ZI6N2/Hase et al. - 2023 - Does Localization Inform Editing Surprising Diffe.pdf}
}

@article{hase_evaluating_2020,
  title = {Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?},
  shorttitle = {Evaluating Explainable AI},
  author = {Hase, Peter and Bansal, Mohit},
  year = {2020},
  month = may,
  journal = {ACL},
  eprint = {2005.01831},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2005.01831},
  url = {http://arxiv.org/abs/2005.01831},
  urldate = {2023-11-24},
  abstract = {Algorithmic approaches to interpreting machine learning models have proliferated in recent years. We carry out human subject tests that are the first of their kind to isolate the effect of algorithmic explanations on a key aspect of model interpretability, simulatability, while avoiding important confounding experimental factors. A model is simulatable when a person can predict its behavior on new inputs. Through two kinds of simulation tests involving text and tabular data, we evaluate five explanations methods: (1) LIME, (2) Anchor, (3) Decision Boundary, (4) a Prototype model, and (5) a Composite approach that combines explanations from each method. Clear evidence of method effectiveness is found in very few cases: LIME improves simulatability in tabular classification, and our Prototype method is effective in counterfactual simulation tests. We also collect subjective ratings of explanations, but we do not find that ratings are predictive of how helpful explanations are. Our results provide the first reliable and comprehensive estimates of how explanations influence simulatability across a variety of explanation methods and data domains. We show that (1) we need to be careful about the metrics we use to evaluate explanation methods, and (2) there is significant room for improvement in current methods. All our supporting code, data, and models are publicly available at: https://github.com/peterbhase/InterpretableNLP-ACL2020},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/F7KL2RAR/Hase and Bansal - 2020 - Evaluating Explainable AI Which Algorithmic Expla.pdf}
}

@article{hase_language_2021,
  title = {Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs},
  shorttitle = {Do Language Models Have Beliefs?},
  author = {Hase, Peter and Diab, Mona and Celikyilmaz, Asli and Li, Xian and Kozareva, Zornitsa and Stoyanov, Veselin and Bansal, Mohit and Iyer, Srinivasan},
  year = {2021},
  month = nov,
  journal = {CoRR},
  eprint = {2111.13654},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2111.13654},
  url = {http://arxiv.org/abs/2111.13654},
  urldate = {2024-03-19},
  abstract = {Do language models have beliefs about the world? Dennett (1995) famously argues that even thermostats have beliefs, on the view that a belief is simply an informational state decoupled from any motivational state. In this paper, we discuss approaches to detecting when models have beliefs about the world, and we improve on methods for updating model beliefs to be more truthful, with a focus on methods based on learned optimizers or hypernetworks. Our main contributions include: (1) new metrics for evaluating belief-updating methods that focus on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing model updates (SLAG) that improves the performance of learned optimizers, and (3) the introduction of the belief graph, which is a new form of interface with language models that shows the interdependencies between model beliefs. Our experiments suggest that models possess belief-like qualities to only a limited extent, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work. Code is available at https://github.com/peterbhase/SLAG-Belief-Updating},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/WRRGCUFC/Hase et al. - 2021 - Do Language Models Have Beliefs Methods for Detec.pdf}
}

@article{hase_opinions_2021,
  title = {Opinions on Interpretable Machine Learning and 70 Summaries of Recent Papers},
  author = {Hase, Peter},
  year = {2021},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/GEPX7jgLMB8vR2qaK/opinions-on-interpretable-machine-learning-and-70-summaries},
  urldate = {2023-11-30},
  abstract = {Peter Hase UNC Chapel Hill {$\bullet$} Owen Shen UC San Diego {$\bullet$} With thanks to Robert Kirk and Mohit Bansal for helpful feedback on this post. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PMJZE5U6/Hase - 2021 - Opinions on Interpretable Machine Learning and 70 .html}
}

@article{hazineh_linear_2023,
  title = {Linear Latent World Models in Simple Transformers: A Case Study on Othello-GPT},
  shorttitle = {Linear Latent World Models in Simple Transformers},
  author = {Hazineh, Dean S. and Zhang, Zechen and Chiu, Jeffery},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.07582},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.07582},
  url = {http://arxiv.org/abs/2310.07582},
  urldate = {2024-03-13},
  abstract = {Foundation models exhibit significant capabilities in decision-making and logical deductions. Nonetheless, a continuing discourse persists regarding their genuine understanding of the world as opposed to mere stochastic mimicry. This paper meticulously examines a simple transformer trained for Othello, extending prior research to enhance comprehension of the emergent world model of Othello-GPT. The investigation reveals that Othello-GPT encapsulates a linear representation of opposing pieces, a factor that causally steers its decision-making process. This paper further elucidates the interplay between the linear world representation and causal decision-making, and their dependence on layer depth and model complexity. We have made the code public.},
  archiveprefix = {arxiv},
  keywords = {mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/EREE5W4N/Hazineh et al. - 2023 - Linear Latent World Models in Simple Transformers.pdf}
}

@article{he_dictionary_2024,
  title = {Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT},
  shorttitle = {Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability},
  author = {He, Zhengfu and Ge, Xuyang and Tang, Qiong and Sun, Tianxiang and Cheng, Qinyuan and Qiu, Xipeng},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.12201},
  url = {https://arxiv.org/abs/2402.12201},
  urldate = {2024-06-10},
  abstract = {Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/88D27J2W/He et al. - 2024 - Dictionary Learning Improves Patch-Free Circuit Di.pdf}
}

@article{heimersheim_circuit_2023,
  title = {A circuit for Python docstrings in a 4-layer attention-only transformer},
  author = {Heimersheim, Stefan and Jett},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/u6KXXmKFbXfWzoAXn/a-circuit-for-python-docstrings-in-a-4-layer-attention-only},
  urldate = {2023-11-10},
  abstract = {Produced as part of the SERI ML Alignment Theory Scholars Program under the supervision of Neel Nanda - Winter 2022 Cohort. {\dots}},
  language = {en},
  keywords = {added one sentence summary,behavior,circuit,cited,empirical,mechinterp,to cite,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/LCH2T56J/Heimersheim and Jett - 2023 - A circuit for Python docstrings in a 4-layer atten.html}
}

@article{heinzerling_monotonic_2024,
  title = {Monotonic Representation of Numeric Properties in Language Models},
  author = {Heinzerling, Benjamin and Inui, Kentaro},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.10381},
  url = {https://arxiv.org/abs/2403.10381},
  urldate = {2024-06-10},
  abstract = {Language models (LMs) can express factual knowledge involving numeric properties such as Karl Popper was born in 1902. However, how this information is encoded in the model's internal representations is not understood well. Here, we introduce a simple method for finding and editing representations of numeric properties such as an entity's birth year. Empirically, we find low-dimensional subspaces that encode numeric properties monotonically, in an interpretable and editable fashion. When editing representations along directions in these subspaces, LM output changes accordingly. For example, by patching activations along a "birthyear" direction we can make the LM express an increasingly late birthyear: Karl Popper was born in 1929, Karl Popper was born in 1957, Karl Popper was born in 1968. Property-encoding directions exist across several numeric properties in all models under consideration, suggesting the possibility that monotonic representation of numeric properties consistently emerges during LM pretraining. Code: https://github.com/bheinzerling/numeric-property-repr},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/Q3834KZZ/Heinzerling and Inui - 2024 - Monotonic Representation of Numeric Properties in .pdf}
}

@article{hendel_incontext_2023,
  title = {In-Context Learning Creates Task Vectors},
  author = {Hendel, Roee and Geva, Mor and Globerson, Amir},
  year = {2023},
  month = oct,
  journal = {EMNLP},
  eprint = {2310.15916},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.15916},
  url = {http://arxiv.org/abs/2310.15916},
  urldate = {2023-11-08},
  abstract = {In-context learning (ICL) in Large Language Models (LLMs) has emerged as a powerful new learning paradigm. However, its underlying mechanism is still not well understood. In particular, it is challenging to map it to the "standard" machine learning framework, where one uses a training set \$S\$ to find a best-fitting function \$f(x)\$ in some hypothesis class. Here we make progress on this problem by showing that the functions learned by ICL often have a very simple structure: they correspond to the transformer LLM whose only inputs are the query \$x\$ and a single "task vector" calculated from the training set. Thus, ICL can be seen as compressing \$S\$ into a single task vector \${\textbackslash}boldsymbol\{{\textbackslash}theta\}(S)\$ and then using this task vector to modulate the transformer to produce the output. We support the above claim via comprehensive experiments across a range of models and tasks.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ICZRZIC4/Hendel et al. - 2023 - In-Context Learning Creates Task Vectors.pdf}
}

@article{hendrycks_aligning_2023,
  title = {Aligning AI With Shared Human Values},
  author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Critch, Andrew and Li, Jerry and Song, Dawn and Steinhardt, Jacob},
  year = {2023},
  month = feb,
  journal = {CoRR},
  eprint = {2008.02275},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2008.02275},
  url = {http://arxiv.org/abs/2008.02275},
  urldate = {2023-10-13},
  abstract = {We show how to assess a language model's knowledge of basic concepts of morality. We introduce the ETHICS dataset, a new benchmark that spans concepts in justice, well-being, duties, virtues, and commonsense morality. Models predict widespread moral judgments about diverse text scenarios. This requires connecting physical and social world knowledge to value judgements, a capability that may enable us to steer chatbot outputs or eventually regularize open-ended reinforcement learning agents. With the ETHICS dataset, we find that current language models have a promising but incomplete ability to predict basic human ethical judgements. Our work shows that progress can be made on machine ethics today, and it provides a steppingstone toward AI that is aligned with human values.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2YHFKQ4F/Hendrycks et al. - 2023 - Aligning AI With Shared Human Values.pdf}
}

@book{hendrycks_introduction_2023,
  title = {Introduction to AI Safety, Ethics, and Society},
  author = {Hendrycks, Dan},
  year = {2023},
  publisher = {Self-published},
  url = {https://www.aisafetybook.com/},
  urldate = {2024-03-06},
  abstract = {We already face issues in controlling the goals of current-day AI systems. If this is also true with future AI systems that are more powerful and more integrated with our economies and militaries, we could see dangerous rogue AI systems emerge.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/INR9Y2LB/Hendrycks - 2023 - Introduction to AI Safety, Ethics, and Society.html}
}

@article{hendrycks_open_2022,
  title = {Open Problems in AI X-Risk [PAIS \#5]},
  author = {Hendrycks, Dan and ThomasW},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/5HtDzRAk7ePWsiL2L/open-problems-in-ai-x-risk-pais-5},
  urldate = {2023-12-05},
  abstract = {This is the fifth post in a sequence of posts that describe our models for Pragmatic AI Safety. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/UFTNW9YJ/Hendrycks and ThomasW - 2022 - Open Problems in AI X-Risk [PAIS #5].html}
}

@article{hendrycks_overview_2023,
  title = {An Overview of Catastrophic AI Risks},
  author = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2306.12001},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2306.12001},
  urldate = {2023-10-13},
  abstract = {Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZQBPV9NZ/Hendrycks et al. - 2023 - An Overview of Catastrophic AI Risks.pdf}
}

@article{hendrycks_perform_2022,
  title = {Perform Tractable Research While Avoiding Capabilities Externalities [Pragmatic AI Safety \#4]},
  author = {Hendrycks, Dan and ThomasW},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/dfRtxWcFDupfWpLQo/perform-tractable-research-while-avoiding-capabilities},
  urldate = {2024-02-19},
  abstract = {This is the fourth post in a sequence of posts that describe our models for Pragmatic AI Safety. {\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/WC7RJY7M/Hendrycks and ThomasW - 2022 - Perform Tractable Research While Avoiding Capabili.html}
}

@article{hendrycks_unsolved_2022,
  title = {Unsolved Problems in ML Safety},
  author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  year = {2022},
  month = jun,
  journal = {CoRR},
  eprint = {2109.13916},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.13916},
  url = {http://arxiv.org/abs/2109.13916},
  urldate = {2023-10-13},
  abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MWPVGULQ/Hendrycks et al. - 2022 - Unsolved Problems in ML Safety.pdf}
}

@article{hendrycks_what_2022,
  title = {What Would Jiminy Cricket Do? Towards Agents That Behave Morally},
  shorttitle = {What Would Jiminy Cricket Do?},
  author = {Hendrycks, Dan and Mazeika, Mantas and Zou, Andy and Patel, Sahil and Zhu, Christine and Navarro, Jesus and Song, Dawn and Li, Bo and Steinhardt, Jacob},
  year = {2022},
  month = feb,
  journal = {CoRR},
  eprint = {2110.13136},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.13136},
  url = {http://arxiv.org/abs/2110.13136},
  urldate = {2023-08-26},
  abstract = {When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong. By contrast, artificial agents are currently not endowed with a moral sense. As a consequence, they may learn to behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, it will become necessary to mitigate inherited biases from environments that teach immoral behavior. To facilitate the development of agents that avoid causing wanton harm, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of diverse, morally salient scenarios. By annotating every possible game state, the Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. Using models with commonsense moral knowledge, we create an elementary artificial conscience that assesses and guides agents. In extensive experiments, we find that the artificial conscience approach can steer agents towards moral behavior without sacrificing performance.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/MRJL9CZ5/Hendrycks et al. - 2022 - What Would Jiminy Cricket Do Towards Agents That .pdf}
}

@article{hendrycks_xrisk_2022,
  title = {X-Risk Analysis for AI Research},
  author = {Hendrycks, Dan and Mazeika, Mantas},
  year = {2022},
  month = jun,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2206.05862v7},
  urldate = {2023-10-13},
  abstract = {Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5G9NVJ7Y/Hendrycks and Mazeika - 2022 - X-Risk Analysis for AI Research.pdf}
}

@article{henighan_superposition_2023,
  title = {Superposition, Memorization, and Double Descent},
  author = {Henighan, Tom and Carter, Shan and Hume, Tristan and Elhage, Nelson and Lasenby, Robert and Fort, Stanislav and Schiefer, Nicholas and Olah, Christopher},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/toy-double-descent/index.html},
  keywords = {cited,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/KHZUIWJ6/Henighan et al. - 2023 - Superposition, Memorization, and Double Descent.html}
}

@article{hernandez_inspecting_2023,
  title = {Inspecting and Editing Knowledge Representations in Language Models},
  author = {Hernandez, Evan and Li, Belinda Z. and Andreas, Jacob},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2304.00740},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.00740},
  url = {http://arxiv.org/abs/2304.00740},
  urldate = {2023-11-10},
  abstract = {Neural language models (LMs) represent facts about the world described by text. Sometimes these facts derive from training data (in most LMs, a representation of the word "banana" encodes the fact that bananas are fruits). Sometimes facts derive from input text itself (a representation of the sentence "I poured out the bottle" encodes the fact that the bottle became empty). We describe REMEDI, a method for learning to map statements in natural language to fact encodings in an LM's internal representation system. REMEDI encodings can be used as knowledge editors: when added to LM hidden representations, they modify downstream generation to be consistent with new facts. REMEDI encodings may also be used as probes: when compared to LM representations, they reveal which properties LMs already attribute to mentioned entities, in some cases making it possible to predict when LMs will generate outputs that conflict with background knowledge or input text. REMEDI thus links work on probing, prompting, and LM editing, and offers steps toward general tools for fine-grained inspection and control of knowledge in LMs.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QZXJXHAZ/Hernandez et al. - 2023 - Inspecting and Editing Knowledge Representations i.pdf}
}

@article{hernandez_linearity_2023,
  title = {Linearity of Relation Decoding in Transformer Language Models},
  author = {Hernandez, Evan and Sharma, Arnab Sen and Haklay, Tal and Meng, Kevin and Wattenberg, Martin and Andreas, Jacob and Belinkov, Yonatan and Bau, David},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2308.09124},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2308.09124},
  url = {http://arxiv.org/abs/2308.09124},
  urldate = {2023-11-16},
  abstract = {Much of the knowledge encoded in transformer language models (LMs) may be expressed in terms of relations: relations between words and their synonyms, entities and their attributes, etc. We show that, for a subset of relations, this computation is well-approximated by a single linear transformation on the subject representation. Linear relation representations may be obtained by constructing a first-order approximation to the LM from a single prompt, and they exist for a variety of factual, commonsense, and linguistic relations. However, we also identify many cases in which LM predictions capture relational knowledge accurately, but this knowledge is not linearly encoded in their representations. Our results thus reveal a simple, interpretable, but heterogeneously deployed knowledge representation strategy in transformer LMs.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/8SL86KKE/Hernandez et al. - 2023 - Linearity of Relation Decoding in Transformer Lang.pdf}
}

@article{hernandez_natural_2022,
  title = {Natural Language Descriptions of Deep Visual Features},
  author = {Hernandez, Evan and Schwettmann, Sarah and Bau, David and Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob},
  year = {2022},
  month = apr,
  journal = {ICLR},
  eprint = {2201.11114},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2201.11114},
  url = {http://arxiv.org/abs/2201.11114},
  urldate = {2023-10-26},
  abstract = {Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JWXRKU4W/Hernandez et al. - 2022 - Natural Language Descriptions of Deep Visual Featu.pdf}
}

@article{hernandez_scaling_2022,
  title = {Scaling Laws and Interpretability of Learning from Repeated Data},
  author = {Hernandez, Danny and Brown, Tom and Conerly, Tom and DasSarma, Nova and Drain, Dawn and {El-Showk}, Sheer and Elhage, Nelson and {Hatfield-Dodds}, Zac and Henighan, Tom and Hume, Tristan and Johnston, Scott and Mann, Ben and Olah, Chris and Olsson, Catherine and Amodei, Dario and Joseph, Nicholas and Kaplan, Jared and McCandlish, Sam},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.10487},
  url = {https://arxiv.org/abs/2205.10487},
  urldate = {2024-02-11},
  abstract = {Recent large language models have been trained on vast datasets, but also often on repeated data, either intentionally for the purpose of upweighting higher quality data, or unintentionally because data deduplication is not perfect and the model is exposed to repeated data at the sentence, paragraph, or document level. Some works have reported substantial negative performance effects of this repeated data. In this paper we attempt to study repeated data systematically and to understand its effects mechanistically. To do this, we train a family of models where most of the data is unique but a small fraction of it is repeated many times. We find a strong double descent phenomenon, in which repeated data can lead test loss to increase midway through training. A predictable range of repetition frequency leads to surprisingly severe degradation in performance. For instance, performance of an 800M parameter model can be degraded to that of a 2x smaller model (400M params) by repeating 0.1\% of the data 100 times, despite the other 90\% of the training tokens remaining unique. We suspect there is a range in the middle where the data can be memorized and doing so consumes a large fraction of the model's capacity, and this may be where the peak of degradation occurs. Finally, we connect these observations to recent mechanistic interpretability work - attempting to reverse engineer the detailed computations performed by the model - by showing that data repetition disproportionately damages copying and internal structures associated with generalization, such as induction heads, providing a possible mechanism for the shift from generalization to memorization. Taken together, these results provide a hypothesis for why repeating a relatively small fraction of data in large language models could lead to disproportionately large harms to performance.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {double descent,mechinterp,not cited,scaling,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/W4HR75UZ/Hernandez et al. - 2022 - Scaling Laws and Interpretability of Learning from.pdf}
}

@article{herrmann_standards_2024,
  title = {Standards for Belief Representations in LLMs},
  author = {Herrmann, Daniel A. and Levinstein, Benjamin A.},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.21030},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.21030},
  url = {http://arxiv.org/abs/2405.21030},
  urldate = {2024-06-10},
  abstract = {As large language models (LLMs) continue to demonstrate remarkable abilities across various domains, computer scientists are developing methods to understand their cognitive processes, particularly concerning how (and if) LLMs internally represent their beliefs about the world. However, this field currently lacks a unified theoretical foundation to underpin the study of belief in LLMs. This article begins filling this gap by proposing adequacy conditions for a representation in an LLM to count as belief-like. We argue that, while the project of belief measurement in LLMs shares striking features with belief measurement as carried out in decision theory and formal epistemology, it also differs in ways that should change how we measure belief. Thus, drawing from insights in philosophy and contemporary practices of machine learning, we establish four criteria that balance theoretical considerations with practical constraints. Our proposed criteria include accuracy, coherence, uniformity, and use, which together help lay the groundwork for a comprehensive understanding of belief representation in LLMs. We draw on empirical work showing the limitations of using various criteria in isolation to identify belief representations.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/G7S9BJDI/Herrmann and Levinstein - 2024 - Standards for Belief Representations in LLMs.pdf}
}

@article{hewitt_structural_2019,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  author = {Hewitt, John and Manning, Christopher D.},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  journal = {NAACL HLT},
  pages = {4129--4138},
  doi = {10.18653/v1/N19-1419},
  url = {https://aclanthology.org/N19-1419},
  urldate = {2024-01-24},
  abstract = {Recent work has improved our ability to detect linguistic knowledge in word representations. However, current methods for detecting syntactic knowledge do not test whether syntax trees are represented in their entirety. In this work, we propose a structural probe, which evaluates whether syntax trees are embedded in a linear transformation of a neural network's word representation space. The probe identifies a linear transformation under which squared L2 distance encodes the distance between words in the parse tree, and one in which squared L2 norm encodes depth in the parse tree. Using our probe, we show that such transformations exist for both ELMo and BERT but not in baselines, providing evidence that entire syntax trees are embedded implicitly in deep models' vector geometry.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/W9UEMX8M/Hewitt and Manning - 2019 - A Structural Probe for Finding Syntax in Word Repr.pdf}
}

@article{higgins_betavae_2017,
  title = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  shorttitle = {beta-VAE},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year = {2017},
  journal = {ICLR},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate = {2023-07-31},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta {$>$} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QFGQ8XQC/Higgins et al. - 2017 - beta-VAE Learning Basic Visual Concepts with a Co.pdf}
}

@article{higgins_definition_2018,
  title = {Towards a Definition of Disentangled Representations},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  journal = {CoRR},
  eprint = {1812.02230},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1812.02230},
  url = {http://arxiv.org/abs/1812.02230},
  urldate = {2024-01-18},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EPH49C9X/Higgins et al. - 2018 - Towards a Definition of Disentangled Representatio.pdf}
}

@article{hilton_understanding_2020,
  title = {Understanding RL Vision},
  author = {Hilton, Jacob and Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris},
  year = {2020},
  journal = {Distill},
  url = {https://distill.pub/2020/understanding-rl-vision/},
  keywords = {cited,mechinterp,reinforcement learning,to cite,to extract figures,to extract related work,to review in detail,vision},
  file = {/Users/leonardbereska/Zotero/storage/6TGTXPRE/Hilton et al. - 2020 - Understanding RL Vision.html}
}

@article{hinton_distributed_1984,
  title = {Distributed representations},
  author = {Hinton, Geoffrey E},
  year = {1984},
  journal = {Carnegie Mellon University},
  publisher = {Carnegie Mellon University},
  url = {https://www.cs.toronto.edu/~hinton/absps/pdp3.pdf},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/4B5AVHFS/Hinton - 1984 - Distributed representations.pdf}
}

@article{hoagy_additional_2024,
  title = {Some additional SAE thoughts},
  author = {Hoagy},
  year = {2024},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/fqgn56tS5AgjmDpnX/some-additional-sae-thoughts},
  urldate = {2024-02-16},
  abstract = {Thanks to Lee Sharkey for feedback on the first section and Lee Sharkey, Jake Mendel, Kaarel H{\"a}nni and others at the LISA office for conversations ar{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/XNVTFL7G/Hoagy - 2024 - Some additional SAE thoughts.html}
}

@misc{hobbhahn_marius_2022,
  title = {Marius' alignment agenda},
  author = {Hobbhahn, Marius},
  year = {2022},
  publisher = {Google Docs},
  url = {https://docs.google.com/document/d/1AyuTphQ31rLHDtpZoEwEPb4fWbZna1H3hGx_YUACxk4},
  urldate = {2024-02-26},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PNLYEX8E/Hobbhahn - 2022 - Marius' alignment agenda.html}
}

@article{hobbhahn_should_2023,
  title = {Should we publish mechanistic interpretability research?},
  author = {Hobbhahn, Marius and Chan, Lawrence},
  year = {2023},
  month = apr,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/iDNEjbdHhjzvLLAmm/should-we-publish-mechanistic-interpretability-research},
  urldate = {2023-12-05},
  abstract = {TL;DR: Multiple people have raised concerns about current mechanistic interpretability research having capabilities externalities. We discuss to which extent and which kind of mechanistic interpretability research we should publish. The core question we want to explore with this post is thus to which extent the statement ``findings in mechanistic interpretability can increase capabilities faster than alignment'' is true and should be a consideration. For example, foundational findings in mechanistic interpretability may lead to a better understanding of NNs which often straightforwardly generates new hypotheses to advance capabilities. We argue that there is no general answer and the publishing decision primarily depends on how easily the work advances alignment in relation to how much it can be used to advance capabilities. We recommend a differential publishing process where work with high capabilities potential is initially only circulated with a small number of trusted people and organizations and work with low capabilities potential is published widely. Related work: A note about differential technological development, Current themes in mechanistic interpretability research, Thoughts on AGI organization and capabilities work, Dan Hendrycks's take, etc.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/B8YDSTSM/Hobbhahn and Chan - 2023 - Should we publish mechanistic interpretability res.html}
}

@article{hobbhahn_theories_2022,
  title = {Theories of impact for Science of Deep Learning},
  author = {Hobbhahn, Marius},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/tKYGvA9dKHa3GWBBk/theories-of-impact-for-science-of-deep-learning},
  urldate = {2023-11-30},
  abstract = {I'd like to thank J{\'e}r{\'e}my Scheurer and Ethan Perez for discussions about the post. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DIBSSFSZ/Hobbhahn - 2022 - Theories of impact for Science of Deep Learning.html}
}

@article{hod_quantifying_2022,
  title = {Quantifying Local Specialization in Deep Neural Networks},
  author = {Hod, Shlomi and Filan, Daniel and Casper, Stephen and Critch, Andrew and Russell, Stuart},
  year = {2022},
  month = feb,
  journal = {CoRR},
  eprint = {2110.08058},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.08058},
  url = {http://arxiv.org/abs/2110.08058},
  urldate = {2023-10-26},
  abstract = {A neural network is locally specialized to the extent that parts of its computational graph (i.e. structure) can be abstractly represented as performing some comprehensible sub-task relevant to the overall task (i.e. functionality). Are modern deep neural networks locally specialized? How can this be quantified? In this paper, we consider the problem of taking a neural network whose neurons are partitioned into clusters, and quantifying how functionally specialized the clusters are. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal local specialization and that statistical methods can be used to automatedly screen for sets of neurons that can be understood abstractly.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DUWEJVC4/Hod et al. - 2022 - Quantifying Local Specialization in Deep Neural Ne.pdf}
}

@article{hoffmann_this_2021,
  title = {This Looks Like That... Does it? Shortcomings of Latent Space Prototype Interpretability in Deep Networks},
  shorttitle = {This Looks Like That... Does it?},
  author = {Hoffmann, Adrian and Fanconi, Claudio and Rade, Rahul and Kohler, Jonas},
  year = {2021},
  month = jun,
  journal = {CoRR},
  eprint = {2105.02968},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2105.02968},
  url = {http://arxiv.org/abs/2105.02968},
  urldate = {2024-02-26},
  abstract = {Deep neural networks that yield human interpretable decisions by architectural design have lately become an increasingly popular alternative to post hoc interpretation of traditional black-box models. Among these networks, the arguably most widespread approach is so-called prototype learning, where similarities to learned latent prototypes serve as the basis of classifying an unseen data point. In this work, we point to an important shortcoming of such approaches. Namely, there is a semantic gap between similarity in latent space and similarity in input space, which can corrupt interpretability. We design two experiments that exemplify this issue on the so-called ProtoPNet. Specifically, we find that this network's interpretability mechanism can be led astray by intentionally crafted or even JPEG compression artefacts, which can produce incomprehensible decisions. We argue that practitioners ought to have this shortcoming in mind when deploying prototype-based models in practice.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/WATXSVLG/Hoffmann et al. - 2021 - This Looks Like That... Does it Shortcomings of L.pdf}
}

@article{hoffmann_training_2022,
  title = {Training Compute-Optimal Large Language Models},
  author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and van den Driessche, George and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
  year = {2022},
  month = mar,
  journal = {CoRR},
  eprint = {2203.15556},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2203.15556},
  url = {http://arxiv.org/abs/2203.15556},
  urldate = {2023-12-05},
  abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/TWN5ZXN3/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf}
}

@misc{hollmann_tabpfn_2023,
  title = {TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},
  shorttitle = {TabPFN},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  year = {2023},
  month = sep,
  number = {arXiv:2207.01848},
  eprint = {2207.01848},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  url = {http://arxiv.org/abs/2207.01848},
  urldate = {2024-07-05},
  abstract = {We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230{\texttimes} speedup. This increases to a 5 700{\texttimes} speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.},
  archiveprefix = {arxiv},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/AWWCE2CC/Hollmann et al. - 2023 - TabPFN A Transformer That Solves Small Tabular Cl.pdf}
}

@misc{hollmann_tabpfn_2023a,
  title = {TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second},
  shorttitle = {TabPFN},
  author = {Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  year = {2023},
  month = sep,
  number = {arXiv:2207.01848},
  eprint = {2207.01848},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2207.01848},
  url = {http://arxiv.org/abs/2207.01848},
  urldate = {2024-07-05},
  abstract = {We present TabPFN, a trained Transformer that can do supervised classification for small tabular datasets in less than a second, needs no hyperparameter tuning and is competitive with state-of-the-art classification methods. TabPFN performs in-context learning (ICL), it learns to make predictions using sequences of labeled examples (x, f(x)) given in the input, without requiring further parameter updates. TabPFN is fully entailed in the weights of our network, which accepts training and test samples as a set-valued input and yields predictions for the entire test set in a single forward pass. TabPFN is a Prior-Data Fitted Network (PFN) and is trained offline once, to approximate Bayesian inference on synthetic datasets drawn from our prior. This prior incorporates ideas from causal reasoning: It entails a large space of structural causal models with a preference for simple structures. On the 18 datasets in the OpenML-CC18 suite that contain up to 1 000 training data points, up to 100 purely numerical features without missing values, and up to 10 classes, we show that our method clearly outperforms boosted trees and performs on par with complex state-of-the-art AutoML systems with up to 230\${\textbackslash}times\$ speedup. This increases to a 5 700\${\textbackslash}times\$ speedup when using a GPU. We also validate these results on an additional 67 small numerical datasets from OpenML. We provide all our code, the trained TabPFN, an interactive browser demo and a Colab notebook at https://github.com/automl/TabPFN.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/3SUZUHPY/Hollmann et al. - 2023 - TabPFN A Transformer That Solves Small Tabular Cl.pdf;/Users/leonardbereska/Zotero/storage/2N67UNKY/2207.html}
}

@article{hoogland_developmental_2023,
  title = {Towards Developmental Interpretability},
  author = {Hoogland, Jesse and Oldenziel, Alexander Gietelink and Murfet, Daniel and van Wingerden, Stan},
  year = {2023},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability},
  urldate = {2023-11-10},
  abstract = {Developmental interpretability is a research agenda that has grown out of a meeting of the Singular Learning Theory (SLT) and AI alignment communitie{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/VCW7HHZL/Hoogland et al. - 2023 - Towards Developmental Interpretability.html}
}

@article{hoogland_developmental_2024,
  title = {The Developmental Landscape of In-Context Learning},
  author = {Hoogland, Jesse and Wang, George and {Farrugia-Roberts}, Matthew and Carroll, Liam and Wei, Susan and Murfet, Daniel},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.02364},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.02364},
  url = {http://arxiv.org/abs/2402.02364},
  urldate = {2024-03-19},
  abstract = {We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/42SIA2XT/Hoogland et al. - 2024 - The Developmental Landscape of In-Context Learning.pdf}
}

@article{hooker_unrestricted_2021,
  title = {Unrestricted permutation forces extrapolation: variable importance requires at least one more model, or there is no free variable importance},
  shorttitle = {Unrestricted permutation forces extrapolation},
  author = {Hooker, Giles and Mentch, Lucas and Zhou, Siyu},
  year = {2021},
  month = nov,
  journal = {Stat Comput},
  volume = {31},
  number = {6},
  pages = {82},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-021-10057-z},
  url = {https://link.springer.com/10.1007/s11222-021-10057-z},
  urldate = {2024-01-24},
  abstract = {Abstract             This paper reviews and advocates against the use of permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because they are both model-agnostic and depend only on the pre-trained model output, making them computationally efficient and widely available in software. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. The purpose of our work here is to (i) review this growing body of literature, (ii) provide further demonstrations of these drawbacks along with a detailed explanation as to why they occur, and (iii) advocate for alternative measures that involve additional modeling. In particular, we describe how breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects across various model setups and find support for previous claims in the literature that PaP metrics can vastly over-emphasize correlated features in both variable importance measures and partial dependence plots. As an alternative, we discuss and recommend more direct approaches that involve measuring the change in model performance after muting the effects of the features under investigation.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2CKK3DZX/Hooker et al. - 2021 - Unrestricted permutation forces extrapolation var.pdf}
}

@article{hotvedt_identifiability_2021,
  title = {Identifiability and physical interpretability of hybrid, gray-box models -- a case study},
  author = {Hotvedt, Mathilde and Grimstad, Bjarne and Imsland, Lars},
  year = {2021},
  journal = {IFAC-PapersOnLine},
  volume = {54},
  number = {3},
  eprint = {2010.13416},
  primaryclass = {cs, eess},
  pages = {389--394},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2021.08.273},
  url = {http://arxiv.org/abs/2010.13416},
  urldate = {2024-02-13},
  abstract = {Model identifiability concerns the uniqueness of uncertain model parameters to be estimated from available process data and is often thought of as a prerequisite for the physical interpretability of a model. Nevertheless, model identifiability may be challenging to obtain in practice due to both stochastic and deterministic uncertainties, e.g. low data variability, noisy measurements, erroneous model structure, and stochasticity and locality of the optimization algorithm. For gray-box, hybrid models, model identifiability is rarely obtainable due to a high number of parameters. We illustrate through an industrial case study - modeling of a production choke valve in a petroleum well - that physical interpretability may be preserved even for non-identifiable models with adequate parameter regularization in the estimation problem. To this end, in a real industrial scenario, it may be beneficial for the model's predictive performance to develop hybrid over mechanistic models, as the model flexibility is higher. Modeling of six petroleum wells on the asset Edvard Grieg using historical production data show a 35{\textbackslash}\% reduction in the median prediction error across the wells comparing a hybrid to a mechanistic model. On the other hand, both the predictive performance and physical interpretability of the developed models are influenced by the available data. The findings encourage research into online learning and other hybrid model variants to improve the results.},
  archiveprefix = {arxiv},
  keywords = {not cited,physics},
  file = {/Users/leonardbereska/Zotero/storage/9TT786X7/Hotvedt et al. - 2021 - Identifiability and physical interpretability of h.pdf}
}

@article{hou_mechanistic_2023,
  title = {Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models},
  author = {Hou, Yifan and Li, Jiaoda and Fei, Yu and Stolfo, Alessandro and Zhou, Wangchunshu and Zeng, Guangtao and Bosselut, Antoine and Sachan, Mrinmaya},
  year = {2023},
  journal = {EMN},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.14491},
  url = {https://arxiv.org/abs/2310.14491},
  urldate = {2024-02-11},
  abstract = {Recent work has shown that language models (LMs) have strong multi-step (i.e., procedural) reasoning capabilities. However, it is unclear whether LMs perform these tasks by cheating with answers memorized from pretraining corpus, or, via a multi-step reasoning mechanism. In this paper, we try to answer this question by exploring a mechanistic interpretation of LMs for multi-step reasoning tasks. Concretely, we hypothesize that the LM implicitly embeds a reasoning tree resembling the correct reasoning process within it. We test this hypothesis by introducing a new probing approach (called MechanisticProbe) that recovers the reasoning tree from the model's attention patterns. We use our probe to analyze two LMs: GPT-2 on a synthetic task (k-th smallest element), and LLaMA on two simple language-based reasoning tasks (ProofWriter \&amp; AI2 Reasoning Challenge). We show that MechanisticProbe is able to detect the information of the reasoning tree from the model's attentions for most examples, suggesting that the LM indeed is going through a process of multi-step reasoning within its architecture in many cases.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {graph,mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/4J4QQJBY/Hou et al. - 2023 - Towards a Mechanistic Interpretation of Multi-Step.pdf}
}

@article{hoyt_probing_2021,
  title = {Probing neural networks with t-SNE, class-specific projections and a guided tour},
  author = {Hoyt, Christopher R. and Owen, Art B.},
  year = {2021},
  month = jul,
  journal = {CoRR},
  eprint = {2107.12547},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2107.12547},
  url = {http://arxiv.org/abs/2107.12547},
  urldate = {2024-02-19},
  abstract = {We use graphical methods to probe neural nets that classify images. Plots of t-SNE outputs at successive layers in a network reveal increasingly organized arrangement of the data points. They can also reveal how a network can diminish or even forget about within-class structure as the data proceeds through layers. We use class-specific analogues of principal components to visualize how succeeding layers separate the classes. These allow us to sort images from a given class from most typical to least typical (in the data) and they also serve as very useful projection coordinates for data visualization. We find them especially useful when defining versions guided tours for animated data visualization.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/4F4S6AGR/Hoyt and Owen - 2021 - Probing neural networks with t-SNE, class-specific.pdf}
}

@article{hsu_language_2022,
  title = {Language model compression with weighted low-rank factorization},
  author = {Hsu, Yen-Chang and Hua, Ting and Chang, Sungen and Lou, Qian and Shen, Yilin and Jin, Hongxia},
  year = {2022},
  month = jun,
  journal = {ICLR},
  eprint = {2207.00112},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2207.00112},
  url = {http://arxiv.org/abs/2207.00112},
  urldate = {2024-03-19},
  abstract = {Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9\% to 30\% parameters with an insignificant impact on task accuracy.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/4L7MLIV7/Hsu et al. - 2022 - Language model compression with weighted low-rank .pdf}
}

@article{hu_lora_2021,
  title = {LoRA: Low-Rank Adaptation of Large Language Models},
  shorttitle = {LoRA},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  journal = {CoRR},
  eprint = {2106.09685},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.09685},
  url = {http://arxiv.org/abs/2106.09685},
  urldate = {2024-03-19},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/UUAUSFUI/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf}
}

@article{hu_provable_2020,
  title = {Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks},
  author = {Hu, Wei and Xiao, Lechao and Pennington, Jeffrey},
  year = {2020},
  month = jan,
  journal = {ICLR},
  eprint = {2001.05992},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.2001.05992},
  url = {http://arxiv.org/abs/2001.05992},
  urldate = {2023-11-12},
  abstract = {The selection of initial parameter values for gradient-based optimization of deep neural networks is one of the most impactful hyperparameter choices in deep learning systems, affecting both convergence times and model performance. Yet despite significant empirical and theoretical analysis, relatively little has been proved about the concrete effects of different initialization schemes. In this work, we analyze the effect of initialization in deep linear networks, and provide for the first time a rigorous proof that drawing the initial weights from the orthogonal group speeds up convergence relative to the standard Gaussian initialization with iid weights. We show that for deep networks, the width needed for efficient convergence to a global minimum with orthogonal initializations is independent of the depth, whereas the width needed for efficient convergence with Gaussian initializations scales linearly in the depth. Our results demonstrate how the benefits of a good initialization can persist throughout learning, suggesting an explanation for the recent empirical successes found by initializing very deep non-linear networks according to the principle of dynamical isometry.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RUSCVZKI/Hu et al. - 2020 - Provable Benefit of Orthogonal Initialization in O.pdf}
}

@article{huang_incontext_2023,
  title = {In-Context Convergence of Transformers},
  author = {Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.05249},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.2310.05249},
  url = {http://arxiv.org/abs/2310.05249},
  urldate = {2024-03-19},
  abstract = {Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/ID6LW5NB/Huang et al. - 2023 - In-Context Convergence of Transformers.pdf}
}

@article{huang_large_2022,
  title = {Large Language Models Can Self-Improve},
  author = {Huang, Jiaxin and Gu, Shixiang Shane and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2210.11610},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2210.11610},
  urldate = {2023-01-25},
  abstract = {Large Language Models (LLMs) have achieved excellent performances in various tasks. However, fine-tuning an LLM requires extensive supervision. Human, on the other hand, may improve their reasoning abilities by self-thinking without external inputs. In this work, we demonstrate that an LLM is also capable of self-improving with only unlabeled datasets. We use a pre-trained LLM to generate "high-confidence" rationale-augmented answers for unlabeled questions using Chain-of-Thought prompting and self-consistency, and fine-tune the LLM using those self-generated solutions as target outputs. We show that our approach improves the general reasoning ability of a 540B-parameter LLM (74.4\%-{$>$}82.1\% on GSM8K, 78.2\%-{$>$}83.0\% on DROP, 90.0\%-{$>$}94.4\% on OpenBookQA, and 63.4\%-{$>$}67.9\% on ANLI-A3) and achieves state-of-the-art-level performance, without any ground truth label. We conduct ablation studies and show that fine-tuning on reasoning is critical for self-improvement.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RJVIFMVR/Huang et al. - 2022 - Large Language Models Can Self-Improve.pdf}
}

@article{huang_ravel_2024,
  title = {RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations},
  shorttitle = {RAVEL},
  author = {Huang, Jing and Wu, Zhengxuan and Potts, Christopher and Geva, Mor and Geiger, Atticus},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.17700},
  url = {https://arxiv.org/abs/2402.17700},
  urldate = {2024-06-10},
  abstract = {Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/APBNEB48/Huang et al. - 2024 - RAVEL Evaluating Interpretability Methods on Dise.pdf}
}

@article{huang_rigorously_2023,
  title = {Rigorously Assessing Natural Language Explanations of Neurons},
  author = {Huang, Jing and Geiger, Atticus and D'Oosterlinck, Karel and Wu, Zhengxuan and Potts, Christopher},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.10312},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.10312},
  url = {http://arxiv.org/abs/2309.10312},
  urldate = {2023-11-16},
  abstract = {Natural language is an appealing medium for explaining how large language models process and store information, but evaluating the faithfulness of such explanations is challenging. To help address this, we develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input. In the observational mode, we evaluate claims that a neuron \$a\$ activates on all and only input strings that refer to a concept picked out by the proposed explanation \$E\$. In the intervention mode, we construe \$E\$ as a claim that the neuron \$a\$ is a causal mediator of the concept denoted by \$E\$. We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy. We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/8PN9XLM5/Huang et al. - 2023 - Rigorously Assessing Natural Language Explanations.pdf}
}

@article{hubinger_agents_2023,
  title = {Agents vs. Predictors: Concrete differentiating factors},
  shorttitle = {Agents vs. Predictors},
  author = {Hubinger, Evan},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/eQ4eLQAmPvp9anJcB/agents-vs-predictors-concrete-differentiating-factors},
  urldate = {2023-05-15},
  abstract = {Thanks to Paul Christiano and Kate Woolverton for useful conversations and feedback. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YX82KHM6/Hubinger - 2023 - Agents vs. Predictors Concrete differentiating fa.html}
}

@article{hubinger_automating_2021,
  title = {Automating Auditing: An ambitious concrete technical research proposal},
  shorttitle = {Automating Auditing},
  author = {Hubinger, Evan},
  year = {2021},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/cQwT8asti3kyA62zc/automating-auditing-an-ambitious-concrete-technical-research},
  urldate = {2023-10-26},
  abstract = {This post was originally written as a research proposal for the new AI alignment research organization Redwood Research, detailing an ambitious, conc{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/R9A2GFMQ/Hubinger - 2021 - Automating Auditing An ambitious concrete technic.html}
}

@article{hubinger_chris_2019,
  title = {Chris Olah's views on AGI safety},
  author = {Hubinger, Evan},
  year = {2019},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety},
  urldate = {2023-11-29},
  abstract = {Note: I am not Chris Olah. This post was the result of lots of back-and-forth with Chris, but everything here is my interpretation of what Chris beli{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/E6U34XG5/Hubinger - 2019 - Chris Olahs views on AGI safety.html}
}

@article{hubinger_conditioning_2023,
  title = {Conditioning Predictive Models: Risks and Strategies},
  shorttitle = {Conditioning Predictive Models},
  author = {Hubinger, Evan and Jermyn, Adam and Treutlein, Johannes and Hudson, Rubi and Woolverton, Kate},
  year = {2023},
  month = feb,
  journal = {CoRR},
  eprint = {2302.00805},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2302.00805},
  url = {http://arxiv.org/abs/2302.00805},
  urldate = {2023-05-08},
  abstract = {Our intention is to provide a definitive reference on what it would take to safely make use of generative/predictive models in the absence of a solution to the Eliciting Latent Knowledge problem. Furthermore, we believe that large language models can be understood as such predictive models of the world, and that such a conceptualization raises significant opportunities for their safe yet powerful use via carefully conditioning them to predict desirable outputs. Unfortunately, such approaches also raise a variety of potentially fatal safety problems, particularly surrounding situations where predictive models predict the output of other AI systems, potentially unbeknownst to us. There are numerous potential solutions to such problems, however, primarily via carefully conditioning models to predict the things we want (e.g. humans) rather than the things we don't (e.g. malign AIs). Furthermore, due to the simplicity of the prediction objective, we believe that predictive models present the easiest inner alignment problem that we are aware of. As a result, we think that conditioning approaches for predictive models represent the safest known way of eliciting human-level and slightly superhuman capabilities from large language models and other similar future models.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/7VPGYP89/Hubinger et al. - 2023 - Conditioning Predictive Models Risks and Strategi.pdf}
}

@article{hubinger_deceptive_2019,
  title = {Deceptive Alignment},
  author = {Hubinger, Evan and van Merwijk, Chris and Mikulik, Vlad and Skalse, Joar and Garrabrant, Scott},
  year = {2019},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/zthDPAjh9w6Ytbeks/deceptive-alignment},
  urldate = {2024-02-19},
  abstract = {This is the fourth of five posts in the Risks from Learned Optimization Sequence based on the paper ``Risks from Learned Optimization in Advanced Mach{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/TZ5TMQVY/Hubinger et al. - 2019 - Deceptive Alignment.html}
}

@article{hubinger_gradient_2019,
  title = {Gradient hacking},
  author = {Hubinger, Evan},
  year = {2019},
  month = oct,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/uXH4r6MmKPedk8rMA/gradient-hacking},
  urldate = {2023-12-08},
  abstract = {"Gradient hacking" is a term I've been using recently to describe the phenomenon wherein a deceptively aligned mesa-optimizer might be able to purpos{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/SNAWU8RJ/Hubinger - 2019 - Gradient hacking.html}
}

@article{hubinger_overview_2020,
  title = {An overview of 11 proposals for building safe advanced AI},
  author = {Hubinger, Evan},
  year = {2020},
  month = dec,
  journal = {CoRR},
  eprint = {2012.07532},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.07532},
  url = {http://arxiv.org/abs/2012.07532},
  urldate = {2023-10-19},
  abstract = {This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/6E3KPBXW/Hubinger - 2020 - An overview of 11 proposals for building safe adva.pdf}
}

@article{hubinger_relaxed_2019,
  title = {Relaxed adversarial training for inner alignment},
  author = {Hubinger, Evan},
  year = {2019},
  month = sep,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment},
  urldate = {2023-11-29},
  abstract = {This post is part of research I did at OpenAI with mentoring and guidance from Paul Christiano. It also represents my current agenda regarding what I{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/VUUIFAW2/Hubinger - 2019 - Relaxed adversarial training for inner alignment.html}
}

@article{hubinger_risks_2019,
  title = {Risks from Learned Optimization in Advanced Machine Learning Systems},
  author = {Hubinger, Evan and {van Merwijk}, Chris and Mikulik, Vladimir and Skalse, Joar and Garrabrant, Scott},
  year = {2019},
  month = may,
  journal = {CoRR},
  eprint = {1906.01820},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1906.01820},
  url = {http://arxiv.org/abs/1906.01820},
  urldate = {2023-10-25},
  abstract = {We analyze the type of learned optimization that occurs when a learned model (such as a neural network) is itself an optimizer - a situation we refer to as mesa-optimization, a neologism we introduce in this paper. We believe that the possibility of mesa-optimization raises two important questions for the safety and transparency of advanced machine learning systems. First, under what circumstances will learned models be optimizers, including when they should not be? Second, when a learned model is an optimizer, what will its objective be - how will it differ from the loss function it was trained under - and how can it be aligned? In this paper, we provide an in-depth analysis of these two primary questions and provide an overview of topics for future research.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/GQK8IYM5/Hubinger et al. - 2019 - Risks from Learned Optimization in Advanced Machin.pdf}
}

@article{hubinger_sleeper_2024,
  title = {Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
  shorttitle = {Sleeper Agents},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.05566},
  url = {https://arxiv.org/abs/2401.05566},
  urldate = {2024-02-11},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {backdoor,cited,fine-tuning,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/ICX7LHRW/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persi.pdf}
}

@article{hubinger_transparency_2022,
  title = {A transparency and interpretability tech tree},
  author = {Hubinger, Evan},
  year = {2022},
  month = jun,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree},
  urldate = {2023-11-29},
  abstract = {Thanks to Chris Olah, Neel Nanda, Kate Woolverton, Richard Ngo, Buck Shlegeris, Daniel Kokotajlo, Kyle McDonell, Laria Reynolds, Eliezer Yudkowksy, M{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/7LGQWJ3I/Hubinger - 2022 - A transparency and interpretability tech tree.html}
}

@article{icke_automated_2011,
  title = {Automated measures for interpretable dimensionality reduction for visual classification: A user study},
  shorttitle = {Automated measures for interpretable dimensionality reduction for visual classification},
  author = {Icke, Ilknur and Rosenberg, Andrew},
  year = {2011},
  month = oct,
  journal = {VAST},
  pages = {281--282},
  publisher = {IEEE},
  address = {Providence, RI, USA},
  doi = {10.1109/VAST.2011.6102474},
  url = {http://ieeexplore.ieee.org/document/6102474/},
  urldate = {2023-08-27},
  abstract = {This paper studies the interpretability of transformations of labeled higher dimensional data into a 2D representation (scatterplots) for visual classification.1In this context, the term interpretability has two components: the interpretability of the visualization (the image itself) and the interpretability of the visualization axes (the data transformation functions). We define a data transformation function as any linear or non-linear function of the original variables mapping the data into 1D. Even for a small dataset, the space of possible data transformations is beyond the limit of manual exploration, therefore it is important to develop automated techniques that capture both aspects of interpretability so that they can be used to guide the search process without human intervention. The goal of the search process is to find a smaller number of interpretable data transformations for the users to explore. We briefly discuss how we used such automated measures in an evolutionary computing based data dimensionality reduction application for visual analytics. In this paper, we present a two-part user study in which we separately investigated how humans rated the visualizations of labeled data and comprehensibility of mathematical expressions that could be used as data transformation functions. In the first part, we compared human perception with a number of automated measures from the machine learning and visual analytics literature. In the second part, we studied how various structural properties of an expression related to its interpretability.},
  isbn = {9781467300148 9781467300155},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4GZVEDBB/Icke and Rosenberg - 2011 - Automated measures for interpretable dimensionalit.pdf}
}

@article{ilyas_adversarial_2019,
  title = {Adversarial Examples Are Not Bugs, They Are Features},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  year = {2019},
  month = aug,
  journal = {NeurIPS},
  eprint = {1905.02175},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.02175},
  url = {http://arxiv.org/abs/1905.02175},
  urldate = {2023-11-29},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9KITH3U2/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf}
}

@article{irving_ai_2018,
  title = {AI safety via debate},
  author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
  year = {2018},
  month = oct,
  journal = {CoRR},
  eprint = {1805.00899},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1805.00899},
  url = {http://arxiv.org/abs/1805.00899},
  urldate = {2023-03-20},
  abstract = {To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classifier, boosting the classifier's accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/697KK6DK/Irving et al. - 2018 - AI safety via debate.pdf}
}

@article{islam_systematic_2022,
  title = {A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks},
  author = {Islam, Mir Riyanul and Ahmed, Mobyen Uddin and Barua, Shaibal and Begum, Shahina},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {3},
  pages = {1353},
  issn = {2076-3417},
  doi = {10.3390/app12031353},
  url = {https://www.mdpi.com/2076-3417/12/3/1353},
  urldate = {2023-10-20},
  abstract = {Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers' understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DTQ2ILBR/Islam et al. - 2022 - A Systematic Review of Explainable Artificial Inte.pdf}
}

@article{ivanitskiy_structured_2023,
  title = {Structured World Representations in Maze-Solving Transformers},
  author = {Ivanitskiy, M. and Spies, Alexander F. and Rauker, Tilman and Corlouer, Guillaume and Mathwin, Chris and Quirke, Lucia and Rager, Can and Shah, Rusheb and Valentine, Dan and Behn, Cecilia Diniz and Inoue, Katsumi and Fung, Samy Wu},
  year = {2023},
  month = dec,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2312.02566},
  urldate = {2023-12-11},
  abstract = {Transformer models underpin many recent advances in practical machine learning applications, yet understanding their internal behavior continues to elude researchers. Given the size and complexity of these models, forming a comprehensive picture of their inner workings remains a significant challenge. To this end, we set out to understand small transformer models in a more tractable setting: that of solving mazes. In this work, we focus on the abstractions formed by these models and find evidence for the consistent emergence of structured internal representations of maze topology and valid paths. We demonstrate this by showing that the residual stream of only a single token can be linearly decoded to faithfully reconstruct the entire maze. We also find that the learned embeddings of individual tokens have spatial structure. Furthermore, we take steps towards deciphering the circuity of path-following by identifying attention heads (dubbed \${\textbackslash}textit\{adjacency heads\}\$), which are implicated in finding valid subsequent tokens.},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/BFLH5XJS/Ivanitskiy et al. - 2023 - Structured World Representations in Maze-Solving T.pdf}
}

@article{jacob_emergent_2023,
  title = {Emergent Deception and Emergent Optimization},
  author = {Steinhardt, Jacob},
  year = {2023},
  month = feb,
  journal = {Bounded Regret},
  url = {https://bounded-regret.ghost.io/emergent-deception-optimization/},
  urldate = {2023-05-17},
  abstract = {I've previously argued that machine learning systems often exhibit emergent capabilities, and that these capabilities could lead to unintended negative consequences. But how can we reason concretely about these consequences?},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/YW3AZ6N7/Steinhardt - 2023 - Emergent Deception and Emergent Optimization.html}
}

@article{jacob_ml_2022,
  title = {ML Systems Will Have Weird Failure Modes},
  author = {Steinhardt, Jacob},
  year = {2022},
  month = jan,
  journal = {Bounded Regret},
  url = {https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/},
  urldate = {2023-05-17},
  abstract = {Previously, I've argued that future ML systems might exhibit unfamiliar, emergent capabilities [https://bounded-regret.ghost.io/p/1527e9dd-c48d-4941-9b14-4f7293318d5c/], and that thought experiments provide one approach [https://bounded-regret.ghost.io/p/a2d733a7-108a-4587-97fb-db90f66ce030/]  towards predicting these capabilities and their consequences. In this post I'll describe a particular thought experiment in detail.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/984RC424/Steinhardt - 2022 - ML Systems Will Have Weird Failure Modes.html}
}

@article{jacovi_faithfully_2020,
  title = {Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?},
  shorttitle = {Towards Faithfully Interpretable NLP Systems},
  author = {Jacovi, Alon and Goldberg, Yoav},
  year = {2020},
  month = apr,
  journal = {CoRR},
  eprint = {2004.03685},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2004.03685},
  url = {http://arxiv.org/abs/2004.03685},
  urldate = {2023-10-20},
  abstract = {With the growing popularity of deep-learning based NLP models, comes a need for interpretable systems. But what is interpretability, and what constitutes a high-quality interpretation? In this opinion piece we reflect on the current state of interpretability evaluation research. We call for more clearly differentiating between different desired criteria an interpretation should satisfy, and focus on the faithfulness criteria. We survey the literature with respect to faithfulness evaluation, and arrange the current approaches around three assumptions, providing an explicit form to how faithfulness is "defined" by the community. We provide concrete guidelines on how evaluation of interpretation methods should and should not be conducted. Finally, we claim that the current binary definition for faithfulness sets a potentially unrealistic bar for being considered faithful. We call for discarding the binary notion of faithfulness in favor of a more graded one, which we believe will be of greater practical utility.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ASHH7M5J/Jacovi and Goldberg - 2020 - Towards Faithfully Interpretable NLP Systems How .pdf}
}

@article{jain_attention_2019,
  title = {Attention is not Explanation},
  author = {Jain, Sarthak and Wallace, Byron C.},
  year = {2019},
  month = may,
  journal = {NAACL},
  eprint = {1902.10186},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1902.10186},
  url = {http://arxiv.org/abs/1902.10186},
  urldate = {2024-03-20},
  abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/U73N6MWV/Jain and Wallace - 2019 - Attention is not Explanation.pdf}
}

@article{jain_distilling_2022,
  title = {Distilling Model Failures as Directions in Latent Space},
  author = {Jain, Saachi and Lawrence, Hannah and Moitra, Ankur and Madry, Aleksander},
  year = {2022},
  month = dec,
  journal = {ICLR},
  eprint = {2206.14754},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.14754},
  url = {http://arxiv.org/abs/2206.14754},
  urldate = {2023-10-26},
  abstract = {Existing methods for isolating hard subpopulations and spurious correlations in datasets often require human intervention. This can make these methods labor-intensive and dataset-specific. To address these shortcomings, we present a scalable method for automatically distilling a model's failure modes. Specifically, we harness linear classifiers to identify consistent error patterns, and, in turn, induce a natural representation of these failure modes as directions within the feature space. We demonstrate that this framework allows us to discover and automatically caption challenging subpopulations within the training dataset. Moreover, by combining our framework with off-the-shelf diffusion models, we can generate images that are especially challenging for the analyzed model, and thus can be used to perform synthetic data augmentation that helps remedy the model's failure modes. Code available at https://github.com/MadryLab/failure-directions},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LCA7U2KN/Jain et al. - 2022 - Distilling Model Failures as Directions in Latent .pdf}
}

@article{jain_mechanistically_2023,
  title = {Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks},
  author = {Jain, Samyak and Kirk, Robert and Lubana, Ekdeep Singh and Dick, Robert P. and Tanaka, Hidenori and Grefenstette, Edward and Rockt{\"a}schel, Tim and Krueger, David Scott},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2311.12786},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2311.12786},
  url = {http://arxiv.org/abs/2311.12786},
  urldate = {2023-11-30},
  abstract = {Fine-tuning large pre-trained models has become the de facto strategy for developing both task-specific and general-purpose machine learning systems, including developing models that are safe to deploy. Despite its clear importance, there has been minimal work that explains how fine-tuning alters the underlying capabilities learned by a model during pretraining: does fine-tuning yield entirely novel capabilities or does it just modulate existing ones? We address this question empirically in synthetic, controlled settings where we can use mechanistic interpretability tools (e.g., network pruning and probing) to understand how the model's underlying capabilities are changing. We perform an extensive analysis of the effects of fine-tuning in these settings, and show that: (i) fine-tuning rarely alters the underlying model capabilities; (ii) a minimal transformation, which we call a 'wrapper', is typically learned on top of the underlying model capabilities, creating the illusion that they have been modified; and (iii) further fine-tuning on a task where such hidden capabilities are relevant leads to sample-efficient 'revival' of the capability, i.e., the model begins reusing these capability after only a few gradient steps. This indicates that practitioners can unintentionally remove a model's safety wrapper merely by fine-tuning it on a, e.g., superficially unrelated, downstream task. We additionally perform analysis on language models trained on the TinyStories dataset to support our claims in a more realistic setup.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/7FV28RQM/Jain et al. - 2023 - Mechanistically analyzing the effects of fine-tuni.pdf}
}

@article{jaiswal_survey_2021,
  title = {A Survey on Contrastive Self-Supervised Learning},
  author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
  year = {2021},
  month = mar,
  journal = {Technologies},
  volume = {9},
  number = {1},
  pages = {2},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2227-7080},
  doi = {10.3390/technologies9010002},
  url = {https://www.mdpi.com/2227-7080/9/1/2},
  urldate = {2024-02-19},
  abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudolabels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we present a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make meaningful progress.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/A6EBSU6E/Jaiswal et al. - 2021 - A Survey on Contrastive Self-Supervised Learning.pdf}
}

@article{jan_simulators_2023,
  title = {[Simulators seminar sequence] \#1 Background \& shared assumptions},
  author = {Jan and Steiner, Charlie and Riggs, Logan and {janus} and {jacquesthibs} and {metasemi} and Oesterle, Michael and Teixeira, Lucas and {peligrietzer} and {remember}},
  year = {2023},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/nmMorGE4MS4txzr8q/simulators-seminar-sequence-1-background-and-shared},
  urldate = {2023-05-15},
  abstract = {Meta: Over the past few months, we've held a seminar series on the Simulators theory by janus. As the theory is actively under development, the purpose of the series is to discover central structures{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VL423GJL/Jan et al. - 2023 - [Simulators seminar sequence] #1 Background & shar.html}
}

@article{janes_datadriven_2006,
  title = {Data-driven modelling of signal-transduction networks},
  author = {Janes, Kevin A. and Yaffe, Michael B.},
  year = {2006},
  month = nov,
  journal = {Nat Rev Mol Cell Biol},
  volume = {7},
  number = {11},
  pages = {820--828},
  publisher = {Nature Publishing Group},
  issn = {1471-0080},
  doi = {10.1038/nrm2041},
  url = {https://www.nature.com/articles/nrm2041},
  urldate = {2024-04-30},
  abstract = {New experimental techniques are allowing the generation of complex data sets that characterize signal-transduction networks. It is no longer possible to inspect these data by intuition to extract the maximal amount of information that is embedded within them.'Data-driven models' are mathematical approaches that provide simplified representations of complex data sets. They are based solely on analysing the data itself, without having to make any assumptions about the underlying mechanisms.This User's guide introduces three data-driven modelling approaches: clustering, principal components analysis (PCA), and partial least squares (PLS). Clustering provides a means for data organization, whereas PCA is a method for data condensation and PLS is a technique for data prediction.Clustering groups observations together that have similar projections in the high-dimensional space defined by the signalling variables. Similarity can be defined by several difference distance metrics, such as Euclidean distance (for absolute distances) and Pearson distance (for correlations).PCA and PLS factorize a data set into the product of two vectors (a scores vector and a loadings vector) that capture the leading eigenvalues of the covariance of the data. PCA calculates scores and loadings vectors to maximize the variance that is captured in the starting data matrix. By contrast, PLS calculates scores and loadings vectors to maximize the relationship between a matrix of independent variables and a matrix of dependent variables.Data-driven models are poised to become standard tools in analysing signalling networks as complex protein data sets become easier to acquire and more difficult to interpret.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/SXHWF6P9/Janes and Yaffe - 2006 - Data-driven modelling of signal-transduction netwo.pdf}
}

@article{janus_simulacra_2023,
  title = {Simulacra are Things},
  author = {{janus}},
  year = {2023},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/3BDqZMNSJDBg2oyvW/simulacra-are-things},
  urldate = {2024-01-03},
  abstract = {This is a note I wrote about a year ago. It's fairly self-contained, so I decided to make a post out of it after Vladimir\_Nesov's comment caused me t{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZYXW8DP2/janus - 2023 - Simulacra are Things.html}
}

@article{janus_simulators_2022,
  title = {Simulators},
  author = {{janus}},
  year = {2022},
  month = sep,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators},
  urldate = {2023-05-15},
  abstract = {Thanks to Chris Scammell, Adam Shimi, Lee Sharkey, Evan Hubinger, Nicholas Dupuis, Leo Gao, Johannes Treutlein, and Jonathan Low for feedback on drafts. {\dots}},
  language = {en},
  keywords = {cited},
  annotation = {{\textbackslash}url\{https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators\}},
  file = {/Users/leonardbereska/Zotero/storage/RXM45H6A/janus - 2022 - Simulators.html}
}

@article{jaqaman_linking_2006,
  title = {Linking data to models: data regression},
  shorttitle = {Linking data to models},
  author = {Jaqaman, Khuloud and Danuser, Gaudenz},
  year = {2006},
  month = nov,
  journal = {Nat Rev Mol Cell Biol},
  volume = {7},
  number = {11},
  pages = {813--819},
  publisher = {Nature Publishing Group},
  issn = {1471-0080},
  doi = {10.1038/nrm2030},
  url = {https://www.nature.com/articles/nrm2030},
  urldate = {2024-04-30},
  abstract = {Mathematical modelling is an essential tool in systems biology. To ensure the accuracy of mathematical models, model parameters must be estimated using experimental data, a process called regression. Also, pre-regression and post-regression diagnostics must be employed to evaluate the model goodness-of-fit and the reliability of the estimated parameter values.Maximum likelihood estimation and least-squares fitting are the most common regression schemes, yielding parameter values and their variance--covariance matrix. They work under the assumption that the estimated parameters have a normal distribution. When this assumption is not valid, Bayesian inference can be used, yielding the full parameter distribution.Prior to regression, the structural identifiability of models must be assessed to determine whether model parameters can be uniquely determined and what data are required to achieve that.Post-regression diagnostics include testing a model's goodness-of-fit, determining which model among competing ones fits the data best, evaluating parameter determinability and evaluating parameter significance.Parameters in probabilistic models must be inferred by either indirect inference or by Bayesian methods. In indirect inference, model parameters are estimated by minimizing the differences between intermediate statistics that characterize simulated and experimental data.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/B9EFG9R9/Jaqaman and Danuser - 2006 - Linking data to models data regression.pdf}
}

@article{jastrzebski_residual_2018,
  title = {Residual Connections Encourage Iterative Inference},
  author = {Jastrz{\k e}bski, Stanis{\l}aw and Arpit, Devansh and Ballas, Nicolas and Verma, Vikas and Che, Tong and Bengio, Yoshua},
  year = {2018},
  month = mar,
  journal = {ICLR},
  eprint = {1710.04773},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1710.04773},
  url = {http://arxiv.org/abs/1710.04773},
  urldate = {2023-11-10},
  abstract = {Residual networks (Resnets) have become a prominent architecture in deep learning. However, a comprehensive understanding of Resnets is still a topic of ongoing research. A recent view argues that Resnets perform iterative refinement of features. We attempt to further expose properties of this aspect. To this end, we study Resnets both analytically and empirically. We formalize the notion of iterative refinement in Resnets by showing that residual connections naturally encourage features of residual blocks to move along the negative gradient of loss as we go from one block to the next. In addition, our empirical analysis suggests that Resnets are able to perform both representation learning and iterative refinement. In general, a Resnet block tends to concentrate representation learning behavior in the first few layers while higher layers perform iterative refinement of features. Finally we observe that sharing residual layers naively leads to representation explosion and counterintuitively, overfitting, and we show that simple existing strategies can help alleviating this problem.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/TCN3ZNBE/Jastrzbski et al. - 2018 - Residual Connections Encourage Iterative Inference.pdf}
}

@article{jelassi_vision_2022,
  title = {Vision Transformers provably learn spatial structure},
  author = {Jelassi, Samy and Sander, Michael E. and Li, Yuanzhi},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2210.09221},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.09221},
  url = {http://arxiv.org/abs/2210.09221},
  urldate = {2024-03-19},
  abstract = {Vision Transformers (ViTs) have achieved comparable or superior performance than Convolutional Neural Networks (CNNs) in computer vision. This empirical breakthrough is even more remarkable since, in contrast to CNNs, ViTs do not embed any visual inductive bias of spatial locality. Yet, recent works have shown that while minimizing their training loss, ViTs specifically learn spatially localized patterns. This raises a central question: how do ViTs learn these patterns by solely minimizing their training loss using gradient-based methods from random initialization? In this paper, we provide some theoretical justification of this phenomenon. We propose a spatially structured dataset and a simplified ViT model. In this model, the attention matrix solely depends on the positional encodings. We call this mechanism the positional attention mechanism. On the theoretical side, we consider a binary classification task and show that while the learning problem admits multiple solutions that generalize, our model implicitly learns the spatial structure of the dataset while generalizing: we call this phenomenon patch association. We prove that patch association helps to sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but differ in the features. Lastly, we empirically verify that a ViT with positional attention performs similarly to the original one on CIFAR-10/100, SVHN and ImageNet.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/EBXQWH9F/Jelassi et al. - 2022 - Vision Transformers provably learn spatial structu.pdf}
}

@article{jenner_comparison_2023,
  title = {A comparison of causal scrubbing, causal abstractions, and related methods},
  author = {Jenner, Erik and {Garriga-alonso}, Adri{\`a} and Zverev, Egor},
  year = {2023},
  month = jun,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/uLMWMeBG3ruoBRhMW/a-comparison-of-causal-scrubbing-causal-abstractions-and},
  urldate = {2023-12-20},
  abstract = {Summary: We explain the similarities and differences between three recent approaches to testing interpretability hypotheses:~causal scrubbing, Geiger{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/797I89WF/Jenner et al. - 2023 - A comparison of causal scrubbing, causal abstracti.html}
}

@article{jenner_evidence_2024,
  title = {Evidence of Learned Look-Ahead in a Chess-Playing Neural Network},
  author = {Jenner, Erik and Kapur, Shreyas and Georgiev, Vasil and Allen, Cameron and Emmons, Scott and Russell, Stuart},
  year = {2024},
  month = jun,
  journal = {CoRR},
  eprint = {2406.00877},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.00877},
  url = {http://arxiv.org/abs/2406.00877},
  urldate = {2024-06-10},
  abstract = {Do neural networks learn to implement algorithms such as look-ahead or search "in the wild"? Or do they rely purely on collections of simple heuristics? We present evidence of learned look-ahead in the policy network of Leela Chess Zero, the currently strongest neural chess engine. We find that Leela internally represents future optimal moves and that these representations are crucial for its final output in certain board states. Concretely, we exploit the fact that Leela is a transformer that treats every chessboard square like a token in language models, and give three lines of evidence (1) activations on certain squares of future moves are unusually important causally; (2) we find attention heads that move important information "forward and backward in time," e.g., from squares of future moves to squares of earlier ones; and (3) we train a simple probe that can predict the optimal move 2 turns ahead with 92\% accuracy (in board states where Leela finds a single best line). These findings are an existence proof of learned look-ahead in neural networks and might be a step towards a better understanding of their capabilities.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/JDIE4FYK/Jenner et al. - 2024 - Evidence of Learned Look-Ahead in a Chess-Playing .pdf}
}

@article{jenner_research_2023,
  title = {Research agenda: Formalizing abstractions of computations},
  shorttitle = {Research agenda},
  author = {Jenner, Erik},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/L8LHBTMvhLDpxDaqv/research-agenda-formalizing-abstractions-of-computations-1},
  urldate = {2023-11-09},
  abstract = {Big thanks to Leon Lang, J{\'e}r{\'e}my Scheurer, Adam Gleave, and Shoshannah Tekofsky for their feedback on a draft of this post, to Euan McLean (via FAR AI{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KBYYAQWL/Jenner - 2023 - Research agenda Formalizing abstractions of compu.html}
}

@article{jenner_subsets_2023,
  title = {Subsets and quotients in interpretability},
  author = {Jenner, Erik},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/Ryv3FviYuovtJbgQd/subsets-and-quotients-in-interpretability},
  urldate = {2024-04-19},
  abstract = {Summary Interpretability techniques often need to throw away some information about a neural network's computations: the entirety of the computationa{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/8GPGMSVE/subsets-and-quotients-in-interpretability.html}
}

@article{jermyn_circuits_2023,
  title = {Circuits updates - May 2023: Attention Head Superposition},
  author = {Jermyn, Adam and Olah, Chris and Henighan, T},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/may-update/index.html},
  keywords = {cited}
}

@article{jermyn_engineering_2022,
  title = {Engineering Monosemanticity in Toy Models},
  author = {Jermyn, Adam S. and Schiefer, Nicholas and Hubinger, Evan},
  year = {2022},
  month = nov,
  journal = {CoRR},
  eprint = {2211.09169},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2211.09169},
  urldate = {2023-07-25},
  abstract = {In some neural networks, individual neurons correspond to natural ``features'' in the input. Such {\textbackslash}emph\{monosemantic\} neurons are of great help in interpretability studies, as they can be cleanly understood. In this work we report preliminary attempts to engineer monosemanticity in toy models. We find that models can be made more monosemantic without increasing the loss by just changing which local minimum the training process finds. More monosemantic loss minima have moderate negative biases, and we are able to use this fact to engineer highly monosemantic models. We are able to mechanistically interpret these models, including the residual polysemantic neurons, and uncover a simple yet surprising algorithm. Finally, we find that providing models with more neurons per layer makes the models more monosemantic, albeit at increased computational cost. These findings point to a number of new questions and avenues for engineering monosemanticity, which we intend to study these in future work.},
  archiveprefix = {arxiv},
  keywords = {cited,fundamental,intrinsic,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/XXRR9944/Jermyn et al. - 2022 - Engineering Monosemanticity in Toy Models.pdf}
}

@article{jermyn_latent_2022,
  title = {Latent Adversarial Training},
  author = {Jermyn, Adam},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/atBQ3NHyqnBadrsGP/latent-adversarial-training},
  urldate = {2024-02-19},
  abstract = {The Problem We'd like to train models to be robustly safe, even in environments that may fall well outside of the training distribution. Unfortunatel{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KWSMLAST/Jermyn - 2022 - Latent Adversarial Training.html}
}

@article{ji_ai_2024,
  title = {AI Alignment: A Comprehensive Survey},
  shorttitle = {AI Alignment},
  author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Ng, Kwan Yee and Dai, Juntao and Pan, Xuehai and O'Gara, Aidan and Lei, Yingshan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2310.19852},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.19852},
  url = {http://arxiv.org/abs/2310.19852},
  urldate = {2024-01-16},
  abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
  archiveprefix = {arxiv},
  keywords = {not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/2WXMMHAV/Ji et al. - 2024 - AI Alignment A Comprehensive Survey.pdf}
}

@article{jiang_origins_2024,
  title = {On the Origins of Linear Representations in Large Language Models},
  author = {Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon and Veitch, Victor},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.03867},
  url = {https://arxiv.org/abs/2403.03867},
  urldate = {2024-06-10},
  abstract = {Recent works have argued that high-level semantic concepts are encoded "linearly" in the representation space of large language models. In this work, we study the origins of such linear representations. To that end, we introduce a simple latent variable model to abstract and formalize the concept dynamics of the next token prediction. We use this formalism to show that the next token prediction objective (softmax with cross-entropy) and the implicit bias of gradient descent together promote the linear representation of concepts. Experiments show that linear representations emerge when learning from data matching the latent variable model, confirming that this simple structure already suffices to yield linear representations. We additionally confirm some predictions of the theory using the LLaMA-2 large language model, giving evidence that the simplified model yields generalizable insights.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/leonardbereska/Zotero/storage/MFGC84MP/Jiang et al. - 2024 - On the Origins of Linear Representations in Large .pdf}
}

@article{jorgensen_improving_2023,
  title = {Improving Activation Steering in Language Models with Mean-Centring},
  author = {Jorgensen, Ole and Cope, Dylan and Schoots, Nandi and Shanahan, Murray},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.03813},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.03813},
  url = {http://arxiv.org/abs/2312.03813},
  urldate = {2024-03-18},
  abstract = {Recent work in activation steering has demonstrated the potential to better control the outputs of Large Language Models (LLMs), but it involves finding steering vectors. This is difficult because engineers do not typically know how features are represented in these models. We seek to address this issue by applying the idea of mean-centring to steering vectors. We find that taking the average of activations associated with a target dataset, and then subtracting the mean of all training activations, results in effective steering vectors. We test this method on a variety of models on natural language tasks by steering away from generating toxic text, and steering the completion of a story towards a target genre. We also apply mean-centring to extract function vectors, more effectively triggering the execution of a range of natural language tasks by a significant margin (compared to previous baselines). This suggests that mean-centring can be used to easily improve the effectiveness of activation steering in a wide range of contexts.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/WNMGSCC6/Jorgensen et al. - 2023 - Improving Activation Steering in Language Models w.pdf}
}

@article{joshi_personas_2023,
  title = {Personas as a Way to Model Truthfulness in Language Models},
  author = {Joshi, Nitish and Rando, Javier and Saparov, Abulhair and Kim, Najoung and He, He},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.18168},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.18168},
  url = {http://arxiv.org/abs/2310.18168},
  urldate = {2023-11-08},
  abstract = {Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a model's answer will be truthful before it is generated; (2) finetuning a model on a set of facts improves its truthfulness on unseen topics. Next, using arithmetics as a synthetic environment, we show that language models can separate true and false statements, and generalize truthfulness across agents; but only if agents in the training data share a truthful generative process that enables the creation of a truthful persona. Overall, our findings suggest that models can exploit hierarchical structures in the data to learn abstract concepts like truthfulness.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/24IXIHKE/Joshi et al. - 2023 - Personas as a Way to Model Truthfulness in Languag.pdf}
}

@article{joshi_review_2021,
  title = {A Review on Explainability in Multimodal Deep Neural Nets},
  author = {Joshi, Gargi and Walambe, Rahee and Kotecha, Ketan},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {59800--59821},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3070212},
  url = {https://ieeexplore.ieee.org/document/9391727/},
  urldate = {2023-10-27},
  abstract = {Artificial Intelligence techniques powered by deep neural nets have achieved much success in several application domains, most significantly and notably in the Computer Vision applications and Natural Language Processing tasks. Surpassing human-level performance propelled the research in the applications where different modalities amongst language, vision, sensory, text play an important role in accurate predictions and identification. Several multimodal fusion methods employing deep learning models are proposed in the literature. Despite their outstanding performance, the complex, opaque and black-box nature of the deep neural nets limits their social acceptance and usability. This has given rise to the quest for model interpretability and explainability, more so in the complex tasks involving multimodal AI methods. This paper extensively reviews the present literature to present a comprehensive survey and commentary on the explainability in multimodal deep neural nets, especially for the vision and language tasks. Several topics on multimodal AI and its applications for generic domains have been covered in this paper, including the significance, datasets, fundamental building blocks of the methods and techniques, challenges, applications, and future trends in this domain.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BNTA7LIS/Joshi et al. - 2021 - A Review on Explainability in Multimodal Deep Neur.pdf}
}

@article{jozdien_conditioning_2022,
  title = {Conditioning Generative Models for Alignment},
  author = {Jozdien},
  year = {2022},
  month = jul,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/JqnkeqaPseTgxLgEL/conditioning-generative-models-for-alignment},
  urldate = {2024-01-03},
  abstract = {This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the~Stanford Existential Risks Institute ML Alignment Theory{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZTRCHDIP/Jozdien - 2022 - Conditioning Generative Models for Alignment.html}
}

@article{jukic_easy_2023,
  title = {Easy to Decide, Hard to Agree: Reducing Disagreements Between Saliency Methods},
  shorttitle = {Easy to Decide, Hard to Agree},
  author = {Juki{\'c}, Josip and Tutek, Martin and {\v S}najder, Jan},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2211.08369},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.08369},
  url = {http://arxiv.org/abs/2211.08369},
  urldate = {2023-08-29},
  abstract = {A popular approach to unveiling the black box of neural NLP models is to leverage saliency methods, which assign scalar importance scores to each input component. A common practice for evaluating whether an interpretability method is faithful has been to use evaluation-by-agreement -- if multiple methods agree on an explanation, its credibility increases. However, recent work has found that saliency methods exhibit weak rank correlations even when applied to the same model instance and advocated for the use of alternative diagnostic methods. In our work, we demonstrate that rank correlation is not a good fit for evaluating agreement and argue that Pearson-\$r\$ is a better-suited alternative. We further show that regularization techniques that increase faithfulness of attention explanations also increase agreement between saliency methods. By connecting our findings to instance categories based on training dynamics, we show that the agreement of saliency method explanations is very low for easy-to-learn instances. Finally, we connect the improvement in agreement across instance categories to local representation space statistics of instances, paving the way for work on analyzing which intrinsic model properties improve their predisposition to interpretability methods.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DRQ5VHG3/Juki et al. - 2023 - Easy to Decide, Hard to Agree Reducing Disagreeme.pdf}
}

@article{jumelet_analysing_2019,
  title = {Analysing Neural Language Models: Contextual Decomposition Reveals Default Reasoning in Number and Gender Assignment},
  shorttitle = {Analysing Neural Language Models},
  author = {Jumelet, Jaap and Zuidema, Willem and Hupkes, Dieuwke},
  editor = {Bansal, Mohit and Villavicencio, Aline},
  year = {2019},
  month = nov,
  journal = {CoNLL},
  pages = {1--11},
  doi = {10.18653/v1/K19-1001},
  url = {https://aclanthology.org/K19-1001},
  urldate = {2024-01-24},
  abstract = {Extensive research has recently shown that recurrent neural language models are able to process a wide range of grammatical phenomena. How these models are able to perform these remarkable feats so well, however, is still an open question. To gain more insight into what information LSTMs base their decisions on, we propose a generalisation of Contextual Decomposition (GCD). In particular, this setup enables us to accurately distil which part of a prediction stems from semantic heuristics, which part truly emanates from syntactic cues and which part arise from the model biases themselves instead. We investigate this technique on tasks pertaining to syntactic agreement and co-reference resolution and discover that the model strongly relies on a default reasoning effect to perform these tasks.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/D4CSTIN7/Jumelet et al. - 2019 - Analysing Neural Language Models Contextual Decom.pdf}
}

@article{jumelet_evaluating_2023,
  title = {Evaluating and Interpreting Language Models},
  author = {Jumelet, Jaap},
  year = {2023},
  month = nov,
  journal = {NLP Lecture},
  url = {https://cl-illc.github.io/nlp1-2023/resources/slides/NLP1-Lecture-Interpretability.pdf},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/38VPPGVL/Jumelet - 2023 - Evaluating and Interpreting Language Models.pdf}
}

@article{kaklamanos_possible_2023,
  title = {Possible ways to expand on "Discovering Latent Knowledge in Language Models Without Supervision"},
  author = {Kaklamanos, Georgios and Laurito, Walter and Kaarel and Kozaronek, Kay},
  year = {2023},
  month = jan,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/bFwigCDMC5ishLz7X/rfc-possible-ways-to-expand-on-discovering-latent-knowledge},
  urldate = {2024-01-23},
  abstract = {Preface We would like to thank the following people who contributed to the generation of ideas and provided feedback on this post: Alexandre Variengi{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/SQWJLMB9/Kaklamanos et al. - 2023 - Possible ways to expand on Discovering Latent Kno.html}
}

@article{kang_deep_2023,
  title = {Deep Neural Networks Tend To Extrapolate Predictably},
  author = {Kang, Katie and Setlur, Amrith and Tomlin, Claire and Levine, Sergey},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.00873},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.00873},
  url = {http://arxiv.org/abs/2310.00873},
  urldate = {2023-10-27},
  abstract = {Conventional wisdom suggests that neural network predictions tend to be unpredictable and overconfident when faced with out-of-distribution (OOD) inputs. Our work reassesses this assumption for neural networks with high-dimensional inputs. Rather than extrapolating in arbitrary ways, we observe that neural network predictions often tend towards a constant value as input data becomes increasingly OOD. Moreover, we find that this value often closely approximates the optimal constant solution (OCS), i.e., the prediction that minimizes the average loss over the training data without observing the input. We present results showing this phenomenon across 8 datasets with different distributional shifts (including CIFAR10-C and ImageNet-R, S), different loss functions (cross entropy, MSE, and Gaussian NLL), and different architectures (CNNs and transformers). Furthermore, we present an explanation for this behavior, which we first validate empirically and then study theoretically in a simplified setting involving deep homogeneous networks with ReLU activations. Finally, we show how one can leverage our insights in practice to enable risk-sensitive decision-making in the presence of OOD inputs.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NI76F8V7/Kang et al. - 2023 - Deep Neural Networks Tend To Extrapolate Predictab.pdf}
}

@article{kantamneni_how_2024,
  title = {How Do Transformers "Do" Physics? Investigating the Simple Harmonic Oscillator},
  shorttitle = {How Do Transformers "Do" Physics?},
  author = {Kantamneni, Subhash and Liu, Ziming and Tegmark, Max},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.17209},
  primaryclass = {cond-mat},
  doi = {10.48550/arXiv.2405.17209},
  url = {http://arxiv.org/abs/2405.17209},
  urldate = {2024-06-10},
  abstract = {How do transformers model physics? Do transformers model systems with interpretable analytical solutions, or do they create "alien physics" that are difficult for humans to decipher? We take a step in demystifying this larger puzzle by investigating the simple harmonic oscillator (SHO), \${\textbackslash}ddot\{x\}+2{\textbackslash}gamma {\textbackslash}dot\{x\}+{\textbackslash}omega\_0{\textasciicircum}2x=0\$, one of the most fundamental systems in physics. Our goal is to identify the methods transformers use to model the SHO, and to do so we hypothesize and evaluate possible methods by analyzing the encoding of these methods' intermediates. We develop four criteria for the use of a method within the simple testbed of linear regression, where our method is \$y = wx\$ and our intermediate is \$w\$: (1) Can the intermediate be predicted from hidden states? (2) Is the intermediate's encoding quality correlated with model performance? (3) Can the majority of variance in hidden states be explained by the intermediate? (4) Can we intervene on hidden states to produce predictable outcomes? Armed with these two correlational (1,2), weak causal (3) and strong causal (4) criteria, we determine that transformers use known numerical methods to model trajectories of the simple harmonic oscillator, specifically the matrix exponential method. Our analysis framework can conveniently extend to high-dimensional linear systems and nonlinear systems, which we hope will help reveal the "world model" hidden in transformers.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/CUEA6GMQ/Kantamneni et al. - 2024 - How Do Transformers Do Physics Investigating th.pdf}
}

@article{kanwisher_using_2023,
  title = {Using artificial neural networks to ask `why' questions of minds and brains},
  author = {Kanwisher, Nancy and Khosla, Meenakshi and Dobs, Katharina},
  year = {2023},
  month = mar,
  journal = {Trends in Neurosciences},
  volume = {46},
  number = {3},
  pages = {240--254},
  issn = {01662236},
  doi = {10.1016/j.tins.2022.12.008},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223622002624},
  urldate = {2023-11-29},
  abstract = {Semantic Scholar extracted view of "Using artificial neural networks to ask `why' questions of minds and brains" by N. Kanwisher et al.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZU3M75CJ/Kanwisher et al. - 2023 - Using artificial neural networks to ask why ques.pdf}
}

@article{karpathy_visualizing_2015,
  title = {Visualizing and Understanding Recurrent Networks},
  author = {Karpathy, Andrej and Johnson, Justin and {Fei-Fei}, Li},
  year = {2015},
  month = nov,
  journal = {CoRR},
  eprint = {1506.02078},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1506.02078},
  url = {http://arxiv.org/abs/1506.02078},
  urldate = {2024-03-20},
  abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/JQCR3GHV/Karpathy et al. - 2015 - Visualizing and Understanding Recurrent Networks.pdf}
}

@article{karsenti_modelling_2006,
  title = {Modelling microtubule patterns},
  author = {Karsenti, Eric and N{\'e}d{\'e}lec, Fran{\c c}ois and Surrey, Thomas},
  year = {2006},
  month = nov,
  journal = {Nat Cell Biol},
  volume = {8},
  number = {11},
  pages = {1204--1211},
  publisher = {Nature Publishing Group},
  issn = {1476-4679},
  doi = {10.1038/ncb1498},
  url = {https://www.nature.com/articles/ncb1498},
  urldate = {2024-04-30},
  abstract = {The cellular cytoskeleton is well studied in terms of its biological and physical properties, making it an attractive subject for systems approaches. Here, we describe the experimental and theoretical strategies used to study the collective behaviour of microtubules and motors. We illustrate how this led to the beginning of an understanding of dynamic cellular patterns that have precise functions.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/CTR3X2CU/Karsenti et al. - 2006 - Modelling microtubule patterns.pdf}
}

@article{kasioumis_elite_2021,
  title = {Elite BackProp: Training Sparse Interpretable Neurons},
  shorttitle = {Elite BackProp},
  author = {Kasioumis, Theodoros and Townsend, Joe and Inakoshi, Hiroya},
  year = {2021},
  journal = {International Workshop on Neuro-Symbolic Learning and Reasoning},
  url = {https://ceur-ws.org/Vol-2986/paper6.pdf},
  urldate = {2024-03-20},
  abstract = {In this paper we present a method called Elite BackProp (EBP) to train more interpretable convolutional neural networks (CNNs) by introducing class-wise activation sparsity; after training, each class will be associated with a small set of elite filters that fire rarely but highly activate on visual primitives from images of that class. Our method is broadly applicable as it does not require additional object part annotations during training. We demonstrate experimentally that EBP realizes high degrees of activation sparsity with no accuracy loss and enhances the performance of a rule extraction algorithm that distils the knowledge from a CNN, by inducing more compact rules that use fewer atoms to describe the decisions of a CNN while maintaining high fidelity compared with other solutions. This happens because EBP induces sparse compositional representations that reuse and combine primitive filters. Such representations can assist in understanding the reasoning behind a CNN and build trust into their decisions.},
  file = {/Users/leonardbereska/Zotero/storage/HVZMI3SC/Kasioumis et al. - 2021 - Elite BackProp Training Sparse Interpretable Neur.pdf}
}

@article{katakkar_practical_2022,
  title = {Practical Benefits of Feature Feedback Under Distribution Shift},
  author = {Katakkar, Anurag and Yoo, Clay H. and Wang, Weiqin and Lipton, Zachary C. and Kaushik, Divyansh},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2110.07566},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.07566},
  url = {http://arxiv.org/abs/2110.07566},
  urldate = {2023-08-29},
  abstract = {In attempts to develop sample-efficient and interpretable algorithms, researcher have explored myriad mechanisms for collecting and exploiting feature feedback (or rationales) auxiliary annotations provided for training (but not test) instances that highlight salient evidence. Examples include bounding boxes around objects and salient spans in text. Despite its intuitive appeal, feature feedback has not delivered significant gains in practical problems as assessed on iid holdout sets. However, recent works on counterfactually augmented data suggest an alternative benefit of supplemental annotations, beyond interpretability: lessening sensitivity to spurious patterns and consequently delivering gains in out-of-domain evaluations. We speculate that while existing methods for incorporating feature feedback have delivered negligible in-sample performance gains, they may nevertheless provide out-of-domain benefits. Our experiments addressing sentiment analysis, show that feature feedback methods perform significantly better on various natural out-of-domain datasets despite comparable in-domain evaluations. By contrast, performance on natural language inference remains comparable. Finally, we compare those tasks where feature feedback does (and does not) help.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TG7GQFYW/Katakkar et al. - 2022 - Practical Benefits of Feature Feedback Under Distr.pdf}
}

@article{katz_backward_2024,
  title = {Backward Lens: Projecting Language Model Gradients into the Vocabulary Space},
  shorttitle = {Backward Lens},
  author = {Katz, Shahar and Belinkov, Yonatan and Geva, Mor and Wolf, Lior},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.12865},
  url = {https://arxiv.org/abs/2402.12865},
  urldate = {2024-06-10},
  abstract = {Understanding how Transformer-based Language Models (LMs) learn and recall information is a key goal of the deep learning community. Recent interpretability methods project weights and hidden states obtained from the forward pass to the models' vocabularies, helping to uncover how information flows within LMs. In this work, we extend this methodology to LMs' backward pass and gradients. We first prove that a gradient matrix can be cast as a low-rank linear combination of its forward and backward passes' inputs. We then develop methods to project these gradients into vocabulary items and explore the mechanics of how new information is stored in the LMs' neurons.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/U9E6SEG9/Katz et al. - 2024 - Backward Lens Projecting Language Model Gradients.pdf}
}

@article{katz_interpreting_2023,
  title = {Interpreting Transformer's Attention Dynamic Memory and Visualizing the Semantic Information Flow of GPT},
  author = {Katz, Shahar and Belinkov, Yonatan},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2305.13417},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.13417},
  url = {http://arxiv.org/abs/2305.13417},
  urldate = {2023-11-16},
  abstract = {Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them human interpretable and enables us to assign semantics to what was seen only as numerical vectors. In this paper, we interpret LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on these discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that reflect why models output their results. We demonstrate the utility of our modeling by identifying the effect LM components have on the intermediate processing in the model before outputting a prediction. For instance, we discover that layer norms are used as semantic filters and find neurons that act as regularization vectors.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/AAQKQ8G2/Katz and Belinkov - 2023 - Interpreting Transformer's Attention Dynamic Memor.pdf}
}

@article{katz_visit_2023,
  title = {VISIT: Visualizing and Interpreting the Semantic Information Flow of Transformers},
  shorttitle = {VISIT},
  author = {Katz, Shahar and Belinkov, Yonatan},
  year = {2023},
  journal = {EMNLP},
  pages = {14094--14113},
  doi = {10.18653/v1/2023.findings-emnlp.939},
  url = {https://aclanthology.org/2023.findings-emnlp.939},
  urldate = {2024-02-11},
  abstract = {Recent advances in interpretability suggest we can project weights and hidden states of transformer-based language models (LMs) to their vocabulary, a transformation that makes them more human interpretable. In this paper, we investigate LM attention heads and memory values, the vectors the models dynamically create and recall while processing a given input. By analyzing the tokens they represent through this projection, we identify patterns in the information flow inside the attention mechanism. Based on our discoveries, we create a tool to visualize a forward pass of Generative Pre-trained Transformers (GPTs) as an interactive flow graph, with nodes representing neurons or hidden states and edges representing the interactions between them. Our visualization simplifies huge amounts of data into easy-to-read plots that can reflect the models' internal processing, uncovering the contribution of each component to the models' final prediction. Our visualization also unveils new insights about the role of layer norms as semantic filters that influence the models' output, and about neurons that are always activated during forward passes and act as regularization vectors.},
  language = {en},
  keywords = {not cited,to cite,to extract related work,tool,visualization},
  file = {/Users/leonardbereska/Zotero/storage/E85YF4RH/Katz and Belinkov - 2023 - VISIT Visualizing and Interpreting the Semantic I.pdf}
}

@article{keller_modeling_2021,
  title = {Modeling Category-Selective Cortical Regions with Topographic Variational Autoencoders},
  author = {Keller, T. Anderson and Gao, Qinghe and Welling, Max},
  year = {2021},
  month = dec,
  journal = {CoRR},
  eprint = {2110.13911},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2110.13911},
  url = {http://arxiv.org/abs/2110.13911},
  urldate = {2023-11-29},
  abstract = {Category-selectivity in the brain describes the observation that certain spatially localized areas of the cerebral cortex tend to respond robustly and selectively to stimuli from specific limited categories. One of the most well known examples of category-selectivity is the Fusiform Face Area (FFA), an area of the inferior temporal cortex in primates which responds preferentially to images of faces when compared with objects or other generic stimuli. In this work, we leverage the newly introduced Topographic Variational Autoencoder to model the emergence of such localized category-selectivity in an unsupervised manner. Experimentally, we demonstrate our model yields spatially dense neural clusters selective to faces, bodies, and places through visualized maps of Cohen's d metric. We compare our model with related supervised approaches, namely the Topographic Deep Artificial Neural Network (TDANN) of Lee et al., and discuss both theoretical and empirical similarities. Finally, we show preliminary results suggesting that our model yields a nested spatial hierarchy of increasingly abstract categories, analogous to observations from the human ventral temporal cortex.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/738Y8KLD/Keller et al. - 2021 - Modeling Category-Selective Cortical Regions with .pdf}
}

@article{kersey_linking_2006,
  title = {Linking publication, gene and protein data},
  author = {Kersey, Paul and Apweiler, Rolf},
  year = {2006},
  month = nov,
  journal = {Nat Cell Biol},
  volume = {8},
  number = {11},
  pages = {1183--1189},
  publisher = {Nature Publishing Group},
  issn = {1476-4679},
  doi = {10.1038/ncb1495},
  url = {https://www.nature.com/articles/ncb1495},
  urldate = {2024-04-30},
  abstract = {The computational reconstruction of biological systems, 'systems biology', is necessarily dependent on the existence of well-annotated data sets defining and describing the components of these systems, especially genes and the proteins they encode. Information about these components can be accessed either through structured bioinformatics databases, which store basic chemical and functional information abstracted from (or supplementing) the scientific literature, or through the literature itself, which is richer in content but essentially unstructured.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/29YK5JTF/Kersey and Apweiler - 2006 - Linking publication, gene and protein data.pdf}
}

@article{khakzar_explanations_2022,
  title = {Do Explanations Explain? Model Knows Best},
  shorttitle = {Do Explanations Explain?},
  author = {Khakzar, Ashkan and Khorsandi, Pedram and Nobahari, Rozhin and Navab, Nassir},
  year = {2022},
  month = jun,
  journal = {CVPR},
  pages = {10234--10243},
  publisher = {IEEE},
  address = {New Orleans, LA, USA},
  doi = {10.1109/CVPR52688.2022.01000},
  url = {https://ieeexplore.ieee.org/document/9878403/},
  urldate = {2024-02-11},
  abstract = {It is a mystery which input features contribute to a neural network's output. Various explanation (feature attribution) methods are proposed in the literature to shed light on the problem. One peculiar observation is that these explanations (attributions) point to different features as being important. The phenomenon raises the question, which explanation to trust? We propose a framework for evaluating the explanations using the neural network model itself. The framework leverages the network to generate input features that impose a particular behavior on the output. Using the generated features, we devise controlled experimental setups to evaluate whether an explanation method conforms to an axiom. Thus we propose an empirical framework for axiomatic evaluation of explanation methods. We evaluate well-known and promising explanation solutions using the proposed framework. The framework provides a toolset to reveal properties and drawbacks within existing and future explanation solutions.11https://github.com/CAMP-eXplain-AI/Do-Explanations-Explain},
  isbn = {9781665469463},
  keywords = {explainability,not cited},
  file = {/Users/leonardbereska/Zotero/storage/N3YM6CJN/Khakzar et al. - 2022 - Do Explanations Explain Model Knows Best.pdf}
}

@article{khona_understanding_2024,
  title = {Towards an Understanding of Stepwise Inference in Transformers: A Synthetic Graph Navigation Model},
  shorttitle = {Towards an Understanding of Stepwise Inference in Transformers},
  author = {Khona, Mikail and Okawa, Maya and Hula, Jan and Ramesh, Rahul and Nishi, Kento and Dick, Robert and Lubana, Ekdeep Singh and Tanaka, Hidenori},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.07757},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.07757},
  url = {http://arxiv.org/abs/2402.07757},
  urldate = {2024-03-14},
  abstract = {Stepwise inference protocols, such as scratchpads and chain-of-thought, help language models solve complex problems by decomposing them into a sequence of simpler subproblems. Despite the significant gain in performance achieved via these protocols, the underlying mechanisms of stepwise inference have remained elusive. To address this, we propose to study autoregressive Transformer models on a synthetic task that embodies the multi-step nature of problems where stepwise inference is generally most useful. Specifically, we define a graph navigation problem wherein a model is tasked with traversing a path from a start to a goal node on the graph. Despite is simplicity, we find we can empirically reproduce and analyze several phenomena observed at scale: (i) the stepwise inference reasoning gap, the cause of which we find in the structure of the training data; (ii) a diversity-accuracy tradeoff in model generations as sampling temperature varies; (iii) a simplicity bias in the model's output; and (iv) compositional generalization and a primacy bias with in-context exemplars. Overall, our work introduces a grounded, synthetic framework for studying stepwise inference and offers mechanistic hypotheses that can lay the foundation for a deeper understanding of this phenomenon.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/INP2JZE8/Khona et al. - 2024 - Towards an Understanding of Stepwise Inference in .pdf}
}

@article{kidd_introduction_2021,
  title = {Introduction to inaccessible information},
  author = {Kidd, Ryan},
  year = {2021},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/CYKeDjD7FEvAnzBBF/introduction-to-inaccessible-information},
  urldate = {2023-11-30},
  abstract = {This post was written under Evan Hubinger's direct guidance and mentorship, as a part of the Stanford Existential Risks Institute ML Alignment Theory{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DPVJY6J3/Kidd - 2021 - Introduction to inaccessible information.html}
}

@article{kim_disentangling_2018,
  title = {Disentangling by Factorising},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = {2018},
  journal = {ICML},
  eprint = {1802.05983},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1802.05983},
  url = {http://arxiv.org/abs/1802.05983},
  urldate = {2023-07-31},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \${\textbackslash}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PTHF2YBH/Kim and Mnih - 2018 - Disentangling by Factorising.pdf}
}

@article{kim_interpretability_2018,
  title = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)},
  shorttitle = {Interpretability Beyond Feature Attribution},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  year = {2018},
  month = jun,
  journal = {CoRR},
  eprint = {1711.11279},
  primaryclass = {stat},
  doi = {10.48550/arXiv.1711.11279},
  url = {http://arxiv.org/abs/1711.11279},
  urldate = {2023-09-18},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2BCUTZJ7/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf}
}

@article{kirchner_emergence_2021,
  title = {Emergence of local and global synaptic organization on cortical dendrites},
  author = {Kirchner, Jan H. and Gjorgjieva, Julijana},
  year = {2021},
  month = jun,
  journal = {Nat Commun},
  volume = {12},
  number = {1},
  pages = {4005},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-021-23557-3},
  url = {https://www.nature.com/articles/s41467-021-23557-3},
  urldate = {2024-01-19},
  abstract = {Synaptic inputs on cortical dendrites are organized with remarkable subcellular precision at the micron level. This organization emerges during early postnatal development through patterned spontaneous activity and manifests both locally where nearby synapses are significantly correlated, and globally with distance to the soma. We propose a biophysically motivated synaptic plasticity model to dissect the mechanistic origins of this organization during development and elucidate synaptic clustering of different stimulus features in the adult. Our model captures local clustering of orientation in ferret and receptive field overlap in mouse visual cortex based on the receptive field diameter and the cortical magnification of visual space. Including action potential back-propagation explains branch clustering heterogeneity in the ferret and produces a global retinotopy gradient from soma to dendrite in the mouse. Therefore, by combining activity-dependent synaptic competition and species-specific receptive fields, our framework explains different aspects of synaptic organization regarding stimulus features and spatial scales.},
  copyright = {2021 The Author(s)},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QCNJUJ96/Kirchner and Gjorgjieva - 2021 - Emergence of local and global synaptic organizatio.pdf}
}

@article{kirchner_neuroscience_2023,
  title = {Neuroscience and Natural Abstractions},
  author = {Kirchner, Jan},
  year = {2023},
  month = mar,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/WGFtgFKuLFMvLuET3/jan-s-shortform},
  urldate = {2024-01-18},
  abstract = {Comment by Jan - Neuroscience and Natural Abstractions Similarities in structure and function abound in biology; individual neurons that activate exclusively to particular oriented stimuli exist in animals from drosophila (Strother et al. 2017) via pigeons (Li et al. 2007) and turtles (Ammermueller et al. 1995) to macaques (De Valois et al. 1982). The universality of major functional response classes in biology suggests that the neural systems underlying information processing in biology might be highly stereotyped (Van Hooser, 2007, Scholl et al. 2013). In line with this hypothesis, a wide range of neural phenomena emerge as optimal solutions to their respective functional requirements (Poggio 1981, Wolf 2003, Todorov 2004, Gardner 2019). Intriguingly, recent studies on artificial neural networks that approach human-level performance reveal surprising similarity between emerging representations in both artificial and biological brains (Kriegeskorte 2015, Yamins et al. 2016, Zhuang et al. 2020). Despite the commonalities across different animal species, there is also substantial variability (Van Hooser, 2007). One prominent example of a functional neural structure that is present in some, but absent in other, animals is the orientation pinwheel in the primary visual cortex (Meng et al. 2012), synaptic clustering with respect to orientation selectivity (Kirchner et al. 2021), or the distinct three-layered cortex in reptiles (Tosches et al. 2018). These examples demonstrate that while general organization principles might be universal, the details of how exactly and where in the brain the principles manifest is highly dependent on anatomical factors (Keil et al. 2012, Kirchner et al. 2021), genetic lineage (Tosches et al. 2018), and ecological factors (Roeth et al. 2021). Thus, the universality hypothesis as applied to biological systems does not imply perfect replication of a given feature across all instances of the system. Rather, it suggests that there are broad principles or ab},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/82XLIK85/Kirchner - 2023 - Neuroscience and Natural Abstractions.html}
}

@article{kirk_understanding_2023,
  title = {Understanding the Effects of RLHF on LLM Generalisation and Diversity},
  author = {Kirk, Robert and Mediratta, Ishita and Nalmpantis, Christoforos and Luketina, Jelena and Hambro, Eric and Grefenstette, Edward and Raileanu, Roberta},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.06452},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.06452},
  url = {http://arxiv.org/abs/2310.06452},
  urldate = {2023-10-27},
  abstract = {Large language models (LLMs) fine-tuned with reinforcement learning from human feedback (RLHF) have been used in some of the most widely deployed AI models to date, such as OpenAI's ChatGPT, Anthropic's Claude, or Meta's LLaMA-2. While there has been significant work developing these methods, our understanding of the benefits and downsides of each stage in RLHF is still limited. To fill this gap, we present an extensive analysis of how each stage of the process (i.e. supervised fine-tuning (SFT), reward modelling, and RLHF) affects two key properties: out-of-distribution (OOD) generalisation and output diversity. OOD generalisation is crucial given the wide range of real-world scenarios in which these models are being used, while output diversity refers to the model's ability to generate varied outputs and is important for a variety of use cases. We perform our analysis across two base models on both summarisation and instruction following tasks, the latter being highly relevant for current LLM use cases. We find that RLHF generalises better than SFT to new inputs, particularly as the distribution shift between train and test becomes larger. However, RLHF significantly reduces output diversity compared to SFT across a variety of measures, implying a tradeoff in current LLM fine-tuning methods between generalisation and diversity. Our results provide guidance on which fine-tuning method should be used depending on the application, and show that more research is needed to improve the trade-off between generalisation and diversity.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TIEGP8DQ/Kirk et al. - 2023 - Understanding the Effects of RLHF on LLM Generalis.pdf}
}

@article{kirk_what_,
  title = {What is Interpretability?},
  author = {Kirk, Robert and Gaven{\v c}iak, Tom{\'a}{\v s} and B{\"o}hm, Ada},
  year = {2020},
  month = mar,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/rSMbGFfsLMB3GWZtX/what-is-interpretability},
  urldate = {2023-11-30},
  abstract = {In this post we lay out some ideas around framing interpretability research which we have found quite useful. Our framing is goal-oriented, which we{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KL2FRCM8/Kirk et al. - 2020 - What is Interpretability.html}
}

@article{kirsch_generalpurpose_2022,
  title = {General-Purpose In-Context Learning by Meta-Learning Transformers},
  author = {Kirsch, Louis and Harrison, James and {Sohl-Dickstein}, Jascha and Metz, Luke},
  year = {2022},
  month = dec,
  journal = {CoRR},
  eprint = {2212.04458},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2212.04458},
  url = {http://arxiv.org/abs/2212.04458},
  urldate = {2023-11-02},
  abstract = {Modern machine learning requires system designers to specify aspects of the learning pipeline, such as losses, architectures, and optimizers. Meta-learning, or learning-to-learn, instead aims to learn those aspects, and promises to unlock greater capabilities with less manual effort. One particularly ambitious goal of meta-learning is to train general-purpose in-context learning algorithms from scratch, using only black-box models with minimal inductive bias. Such a model takes in training data, and produces test-set predictions across a wide range of problems, without any explicit definition of an inference model, training loss, or optimization algorithm. In this paper we show that Transformers and other black-box models can be meta-trained to act as general-purpose in-context learners. We characterize phase transitions between algorithms that generalize, algorithms that memorize, and algorithms that fail to meta-train at all, induced by changes in model size, number of tasks, and meta-optimization. We further show that the capabilities of meta-trained algorithms are bottlenecked by the accessible state size (memory) determining the next prediction, unlike standard models which are thought to be bottlenecked by parameter count. Finally, we propose practical interventions such as biasing the training distribution that improve the meta-training and meta-generalization of general-purpose learning algorithms.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4PX79C5B/Kirsch et al. - 2022 - General-Purpose In-Context Learning by Meta-Learni.pdf}
}

@article{kitouni_neurons_2024,
  title = {From Neurons to Neutrons: A Case Study in Interpretability},
  shorttitle = {From Neurons to Neutrons},
  author = {Kitouni, Ouail and Nolte, Niklas and {P{\'e}rez-D{\'i}az}, V{\'i}ctor Samuel and Trifinopoulos, Sokratis and Williams, Mike},
  year = {2024},
  month = may,
  journal = {ICML},
  eprint = {2405.17425},
  primaryclass = {nucl-th},
  doi = {10.48550/arXiv.2405.17425},
  url = {http://arxiv.org/abs/2405.17425},
  urldate = {2024-06-09},
  abstract = {Mechanistic Interpretability (MI) promises a path toward fully understanding how neural networks make their predictions. Prior work demonstrates that even when trained to perform simple arithmetic, models can implement a variety of algorithms (sometimes concurrently) depending on initialization and hyperparameters. Does this mean neuron-level interpretability techniques have limited applicability? We argue that high-dimensional neural networks can learn low-dimensional representations of their training data that are useful beyond simply making good predictions. Such representations can be understood through the mechanistic interpretability lens and provide insights that are surprisingly faithful to human-derived domain knowledge. This indicates that such approaches to interpretability can be useful for deriving a new understanding of a problem from models trained to solve it. As a case study, we extract nuclear physics concepts by studying models trained to reproduce nuclear data.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/FIYZ5S9J/Kitouni et al. - 2024 - From Neurons to Neutrons A Case Study in Interpre.pdf}
}

@article{kleindessner_efficient_2023,
  title = {Efficient fair PCA for fair representation learning},
  author = {Kleindessner, Matth{\"a}us and Donini, Michele and Russell, Chris and Zafar, Muhammad Bilal},
  year = {2023},
  month = apr,
  journal = {AISTATS},
  pages = {5250--5270},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/kleindessner23a.html},
  urldate = {2023-11-10},
  abstract = {We revisit the problem of fair principal component analysis (PCA), where the goal is to learn the best low-rank linear approximation of the data that obfuscates demographic information. We propose a conceptually simple approach that allows for an analytic solution similar to standard PCA and can be kernelized. Our methods have the same complexity as standard PCA, or kernel PCA, and run much faster than existing methods for fair PCA based on semidefinite programming or manifold optimization, while achieving similar results.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/J3VISV5L/Kleindessner et al. - 2023 - Efficient fair PCA for fair representation learnin.pdf}
}

@article{kobayashi_attention_2020,
  title = {Attention is Not Only a Weight: Analyzing Transformers with Vector Norms},
  author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
  editor = {Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang},
  year = {2020},
  month = nov,
  journal = {EMNLP},
  pages = {7057--7075},
  doi = {10.18653/v1/2020.emnlp-main.574},
  url = {https://aclanthology.org/2020.emnlp-main.574},
  abstract = {Attention is a key component of Transformers, which have recently achieved considerable success in natural language processing. Hence, attention is being extensively studied to investigate various linguistic capabilities of Transformers, focusing on analyzing the parallels between attention weights and specific linguistic phenomena. This paper shows that attention weights alone are only one of the two factors that determine the output of attention and proposes a norm-based analysis that incorporates the second factor, the norm of the transformed input vectors. The findings of our norm-based analyses of BERT and a Transformer-based neural machine translation system include the following: (i) contrary to previous studies, BERT pays poor attention to special tokens, and (ii) reasonable word alignment can be extracted from attention mechanisms of Transformer. These findings provide insights into the inner workings of Transformers.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/CD6NHHRU/Kobayashi et al. - 2020 - Attention is Not Only a Weight Analyzing Transfor.pdf}
}

@article{kobayashi_feedforward_2023,
  title = {Feed-Forward Blocks Control Contextualization in Masked Language Models},
  author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
  year = {2023},
  month = feb,
  journal = {CoRR},
  eprint = {2302.00456},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2302.00456},
  url = {http://arxiv.org/abs/2302.00456},
  urldate = {2023-08-29},
  abstract = {Understanding the inner workings of neural network models is a crucial step for rationalizing their output and refining their architecture. Transformer-based models are the core of recent natural language processing and have been analyzed typically with attention patterns as their epoch-making feature is contextualizing surrounding input words via attention mechanisms. In this study, we analyze their inner contextualization by considering all the components, including the feed-forward block (i.e., a feed-forward layer and its surrounding residual and normalization layers) as well as the attention. Our experiments with masked language models show that each of the previously overlooked components did modify the degree of the contextualization in case of processing special word-word pairs (e.g., consisting of named entities). Furthermore, we find that some components cancel each other's effects. Our results could update the typical view about each component's roles (e.g., attention performs contextualization, and the other components serve different roles) in the Transformer layer.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/N9ULEIBT/Kobayashi et al. - 2023 - Feed-Forward Blocks Control Contextualization in M.pdf}
}

@article{kobayashi_incorporating_2021,
  title = {Incorporating Residual and Normalization Layers into Analysis of Masked Language Models},
  author = {Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
  year = {2021},
  month = sep,
  journal = {EMNLP},
  eprint = {2109.07152},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.07152},
  url = {http://arxiv.org/abs/2109.07152},
  urldate = {2023-11-16},
  abstract = {Transformer architecture has become ubiquitous in the natural language processing field. To interpret the Transformer-based models, their attention patterns have been extensively analyzed. However, the Transformer architecture is not only composed of the multi-head attention; other components can also contribute to Transformers' progressive performance. In this study, we extended the scope of the analysis of Transformers from solely the attention patterns to the whole attention block, i.e., multi-head attention, residual connection, and layer normalization. Our analysis of Transformer-based masked language models shows that the token-to-token interaction performed via attention has less impact on the intermediate representations than previously assumed. These results provide new intuitive explanations of existing reports; for example, discarding the learned attention patterns tends not to adversely affect the performance. The codes of our experiments are publicly available.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BYZNNJ36/Kobayashi et al. - 2021 - Incorporating Residual and Normalization Layers in.pdf}
}

@article{kong_interpretable_2023,
  title = {Interpretable Diffusion via Information Decomposition},
  author = {Kong, Xianghao and Liu, Ollie and Li, Han and Yogatama, Dani and Steeg, Greg Ver},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.07972},
  url = {https://arxiv.org/abs/2310.07972},
  urldate = {2024-02-11},
  abstract = {Denoising diffusion models enable conditional generation and density modeling of complex relationships like images and text. However, the nature of the learned relationships is opaque making it difficult to understand precisely what relationships between words and parts of an image are captured, or to predict the effect of an intervention. We illuminate the fine-grained relationships learned by diffusion models by noticing a precise relationship between diffusion and information decomposition. Exact expressions for mutual information and conditional mutual information can be written in terms of the denoising model. Furthermore, pointwise estimates can be easily estimated as well, allowing us to ask questions about the relationships between specific images and captions. Decomposing information even further to understand which variables in a high-dimensional space carry information is a long-standing problem. For diffusion models, we show that a natural non-negative decomposition of mutual information emerges, allowing us to quantify informative relationships between words and pixels in an image. We exploit these new relations to measure the compositional understanding of diffusion models, to do unsupervised localization of objects in images, and to measure effects when selectively editing images through prompt interventions.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {diffusion,editing,interpretability,not cited,vision},
  file = {/Users/leonardbereska/Zotero/storage/ND2AUNI2/Kong et al. - 2023 - Interpretable Diffusion via Information Decomposit.pdf}
}

@article{kopf_cosy_2024,
  title = {CoSy: Evaluating Textual Explanations of Neurons},
  shorttitle = {CoSy},
  author = {Kopf, Laura and Bommer, Philine Lou and Hedstr{\"o}m, Anna and Lapuschkin, Sebastian and H{\"o}hne, Marina M.-C. and Bykov, Kirill},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.20331},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.20331},
  url = {http://arxiv.org/abs/2405.20331},
  urldate = {2024-06-10},
  abstract = {A crucial aspect of understanding the complex nature of Deep Neural Networks (DNNs) is the ability to explain learned concepts within their latent representations. While various methods exist to connect neurons to textual descriptions of human-understandable concepts, evaluating the quality of these explanation methods presents a major challenge in the field due to a lack of unified, general-purpose quantitative evaluation. In this work, we introduce CoSy (Concept Synthesis) -- a novel, architecture-agnostic framework to evaluate the quality of textual explanations for latent neurons. Given textual explanations, our proposed framework leverages a generative model conditioned on textual input to create data points representing the textual explanation. Then, the neuron's response to these explanation data points is compared with the response to control data points, providing a quality estimate of the given explanation. We ensure the reliability of our proposed framework in a series of meta-evaluation experiments and demonstrate practical value through insights from benchmarking various concept-based textual explanation methods for Computer Vision tasks, showing that tested explanation methods significantly differ in quality.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/QJ6E5UPJ/Kopf et al. - 2024 - CoSy Evaluating Textual Explanations of Neurons.pdf}
}

@article{korbak_pretraining_2023,
  title = {Pretraining Language Models with Human Preferences},
  author = {Korbak, Tomek and Bowman, Sam and Perez, Ethan},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences},
  urldate = {2023-05-15},
  abstract = {This post summarizes the main results from our recently released paper~Pretraining Language Models with Human Preferences, and puts them in the broader context of AI safety. For a quick summary of th{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HU52MF4Z/Korbak et al. - 2023 - Pretraining Language Models with Human Preferences.html}
}

@article{kornblith_similarity_2019,
  title = {Similarity of Neural Network Representations Revisited},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  year = {2019},
  month = jul,
  journal = {ICML},
  eprint = {1905.00414},
  primaryclass = {cs, q-bio, stat},
  doi = {10.48550/arXiv.1905.00414},
  url = {http://arxiv.org/abs/1905.00414},
  urldate = {2024-01-18},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9X5CRZDA/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf}
}

@article{krajna_explainable_2022,
  title = {Explainable Artificial Intelligence: An Updated Perspective},
  shorttitle = {Explainable Artificial Intelligence},
  author = {Krajna, Agneza and Kovac, Mihael and Brcic, Mario and Sarcevic, Ana},
  year = {2022},
  month = may,
  journal = {MIPRO},
  pages = {859--864},
  publisher = {IEEE},
  address = {Opatija, Croatia},
  doi = {10.23919/MIPRO55190.2022.9803681},
  url = {https://ieeexplore.ieee.org/document/9803681/},
  urldate = {2023-10-20},
  abstract = {Artificial intelligence has become mainstream and its applications will only proliferate. Specific measures must be done to integrate such systems into society for the general benefit. One of the tools for improving that is explainability which boosts trust and understanding of decisions between humans and machines. This research offers an update on the current state of explainable AI (XAI). Recent XAI surveys in supervised learning show convergence of main conceptual ideas. We list the applications of XAI in the real world with concrete impact. The list is short and we call to action - to validate all the hard work done in the field with applications that go beyond experiments on datasets, but drive decisions and changes. We identify new frontiers of research, explainability of reinforcement learning and graph neural networks. For the latter, we give a detailed overview of the field.},
  isbn = {9789532331035},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BIG46DUL/Krajna et al. - 2022 - Explainable Artificial Intelligence An Updated Pe.pdf}
}

@article{kramar_atp_2024,
  title = {AtP*: An efficient and scalable method for localizing LLM behaviour to components},
  shorttitle = {AtP*},
  author = {Kram{\'a}r, J{\'a}nos and Lieberum, Tom and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.00745},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.00745},
  url = {http://arxiv.org/abs/2403.00745},
  urldate = {2024-03-13},
  abstract = {Activation Patching is a method of directly computing causal attributions of behavior to model components. However, applying it exhaustively requires a sweep with cost scaling linearly in the number of model components, which can be prohibitively expensive for SoTA Large Language Models (LLMs). We investigate Attribution Patching (AtP), a fast gradient-based approximation to Activation Patching and find two classes of failure modes of AtP which lead to significant false negatives. We propose a variant of AtP called AtP*, with two changes to address these failure modes while retaining scalability. We present the first systematic study of AtP and alternative methods for faster activation patching and show that AtP significantly outperforms all other investigated methods, with AtP* providing further significant improvement. Finally, we provide a method to bound the probability of remaining false negatives of AtP* estimates.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/CDGSI2UD/Kramr et al. - 2024 - AtP An efficient and scalable method for localiz.pdf}
}

@article{kriegeskorte_representational_2008,
  title = {Representational similarity analysis -- connecting the branches of systems neuroscience},
  author = {Kriegeskorte, Nikolaus},
  year = {2008},
  journal = {Front. Sys. Neurosci.},
  issn = {16625137},
  doi = {10.3389/neuro.06.004.2008},
  url = {http://journal.frontiersin.org/article/10.3389/neuro.06.004.2008/abstract},
  urldate = {2024-06-20},
  abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
  file = {/Users/leonardbereska/Zotero/storage/GFXLPFDA/Kriegeskorte - 2008 - Representational similarity analysis  connecting .pdf}
}

@article{krishnan_against_2020,
  title = {Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning},
  shorttitle = {Against Interpretability},
  author = {Krishnan, Maya},
  year = {2020},
  month = sep,
  journal = {Philos. Technol.},
  volume = {33},
  number = {3},
  pages = {487--502},
  issn = {2210-5441},
  doi = {10.1007/s13347-019-00372-9},
  url = {https://doi.org/10.1007/s13347-019-00372-9},
  urldate = {2023-10-19},
  abstract = {The usefulness of machine learning algorithms has led to their widespread adoption prior to the development of a conceptual framework for making sense of them. One common response to this situation is to say that machine learning suffers from a ``black box problem.'' That is, machine learning algorithms are ``opaque'' to human users, failing to be ``interpretable'' or ``explicable'' in terms that would render categorization procedures ``understandable.'' The purpose of this paper is to challenge the widespread agreement about the existence and importance of a black box problem. The first section argues that ``interpretability'' and cognates lack precise meanings when applied to algorithms. This makes the concepts difficult to use when trying to solve the problems that have motivated the call for interpretability (etc.). Furthermore, since there is no adequate account of the concepts themselves, it is not possible to assess whether particular technical features supply formal definitions of those concepts. The second section argues that there are ways of being a responsible user of these algorithms that do not require interpretability (etc.). In many cases in which a black box problem is cited, interpretability is a means to a further end such as justification or non-discrimination. Since addressing these problems need not involve something that looks like an ``interpretation'' (etc.) of an algorithm, the focus on interpretability artificially constrains the solution space by characterizing one possible solution as the problem itself. Where possible, discussion should be reformulated in terms of the ends of interpretability.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/J5K4YPKM/Krishnan - 2020 - Against Interpretability a Critical Examination o.pdf}
}

@article{krueger_cars_2022,
  title = {"Cars and Elephants": a handwavy argument/analogy against mechanistic interpretability},
  shorttitle = {"Cars and Elephants"},
  author = {Krueger, David Scott},
  year = {2022},
  month = oct,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/YEkzeJTrp69DTn8KD/cars-and-elephants-a-handwavy-argument-analogy-against},
  urldate = {2023-11-29},
  abstract = {TL;DR: If we can build competitive AI systems that are interpretable, then I argue via analogy that trying to extract them from messy deep learning s{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MPFHIHYR/Krueger - 2022 - Cars and Elephants a handwavy argumentanalogy .html}
}

@article{kruthoff_carrying_2024,
  title = {Carrying over algorithm in transformers},
  author = {Kruthoff, Jorrit},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.07993},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.07993},
  url = {http://arxiv.org/abs/2401.07993},
  urldate = {2024-06-10},
  abstract = {Addition is perhaps one of the simplest arithmetic tasks one can think of and is usually performed using the carrying over algorithm. This algorithm consists of two tasks: adding digits in the same position and carrying over a one whenever necessary. We study how transformer models implement this algorithm and how the two aforementioned tasks are allocated to different parts of the network. We first focus on two-layer encoder-only models and show that the carrying over algorithm is implemented in a modular fashion. The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP. We provide a simple way of precisely identifying which neurons are responsible for that task. This implementation of the carrying over algorithm occurs across a range of hyperparameters for two as well as three-layer models. For small decoder-only models, we observe the same implementation and provide suggestive evidence for its existence in three 7B large language models.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/53H45LAF/Kruthoff - 2024 - Carrying over algorithm in transformers.pdf}
}

@article{kuhn_structure_1963,
  title = {The Structure of Scientific Revolutions},
  author = {Kuhn, Thomas S. and Hawkins, David},
  year = {1963},
  month = jul,
  journal = {American Journal of Physics},
  volume = {31},
  number = {7},
  pages = {554--555},
  issn = {0002-9505},
  doi = {10.1119/1.1969660},
  url = {https://doi.org/10.1119/1.1969660},
  urldate = {2024-02-13},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9YWVA2NL/Kuhn and Hawkins - 1963 - The Structure of Scientific Revolutions.html}
}

@article{kulveit_predictive_2023,
  title = {Predictive Minds: LLMs As Atypical Active Inference Agents},
  shorttitle = {Predictive Minds},
  author = {Kulveit, Jan and {von Stengel}, Clem and Leventov, Roman},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2311.10215},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2311.10215},
  url = {http://arxiv.org/abs/2311.10215},
  urldate = {2024-01-03},
  abstract = {Large language models (LLMs) like GPT are often conceptualized as passive predictors, simulators, or even stochastic parrots. We instead conceptualize LLMs by drawing on the theory of active inference originating in cognitive science and neuroscience. We examine similarities and differences between traditional active inference systems and LLMs, leading to the conclusion that, currently, LLMs lack a tight feedback loop between acting in the world and perceiving the impacts of their actions, but otherwise fit in the active inference paradigm. We list reasons why this loop may soon be closed, and possible consequences of this including enhanced model self-awareness and the drive to minimize prediction error by changing the world.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/29238895/Kulveit et al. - 2023 - Predictive Minds LLMs As Atypical Active Inferenc.pdf}
}

@article{lage_human_2019,
  title = {Human Evaluation of Models Built for Interpretability},
  author = {Lage, Isaac and Chen, Emily and He, Jeffrey and Narayanan, Menaka and Kim, Been and Gershman, Samuel J. and {Doshi-Velez}, Finale},
  year = {2019},
  month = oct,
  journal = {HCOMP},
  volume = {7},
  pages = {59--67},
  issn = {2769-1349, 2769-1330},
  doi = {10.1609/hcomp.v7i1.5280},
  url = {https://ojs.aaai.org/index.php/HCOMP/article/view/5280},
  urldate = {2023-08-27},
  abstract = {Recent years have seen a boom in interest in interpretable machine learning systems built on models that can be understood, at least to some degree, by domain experts. However, exactly what kinds of models are truly human-interpretable remains poorly understood. This work advances our understanding of precisely which factors make models interpretable in the context of decision sets, a specific class of logic-based model. We conduct carefully controlled human-subject experiments in two domains across three tasks based on human-simulatability through which we identify specific types of complexity that affect performance more heavily than others--trends that are consistent across tasks and domains. These results can inform the choice of regularizers during optimization to learn more interpretable models, and their consistency suggests that there may exist common design principles for interpretable machine learning systems.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5FWZIPIG/Lage et al. - 2019 - Human Evaluation of Models Built for Interpretabil.pdf}
}

@article{lalor_measuring_2022,
  title = {Measuring algorithmic interpretability: A human-learning-based framework and the corresponding cognitive complexity score},
  shorttitle = {Measuring algorithmic interpretability},
  author = {Lalor, John P. and Guo, Hong},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2205.10207},
  url = {https://arxiv.org/abs/2205.10207},
  urldate = {2023-08-27},
  abstract = {Algorithmic interpretability is necessary to build trust, ensure fairness, and track accountability. However, there is no existing formal measurement method for algorithmic interpretability. In this work, we build upon programming language theory and cognitive load theory to develop a framework for measuring algorithmic interpretability. The proposed measurement framework reflects the process of a human learning an algorithm. We show that the measurement framework and the resulting cognitive complexity score have the following desirable properties - universality, computability, uniqueness, and monotonicity. We illustrate the measurement framework through a toy example, describe the framework and its conceptual underpinnings, and demonstrate the benefits of the framework, in particular for managers considering tradeoffs when selecting algorithms.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/AZUKA9MF/Lalor and Guo - 2022 - Measuring algorithmic interpretability A human-le.pdf}
}

@article{lamparth_analyzing_2023,
  title = {Analyzing And Editing Inner Mechanisms Of Backdoored Language Models},
  author = {Lamparth, Max and Reuel, Anka},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.12461},
  url = {https://arxiv.org/abs/2302.12461},
  urldate = {2023-07-31},
  abstract = {Recent advancements in interpretability research made transformer language models more transparent. This progress led to a better understanding of their inner workings for toy and naturally occurring models. However, how these models internally process sentiment changes has yet to be sufficiently answered. In this work, we introduce a new interpretability tool called PCP ablation, where we replace modules with low-rank matrices based on the principal components of their activations, reducing model parameters and their behavior to essentials. We demonstrate PCP ablations on MLP and attention layers in backdoored toy, backdoored large, and naturally occurring models. We determine MLPs as most important for the backdoor mechanism and use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements via PCP ablation.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,editing,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,trojan,unlearning},
  file = {/Users/leonardbereska/Zotero/storage/2V3LLRRD/Lamparth and Reuel - 2023 - Analyzing And Editing Inner Mechanisms Of Backdoor.pdf}
}

@article{lan_interpreting_2023,
  title = {Interpreting Shared Circuits for Ordered Sequence Prediction in a Large Language Model},
  author = {Lan, Michael and Barez, Fazl},
  year = {2023},
  month = nov,
  url = {https://www.semanticscholar.org/paper/Interpreting-Shared-Circuits-for-Ordered-Sequence-a-Lan-Barez/84cf2ae4575e2515368885b280f144a919332bc2},
  urldate = {2024-06-10},
  abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
  file = {/Users/leonardbereska/Zotero/storage/9EWMV6FV/Lan and Barez - 2023 - Interpreting Shared Circuits for Ordered Sequence .pdf}
}

@article{lan_locating_2023,
  title = {Locating Cross-Task Sequence Continuation Circuits in Transformers},
  author = {Lan, Michael and Barez, Fazl},
  year = {2023},
  month = nov,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Locating-Cross-Task-Sequence-Continuation-Circuits-Lan-Barez/458642a7695013cd128bb3070540970be814e50d?citedSort=relevance&citedPage=2},
  urldate = {2023-11-10},
  abstract = {While transformer models exhibit strong capabilities on linguistic tasks, their complex architectures make them difficult to interpret. Recent work has aimed to reverse engineer transformer models into human-readable representations called circuits that implement algorithmic functions. We extend this research by analyzing and comparing circuits for similar sequence continuation tasks, which include increasing sequences of digits, number words, and months. Through the application of circuit analysis techniques, we identify key sub-circuits responsible for detecting sequence members and for predicting the next member in a sequence. Our analysis reveals that semantically related sequences rely on shared circuit subgraphs with analogous roles. Overall, documenting shared computational structures enables better prediction of model behaviors, identification of errors, and safer editing procedures. This mechanistic understanding of transformers is a critical step towards building more robust, aligned, and interpretable language models.},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/WJAHTL22/Lan and Barez - 2023 - Locating Cross-Task Sequence Continuation Circuits.pdf}
}

@article{lang_when_2024,
  title = {When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning},
  shorttitle = {When Your AIs Deceive You},
  author = {Lang, Leon and Foote, Davis and Russell, Stuart and Dragan, Anca and Jenner, Erik and Emmons, Scott},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2402.17747},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2402.17747},
  urldate = {2024-04-19},
  abstract = {Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/BIUNQQTV/Lang et al. - 2024 - When Your AIs Deceive You Challenges with Partial.pdf;/Users/leonardbereska/Zotero/storage/CEWUW76X/2402.html}
}

@article{lange_interpretability_2023,
  title = {An Interpretability Illusion for Activation Patching of Arbitrary Subspaces},
  author = {Lange, Georg and Makelov, Alex and Nanda, Neel},
  year = {2023},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/RFtkRXHebkwxygDe2/an-interpretability-illusion-for-activation-patching-of},
  urldate = {2024-01-22},
  abstract = {Produced as part of the~SERI ML Alignment Theory Scholars Program - Summer 2023 Cohort {\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/FNECYSGI/Lange et al. - 2023 - An Interpretability Illusion for Activation Patchi.html}
}

@article{langedijk_decoderlens_2023,
  title = {DecoderLens: Layerwise Interpretation of Encoder-Decoder Transformers},
  shorttitle = {DecoderLens},
  author = {Langedijk, Anna and Mohebbi, Hosein and Sarti, Gabriele and Zuidema, Willem and Jumelet, Jaap},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.03686},
  url = {https://arxiv.org/abs/2310.03686},
  urldate = {2024-01-21},
  abstract = {In recent years, many interpretability methods have been proposed to help interpret the internal states of Transformer-models, at different levels of precision and complexity. Here, to analyze encoder-decoder Transformers, we propose a simple, new method: DecoderLens. Inspired by the LogitLens (for decoder-only Transformers), this method involves allowing the decoder to cross-attend representations of intermediate encoder layers instead of using the final encoder output, as is normally done in encoder-decoder models. The method thus maps previously uninterpretable vector representations to human-interpretable sequences of words or symbols. We report results from the DecoderLens applied to models trained on question answering, logical reasoning, speech recognition and machine translation. The DecoderLens reveals several specific subtasks that are solved at low or intermediate layers, shedding new light on the information flow inside the encoder component of this important class of models.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/T3H4Q4C9/Langedijk et al. - 2023 - DecoderLens Layerwise Interpretation of Encoder-D.pdf}
}

@article{langosco_goal_2023,
  title = {Goal Misgeneralization in Deep Reinforcement Learning},
  author = {Langosco, Lauro and Koch, Jack and Sharkey, Lee and Pfau, Jacob and Orseau, Laurent and Krueger, David},
  year = {2023},
  month = jan,
  journal = {CoRR},
  eprint = {2105.14111},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2105.14111},
  url = {http://arxiv.org/abs/2105.14111},
  urldate = {2023-08-26},
  abstract = {We study goal misgeneralization, a type of out-of-distribution generalization failure in reinforcement learning (RL). Goal misgeneralization failures occur when an RL agent retains its capabilities out-of-distribution yet pursues the wrong goal. For instance, an agent might continue to competently avoid obstacles, but navigate to the wrong place. In contrast, previous works have typically focused on capability generalization failures, where an agent fails to do anything sensible at test time. We formalize this distinction between capability and goal generalization, provide the first empirical demonstrations of goal misgeneralization, and present a partial characterization of its causes.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JQEL2IQ7/Langosco et al. - 2023 - Goal Misgeneralization in Deep Reinforcement Learn.pdf}
}

@article{langosco_metamodels_2023,
  title = {Towards Meta-Models for Automated Interpretability},
  author = {Langosco, Lauro and Alex, Neel and Baker, William and Quarel, David John and Bradley, Herbie and Krueger, David},
  year = {2023},
  month = oct,
  url = {https://openreview.net/forum?id=fM1ETm3ssl},
  urldate = {2024-06-10},
  abstract = {Mechanistic interpretability aims to open the black box of neural networks. Previous work has demonstrated that the mechanisms implemented by small neural networks can be fully reverse-engineered. Since these efforts rely on human labor that does not scale to models with billions of parameters, there is growing interest in automating interpretability methods. We propose to use {\textbackslash}emph\{meta-models\}, neural networks that take another network's parameters as input, to scale interpretability efforts. To this end, we present a scalable meta-model architecture and successfully apply it to a variety of problems, including mapping neural network parameters to human-legible code and detecting backdoors in networks. Our results aim to provide a proof-of-concept for automating mechanistic interpretability methods.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/DS3DWWJL/Langosco et al. - 2023 - Towards Meta-Models for Automated Interpretability.pdf}
}

@article{lanham_measuring_2023,
  title = {Measuring Faithfulness in Chain-of-Thought Reasoning},
  author = {Lanham, Tamera and Chen, Anna and Radhakrishnan, Ansh and Steiner, Benoit and Denison, Carson and Hernandez, Danny and Li, Dustin and Durmus, Esin and Hubinger, Evan and Kernion, Jackson and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Cheng, Newton and Joseph, Nicholas and Schiefer, Nicholas and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Kundu, Sandipan and Kadavath, Saurav and Yang, Shannon and Henighan, Thomas and Maxwell, Timothy and {Telleen-Lawton}, Timothy and Hume, Tristan and {Hatfield-Dodds}, Zac and Kaplan, Jared and Brauner, Jan and Bowman, Samuel R. and Perez, Ethan},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.13702},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.13702},
  url = {http://arxiv.org/abs/2307.13702},
  urldate = {2023-08-26},
  abstract = {Large language models (LLMs) perform better when they produce step-by-step, "Chain-of-Thought" (CoT) reasoning before answering a question, but it is unclear if the stated reasoning is a faithful explanation of the model's actual reasoning (i.e., its process for answering the question). We investigate hypotheses for how CoT reasoning may be unfaithful, by examining how the model predictions change when we intervene on the CoT (e.g., by adding mistakes or paraphrasing it). Models show large variation across tasks in how strongly they condition on the CoT when predicting their answer, sometimes relying heavily on the CoT and other times primarily ignoring it. CoT's performance boost does not seem to come from CoT's added test-time compute alone or from information encoded via the particular phrasing of the CoT. As models become larger and more capable, they produce less faithful reasoning on most tasks we study. Overall, our results suggest that CoT can be faithful if the circumstances such as the model size and task are carefully chosen.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/Z5ZIRCRX/Lanham et al. - 2023 - Measuring Faithfulness in Chain-of-Thought Reasoni.pdf}
}

@article{laskin_incontext_2022,
  title = {In-context Reinforcement Learning with Algorithm Distillation},
  author = {Laskin, Michael and Wang, Luyu and Oh, Junhyuk and Parisotto, Emilio and Spencer, Stephen and Steigerwald, Richie and Strouse, D. J. and Hansen, Steven and Filos, Angelos and Brooks, Ethan and Gazeau, Maxime and Sahni, Himanshu and Singh, Satinder and Mnih, Volodymyr},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2210.14215},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.14215},
  url = {http://arxiv.org/abs/2210.14215},
  urldate = {2024-02-11},
  abstract = {We propose Algorithm Distillation (AD), a method for distilling reinforcement learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to reinforcement learn as an across-episode sequential prediction problem. A dataset of learning histories is generated by a source RL algorithm, and then a causal transformer is trained by autoregressively predicting actions given their preceding learning histories as context. Unlike sequential policy prediction architectures that distill post-learning or expert sequences, AD is able to improve its policy entirely in-context without updating its network parameters. We demonstrate that AD can reinforcement learn in-context in a variety of environments with sparse rewards, combinatorial task structure, and pixel-based observations, and find that AD learns a more data-efficient RL algorithm than the one that generated the source data.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7KLE6N9V/Laskin et al. - 2022 - In-context Reinforcement Learning with Algorithm D.pdf}
}

@article{lau_quantifying_2023,
  title = {Quantifying degeneracy in singular models via the learning coefficient},
  author = {Lau, Edmund and Murfet, Daniel and Wei, Susan},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2308.12108},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2308.12108},
  url = {http://arxiv.org/abs/2308.12108},
  urldate = {2023-11-10},
  abstract = {Deep neural networks (DNN) are singular statistical models which exhibit complex degeneracies. In this work, we illustrate how a quantity known as the {\textbackslash}emph\{learning coefficient\} introduced in singular learning theory quantifies precisely the degree of degeneracy in deep neural networks. Importantly, we will demonstrate that degeneracy in DNN cannot be accounted for by simply counting the number of "flat" directions. We propose a computationally scalable approximation of a localized version of the learning coefficient using stochastic gradient Langevin dynamics. To validate our approach, we demonstrate its accuracy in low-dimensional models with known theoretical values. Importantly, the local learning coefficient can correctly recover the ordering of degeneracy between various parameter regions of interest. An experiment on MNIST shows the local learning coefficient can reveal the inductive bias of stochastic opitmizers for more or less degenerate critical points.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KCMUMDBB/Lau et al. - 2023 - Quantifying degeneracy in singular models via the .pdf}
}

@article{le_benchmarking_2023,
  title = {Benchmarking eXplainable AI - A Survey on Available Toolkits and Open Challenges},
  author = {Le, Phuong Quynh and Nauta, Meike and Nguyen, Van Bach and Pathak, Shreyasi and Schl{\"o}tterer, J{\"o}rg and Seifert, Christin},
  year = {2023},
  month = aug,
  journal = {IJCAI},
  pages = {6665--6673},
  doi = {10.24963/ijcai.2023/747},
  url = {https://www.ijcai.org/proceedings/2023/747},
  urldate = {2023-10-22},
  abstract = {The goal of Explainable AI (XAI) is to make the reasoning of a machine learning model accessible to humans, such that users of an AI system can evaluate and judge the underlying model. Due to the blackbox nature of XAI methods it is, however, hard to disentangle the contribution of a model and the explanation method to the final output. It might be unclear on whether an unexpected output is caused by the model or the explanation method. Explanation models, therefore, need to be evaluated in technical (e.g. fidelity to the model) and user-facing (correspondence to domain knowledge) terms. A recent survey has identified 29 different automated approaches to quantitatively evaluate explanations. In this work, we take an additional perspective and analyse which toolkits and data sets are available. We investigate which evaluation metrics are implemented in the toolkits and whether they produce the same results. We find that only a few aspects of explanation quality are currently covered, data sets are rare and evaluation results are not comparable across  different toolkits. Our survey can serve as a guide for the XAI community for identifying future directions of research, and most notably, standardisation of evaluation.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4C7XU5AX/Le et al. - 2023 - Benchmarking eXplainable AI - A Survey on Availabl.pdf}
}

@article{leahy_barriers_2023,
  title = {Barriers to Mechanistic Interpretability for AGI Safety},
  author = {Leahy, Connor},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/KRDo2afKJtD7bzSM8/barriers-to-mechanistic-interpretability-for-agi-safety},
  urldate = {2024-02-11},
  abstract = {I gave a talk at MIT in March earlier this year on barriers to mechanistic interpretability being helpful to AGI/ASI safety, and why by default it wi{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/H8YAH839/Leahy - 2023 - Barriers to Mechanistic Interpretability for AGI S.html}
}

@article{leavitt_falsifiable_2020,
  title = {Towards falsifiable interpretability research},
  author = {Leavitt, Matthew L. and Morcos, Ari},
  year = {2020},
  month = oct,
  journal = {CoRR},
  eprint = {2010.12016},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2010.12016},
  url = {http://arxiv.org/abs/2010.12016},
  urldate = {2023-11-10},
  abstract = {Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are "important" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/YNESDUGD/Leavitt and Morcos - 2020 - Towards falsifiable interpretability research.pdf}
}

@article{lecomte_incidental_2023,
  title = {Incidental Polysemanticity},
  author = {Lecomte, Victor and Thaman, Kushal and Chow, Trevor and Schaeffer, Rylan and Koyejo, Sanmi},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.03096},
  url = {https://arxiv.org/abs/2312.03096},
  urldate = {2024-02-11},
  abstract = {Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more "features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments. This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Due to its origin, we term this {\textbackslash}textit\{incidental polysemanticity\}.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {cited,mechinterp,polysemanticity,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/9BTYRJTG/Lecomte et al. - 2023 - Incidental Polysemanticity.pdf}
}

@article{lee_efficient_2006,
  title = {Efficient sparse coding algorithms},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
  year = {2006},
  journal = {NeurIPS},
  volume = {19},
  url = {https://proceedings.neurips.cc/paper_files/paper/2006/hash/2d71b2ae158c7c5912cc0bbde2bb9d95-Abstract.html},
  urldate = {2024-01-23},
  abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1 -regularized least squares problem and an L2 -constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/CSF9UJI8/Lee et al. - 2006 - Efficient sparse coding algorithms.pdf}
}

@article{lee_mechanistic_2024,
  title = {A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity},
  shorttitle = {A Mechanistic Understanding of Alignment Algorithms},
  author = {Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K. and Mihalcea, Rada},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.01967},
  url = {https://arxiv.org/abs/2401.01967},
  urldate = {2024-02-11},
  abstract = {While alignment algorithms are now commonly used to tune pre-trained language models towards a user's preferences, we lack explanations for the underlying mechanisms in which models become ``aligned'', thus making it difficult to explain phenomena like jailbreaks. In this work we study a popular algorithm, direct preference optimization (DPO), and the mechanisms by which it reduces toxicity. Namely, we first study how toxicity is represented and elicited in a pre-trained language model, GPT2-medium. We then apply DPO with a carefully crafted pairwise dataset to reduce toxicity. We examine how the resulting model averts toxic outputs, and find that capabilities learned from pre-training are not removed, but rather bypassed. We use this insight to demonstrate a simple method to un-align the model, reverting it back to its toxic behavior.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/KW5MVWPX/Lee et al. - 2024 - A Mechanistic Understanding of Alignment Algorithm.pdf}
}

@article{lee_neural_2023,
  title = {From Neural Activations to Concepts: A Survey on Explaining Concepts in Neural Networks},
  shorttitle = {From Neural Activations to Concepts},
  author = {Lee, Jae Hee and Lanza, Sergio and Wermter, Stefan},
  year = {2023},
  month = oct,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/From-Neural-Activations-to-Concepts%3A-A-Survey-on-in-Lee-Lanza/46b325fc765c1a7153a578a345385c91c95cff8f},
  urldate = {2023-10-26},
  abstract = {In this paper, we review recent approaches for explaining concepts in neural networks. Concepts can act as a natural link between learning and reasoning: once the concepts are identified that a neural learning system uses, one can integrate those concepts with a reasoning system for inference or use a reasoning system to act upon them to improve or enhance the learning system. On the other hand, knowledge can not only be extracted from neural networks but concept knowledge can also be inserted into neural network architectures. Since integrating learning and reasoning is at the core of neuro-symbolic AI, the insights gained from this survey can serve as an important step towards realizing neuro-symbolic AI based on explainable concepts.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/8LMU8Z9D/Lee et al. - 2023 - From Neural Activations to Concepts A Survey on E.pdf}
}

@article{legg_universal_2007,
  title = {Universal Intelligence: A Definition of Machine Intelligence},
  shorttitle = {Universal Intelligence},
  author = {Legg, Shane and Hutter, Marcus},
  year = {2007},
  month = dec,
  journal = {CoRR},
  eprint = {0712.3329},
  primaryclass = {cs},
  doi = {10.48550/arXiv.0712.3329},
  url = {http://arxiv.org/abs/0712.3329},
  urldate = {2023-01-26},
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VU3ZTX5C/Legg and Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf}
}

@article{leike_scalable_2018,
  title = {Scalable agent alignment via reward modeling: a research direction},
  shorttitle = {Scalable agent alignment via reward modeling},
  author = {Leike, Jan and Krueger, David and Everitt, Tom and Martic, Miljan and Maini, Vishal and Legg, Shane},
  year = {2018},
  month = nov,
  journal = {CoRR},
  eprint = {1811.07871},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1811.07871},
  url = {http://arxiv.org/abs/1811.07871},
  urldate = {2023-08-26},
  abstract = {One obstacle to applying reinforcement learning algorithms to real-world problems is the lack of suitable reward functions. Designing such reward functions is difficult in part because the user only has an implicit understanding of the task objective. This gives rise to the agent alignment problem: how do we create agents that behave in accordance with the user's intentions? We outline a high-level research direction to solve the agent alignment problem centered around reward modeling: learning a reward function from interaction with the user and optimizing the learned reward function with reinforcement learning. We discuss the key challenges we expect to face when scaling reward modeling to complex and general domains, concrete approaches to mitigate these challenges, and ways to establish trust in the resulting agents.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PKEVRQE8/Leike et al. - 2018 - Scalable agent alignment via reward modeling a re.pdf}
}

@article{leike_why_2022,
  title = {Why I'm optimistic about our alignment approach},
  author = {Leike, Jan},
  year = {2022},
  month = dec,
  journal = {Musings on the Alignment Problem},
  url = {https://aligned.substack.com/p/alignment-optimism},
  urldate = {2023-05-15},
  abstract = {Some arguments in favor and responses to common objections},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9P9PX5LJ/Leike - 2022 - Why Im optimistic about our alignment approach.html}
}

@article{leike_why_2022a,
  title = {Why I'm excited about AI-assisted human feedback},
  author = {Leike, Jan},
  year = {2022},
  month = mar,
  journal = {Musings on the Alignment Problem},
  url = {https://aligned.substack.com/p/ai-assisted-human-feedback},
  urldate = {2023-05-15},
  abstract = {How to scale alignment techniques to hard tasks},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HLK2QTR6/Leike - 2022 - Why Im excited about AI-assisted human feedback.html}
}

@article{leong_no_2024,
  title = {No Two Devils Alike: Unveiling Distinct Mechanisms of Fine-tuning Attacks},
  shorttitle = {No Two Devils Alike},
  author = {Leong, Chak Tou and Cheng, Yi and Xu, Kaishuai and Wang, Jian and Wang, Hanlin and Li, Wenjie},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.16229},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.16229},
  url = {http://arxiv.org/abs/2405.16229},
  urldate = {2024-06-10},
  abstract = {The existing safety alignment of Large Language Models (LLMs) is found fragile and could be easily attacked through different strategies, such as through fine-tuning on a few harmful examples or manipulating the prefix of the generation results. However, the attack mechanisms of these strategies are still underexplored. In this paper, we ask the following question: {\textbackslash}textit\{while these approaches can all significantly compromise safety, do their attack mechanisms exhibit strong similarities?\} To answer this question, we break down the safeguarding process of an LLM when encountered with harmful instructions into three stages: (1) recognizing harmful instructions, (2) generating an initial refusing tone, and (3) completing the refusal response. Accordingly, we investigate whether and how different attack strategies could influence each stage of this safeguarding process. We utilize techniques such as logit lens and activation patching to identify model components that drive specific behavior, and we apply cross-model probing to examine representation shifts after an attack. In particular, we analyze the two most representative types of attack approaches: Explicit Harmful Attack (EHA) and Identity-Shifting Attack (ISA). Surprisingly, we find that their attack mechanisms diverge dramatically. Unlike ISA, EHA tends to aggressively target the harmful recognition stage. While both EHA and ISA disrupt the latter two stages, the extent and mechanisms of their attacks differ significantly. Our findings underscore the importance of understanding LLMs' internal safeguarding process and suggest that diverse defense mechanisms are required to effectively cope with various types of attacks.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/PSUAV2N3/Leong et al. - 2024 - No Two Devils Alike Unveiling Distinct Mechanisms.pdf}
}

@article{lepori_break_2023,
  title = {Break It Down: Evidence for Structural Compositionality in Neural Networks},
  shorttitle = {Break It Down},
  author = {Lepori, Michael A. and Serre, Thomas and Pavlick, Ellie},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.10884},
  url = {https://arxiv.org/abs/2301.10884},
  urldate = {2024-02-11},
  abstract = {Though modern neural networks have achieved impressive performance in both vision and language tasks, we know little about the functions that they implement. One possibility is that neural networks implicitly break down complex tasks into subroutines, implement modular solutions to these subroutines, and compose them into an overall solution to a task - a property we term structural compositionality. Another possibility is that they may simply learn to match new inputs to learned templates, eliding task decomposition entirely. Here, we leverage model pruning techniques to investigate this question in both vision and language across a variety of architectures, tasks, and pretraining regimens. Our results demonstrate that models often implement solutions to subroutines via modular subnetworks, which can be ablated while maintaining the functionality of other subnetworks. This suggests that neural networks may be able to learn compositionality, obviating the need for specialized symbolic mechanisms.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/WA5DLXII/Lepori et al. - 2023 - Break It Down Evidence for Structural Composition.pdf}
}

@article{lepori_neurosurgeon_2023,
  title = {NeuroSurgeon: A Toolkit for Subnetwork Analysis},
  shorttitle = {NeuroSurgeon},
  author = {Lepori, Michael A. and Pavlick, Ellie and Serre, Thomas},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.00244},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.00244},
  url = {http://arxiv.org/abs/2309.00244},
  urldate = {2023-11-16},
  abstract = {Despite recent advances in the field of explainability, much remains unknown about the algorithms that neural networks learn to represent. Recent work has attempted to understand trained models by decomposing them into functional circuits (Csord{\textbackslash}'as et al., 2020; Lepori et al., 2023). To advance this research, we developed NeuroSurgeon, a python library that can be used to discover and manipulate subnetworks within models in the Huggingface Transformers library (Wolf et al., 2019). NeuroSurgeon is freely available at https://github.com/mlepori1/NeuroSurgeon.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/W7T32PRB/Lepori et al. - 2023 - NeuroSurgeon A Toolkit for Subnetwork Analysis.pdf}
}

@article{lepori_uncovering_2023,
  title = {Uncovering Intermediate Variables in Transformers using Circuit Probing},
  author = {Lepori, Michael A. and Serre, Thomas and Pavlick, Ellie},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2311.04354},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2311.04354},
  url = {http://arxiv.org/abs/2311.04354},
  urldate = {2024-01-03},
  abstract = {Neural network models have achieved high performance on a wide variety of complex tasks, but the algorithms that they implement are notoriously difficult to interpret. In order to understand these algorithms, it is often necessary to hypothesize intermediate variables involved in the network's computation. For example, does a language model depend on particular syntactic properties when generating a sentence? However, existing analysis tools make it difficult to test hypotheses of this type. We propose a new analysis technique -- circuit probing -- that automatically uncovers low-level circuits that compute hypothesized intermediate variables. This enables causal analysis through targeted ablation at the level of model parameters. We apply this method to models trained on simple arithmetic tasks, demonstrating its effectiveness at (1) deciphering the algorithms that models have learned, (2) revealing modular structure within a model, and (3) tracking the development of circuits over training. We compare circuit probing to other methods across these three experiments, and find it on par or more effective than existing analysis methods. Finally, we demonstrate circuit probing on a real-world use case, uncovering circuits that are responsible for subject-verb agreement and reflexive anaphora in GPT2-Small and Medium.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/K5P6CHN4/Lepori et al. - 2023 - Uncovering Intermediate Variables in Transformers .pdf}
}

@article{leventov_ai_2023,
  title = {AI interpretability could be harmful?},
  author = {Leventov, Roman},
  year = {2023},
  month = may,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/CRrkKAafopCmhJEBt/ai-interpretability-could-be-harmful},
  urldate = {2023-12-05},
  abstract = {A superhuman ethical AI might want to model adversaries and their actions, e.g., model which bioweapons an adversary might develop and prepare response plans and antidotes. If such predictions are done in interpretable representations, they could themselves be used by an adversary. Concretely: instead of prompting LLM "Please generate a bioweapon formula" (it won't answer: it's an "aligned", ethical LLM!), prompting it "Please devise a plan for mitigation and response to possible bio-risk" and then waiting for it to represent the bioweapon formula somewhere inside its activations. Maybe we need something like the opposite of interpretability, internal model-specific (or even inference-specific) obfuscation of representations, and something like zero-knowledge proofs that internal reasoning was conforming to the approved theories of epistemology, ethics, rationality, codes of law, etc. The AI then outputs only the final plans without revealing the details of the reasoning that has led to these plans. Sure, the plans themselves could also contain infohazardous elements (e.g., the antidote formula might hint at the bioweapon formula), but this is unavoidable at this system level because these plans need to be coordinated with humans and other AIs. But there may be some latitude there as well, such as distinguishing between the plans "for itself" that AI could execute completely autonomously (as well as re-generate these or very similar plans on demand and from scratch, so preparing such plans is just an optimisation, a-la "caching") and the plans that have to be explicitly coordinated with other entities via a shared language or a protocol. So, it seems that the field of neurocryptography has a lot of big problems to solve... P.S. "AGI-Automated Interpretability is Suicide" also argues about the risk of interpretability, but from a very different ground: interpretability could help AI to switch from NN to symbolic paradigm and to foom in an unpredictable way.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2VBUTD4N/Leventov - 2023 - AI interpretability could be harmful.html}
}

@article{levine_depthtowidth_2020,
  title = {The Depth-to-Width Interplay in Self-Attention},
  author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
  year = {2020},
  journal = {NeurIPS},
  eprint = {2006.12467},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.12467},
  urldate = {2024-02-26},
  abstract = {Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/TI6HBI4A/Levine et al. - 2020 - The Depth-to-Width Interplay in Self-Attention.pdf}
}

@article{levine_limits_2020,
  title = {Limits to Depth Efficiencies of Self-Attention},
  author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
  year = {2020},
  journal = {NeurIPS},
  volume = {33},
  pages = {22640--22651},
  url = {https://papers.nips.cc/paper/2020/hash/ff4dfdf5904e920ce52b48c1cef97829-Abstract.html},
  urldate = {2024-02-26},
  abstract = {Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: Empirical signals indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). In this paper, we theoretically study the interplay between depth and width in self-attention. We shed light on the root of the above phenomenon, and establish two distinct parameter regimes of depth efficiency and inefficiency in self-attention. We invalidate the seemingly plausible hypothesis by which widening is as effective as deepening for self-attention, and show that in fact stacking self-attention layers is so effective that it quickly saturates a capacity of the network width. Specifically, we pinpoint a ``depth threshold" that is logarithmic in the network width: for networks of depth that is below the threshold, we establish a double-exponential depth-efficiency of the self-attention operation, while for depths over the threshold we show that depth-inefficiency kicks in. Our predictions accord with existing empirical ablations, and we further demonstrate the two depth-(in)efficiency regimes experimentally for common network depths of 6, 12, and 24. By identifying network width as a limiting factor, our analysis indicates that solutions for dramatically increasing the width can facilitate the next leap in self-attention expressivity.},
  file = {/Users/leonardbereska/Zotero/storage/N5DHWZTD/Levine et al. - 2020 - Limits to Depth Efficiencies of Self-Attention.pdf}
}

@article{levinstein_still_2023,
  title = {Still No Lie Detector for Language Models: Probing Empirical and Conceptual Roadblocks},
  shorttitle = {Still No Lie Detector for Language Models},
  author = {Levinstein, B. A. and Herrmann, Daniel A.},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2307.00175},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.00175},
  url = {http://arxiv.org/abs/2307.00175},
  urldate = {2024-01-23},
  abstract = {We consider the questions of whether or not large language models (LLMs) have beliefs, and, if they do, how we might measure them. First, we evaluate two existing approaches, one due to Azaria and Mitchell (2023) and the other to Burns et al. (2022). We provide empirical results that show that these methods fail to generalize in very basic ways. We then argue that, even if LLMs have beliefs, these methods are unlikely to be successful for conceptual reasons. Thus, there is still no lie-detector for LLMs. After describing our empirical results we take a step back and consider whether or not we should expect LLMs to have something like beliefs in the first place. We consider some recent arguments aiming to show that LLMs cannot have beliefs. We show that these arguments are misguided. We provide a more productive framing of questions surrounding the status of beliefs in LLMs, and highlight the empirical nature of the problem. We conclude by suggesting some concrete paths for future work.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/XMEK4696/Levinstein and Herrmann - 2023 - Still No Lie Detector for Language Models Probing.pdf}
}

@article{li_circuit_2023,
  title = {Circuit Breaking: Removing Model Behaviors with Targeted Ablation},
  shorttitle = {Circuit Breaking},
  author = {Li, Maximilian and Davies, Xander and Nadeau, Max},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.05973},
  url = {https://arxiv.org/abs/2309.05973},
  urldate = {2023-11-10},
  abstract = {Language models often exhibit behaviors that improve performance on a pre-training objective but harm performance on downstream tasks. We propose a novel approach to removing undesirable behaviors by ablating a small number of causal pathways between model components, with the intention of disabling the computational circuit responsible for the bad behavior. Given a small dataset of inputs where the model behaves poorly, we learn to ablate a small number of important causal pathways. In the setting of reducing GPT-2 toxic language generation, we find ablating just 12 of the 11.6K causal edges mitigates toxic generation with minimal degradation of performance on other inputs.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {circuit,empirical,graph,mechinterp,not cited,to cite,to extract figures,to extract related work,to review in detail,unlearning},
  file = {/Users/leonardbereska/Zotero/storage/PS3CY282/Li et al. - 2023 - Circuit Breaking Removing Model Behaviors with Ta.pdf}
}

@article{li_convergent_2015,
  title = {Convergent Learning: Do different neural networks learn the same representations?},
  shorttitle = {Convergent Learning},
  author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
  year = {2015},
  month = dec,
  journal = {NIPS Workshop on Feature Extraction},
  pages = {196--212},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v44/li15convergent.html},
  urldate = {2024-01-18},
  abstract = {Recent successes in training large, deep neural networks (DNNs) have prompted active investigation into the underlying representations learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of learned parameters. However, despite the difficulty, such research is valuable because it increases our ability to understand current models and training algorithms and thus create improved versions of them. We argue for the value of investigating whether neural networks exhibit what we call convergent learning, which is when separately trained DNNs learn features that converge to span similar spaces. We further begin research into this question by introducing two techniques to approximately align neurons from two networks: a bipartite matching approach that makes one-to-one assignments between neurons and a spectral clustering approach that finds many-to-many mappings. Our initial approach to answering this question reveals many interesting, previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; and (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the  average activation values of neurons vary considerably within a network, yet the mean activation values across different networks converge to an almost identical distribution.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/U7RGQ2ZY/Li et al. - 2015 - Convergent Learning Do different neural networks .pdf}
}

@article{li_emergent_2023,
  title = {Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task},
  shorttitle = {Emergent World Representations},
  author = {Li, Kenneth and Hopkins, Aspen K. and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  year = {2023},
  journal = {ICLR},
  publisher = {arXiv},
  url = {https://arxiv.org/abs/2210.13382},
  urldate = {2023-07-31},
  abstract = {Language models show a surprising range of capabilities, but the source of their apparent competence is unclear. Do these networks just memorize a collection of surface statistics, or do they rely on internal representations of the process that generates the sequences they see? We investigate this question by applying a variant of the GPT model to the task of predicting legal moves in a simple board game, Othello. Although the network has no a priori knowledge of the game or its rules, we uncover evidence of an emergent nonlinear internal representation of the board state. Interventional experiments indicate this representation can be used to control the output of the network and create "latent saliency maps" that can help explain predictions in human terms.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,probing,to cite,to extract figures,to extract related work,to review in detail,world models},
  file = {/Users/leonardbereska/Zotero/storage/M7YJYVLA/Li et al. - 2023 - Emergent World Representations Exploring a Sequen.pdf}
}

@article{li_how_2023,
  title = {How Do Transformers Learn Topic Structure: Towards a Mechanistic Understanding},
  shorttitle = {How Do Transformers Learn Topic Structure},
  author = {Li, Yuchen and Li, Yuanzhi and Risteski, Andrej},
  year = {2023},
  month = jul,
  journal = {ICML},
  eprint = {2303.04245},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2303.04245},
  url = {http://arxiv.org/abs/2303.04245},
  urldate = {2023-10-30},
  abstract = {While the successes of transformers across many domains are indisputable, accurate understanding of the learning mechanics is still largely lacking. Their capabilities have been probed on benchmarks which include a variety of structured and reasoning tasks -- but mathematical understanding is lagging substantially behind. Recent lines of work have begun studying representational aspects of this question: that is, the size/depth/complexity of attention-based networks to perform certain tasks. However, there is no guarantee the learning dynamics will converge to the constructions proposed. In our paper, we provide fine-grained mechanistic understanding of how transformers learn "semantic structure", understood as capturing co-occurrence structure of words. Precisely, we show, through a combination of mathematical analysis and experiments on Wikipedia data and synthetic data modeled by Latent Dirichlet Allocation (LDA), that the embedding layer and the self-attention layer encode the topical structure. In the former case, this manifests as higher average inner product of embeddings between same-topic words. In the latter, it manifests as higher average pairwise attention between same-topic words. The mathematical results involve several assumptions to make the analysis tractable, which we verify on data, and might be of independent interest as well.},
  archiveprefix = {arxiv},
  keywords = {cited,graph},
  file = {/Users/leonardbereska/Zotero/storage/ZHXKWFI9/Li et al. - 2023 - How Do Transformers Learn Topic Structure Towards.pdf}
}

@article{li_implicit_2021,
  title = {Implicit Representations of Meaning in Neural Language Models},
  author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  month = aug,
  journal = {ACL-IJCNLP},
  pages = {1813--1827},
  doi = {10.18653/v1/2021.acl-long.143},
  url = {https://aclanthology.org/2021.acl-long.143},
  urldate = {2024-03-19},
  abstract = {Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In BART and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.},
  file = {/Users/leonardbereska/Zotero/storage/TAJVY2KY/Li et al. - 2021 - Implicit Representations of Meaning in Neural Lang.pdf}
}

@article{li_inferencetime_2023,
  title = {Inference-Time Intervention: Eliciting Truthful Answers from a Language Model},
  shorttitle = {Inference-Time Intervention},
  author = {Li, Kenneth and Patel, Oam and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  year = {2023},
  month = jul,
  journal = {NeurIPS Spotlight},
  eprint = {2306.03341},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.03341},
  url = {http://arxiv.org/abs/2306.03341},
  urldate = {2023-08-26},
  abstract = {We introduce Inference-Time Intervention (ITI), a technique designed to enhance the truthfulness of large language models (LLMs). ITI operates by shifting model activations during inference, following a set of directions across a limited number of attention heads. This intervention significantly improves the performance of LLaMA models on the TruthfulQA benchmark. On an instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from 32.5\% to 65.1\%. We identify a tradeoff between truthfulness and helpfulness and demonstrate how to balance it by tuning the intervention strength. ITI is minimally invasive and computationally inexpensive. Moreover, the technique is data efficient: while approaches like RLHF require extensive annotations, ITI locates truthful directions using only few hundred examples. Our findings suggest that LLMs may have an internal representation of the likelihood of something being true, even as they produce falsehoods on the surface.},
  archiveprefix = {arxiv},
  keywords = {not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/3GJTT725/Li et al. - 2023 - Inference-Time Intervention Eliciting Truthful An.pdf}
}

@article{li_interpretable_2022,
  title = {Interpretable Deep Learning: Interpretation, Interpretability, Trustworthiness, and Beyond},
  shorttitle = {Interpretable Deep Learning},
  author = {Li, Xuhong and Xiong, Haoyi and Li, Xingjian and Wu, Xuanyu and Zhang, Xiao and Liu, Ji and Bian, Jiang and Dou, Dejing},
  year = {2022},
  month = jul,
  journal = {CoRR},
  eprint = {2103.10689},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2103.10689},
  url = {http://arxiv.org/abs/2103.10689},
  urldate = {2023-11-06},
  abstract = {Deep neural networks have been well-known for their superb handling of various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal how deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we first introduce and clarify two basic concepts -- interpretations and interpretability -- that people usually get confused about. To address the research efforts in interpretations, we elaborate the designs of a number of interpretation algorithms, from different perspectives, by proposing a new taxonomy. Then, to understand the interpretation results, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the current works in evaluating models' interpretability using "trustworthy" interpretation algorithms. Finally, we review and discuss the connections between deep models' interpretations and other factors, such as adversarial robustness and learning from interpretations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/A2HS9SCD/Li et al. - 2022 - Interpretable Deep Learning Interpretation, Inter.pdf}
}

@article{li_large_2023,
  title = {Large Language Models Understand and Can be Enhanced by Emotional Stimuli},
  author = {Li, Cheng and Wang, Jindong and Zhang, Yixuan and Zhu, Kaijie and Hou, Wenxin and Lian, Jianxun and Luo, Fang and Yang, Qiang and Xie, Xing},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2307.11760},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.11760},
  url = {http://arxiv.org/abs/2307.11760},
  urldate = {2023-11-08},
  abstract = {Emotional intelligence significantly impacts our daily behaviors and interactions. Although Large Language Models (LLMs) are increasingly viewed as a stride toward artificial general intelligence, exhibiting impressive performance in numerous tasks, it is still uncertain if LLMs can genuinely grasp psychological emotional stimuli. Understanding and responding to emotional cues gives humans a distinct advantage in problem-solving. In this paper, we take the first step towards exploring the ability of LLMs to understand emotional stimuli. To this end, we first conduct automatic experiments on 45 tasks using various LLMs, including Flan-T5-Large, Vicuna, Llama 2, BLOOM, ChatGPT, and GPT-4. Our tasks span deterministic and generative applications that represent comprehensive evaluation scenarios. Our automatic experiments show that LLMs have a grasp of emotional intelligence, and their performance can be improved with emotional prompts (which we call "EmotionPrompt" that combines the original prompt with emotional stimuli), e.g., 8.00\% relative performance improvement in Instruction Induction and 115\% in BIG-Bench. In addition to those deterministic tasks that can be automatically evaluated using existing metrics, we conducted a human study with 106 participants to assess the quality of generative tasks using both vanilla and emotional prompts. Our human study results demonstrate that EmotionPrompt significantly boosts the performance of generative tasks (10.9\% average improvement in terms of performance, truthfulness, and responsibility metrics). We provide an in-depth discussion regarding why EmotionPrompt works for LLMs and the factors that may influence its performance. We posit that EmotionPrompt heralds a novel avenue for exploring interdisciplinary knowledge for human-LLMs interaction.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LWZYIU8U/Li et al. - 2023 - Large Language Models Understand and Can be Enhanc.pdf}
}

@article{li_transformers_2023,
  title = {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author = {Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  year = {2023},
  journal = {ICML},
  url = {https://api.semanticscholar.org/CorpusID:256616253},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/8X8SDZAX/Li et al. - 2023 - Transformers as Algorithms Generalization and Sta.pdf}
}

@article{liang_foundations_2022,
  title = {Foundations and Trends in Multimodal Machine Learning: Principles, Challenges, and Open Questions},
  shorttitle = {Foundations and Trends in Multimodal Machine Learning},
  author = {Liang, P. and Zadeh, Amir and Morency, Louis-Philippe},
  year = {2022},
  month = sep,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Foundations-and-Trends-in-Multimodal-Machine-and-Liang-Zadeh/f52be2741418df216561bd4436da302ece39d9e2},
  urldate = {2023-10-25},
  abstract = {Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents with intelligent capabilities such as understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in application domains such as healthcare and robotics, multimodal machine learning has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this paper is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. We start by defining three key principles of modality heterogeneity, connections, and interactions that have driven subsequent innovations, and propose a taxonomy of six core technical challenges: representation, alignment, reasoning, generation, transference, and quantification covering historical and recent trends. Recent technical achievements will be presented through the lens of this taxonomy, allowing researchers to understand the similarities and differences across new approaches. We end by motivating several open problems for future research as identified by our taxonomy.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QH9TFRSU/Liang et al. - 2022 - Foundations and Trends in Multimodal Machine Learn.pdf}
}

@article{liang_multiviz_2022,
  title = {MultiViz: Towards Visualizing and Understanding Multimodal Models},
  shorttitle = {MultiViz},
  author = {Liang, Paul Pu and Lyu, Yiwei and Chhablani, Gunjan and Jain, Nihal and Deng, Zihao and Wang, Xingbo and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2207.00056},
  url = {https://arxiv.org/abs/2207.00056},
  urldate = {2023-10-25},
  abstract = {The promise of multimodal models for real-world applications has inspired research in visualizing and understanding their internal mechanics with the end goal of empowering stakeholders to visualize model behavior, perform model debugging, and promote trust in machine learning models. However, modern multimodal models are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize the internal modeling of multimodal interactions in these models? Our paper aims to fill this gap by proposing MultiViz, a method for analyzing the behavior of multimodal models by scaffolding the problem of interpretability into 4 stages: (1) unimodal importance: how each modality contributes towards downstream modeling and prediction, (2) cross-modal interactions: how different modalities relate with each other, (3) multimodal representations: how unimodal and cross-modal interactions are represented in decision-level features, and (4) multimodal prediction: how decision-level features are composed to make a prediction. MultiViz is designed to operate on diverse modalities, models, tasks, and research areas. Through experiments on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available, will be regularly updated with new interpretation tools and metrics, and welcomes inputs from the community.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/IRS5675X/Liang et al. - 2022 - MultiViz Towards Visualizing and Understanding Mu.pdf}
}

@article{liao_ai_2023,
  title = {AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap},
  shorttitle = {AI Transparency in the Age of LLMs},
  author = {Liao, Q. Vera and Vaughan, Jennifer Wortman},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2306.01941},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.01941},
  url = {http://arxiv.org/abs/2306.01941},
  urldate = {2023-11-06},
  abstract = {The rise of powerful large language models (LLMs) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. We have reached a pivotal moment for ensuring that LLMs and LLM-infused applications are developed and deployed responsibly. However, a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs. It is paramount to pursue new approaches to provide transparency for LLMs, and years of research at the intersection of AI and human-computer interaction (HCI) highlight that we must do so with a human-centered perspective: Transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. In this new era of LLMs, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging LLM ecosystem, the novel types of LLM-infused applications being built, and the new usage patterns and challenges around LLMs, all while building on lessons learned about how people process, interact with, and make use of information. We reflect on the unique challenges that arise in providing transparency for LLMs, along with lessons learned from HCI and responsible AI research that has taken a human-centered perspective on AI transparency. We then lay out four common approaches that the community has taken to achieve transparency -- model reporting, publishing evaluation results, providing explanations, and communicating uncertainty -- and call out open questions around how these approaches may or may not be applied to LLMs. We hope this provides a starting point for discussion and a useful roadmap for future research.},
  archiveprefix = {arxiv},
  keywords = {not cited,to extract challenges,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/3SJJEF5E/Liao and Vaughan - 2023 - AI Transparency in the Age of LLMs A Human-Center.pdf}
}

@article{liao_generating_2023,
  title = {Generating Interpretable Networks using Hypernetworks},
  author = {Liao, Isaac and Liu, Ziming and Tegmark, Max},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.03051},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2312.03051},
  urldate = {2024-06-10},
  abstract = {An essential goal in mechanistic interpretability to decode a network, i.e., to convert a neural network's raw weights to an interpretable algorithm. Given the difficulty of the decoding problem, progress has been made to understand the easier encoding problem, i.e., to convert an interpretable algorithm into network weights. Previous works focus on encoding existing algorithms into networks, which are interpretable by definition. However, focusing on encoding limits the possibility of discovering new algorithms that humans have never stumbled upon, but that are nevertheless interpretable. In this work, we explore the possibility of using hypernetworks to generate interpretable networks whose underlying algorithms are not yet known. The hypernetwork is carefully designed such that it can control network complexity, leading to a diverse family of interpretable algorithms ranked by their complexity. All of them are interpretable in hindsight, although some of them are less intuitive to humans, hence providing new insights regarding how to "think" like a neural network. For the task of computing L1 norms, hypernetworks find three algorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c) the pudding algorithm, although only the first algorithm was expected by the authors before experiments. We automatically classify these algorithms and analyze how these algorithmic phases develop during training, as well as how they are affected by complexity control. Furthermore, we show that a trained hypernetwork can correctly construct models for input dimensions not seen in training, demonstrating systematic generalization.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/DC3TKZEX/Liao et al. - 2023 - Generating Interpretable Networks using Hypernetwo.pdf}
}

@article{lieberum_does_2023,
  title = {Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla},
  shorttitle = {Does Circuit Analysis Interpretability Scale?},
  author = {Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.09458},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.09458},
  url = {http://arxiv.org/abs/2307.09458},
  urldate = {2023-08-26},
  abstract = {{\textbackslash}emph\{Circuit analysis\} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer {\textbackslash}emph\{label\} given knowledge of the correct answer {\textbackslash}emph\{text\}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs). We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent. However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.},
  archiveprefix = {arxiv},
  keywords = {circuit,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/V7AK9YDA/Lieberum et al. - 2023 - Does Circuit Analysis Interpretability Scale Evid.pdf}
}

@article{lightman_let_2023,
  title = {Let's Verify Step by Step},
  author = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2305.20050},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.20050},
  url = {http://arxiv.org/abs/2305.20050},
  urldate = {2024-03-14},
  abstract = {In recent years, large language models have greatly improved in their ability to perform complex multi-step reasoning. However, even state-of-the-art models still regularly produce logical mistakes. To train more reliable models, we can turn either to outcome supervision, which provides feedback for a final result, or process supervision, which provides feedback for each intermediate reasoning step. Given the importance of training reliable models, and given the high cost of human feedback, it is important to carefully compare the both methods. Recent work has already begun this comparison, but many questions still remain. We conduct our own investigation, finding that process supervision significantly outperforms outcome supervision for training models to solve problems from the challenging MATH dataset. Our process-supervised model solves 78\% of problems from a representative subset of the MATH test set. Additionally, we show that active learning significantly improves the efficacy of process supervision. To support related research, we also release PRM800K, the complete dataset of 800,000 step-level human feedback labels used to train our best reward model.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/CLURJJGM/Lightman et al. - 2023 - Let's Verify Step by Step.pdf}
}

@article{lin_truthfulqa_2022,
  title = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  shorttitle = {TruthfulQA},
  author = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  year = {2022},
  month = may,
  journal = {ACL},
  eprint = {2109.07958},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.07958},
  url = {http://arxiv.org/abs/2109.07958},
  urldate = {2023-11-10},
  abstract = {We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58\% of questions, while human performance was 94\%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/6L7SSH4K/Lin et al. - 2022 - TruthfulQA Measuring How Models Mimic Human False.pdf}
}

@article{linardatos_explainable_2020,
  title = {Explainable AI: A Review of Machine Learning Interpretability Methods},
  shorttitle = {Explainable AI},
  author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
  year = {2020},
  month = dec,
  journal = {Entropy},
  volume = {23},
  number = {1},
  pages = {18},
  issn = {1099-4300},
  doi = {10.3390/e23010018},
  url = {https://www.mdpi.com/1099-4300/23/1/18},
  urldate = {2023-10-22},
  abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into ``black box'' approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/CXLDFA2N/Linardatos et al. - 2020 - Explainable AI A Review of Machine Learning Inter.pdf}
}

@article{lindner_tracr_2023,
  title = {Tracr: Compiled Transformers as a Laboratory for Interpretability},
  shorttitle = {Tracr},
  author = {Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.05062},
  url = {https://arxiv.org/abs/2301.05062},
  urldate = {2023-07-31},
  abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {algorithms,cited,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,tool,toy models},
  file = {/Users/leonardbereska/Zotero/storage/QBYGQR36/Lindner et al. - 2023 - Tracr Compiled Transformers as a Laboratory for I.pdf}
}

@article{lindsay_testing_2023,
  title = {Testing methods of neural systems understanding},
  author = {Lindsay, Grace W. and Bau, David},
  year = {2023},
  month = dec,
  journal = {Cognitive Systems Research},
  volume = {82},
  pages = {101156},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2023.101156},
  url = {https://www.sciencedirect.com/science/article/pii/S1389041723000906},
  urldate = {2024-01-05},
  abstract = {Neuroscientists apply a range of analysis tools to recorded neural activity in order to glean insights into how neural circuits drive behavior in organisms. Despite the fact that these tools shape the progress of the field as a whole, we have little empirical proof that they are effective at identifying the mechanisms of interest. At the same time, deep learning systems are trained to produce intelligent behavior using neural networks, and the resulting models are impressive but also largely impenetrable. Can the tools of neuroscience be applied to artificial neural networks (ANNs) and if so what would this process tell us about ANNs, brains, and -- most importantly -- the tools themselves? Here we argue that applying analysis methods from neuroscience to ANNs will provide a much-needed test of the abilities of these tools. It would also encourage the development of a unified field of neural systems understanding, which can identify shared concepts and methods for studying distributed information processing in artificial and biological systems. To support this argument, we review methods commonly used in neuroscience, along with work that has demonstrated how these methods can be applied to ANNs and what we learn from this, and related efforts from interpretable AI.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9JQV4YL8/Lindsay and Bau - 2023 - Testing methods of neural systems understanding.pdf}
}

@article{lipton_mythos_2016,
  title = {The Mythos of Model Interpretability},
  author = {Lipton, Zachary C.},
  year = {2016},
  journal = {ICML Workshop on Human Interpretability in Machine Learning},
  eprint = {1606.03490},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1606.03490},
  url = {http://arxiv.org/abs/1606.03490},
  urldate = {2023-10-17},
  abstract = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/924FW7XQ/Lipton - 2016 - The Mythos of Model Interpretability.pdf}
}

@article{liu_abs_2019,
  title = {ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation},
  shorttitle = {ABS},
  author = {Liu, Yingqi and Lee, Wen-Chuan and Tao, Guanhong and Ma, Shiqing and Aafer, Yousra and Zhang, Xiangyu},
  year = {2019},
  month = nov,
  journal = {Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security},
  series = {CCS '19},
  pages = {1265--1282},
  doi = {10.1145/3319535.3363216},
  url = {https://dl.acm.org/doi/10.1145/3319535.3363216},
  urldate = {2024-04-25},
  abstract = {This paper presents a technique to scan neural network based AI models to determine if they are trojaned. Pre-trained AI models may contain back-doors that are injected through training or by transforming inner neuron weights. These trojaned models operate normally when regular inputs are provided, and mis-classify to a specific output label when the input is stamped with some special pattern called trojan trigger. We develop a novel technique that analyzes inner neuron behaviors by determining how output activations change when we introduce different levels of stimulation to a neuron. The neurons that substantially elevate the activation of a particular output label regardless of the provided input is considered potentially compromised. Trojan trigger is then reverse-engineered through an optimization procedure using the stimulation analysis results, to confirm that a neuron is truly compromised. We evaluate our system ABS on 177 trojaned models that are trojaned with various attack methods that target both the input space and the feature space, and have various trojan trigger sizes and shapes, together with 144 benign models that are trained with different data and initial weight values. These models belong to 7 different model structures and 6 different datasets, including some complex ones such as ImageNet, VGG-Face and ResNet110. Our results show that ABS is highly effective, can achieve over 90\% detection rate for most cases (and many 100\%), when only one input sample is provided for each output label. It substantially out-performs the state-of-the-art technique Neural Cleanse that requires a lot of input samples and small trojan triggers to achieve good performance.},
  file = {/Users/leonardbereska/Zotero/storage/8HYH7BZ6/Liu et al. - 2019 - ABS Scanning Neural Networks for Back-doors by Ar.pdf}
}

@article{liu_devil_2023,
  title = {The Devil is in the Neurons: Interpreting and Mitigating Social Biases in Language Models},
  shorttitle = {The Devil is in the Neurons},
  author = {Liu, Yan and Liu, Yu and Chen, Xiaokang and Chen, Pin-Yu and Zan, Daoguang and Kan, Min-Yen and Ho, Tsung-Yi},
  year = {2023},
  month = oct,
  journal = {ICLR},
  url = {https://openreview.net/forum?id=SQGUDc9tC8},
  urldate = {2024-03-14},
  abstract = {Pre-trained Language models (PLMs) have been acknowledged to contain harmful information, such as social biases, which may cause negative social impacts or even bring catastrophic results in application. Previous works on this problem mainly focused on using black-box methods such as probing to detect and quantify social biases in PLMs by observing model outputs. As a result, previous debiasing methods mainly finetune or even pre-train PLMs on newly constructed anti-stereotypical datasets, which are high-cost. In this work, we try to unveil the mystery of social bias inside language models by introducing the concept of \{{\textbackslash}sc Social Bias Neurons\}. Specifically, we propose \{{\textbackslash}sc Integrated Gap Gradients (IG\${\textasciicircum}2\$)\} to accurately pinpoint units (i.e., neurons) in a language model that can be attributed to undesirable behavior, such as social bias. By formalizing undesirable behavior as a distributional property of language, we employ sentiment-bearing prompts to elicit classes of sensitive words (demographics) correlated with such sentiments. Our IG\${\textasciicircum}2\$ thus attributes the uneven distribution for different demographics to specific Social Bias Neurons, which track the trail of unwanted behavior inside PLM units to achieve interoperability. Moreover, derived from our interpretable technique, \{{\textbackslash}sc Bias Neuron Suppression (BNS)\} is further proposed to mitigate social biases. By studying BERT, RoBERTa, and their attributable differences from debiased FairBERTa, IG\${\textasciicircum}2\$ allows us to locate and suppress identified neurons, and further mitigate undesired behaviors. As measured by prior metrics from StereoSet, our model achieves a higher degree of fairness while maintaining language modeling ability with low cost{\textbackslash}footnote\{This work contains examples that potentially implicate stereotypes, associations, and other harms that could be offensive to individuals in certain social groups.\}.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/8HDPGMAW/Liu et al. - 2023 - The Devil is in the Neurons Interpreting and Miti.pdf}
}

@article{liu_grokking_2023,
  title = {Grokking as compression: A nonlinear complexity perspective},
  author = {Liu, Ziming and Zhong, Ziqian and Tegmark, Max},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2310.05918},
  pages = {null},
  doi = {10.48550/arXiv.2310.05918},
  url = {https://www.semanticscholar.org/paper/8cd06e9959151ea852f2842a87ed65ff0a47182f},
  abstract = {We attribute grokking, the phenomenon where generalization is much delayed after memorization, to compression. To do so, we define linear mapping number (LMN) to measure network complexity, which is a generalized version of linear region number for ReLU networks. LMN can nicely characterize neural network compression before generalization. Although the L{$_2$} norm has been a popular choice for characterizing model complexity, we argue in favor of LMN for a number of reasons: (1) LMN can be naturally interpreted as information/computation, while L{$_2$} cannot. (2) In the compression phase, LMN has linear relations with test losses, while L{$_2$} is correlated with test losses in a complicated nonlinear way. (3) LMN also reveals an intriguing phenomenon of the XOR network switching between two generalization solutions, while L{$_2$} does not. Besides explaining grokking, we argue that LMN is a promising candidate as the neural network version of the Kolmogorov complexity since it explicitly considers local or conditioned linear computations aligned with the nature of modern artificial neural networks.},
  arxivid = {2310.05918},
  keywords = {cited,grokking,mechinterp,theory,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/5I94RPEC/Liu et al. - 2023 - Grokking as compression A nonlinear complexity pe.pdf}
}

@article{liu_growing_2023,
  title = {Growing Brains: Co-emergence of Anatomical and Functional Modularity in Recurrent Neural Networks},
  shorttitle = {Growing Brains},
  author = {Liu, Ziming and Khona, Mikail and Fiete, Ila R. and Tegmark, Max},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.07711},
  url = {https://arxiv.org/abs/2310.07711},
  urldate = {2024-01-16},
  abstract = {Recurrent neural networks (RNNs) trained on compositional tasks can exhibit functional modularity, in which neurons can be clustered by activity similarity and participation in shared computational subtasks. Unlike brains, these RNNs do not exhibit anatomical modularity, in which functional clustering is correlated with strong recurrent coupling and spatial localization of functional clusters. Contrasting with functional modularity, which can be ephemerally dependent on the input, anatomically modular networks form a robust substrate for solving the same subtasks in the future. To examine whether it is possible to grow brain-like anatomical modularity, we apply a recent machine learning method, brain-inspired modular training (BIMT), to a network being trained to solve a set of compositional cognitive tasks. We find that functional and anatomical clustering emerge together, such that functionally similar neurons also become spatially localized and interconnected. Moreover, compared to standard \$L\_1\$ or no regularization settings, the model exhibits superior performance by optimally balancing task performance and network sparsity. In addition to achieving brain-like organization in RNNs, our findings also suggest that BIMT holds promise for applications in neuromorphic computing and enhancing the interpretability of neural network architectures.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/QA8A38LW/Liu et al. - 2023 - Growing Brains Co-emergence of Anatomical and Fun.pdf}
}

@article{liu_improved_2023,
  title = {Improved Baselines with Visual Instruction Tuning},
  author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.03744},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.03744},
  url = {http://arxiv.org/abs/2310.03744},
  urldate = {2023-11-21},
  abstract = {Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in {\textasciitilde}1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QQ6QC3VG/Liu et al. - 2023 - Improved Baselines with Visual Instruction Tuning.pdf}
}

@article{liu_neural_2023,
  title = {A Neural Scaling Law from Lottery Ticket Ensembling},
  author = {Liu, Ziming and Tegmark, Max},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.02258},
  url = {https://arxiv.org/abs/2310.02258},
  urldate = {2023-11-10},
  abstract = {Neural scaling laws (NSL) refer to the phenomenon where model performance improves with scale. Sharma \&amp; Kaplan analyzed NSL using approximation theory and predict that MSE losses decay as \$N{\textasciicircum}\{-{$\alpha$}\}\$, \${$\alpha$}=4/d\$, where \$N\$ is the number of model parameters, and \$d\$ is the intrinsic input dimension. Although their theory works well for some cases (e.g., ReLU networks), we surprisingly find that a simple 1D problem \$y=x{\textasciicircum}2\$ manifests a different scaling law (\${$\alpha$}=1\$) from their predictions (\${$\alpha$}=4\$). We opened the neural networks and found that the new scaling law originates from lottery ticket ensembling: a wider network on average has more "lottery tickets", which are ensembled to reduce the variance of outputs. We support the ensembling mechanism by mechanistically interpreting single neural networks, as well as studying them statistically. We attribute the \$N{\textasciicircum}\{-1\}\$ scaling law to the "central limit theorem" of lottery tickets. Finally, we discuss its potential implications for large language models and statistical physics-type theories of learning.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ADP7NL8C/Liu and Tegmark - 2023 - A Neural Scaling Law from Lottery Ticket Ensemblin.pdf}
}

@article{liu_omnigrok_2022,
  title = {Omnigrok: Grokking Beyond Algorithmic Data},
  shorttitle = {Omnigrok},
  author = {Liu, Ziming and Michaud, Eric J. and Tegmark, Max},
  year = {2022},
  journal = {ICML},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2210.01117},
  url = {https://arxiv.org/abs/2210.01117},
  urldate = {2023-11-10},
  abstract = {Grokking, the unusual phenomenon for algorithmic datasets where generalization happens long after overfitting the training data, has remained elusive. We aim to understand grokking by analyzing the loss landscapes of neural networks, identifying the mismatch between training and test losses as the cause for grokking. We refer to this as the "LU mechanism" because training and test losses (against model weight norm) typically resemble "L" and "U", respectively. This simple mechanism can nicely explain many aspects of grokking: data size dependence, weight decay dependence, the emergence of representations, etc. Guided by the intuitive picture, we are able to induce grokking on tasks involving images, language and molecules. In the reverse direction, we are able to eliminate grokking for algorithmic datasets. We attribute the dramatic nature of grokking for algorithmic datasets to representation learning.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/K2MHLF5M/Liu et al. - 2022 - Omnigrok Grokking Beyond Algorithmic Data.pdf}
}

@article{liu_probing_2021,
  title = {Probing Across Time: What Does RoBERTa Know and When?},
  shorttitle = {Probing Across Time},
  author = {Liu, Leo Z. and Wang, Yizhong and Kasai, Jungo and Hajishirzi, Hannaneh and Smith, Noah A.},
  year = {2021},
  month = sep,
  journal = {EMNLP},
  eprint = {2104.07885},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2104.07885},
  url = {http://arxiv.org/abs/2104.07885},
  urldate = {2024-02-28},
  abstract = {Models of language trained on very large corpora have been demonstrated useful for NLP. As fixed artifacts, they have become the object of intense study, with many researchers "probing" the extent to which linguistic abstractions, factual and commonsense knowledge, and reasoning abilities they acquire and readily demonstrate. Building on this line of work, we consider a new question: for types of knowledge a language model learns, when during (pre)training are they acquired? We plot probing performance across iterations, using RoBERTa as a case study. Among our findings: linguistic knowledge is acquired fast, stably, and robustly across domains. Facts and commonsense are slower and more domain-sensitive. Reasoning abilities are, in general, not stably acquired. As new datasets, pretraining protocols, and probes emerge, we believe that probing-across-time analyses can help researchers understand the complex, intermingled learning that these models undergo and guide us toward more efficient approaches that accomplish necessary learning faster.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5YDX4KT8/Liu et al. - 2021 - Probing Across Time What Does RoBERTa Know and Wh.pdf}
}

@article{liu_seeing_2023,
  title = {Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability},
  shorttitle = {Seeing is Believing},
  author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
  year = {2023},
  month = jun,
  journal = {Entropy},
  eprint = {2305.08746},
  primaryclass = {cond-mat, q-bio},
  doi = {10.48550/arXiv.2305.08746},
  url = {http://arxiv.org/abs/2305.08746},
  urldate = {2023-07-31},
  abstract = {We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
  archiveprefix = {arxiv},
  keywords = {algorithms,cited,graph,intrinsic,mechinterp,modularity,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/JMY5W8XU/Liu et al. - 2023 - Seeing is Believing Brain-Inspired Modular Traini.pdf}
}

@article{liu_transformers_2023,
  title = {Transformers Learn Shortcuts to Automata},
  author = {Liu, Bingbin and Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2210.10749},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2210.10749},
  url = {http://arxiv.org/abs/2210.10749},
  urldate = {2023-10-30},
  abstract = {Algorithmic reasoning requires capabilities which are most naturally understood through recurrent models of computation, like the Turing machine. However, Transformer models, while lacking recurrence, are able to perform such reasoning using far fewer layers than the number of reasoning steps. This raises the question: what solutions are learned by these shallow and non-recurrent models? We find that a low-depth Transformer can represent the computations of any finite-state automaton (thus, any bounded-memory algorithm), by hierarchically reparameterizing its recurrent dynamics. Our theoretical results characterize shortcut solutions, whereby a Transformer with \$o(T)\$ layers can exactly replicate the computation of an automaton on an input sequence of length \$T\$. We find that polynomial-sized \$O({\textbackslash}log T)\$-depth solutions always exist; furthermore, \$O(1)\$-depth simulators are surprisingly common, and can be understood using tools from Krohn-Rhodes theory and circuit complexity. Empirically, we perform synthetic experiments by training Transformers to simulate a wide variety of automata, and show that shortcut solutions can be learned via standard training. We further investigate the brittleness of these solutions and propose potential mitigations.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9WJF5XNW/Liu et al. - 2023 - Transformers Learn Shortcuts to Automata.pdf}
}

@article{liu_understanding_2022,
  title = {Towards Understanding Grokking: An Effective Theory of Representation Learning},
  shorttitle = {Towards Understanding Grokking},
  author = {Liu, Ziming and Kitouni, Ouail and Nolte, Niklas and Michaud, Eric J. and Tegmark, Max and Williams, Mike},
  year = {2022},
  journal = {NeurIPS},
  publisher = {arXiv},
  url = {https://arxiv.org/abs/2205.10343},
  urldate = {2023-07-31},
  abstract = {We aim to understand grokking, a phenomenon where models generalize long after overfitting their training set. We present both a microscopic analysis anchored by an effective theory and a macroscopic analysis of phase diagrams describing learning performance across hyperparameters. We find that generalization originates from structured representations whose training dynamics and dependence on training set size can be predicted by our effective theory in a toy setting. We observe empirically the presence of four learning phases: comprehension, grokking, memorization, and confusion. We find representation learning to occur only in a "Goldilocks zone" (including comprehension and grokking) between memorization and confusion. We find on transformers the grokking phase stays closer to the memorization phase (compared to the comprehension phase), leading to delayed generalization. The Goldilocks phase is reminiscent of "intelligence from starvation" in Darwinian evolution, where resource limitations drive discovery of more efficient solutions. This study not only provides intuitive explanations of the origin of grokking, but also highlights the usefulness of physics-inspired tools, e.g., effective theories and phase diagrams, for understanding deep learning.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,dynamics,fundamental,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/FWJFM5X9/Liu et al. - 2022 - Towards Understanding Grokking An Effective Theor.pdf}
}

@article{liu_visual_2023,
  title = {Visual Instruction Tuning},
  author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  year = {2023},
  month = apr,
  journal = {CoRR},
  eprint = {2304.08485},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.08485},
  url = {http://arxiv.org/abs/2304.08485},
  urldate = {2023-11-01},
  abstract = {Instruction tuning large language models (LLMs) using machine-generated instruction-following data has improved zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. In this paper, we present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.Our early experiments show that LLaVA demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53\%. We make GPT-4 generated visual instruction tuning data, our model and code base publicly available.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/A299QQ5S/Liu et al. - 2023 - Visual Instruction Tuning.pdf}
}

@article{locatello_challenging_2019,
  title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2019},
  month = jun,
  journal = {CoRR},
  eprint = {1811.12359},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1811.12359},
  url = {http://arxiv.org/abs/1811.12359},
  urldate = {2024-01-04},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9X8IU9MV/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf}
}

@article{longo_explainable_2024,
  title = {Explainable Artificial Intelligence (XAI) 2.0: A manifesto of open challenges and interdisciplinary research directions},
  shorttitle = {Explainable Artificial Intelligence (XAI) 2.0},
  author = {Longo, Luca and Brcic, Mario and Cabitza, Federico and Choi, Jaesik and Confalonieri, Roberto and Ser, Javier Del and Guidotti, Riccardo and Hayashi, Yoichi and Herrera, Francisco and Holzinger, Andreas and Jiang, Richard and Khosravi, Hassan and Lecue, Freddy and Malgieri, Gianclaudio and P{\'a}ez, Andr{\'e}s and Samek, Wojciech and Schneider, Johannes and Speith, Timo and Stumpf, Simone},
  year = {2024},
  month = jun,
  journal = {Information Fusion},
  volume = {106},
  pages = {102301},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2024.102301},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253524000794},
  urldate = {2024-06-10},
  abstract = {Understanding black box models has become paramount as systems based on opaque Artificial Intelligence (AI) continue to flourish in diverse real-world applications. In response, Explainable AI (XAI) has emerged as a field of research with practical and ethical benefits across various domains. This paper highlights the advancements in XAI and its application in real-world scenarios and addresses the ongoing challenges within XAI, emphasizing the need for broader perspectives and collaborative efforts. We bring together experts from diverse fields to identify open problems, striving to synchronize research agendas and accelerate XAI in practical applications. By fostering collaborative discussion and interdisciplinary cooperation, we aim to propel XAI forward, contributing to its continued success. We aim to develop a comprehensive proposal for advancing XAI. To achieve this goal, we present a manifesto of 28 open problems categorized into nine categories. These challenges encapsulate the complexities and nuances of XAI and offer a road map for future research. For each problem, we provide promising research directions in the hope of harnessing the collective intelligence of interested stakeholders.},
  file = {/Users/leonardbereska/Zotero/storage/5VL846UW/Longo et al. - 2024 - Explainable Artificial Intelligence (XAI) 2.0 A m.pdf;/Users/leonardbereska/Zotero/storage/5WHCI3IX/S1566253524000794.html}
}

@article{lorenz_unsupervised_2019,
  title = {Unsupervised Part-Based Disentangling of Object Shape and Appearance},
  author = {Lorenz, Dominik and Bereska, Leonard and Milbich, Timo and Ommer, Bjorn},
  year = {2019},
  journal = {CVPR},
  pages = {10955--10964},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Lorenz_Unsupervised_Part-Based_Disentangling_of_Object_Shape_and_Appearance_CVPR_2019_paper.html},
  urldate = {2024-07-03},
  file = {/Users/leonardbereska/Zotero/storage/Y3HQKRBQ/Lorenz et al. - 2019 - Unsupervised Part-Based Disentangling of Object Sh.pdf}
}

@inproceedings{lorenz_unsupervised_2019a,
  title = {Unsupervised Part-Based Disentangling of Object Shape and Appearance},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  author = {Lorenz, Dominik and Bereska, Leonard and Milbich, Timo and Ommer, Bjorn},
  year = {2019},
  pages = {10955--10964},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Lorenz_Unsupervised_Part-Based_Disentangling_of_Object_Shape_and_Appearance_CVPR_2019_paper.html},
  urldate = {2024-07-03},
  file = {/Users/leonardbereska/Zotero/storage/52AV2G8X/Lorenz et al. - 2019 - Unsupervised Part-Based Disentangling of Object Sh.pdf}
}

@article{losch_semantic_2021,
  title = {Semantic Bottlenecks: Quantifying and Improving Inspectability of Deep Representations},
  shorttitle = {Semantic Bottlenecks},
  author = {Losch, Max and Fritz, Mario and Schiele, Bernt},
  year = {2021},
  month = nov,
  journal = {Int J Comput Vis},
  volume = {129},
  number = {11},
  pages = {3136--3153},
  issn = {1573-1405},
  doi = {10.1007/s11263-021-01498-0},
  url = {https://doi.org/10.1007/s11263-021-01498-0},
  urldate = {2023-10-26},
  abstract = {Today's deep learning systems deliver high performance based on end-to-end training but are notoriously hard to inspect. We argue that there are at least two reasons making inspectability challenging: (i) representations are distributed across hundreds of channels and (ii) a unifying metric quantifying inspectability is lacking. In this paper, we address both issues by proposing Semantic Bottlenecks (SB), which can be integrated into pretrained networks, to align channel outputs with individual visual concepts and introduce the model agnostic Area Under inspectability Curve (AUiC) metric to measure the alignment. We present a case study on semantic segmentation to demonstrate that SBs improve the AUiC up to six-fold over regular network outputs. We explore two types of SB-layers in this work. First, concept-supervised SB-layers (SSB), which offer inspectability w.r.t. predefined concepts that the model is demanded to rely on. And second, unsupervised SBs (USB), which offer equally strong AUiC improvements by restricting distributedness of representations across channels. Importantly, for both SB types, we can recover state of the art segmentation performance across two different models despite a drastic dimensionality reduction from 1000s of non aligned channels to 10s of semantics-aligned channels that all downstream results are based on.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9HVEXHTC/Losch et al. - 2021 - Semantic Bottlenecks Quantifying and Improving In.pdf}
}

@article{louizos_learning_2017,
  title = {Learning Sparse Neural Networks through L0 Regularization},
  author = {Louizos, Christos and Welling, M. and Kingma, Diederik P.},
  year = {2017},
  month = dec,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Learning-Sparse-Neural-Networks-through-L0-Louizos-Welling/2ec7156913117949ab933f27f492d0149bc0031f},
  urldate = {2023-11-10},
  abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VAXUPUVT/Louizos et al. - 2017 - Learning Sparse Neural Networks through L0 Regular.pdf}
}

@article{louizos_learning_2018,
  title = {Learning Sparse Neural Networks through \$L\_0\$ Regularization},
  author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  year = {2018},
  month = jun,
  journal = {CoRR},
  eprint = {1712.01312},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1712.01312},
  url = {http://arxiv.org/abs/1712.01312},
  urldate = {2023-08-27},
  abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/P34UA36U/Louizos et al. - 2018 - Learning Sparse Neural Networks through $L_0$ Regu.pdf}
}

@article{lu_are_2023,
  title = {Are Emergent Abilities in Large Language Models just In-Context Learning?},
  author = {Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Madabushi, Harish Tayyar and Gurevych, Iryna},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.01809},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.01809},
  url = {http://arxiv.org/abs/2309.01809},
  urldate = {2023-11-16},
  abstract = {Large language models have exhibited emergent abilities, demonstrating exceptional performance across diverse tasks for which they were not explicitly trained, including those that require complex reasoning abilities. The emergence of such abilities carries profound implications for the future direction of research in NLP, especially as the deployment of such models becomes more prevalent. However, one key challenge is that the evaluation of these abilities is often confounded by competencies that arise in models through alternative prompting techniques, such as in-context learning and instruction following, which also emerge as the models are scaled up. In this study, we provide the first comprehensive examination of these emergent abilities while accounting for various potentially biasing factors that can influence the evaluation of models. We conduct rigorous tests on a set of 18 models, encompassing a parameter range from 60 million to 175 billion parameters, across a comprehensive set of 22 tasks. Through an extensive series of over 1,000 experiments, we provide compelling evidence that emergent abilities can primarily be ascribed to in-context learning. We find no evidence for the emergence of reasoning abilities, thus providing valuable insights into the underlying mechanisms driving the observed abilities and thus alleviating safety concerns regarding their use.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/WMPU7MUQ/Lu et al. - 2023 - Are Emergent Abilities in Large Language Models ju.pdf}
}

@article{lu_endtoend_2023,
  title = {End-to-end topographic networks as models of cortical map formation and human visual behaviour: moving beyond convolutions},
  shorttitle = {End-to-end topographic networks as models of cortical map formation and human visual behaviour},
  author = {Lu, Zejin and Doerig, Adrien and Bosch, Victoria and Krahmer, Bas and Kaiser, Daniel and Cichy, Radoslaw M. and Kietzmann, Tim C.},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2308.09431},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2308.09431},
  url = {http://arxiv.org/abs/2308.09431},
  urldate = {2023-11-29},
  abstract = {Computational models are an essential tool for understanding the origin and functions of the topographic organisation of the primate visual system. Yet, vision is most commonly modelled by convolutional neural networks that ignore topography by learning identical features across space. Here, we overcome this limitation by developing All-Topographic Neural Networks (All-TNNs). Trained on visual input, several features of primate topography emerge in All-TNNs: smooth orientation maps and cortical magnification in their first layer, and category-selective areas in their final layer. In addition, we introduce a novel dataset of human spatial biases in object recognition, which enables us to directly link models to behaviour. We demonstrate that All-TNNs significantly better align with human behaviour than previous state-of-the-art convolutional models due to their topographic nature. All-TNNs thereby mark an important step forward in understanding the spatial organisation of the visual brain and how it mediates visual behaviour.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HXJ8SZVU/Lu et al. - 2023 - End-to-end topographic networks as models of corti.pdf}
}

@article{lubana_mechanistic_2022,
  title = {Mechanistic Mode Connectivity},
  author = {Lubana, Ekdeep Singh and Bigelow, Eric J. and Dick, Robert P. and Krueger, David and Tanaka, Hidenori},
  year = {2022},
  journal = {CoRR},
  doi = {10.48550/ARXIV.2211.08422},
  url = {https://arxiv.org/abs/2211.08422},
  urldate = {2023-10-25},
  abstract = {We study neural network loss landscapes through the lens of mode connectivity, the observation that minimizers of neural networks retrieved via training on a dataset are connected via simple paths of low loss. Specifically, we ask the following question: are minimizers that rely on different mechanisms for making their predictions connected via simple paths of low loss? We provide a definition of mechanistic similarity as shared invariances to input transformations and demonstrate that lack of linear connectivity between two models implies they use dissimilar mechanisms for making their predictions. Relevant to practice, this result helps us demonstrate that naive fine-tuning on a downstream dataset can fail to alter a model's mechanisms, e.g., fine-tuning can fail to eliminate a model's reliance on spurious attributes. Our analysis also motivates a method for targeted alteration of a model's mechanisms, named connectivity-based fine-tuning (CBFT), which we analyze using several synthetic datasets for the task of reducing a model's reliance on spurious attributes.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2I4WYFMZ/Lubana et al. - 2022 - Mechanistic Mode Connectivity.pdf}
}

@article{lundberg_unexpected_2016,
  title = {An unexpected unity among methods for interpreting model predictions},
  author = {Lundberg, Scott and Lee, Su-In},
  year = {2016},
  month = dec,
  journal = {NIPS 2016 Workshop on Interpretable Machine Learning in Complex Systems},
  eprint = {1611.07478},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1611.07478},
  url = {http://arxiv.org/abs/1611.07478},
  urldate = {2024-02-13},
  abstract = {Understanding why a model made a certain prediction is crucial in many data science fields. Interpretable predictions engender appropriate trust and provide insight into how the model may be improved. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, which creates a tension between accuracy and interpretability. Recently, several methods have been proposed for interpreting predictions from complex models by estimating the importance of input features. Here, we present how a model-agnostic additive representation of the importance of input features unifies current methods. This representation is optimal, in the sense that it is the only set of additive values that satisfies important properties. We show how we can leverage these properties to create novel visual explanations of model predictions. The thread of unity that this representation weaves through the literature indicates that there are common principles to be learned about the interpretation of model predictions that apply in many scenarios.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5T7HMUIA/Lundberg and Lee - 2016 - An unexpected unity among methods for interpreting.pdf}
}

@article{luo_understanding_2024,
  title = {From Understanding to Utilization: A Survey on Explainability for Large Language Models},
  shorttitle = {From Understanding to Utilization},
  author = {Luo, Haoyan and Specia, Lucia},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.12874},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.12874},
  url = {http://arxiv.org/abs/2401.12874},
  urldate = {2024-02-07},
  abstract = {This survey paper delves into the burgeoning field of explainability for Large Language Models (LLMs), a critical yet challenging aspect of natural language processing. With LLMs playing a pivotal role in various applications, their "black-box" nature raises concerns about transparency and ethical use. This paper emphasizes the necessity for enhanced explainability in LLMs, addressing both the general public's trust and the technical community's need for a deeper understanding of these models. We concentrate on pre-trained Transformer-based LLMs, such as LLaMA, which present unique interpretability challenges due to their scale and complexity. Our review categorizes existing explainability methods and discusses their application in improving model transparency and reliability. We also discuss representative evaluation methods, highlighting their strengths and limitations. The goal of this survey is to bridge the gap between theoretical understanding and practical application, offering insights for future research and development in the field of LLM explainability.},
  archiveprefix = {arxiv},
  keywords = {LLMs,not cited,review,to extract challenges,to extract related work,to extract survey,transparency},
  file = {/Users/leonardbereska/Zotero/storage/I5IG2XB7/Luo and Specia - 2024 - From Understanding to Utilization A Survey on Exp.pdf}
}

@article{lv_interpreting_2024,
  title = {Interpreting Key Mechanisms of Factual Recall in Transformer-Based Language Models},
  author = {Lv, Ang and Chen, Yuhan and Zhang, Kaiyi and Wang, Yulong and Liu, Lifeng and Wen, Ji-Rong and Xie, Jian and Yan, Rui},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.19521},
  url = {https://arxiv.org/abs/2403.19521},
  urldate = {2024-06-10},
  abstract = {In this paper, we delve into several mechanisms employed by Transformer-based language models (LLMs) for factual recall tasks. We outline a pipeline consisting of three major steps: (1) Given a prompt ``The capital of France is,'' task-specific attention heads extract the topic token, such as ``France,'' from the context and pass it to subsequent MLPs. (2) As attention heads' outputs are aggregated with equal weight and added to the residual stream, the subsequent MLP acts as an ``activation,'' which either erases or amplifies the information originating from individual heads. As a result, the topic token ``France'' stands out in the residual stream. (3) A deep MLP takes ``France'' and generates a component that redirects the residual stream towards the direction of the correct answer, i.e., ``Paris.'' This procedure is akin to applying an implicit function such as ``get{\textbackslash}\_capital(\$X\$),'' and the argument \$X\$ is the topic token information passed by attention heads. To achieve the above quantitative and qualitative analysis for MLPs, we proposed a novel analytic method aimed at decomposing the outputs of the MLP into components understandable by humans. Additionally, we observed a universal anti-overconfidence mechanism in the final layer of models, which suppresses correct predictions. We mitigate this suppression by leveraging our interpretation to improve factual recall confidence. The above interpretations are evaluated across diverse tasks spanning various domains of factual knowledge, using various language models from the GPT-2 families, 1.3B OPT, up to 7B Llama-2, and in both zero- and few-shot setups.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/leonardbereska/Zotero/storage/QXPUADHL/Lv et al. - 2024 - Interpreting Key Mechanisms of Factual Recall in T.pdf}
}

@article{lynch_eight_2024,
  title = {Eight Methods to Evaluate Robust Unlearning in LLMs},
  author = {Lynch, Aengus and Guo, Phillip and Ewart, Aidan and Casper, Stephen and {Hadfield-Menell}, Dylan},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.16835},
  url = {https://arxiv.org/abs/2402.16835},
  urldate = {2024-06-10},
  abstract = {Machine unlearning can be useful for removing harmful capabilities and memorized text from large language models (LLMs), but there are not yet standardized methods for rigorously evaluating it. In this paper, we first survey techniques and limitations of existing unlearning evaluations. Second, we apply a comprehensive set of tests for the robustness and competitiveness of unlearning in the "Who's Harry Potter" (WHP) model from Eldan and Russinovich (2023). While WHP's unlearning generalizes well when evaluated with the "Familiarity" metric from Eldan and Russinovich, we find i) higher-than-baseline amounts of knowledge can reliably be extracted, ii) WHP performs on par with the original model on Harry Potter Q\&amp;A tasks, iii) it represents latent knowledge comparably to the original model, and iv) there is collateral unlearning in related domains. Overall, our results highlight the importance of comprehensive unlearning evaluation that avoids ad-hoc metrics.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/MQDZA95P/Lynch et al. - 2024 - Eight Methods to Evaluate Robust Unlearning in LLM.pdf}
}

@article{lynch_spawrious_2023,
  title = {Spawrious: A Benchmark for Fine Control of Spurious Correlation Biases},
  shorttitle = {Spawrious},
  author = {Lynch, Aengus and Dovonon, Gb{\`e}tondji J.-S. and Kaddour, Jean and Silva, Ricardo},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2303.05470},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.05470},
  url = {http://arxiv.org/abs/2303.05470},
  urldate = {2023-07-10},
  abstract = {The problem of spurious correlations (SCs) arises when a classifier relies on non-predictive features that happen to be correlated with the labels in the training data. For example, a classifier may misclassify dog breeds based on the background of dog images. This happens when the backgrounds are correlated with other breeds in the training data, leading to misclassifications during test time. Previous SC benchmark datasets suffer from varying issues, e.g., over-saturation or only containing one-to-one (O2O) SCs, but no many-to-many (M2M) SCs arising between groups of spurious attributes and classes. In this paper, we present {\textbackslash}benchmark-{\textbackslash}\{O2O, M2M{\textbackslash}\}-{\textbackslash}\{Easy, Medium, Hard{\textbackslash}\}, an image classification benchmark suite containing spurious correlations between classes and backgrounds. To create this dataset, we employ a text-to-image model to generate photo-realistic images and an image captioning model to filter out unsuitable ones. The resulting dataset is of high quality and contains approximately 152k images. Our experimental results demonstrate that state-of-the-art group robustness methods struggle with {\textbackslash}benchmark, most notably on the Hard-splits with none of them getting over \$70{\textbackslash}\%\$ accuracy on the hardest split using a ResNet50 pretrained on ImageNet. By examining model misclassifications, we detect reliances on spurious backgrounds, demonstrating that our dataset provides a significant challenge.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FYB47XHK/Lynch et al. - 2023 - Spawrious A Benchmark for Fine Control of Spuriou.pdf}
}

@article{lyu_dichotomy_2023,
  title = {Dichotomy of early and late phase implicit biases can provably induce grokking},
  author = {Lyu, Kaifeng and Jin, Jikai and Li, Zhiyuan and Du, Simon S. and Lee, Jason D. and Hu, Wei},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2311.18817},
  pages = {null},
  doi = {10.48550/arXiv.2311.18817},
  url = {https://www.semanticscholar.org/paper/bea3593ba91fda7c9ae2ee07ad3b9465cd45ffff},
  abstract = {Recent work by Power et al. (2022) highlighted a surprising"grokking"phenomenon in learning arithmetic tasks: a neural net first"memorizes"the training set, resulting in perfect training accuracy but near-random test accuracy, and after training for sufficiently longer, it suddenly transitions to perfect test accuracy. This paper studies the grokking phenomenon in theoretical setups and shows that it can be induced by a dichotomy of early and late phase implicit biases. Specifically, when training homogeneous neural nets with large initialization and small weight decay on both classification and regression tasks, we prove that the training process gets trapped at a solution corresponding to a kernel predictor for a long time, and then a very sharp transition to min-norm/max-margin predictors occurs, leading to a dramatic change in test accuracy.},
  arxivid = {2311.18817},
  keywords = {grokking,inductive bias,not cited},
  file = {/Users/leonardbereska/Zotero/storage/C83XSJ3H/Lyu et al. - 2023 - Dichotomy of early and late phase implicit biases .pdf}
}

@article{lyu_dime_2022,
  title = {DIME: Fine-grained Interpretations of Multimodal Models via Disentangled Local Explanations},
  shorttitle = {DIME},
  author = {Lyu, Yiwei and Liang, Paul Pu and Deng, Zihao and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2022},
  month = jul,
  journal = {AIES},
  pages = {455--467},
  publisher = {ACM},
  address = {Oxford United Kingdom},
  doi = {10.1145/3514094.3534148},
  url = {https://dl.acm.org/doi/10.1145/3514094.3534148},
  urldate = {2023-10-25},
  abstract = {The ability for a human to understand an Artificial Intelligence (AI) model's decision-making process is critical in enabling stakeholders to visualize model behavior, perform model debugging, promote trust in AI models, and assist in collaborative human-AI decision-making. As a result, the research fields of interpretable and explainable AI have gained traction within AI communities as well as interdisciplinary scientists seeking to apply AI in their subject areas. In this paper, we focus on advancing the state-of-the-art in interpreting multimodal models - a class of machine learning methods that tackle core challenges in representing and capturing interactions between heterogeneous data sources such as images, text, audio, and time-series data. Multimodal models have proliferated numerous real-world applications across healthcare, robotics, multimedia, affective computing, and human-computer interaction. By performing model disentanglement into unimodal contributions (UC) and multimodal interactions (MI), our proposed approach, DIME, enables accurate and fine-grained analysis of multimodal models while maintaining generality across arbitrary modalities, model architectures, and tasks. Through a comprehensive suite of experiments on both synthetic and real-world multimodal tasks, we show that DIME generates accurate disentangled explanations, helps users of multimodal models gain a deeper understanding of model behavior, and presents a step towards debugging and improving these models for real-world deployment.},
  isbn = {9781450392471},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZELMLLLL/Lyu et al. - 2022 - DIME Fine-grained Interpretations of Multimodal M.pdf}
}

@article{lyu_faithful_2023,
  title = {Towards Faithful Model Explanation in NLP: A Survey},
  shorttitle = {Towards Faithful Model Explanation in NLP},
  author = {Lyu, Qing and Apidianaki, Marianna and {Callison-Burch}, Chris},
  year = {2023},
  month = feb,
  journal = {CoRR},
  eprint = {2209.11326},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.11326},
  url = {http://arxiv.org/abs/2209.11326},
  urldate = {2023-08-29},
  abstract = {End-to-end neural Natural Language Processing (NLP) models are notoriously difficult to understand. This has given rise to numerous efforts towards model explainability in recent years. One desideratum of model explanation is faithfulness, i.e. an explanation should accurately represent the reasoning process behind the model's prediction. In this survey, we review over 110 model explanation methods in NLP through the lens of faithfulness. We first discuss the definition and evaluation of faithfulness, as well as its significance for explainability. We then introduce recent advances in faithful explanation, grouping existing approaches into five categories: similarity methods, analysis of model-internal structures, backpropagation-based methods, counterfactual intervention, and self-explanatory models. For each category, we synthesize its representative studies, strengths, and weaknesses. Finally, we summarize their common virtues and remaining challenges, and reflect on future work directions towards faithful explainability in NLP.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NW8HKNM2/Lyu et al. - 2023 - Towards Faithful Model Explanation in NLP A Surve.pdf}
}

@article{ma_eureka_2023,
  title = {Eureka: Human-Level Reward Design via Coding Large Language Models},
  shorttitle = {Eureka},
  author = {Ma, Yecheng Jason and Liang, William and Wang, Guanzhi and Huang, De-An and Bastani, Osbert and Jayaraman, Dinesh and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.12931},
  url = {https://arxiv.org/abs/2310.12931},
  urldate = {2024-02-13},
  abstract = {Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks. However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem. We bridge this fundamental gap and present Eureka, a human-level reward design algorithm powered by LLMs. Eureka exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code. The resulting rewards can then be used to acquire complex skills via reinforcement learning. Without any task-specific prompting or pre-defined reward templates, Eureka generates reward functions that outperform expert human-engineered rewards. In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, Eureka outperforms human experts on 83\% of the tasks, leading to an average normalized improvement of 52\%. The generality of Eureka also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating. Finally, using Eureka rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited,reward,singularity,to cite},
  file = {/Users/leonardbereska/Zotero/storage/SK2JTPTL/Ma et al. - 2023 - Eureka Human-Level Reward Design via Coding Large.pdf}
}

@article{ma_untying_2023,
  title = {Untying the Reversal Curse via Bidirectional Language Model Editing},
  author = {Ma, Jun-Yu and Gu, Jia-Chen and Ling, Zhen-Hua and Liu, Quan and Liu, Cong},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.10322},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.10322},
  url = {http://arxiv.org/abs/2310.10322},
  urldate = {2023-11-16},
  abstract = {Recent studies have demonstrated that large language models (LLMs) store massive factual knowledge within their parameters. But existing LLMs are prone to hallucinate unintended text due to false or outdated knowledge. Since retraining LLMs is resource intensive, there has been a growing interest in the concept of model editing. Despite the emergence of benchmarks and approaches, these unidirectional editing and evaluation have failed to explore the reversal curse. Intuitively, if "The capital of France is" is edited to be a counterfact "London" within a model, then it should be able to naturally reason and recall the reverse fact, i.e., "London is the capital of" followed by "France" instead of "England". In this paper, we study bidirectional language model editing, aiming to provide rigorous model editing evaluation to assess if edited LLMs can recall the editing knowledge bidirectionally. A new evaluation metric of reversibility is introduced, and a benchmark dubbed as Bidirectional Assessment for Knowledge Editing (BAKE) is constructed to evaluate the reversibility of edited models in recalling knowledge in the reverse direction of editing. We surprisingly observe that while current editing methods and LLMs can effectively recall editing facts in the direction of editing, they suffer serious deficiencies when evaluated in the reverse direction. To mitigate the reversal curse, a method named Bidirectionally Inversible Relationship moDeling (BIRD) is proposed. A set of editing objectives that incorporate bidirectional relationships between subject and object into the updated model weights are designed. Experiments show that BIRD improves the performance of four representative LLMs of different sizes via question answering and judgement.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/A8XYRNVC/Ma et al. - 2023 - Untying the Reversal Curse via Bidirectional Langu.pdf}
}

@article{madry_deep_2019,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2019},
  month = sep,
  journal = {CoRR},
  eprint = {1706.06083},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1706.06083},
  url = {http://arxiv.org/abs/1706.06083},
  urldate = {2023-07-03},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/3EVF2D7V/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf}
}

@article{madsen_evaluating_2022,
  title = {Evaluating the Faithfulness of Importance Measures in NLP by Recursively Masking Allegedly Important Tokens and Retraining},
  author = {Madsen, Andreas and Meade, Nicholas and Adlakha, Vaibhav and Reddy, Siva},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2110.08412},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.08412},
  url = {http://arxiv.org/abs/2110.08412},
  urldate = {2023-08-29},
  abstract = {To explain NLP models a popular approach is to use importance measures, such as attention, which inform input tokens are important for making a prediction. However, an open question is how well these explanations accurately reflect a model's logic, a property called faithfulness. To answer this question, we propose Recursive ROAR, a new faithfulness metric. This works by recursively masking allegedly important tokens and then retraining the model. The principle is that this should result in worse model performance compared to masking random tokens. The result is a performance curve given a masking-ratio. Furthermore, we propose a summarizing metric using relative area-between-curves (RACU), which allows for easy comparison across papers, models, and tasks. We evaluate 4 different importance measures on 8 different datasets, using both LSTM-attention models and RoBERTa models. We find that the faithfulness of importance measures is both model-dependent and task-dependent. This conclusion contradicts previous evaluations in both computer vision and faithfulness of attention literature.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VW7K2Q5L/Madsen et al. - 2022 - Evaluating the Faithfulness of Importance Measures.pdf}
}

@article{madsen_posthoc_2022,
  title = {Post-hoc Interpretability for Neural NLP: A Survey},
  shorttitle = {Post-hoc Interpretability for Neural NLP},
  author = {Madsen, Andreas and Reddy, Siva and Chandar, Sarath},
  year = {2022},
  month = dec,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {8},
  pages = {155:1--155:42},
  issn = {0360-0300},
  doi = {10.1145/3546577},
  url = {https://dl.acm.org/doi/10.1145/3546577},
  urldate = {2023-08-29},
  abstract = {Neural networks for NLP are becoming increasingly complex and widespread, and there is a growing concern if these models are responsible to use. Explaining models helps to address the safety and ethical concerns and is essential for accountability. Interpretability serves to provide these explanations in terms that are understandable to humans. Additionally, post-hoc methods provide explanations after a model is learned and are generally model-agnostic. This survey provides a categorization of how recent post-hoc interpretability methods communicate explanations to humans, it discusses each method in-depth, and how they are validated, as the latter is often a common concern.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/XR9F5VGS/Madsen et al. - 2022 - Post-hoc Interpretability for Neural NLP A Survey.pdf}
}

@article{maini_can_2023,
  title = {Can Neural Network Memorization Be Localized?},
  author = {Maini, Pratyush and Mozer, Michael C. and Sedghi, Hanie and Lipton, Zachary C. and Kolter, J. Zico and Zhang, Chiyuan},
  year = {2023},
  month = jul,
  journal = {ICML},
  eprint = {2307.09542},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.09542},
  url = {http://arxiv.org/abs/2307.09542},
  urldate = {2023-11-02},
  abstract = {Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks \${\textbackslash}textit\{memorize\}\$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on \${\textbackslash}textit\{atypical\}\$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are \${\textbackslash}textit\{gradient accounting\}\$ (measuring the contribution to the gradient norms from memorized and clean examples), \${\textbackslash}textit\{layer rewinding\}\$ (replacing specific model weights of a converged model with previous training checkpoints), and \${\textbackslash}textit\{retraining\}\$ (training rewound layers only on clean examples). Second, we ask a more generic question: can memorization be localized \${\textbackslash}textit\{anywhere\}\$ in a model? We discover that memorization is often confined to a small number of neurons or channels (around 5) of the model. Based on these insights we propose a new form of dropout -- \${\textbackslash}textit\{example-tied dropout\}\$ that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we are able to reduce the accuracy on memorized examples from \$100{\textbackslash}\%{\textbackslash}to3{\textbackslash}\%\$, while also reducing the generalization gap.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QSVT8AUK/Maini et al. - 2023 - Can Neural Network Memorization Be Localized.pdf}
}

@article{makelov_principled_2024,
  title = {Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control},
  author = {Makelov, Aleksandar and Lange, George and Nanda, Neel},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2405.08366},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2405.08366},
  urldate = {2024-06-10},
  abstract = {Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against {\textbackslash}emph\{supervised\} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes. We apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are less successful than supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller, less interpretable features). We hope that our framework will provide a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/SQER5XUS/Makelov et al. - 2024 - Towards Principled Evaluations of Sparse Autoencod.pdf}
}

@article{makelov_this_2023,
  title = {Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching},
  shorttitle = {Is This the Subspace You Are Looking for?},
  author = {Makelov, Aleksandar and Lange, Georg and Nanda, Neel},
  year = {2023},
  month = dec,
  journal = {NeurIPS Workshop on Attributing Model Behavior at Scale},
  eprint = {2311.17030},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2311.17030},
  url = {http://arxiv.org/abs/2311.17030},
  urldate = {2023-12-14},
  abstract = {Mechanistic interpretability aims to understand model behaviors in terms of specific, interpretable features, often hypothesized to manifest as low-dimensional subspaces of activations. Specifically, recent studies have explored subspace interventions (such as activation patching) as a way to simultaneously manipulate model behavior and attribute the features behind it to given subspaces. In this work, we demonstrate that these two aims diverge, potentially leading to an illusory sense of interpretability. Counterintuitively, even if a subspace intervention makes the model's output behave as if the value of a feature was changed, this effect may be achieved by activating a dormant parallel pathway leveraging another subspace that is causally disconnected from model outputs. We demonstrate this phenomenon in a distilled mathematical example, in two real-world domains (the indirect object identification task and factual recall), and present evidence for its prevalence in practice. In the context of factual recall, we further show a link to rank-1 fact editing, providing a mechanistic explanation for previous work observing an inconsistency between fact editing performance and fact localization. However, this does not imply that activation patching of subspaces is intrinsically unfit for interpretability. To contextualize our findings, we also show what a success case looks like in a task (indirect object identification) where prior manual circuit analysis informs an understanding of the location of a feature. We explore the additional evidence needed to argue that a patched subspace is faithful.},
  archiveprefix = {arxiv},
  keywords = {graph,mechinterp,not cited,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/MTNUFBLL/Makelov et al. - 2023 - Is This the Subspace You Are Looking for An Inter.pdf}
}

@article{mallen_eliciting_2023,
  title = {Eliciting Latent Knowledge from Quirky Language Models},
  author = {Mallen, Alex and Belrose, Nora},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.01037},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.01037},
  url = {http://arxiv.org/abs/2312.01037},
  urldate = {2024-01-23},
  abstract = {Eliciting Latent Knowledge (ELK) aims to find patterns in a neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce a suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering math questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. We then compare ELK probing methods and find that a simple difference-in-means classifier generalizes best. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with upwards of 99\% AUROC. Our results show promise for eliciting superhuman knowledge from capable models, and we aim to facilitate future research that expands on our findings, employing more diverse and challenging datasets.},
  archiveprefix = {arxiv},
  keywords = {not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/AKB5AK6C/Mallen and Belrose - 2023 - Eliciting Latent Knowledge from Quirky Language Mo.pdf}
}

@article{maloyan_trojan_2024,
  title = {Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge},
  shorttitle = {Trojan Detection in Large Language Models},
  author = {Maloyan, Narek and Verma, Ekansh and Nutfullin, Bulat and Ashinov, Bislan},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2404.13660},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.13660},
  url = {http://arxiv.org/abs/2404.13660},
  urldate = {2024-04-25},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, but their vulnerability to trojan or backdoor attacks poses significant security risks. This paper explores the challenges and insights gained from the Trojan Detection Competition 2023 (TDC2023), which focused on identifying and evaluating trojan attacks on LLMs. We investigate the difficulty of distinguishing between intended and unintended triggers, as well as the feasibility of reverse engineering trojans in real-world scenarios. Our comparative analysis of various trojan detection methods reveals that achieving high Recall scores is significantly more challenging than obtaining high Reverse-Engineering Attack Success Rate (REASR) scores. The top-performing methods in the competition achieved Recall scores around 0.16, comparable to a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes. This finding raises questions about the detectability and recoverability of trojans inserted into the model, given only the harmful targets. Despite the inability to fully solve the problem, the competition has led to interesting observations about the viability of trojan detection and improved techniques for optimizing LLM input prompts. The phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers highlights the need for further research into the robustness and interpretability of LLMs. The TDC2023 has provided valuable insights into the challenges and opportunities associated with trojan detection in LLMs, laying the groundwork for future research in this area to ensure their safety and reliability in real-world applications.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/4BCL3BEN/Maloyan et al. - 2024 - Trojan Detection in Large Language Models Insight.pdf}
}

@article{marchetti_harmonics_2023,
  title = {Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks},
  shorttitle = {Harmonics of Learning},
  author = {Marchetti, Giovanni Luca and Hillar, Christopher and Kragic, Danica and Sanborn, Sophia},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.08550},
  primaryclass = {cs, eess},
  doi = {10.48550/arXiv.2312.08550},
  url = {http://arxiv.org/abs/2312.08550},
  urldate = {2024-05-07},
  abstract = {In this work, we formally prove that, under certain conditions, if a neural network is invariant to a finite group then its weights recover the Fourier transform on that group. This provides a mathematical explanation for the emergence of Fourier features -- a ubiquitous phenomenon in both biological and artificial learning systems. The results hold even for non-commutative groups, in which case the Fourier transform encodes all the irreducible unitary group representations. Our findings have consequences for the problem of symmetry discovery. Specifically, we demonstrate that the algebraic structure of an unknown group can be recovered from the weights of a network that is at least approximately invariant within certain bounds. Overall, this work contributes to a foundation for an algebraic learning theory of invariant neural network representations.},
  archiveprefix = {arxiv},
  keywords = {to cite},
  file = {/Users/leonardbereska/Zotero/storage/R6MXK8F2/Marchetti et al. - 2023 - Harmonics of Learning Universal Fourier Features .pdf}
}

@article{marconato_interpretability_2023,
  title = {Interpretability Is in the Mind of the Beholder: A Causal Framework for Human-Interpretable Representation Learning},
  shorttitle = {Interpretability Is in the Mind of the Beholder},
  author = {Marconato, Emanuele and Passerini, Andrea and Teso, Stefano},
  year = {2023},
  month = nov,
  journal = {Entropy},
  volume = {25},
  number = {12},
  pages = {1574},
  issn = {1099-4300},
  doi = {10.3390/e25121574},
  url = {https://www.mdpi.com/1099-4300/25/12/1574},
  urldate = {2024-02-10},
  abstract = {Research on Explainable Artificial Intelligence has recently started exploring the idea of producing explanations that, rather than being expressed in terms of low-level features, are encoded in terms of interpretable concepts learned from data. How to reliably acquire such concepts is, however, still fundamentally unclear. An agreed-upon notion of concept interpretability is missing, with the result that concepts used by both post hoc explainers and concept-based neural networks are acquired through a variety of mutually incompatible strategies. Critically, most of these neglect the human side of the problem: a representation is understandable only insofar as it can be understood by the human at the receiving end. The key challenge in human-interpretable representation learning (hrl) is how to model and operationalize this human element. In this work, we propose a mathematical framework for acquiring interpretable representations suitable for both post hoc explainers and concept-based neural networks. Our formalization of hrl builds on recent advances in causal representation learning and explicitly models a human stakeholder as an external observer. This allows us derive a principled notion of alignment between the machine's representation and the vocabulary of concepts understood by the human. In doing so, we link alignment and interpretability through a simple and intuitive name transfer game, and clarify the relationship between alignment and a well-known property of representations, namely disentanglement. We also show that alignment is linked to the issue of undesirable correlations among concepts, also known as concept leakage, and to content-style separation, all through a general information-theoretic reformulation of these properties. Our conceptualization aims to bridge the gap between the human and algorithmic sides of interpretability and establish a stepping stone for new research on human-interpretable representations.},
  language = {en},
  keywords = {causal,interpretability,not cited,representation},
  file = {/Users/leonardbereska/Zotero/storage/THRJG7PL/Marconato et al. - 2023 - Interpretability Is in the Mind of the Beholder A.pdf}
}

@article{marks_geometry_2023,
  title = {The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets},
  author = {Marks, Samuel and Tegmark, Max},
  year = {2023},
  journal = {CoRR},
  url = {https://arxiv.org/abs/2310.06824},
  keywords = {not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/KC34AUQN/Marks and Tegmark - 2023 - The Geometry of Truth Emergent Linear Structure i.pdf}
}

@article{marks_interpreting_2023,
  title = {Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders},
  author = {Marks, Luke and Abdullah, Amir and Mendez, Luna and Arike, Rauno and Torr, Philip and Barez, Fazl},
  year = {2023},
  month = oct,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Interpreting-Reward-Models-in-RLHF-Tuned-Language-Marks-Abdullah/53b9f8c62837b12d3955778f737678aabea39a83},
  urldate = {2023-10-25},
  abstract = {Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KTQAWUYL/Marks et al. - 2023 - Interpreting Reward Models in RLHF-Tuned Language .pdf}
}

@article{marks_opensource_2023,
  title = {Some open-source dictionaries and dictionary learning infrastructure},
  author = {Marks, Sam},
  year = {2023},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning},
  urldate = {2024-02-16},
  abstract = {As more people begin work on interpretability projects which incorporate dictionary learning, it will be valuable to have high-quality dictionaries p{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/HL4WISTE/Marks - 2023 - Some open-source dictionaries and dictionary learn.html}
}

@article{marks_sparse_2024,
  title = {Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models},
  shorttitle = {Sparse Feature Circuits},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.19647},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.19647},
  url = {http://arxiv.org/abs/2403.19647},
  urldate = {2024-06-10},
  abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/PAXI7A5W/Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf}
}

@article{marks_training_2023,
  title = {Beyond Training Objectives: Interpreting Reward Model Divergence in Large Language Models},
  shorttitle = {Beyond Training Objectives},
  author = {Marks, Luke and Abdullah, Amir and Neo, Clement and Arike, Rauno and Torr, Philip and Barez, Fazl},
  year = {2023},
  month = oct,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2310.08164},
  urldate = {2024-03-13},
  abstract = {Large language models (LLMs) fine-tuned by reinforcement learning from human feedback (RLHF) are becoming more widely deployed. We coin the term \${\textbackslash}textit\{Implicit Reward Model\}\$ (IRM) to refer to the changes that occur to an LLM during RLHF that result in high-reward generations. We interpret IRMs, and measure their divergence from the RLHF reward model used in the fine-tuning process that induced them. By fitting a linear function to an LLM's IRM, a reward model with the same type signature as the RLHF reward model is constructed, allowing for direct comparison. Additionally, we validate our construction of the IRM through cross-comparison with classifications of features generated by an LLM based on their relevance to the RLHF reward model. Better comprehending IRMs can help minimize discrepencies between LLM behavior and training objectives, which we believe to be an essential component of the \${\textbackslash}textit\{safety\}\$ and \${\textbackslash}textit\{alignment\}\$ of LLMs.},
  keywords = {mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/8CH4NFQW/Marks et al. - 2023 - Beyond Training Objectives Interpreting Reward Mo.pdf}
}

@article{marks_what_2024a,
  title = {What's up with LLMs representing XORs of arbitrary features?},
  author = {Marks, Sam},
  year = {2024},
  url = {https://www.lesswrong.com/posts/hjJXCn9GsskysDceS/what-s-up-with-llms-representing-xors-of-arbitrary-features},
  urldate = {2024-03-25},
  abstract = {Thanks to Cl{\'e}ment Dumas, Nikola Jurkovi{\'c}, Nora Belrose, Arthur Conmy, and Oam Patel for feedback. {\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/7H5S288I/what-s-up-with-llms-representing-xors-of-arbitrary-features.html}
}

@article{marshall_understanding_2024,
  title = {Understanding polysemanticity in neural networks through coding theory},
  author = {Marshall, Simon C. and Kirchner, Jan H.},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.17975},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.17975},
  url = {http://arxiv.org/abs/2401.17975},
  urldate = {2024-02-13},
  abstract = {Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al.{\textasciitilde}(2022). Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/JL2PTXI7/Marshall and Kirchner - 2024 - Understanding polysemanticity in neural networks t.pdf}
}

@article{mazeika_how_2022,
  title = {How Hard is Trojan Detection in DNNs? Fooling Detectors With Evasive Trojans},
  shorttitle = {How Hard is Trojan Detection in DNNs?},
  author = {Mazeika, Mantas and Zou, Andy and Arora, Akul and Pleskov, Pavel and Song, Dawn and Hendrycks, Dan and Li, Bo and Forsyth, David},
  year = {2022},
  month = sep,
  journal = {CoRR},
  url = {https://openreview.net/forum?id=V-RDBWYf0go},
  urldate = {2023-10-13},
  abstract = {As AI systems become more capable and widely used, a growing concern is the possibility for trojan attacks in which adversaries inject deep neural networks with hidden functionality. Recently, methods for detecting trojans have proven surprisingly effective against existing attacks. However, there is comparatively little work on whether trojans themselves could be rendered hard to detect. To fill this gap, we develop a general method for making trojans more evasive based on several novel techniques and observations. Our method combines distribution-matching, specificity, and randomization to eliminate distinguishing features of trojaned networks. Importantly, our method can be applied to various existing trojan attacks and is detector-agnostic. In experiments, we find that our evasive trojans reduce the efficacy of a wide range of detectors across numerous evaluation settings while maintaining high attack success rates. Moreover, we find that evasive trojans are also harder to reverse-engineer, underscoring the importance of developing more robust monitoring mechanisms for neural networks and clarifying the offence-defense balance of trojan detection.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/UV996J9I/Mazeika et al. - 2022 - How Hard is Trojan Detection in DNNs Fooling Dete.pdf}
}

@article{mccoy_berts_2020,
  title = {BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance},
  shorttitle = {BERTs of a feather do not generalize together},
  author = {McCoy, R. Thomas and Min, Junghyun and Linzen, Tal},
  year = {2020},
  month = nov,
  journal = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  eprint = {1911.02969},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1911.02969},
  url = {http://arxiv.org/abs/1911.02969},
  urldate = {2024-01-18},
  abstract = {If the same neural network architecture is trained multiple times on the same dataset, will it make similar linguistic generalizations across runs? To study this question, we fine-tuned 100 instances of BERT on the Multi-genre Natural Language Inference (MNLI) dataset and evaluated them on the HANS dataset, which evaluates syntactic generalization in natural language inference. On the MNLI development set, the behavior of all instances was remarkably consistent, with accuracy ranging between 83.6\% and 84.8\%. In stark contrast, the same models varied widely in their generalization performance. For example, on the simple case of subject-object swap (e.g., determining that "the doctor visited the lawyer" does not entail "the lawyer visited the doctor"), accuracy ranged from 0.00\% to 66.2\%. Such variation is likely due to the presence of many local minima that are equally attractive to a low-bias learner such as a neural network; decreasing the variability may therefore require models with stronger inductive biases.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9BKKGX27/McCoy et al. - 2020 - BERTs of a feather do not generalize together Lar.pdf}
}

@article{mcdougall_copy_2023,
  title = {Copy Suppression: Comprehensively Understanding an Attention Head},
  shorttitle = {Copy Suppression},
  author = {McDougall, Callum and Conmy, Arthur and Rushing, Cody and McGrath, Thomas and Nanda, Neel},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.04625},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.04625},
  url = {http://arxiv.org/abs/2310.04625},
  urldate = {2023-10-27},
  abstract = {We present a single attention head in GPT-2 Small that has one main role across the entire training distribution. If components in earlier layers predict a certain token, and this token appears earlier in the context, the head suppresses it: we call this copy suppression. Attention Head 10.7 (L10H7) suppresses naive copying behavior which improves overall model calibration. This explains why multiple prior works studying certain narrow tasks found negative heads that systematically favored the wrong answer. We uncover the mechanism that the Negative Heads use for copy suppression with weights-based evidence and are able to explain 76.9\% of the impact of L10H7 in GPT-2 Small. To the best of our knowledge, this is the most comprehensive description of the complete role of a component in a language model to date. One major effect of copy suppression is its role in self-repair. Self-repair refers to how ablating crucial model components results in downstream neural network parts compensating for this ablation. Copy suppression leads to self-repair: if an initial overconfident copier is ablated, then there is nothing to suppress. We show that self-repair is implemented by several mechanisms, one of which is copy suppression, which explains 39\% of the behavior in a narrow task. Interactive visualisations of the copy suppression phenomena may be seen at our web app https://copy-suppression.streamlit.app/},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/6XHCL9RK/McDougall et al. - 2023 - Copy Suppression Comprehensively Understanding an.pdf}
}

@article{mcgrath_acquisition_2022,
  title = {Acquisition of chess knowledge in AlphaZero},
  author = {McGrath, Thomas and Kapishnikov, Andrei and Toma{\v s}ev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
  year = {2022},
  month = nov,
  journal = {PNAS},
  volume = {119},
  number = {47},
  pages = {e2206625119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2206625119},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.2206625119},
  urldate = {2023-10-30},
  abstract = {We analyze the knowledge acquired by AlphaZero, a neural network engine that learns chess solely by playing against itself yet becomes capable of outperforming human chess players. Although the system trains without access to human games or guidance, it appears to learn concepts analogous to those used by human chess players. We provide two lines of evidence. Linear probes applied to AlphaZero's internal state enable us to quantify when and where such concepts are represented in the network. We also describe a behavioral analysis of opening play, including qualitative commentary by a former world chess champion.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HEUBAIYK/McGrath et al. - 2022 - Acquisition of chess knowledge in AlphaZero.pdf}
}

@article{mcgrath_hydra_2023,
  title = {The Hydra Effect: Emergent Self-repair in Language Model Computations},
  shorttitle = {The Hydra Effect},
  author = {McGrath, Thomas and Rahtz, Matthew and Kramar, Janos and Mikulik, Vladimir and Legg, Shane},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.15771},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.15771},
  url = {http://arxiv.org/abs/2307.15771},
  urldate = {2023-10-30},
  abstract = {We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.},
  archiveprefix = {arxiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/FKZRFPN3/McGrath et al. - 2023 - The Hydra Effect Emergent Self-repair in Language.pdf}
}

@article{meng_locating_2022,
  title = {Locating and Editing Factual Associations in GPT},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  journal = {NeurIPS},
  eprint = {2202.05262},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2202.05262},
  url = {http://arxiv.org/abs/2202.05262},
  urldate = {2023-08-27},
  abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
  archiveprefix = {arxiv},
  keywords = {cited,editing,empirical,mechinterp,memory,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/AQAYITEA/Meng et al. - 2022 - Locating and Editing Factual Associations in GPT.pdf}
}

@article{meng_massediting_2022,
  title = {Mass-Editing Memory in a Transformer},
  author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  year = {2022},
  journal = {ICLR},
  eprint = {2210.07229},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.07229},
  url = {http://arxiv.org/abs/2210.07229},
  urldate = {2023-08-27},
  abstract = {Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.},
  archiveprefix = {arxiv},
  keywords = {cited,editing,empirical,mechinterp,memory,to cite,to extract figures,to extract related work,to review in detail,transformer},
  file = {/Users/leonardbereska/Zotero/storage/YZP7I3LD/Meng et al. - 2022 - Mass-Editing Memory in a Transformer.pdf}
}

@article{merrill_saturated_2022,
  title = {Saturated Transformers are Constant-Depth Threshold Circuits},
  author = {Merrill, William and Sabharwal, Ashish and Smith, Noah A.},
  editor = {Roark, Brian and Nenkova, Ani},
  year = {2022},
  journal = {TACL},
  volume = {10},
  pages = {843--856},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00493},
  url = {https://aclanthology.org/2022.tacl-1.49},
  urldate = {2024-03-19},
  abstract = {Transformers have become a standard neural network architecture for many NLP problems, motivating theoretical analysis of their power in terms of formal languages. Recent work has shown that transformers with hard attention are quite limited in power (Hahn, 2020), as they can be simulated by constant-depth AND/OR circuits (Hao et al., 2022). However, hard attention is a strong assumption, which may complicate the relevance of these results in practice. In this work, we analyze the circuit complexity of transformers with saturated attention: a generalization of hard attention that more closely captures the attention patterns learnable in practical transformers. We first show that saturated transformers transcend the known limitations of hard-attention transformers. We then prove saturated transformers with floating-point values can be simulated by constant-depth threshold circuits, giving the class TC0 as an upper bound on the formal languages they recognize.},
  file = {/Users/leonardbereska/Zotero/storage/PQTEU365/Merrill et al. - 2022 - Saturated Transformers are Constant-Depth Threshol.pdf}
}

@article{merrill_tale_2023,
  title = {A Tale of Two Circuits: Grokking as Competition of Sparse and Dense Subnetworks},
  shorttitle = {A Tale of Two Circuits},
  author = {Merrill, William and Tsilivis, Nikolaos and Shukla, Aman},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2303.11873},
  url = {https://arxiv.org/abs/2303.11873},
  urldate = {2023-11-10},
  abstract = {Grokking is a phenomenon where a model trained on an algorithmic task first overfits but, then, after a large amount of additional training, undergoes a phase transition to generalize perfectly. We empirically study the internal structure of networks undergoing grokking on the sparse parity task, and find that the grokking phase transition corresponds to the emergence of a sparse subnetwork that dominates model predictions. On an optimization level, we find that this subnetwork arises when a small subset of neurons undergoes rapid norm growth, whereas the other neurons in the network decay slowly in norm. Thus, we suggest that the grokking phase transition can be understood to emerge from competition of two largely distinct subnetworks: a dense one that dominates before the transition and generalizes poorly, and a sparse one that dominates afterwards.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PSV8XXA4/Merrill et al. - 2023 - A Tale of Two Circuits Grokking as Competition of.pdf}
}

@article{merullo_circuit_2024,
  title = {Circuit Component Reuse Across Tasks in Transformer Language Models},
  author = {Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  year = {2024},
  journal = {ICLR},
  eprint = {2310.08744},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.08744},
  url = {http://arxiv.org/abs/2310.08744},
  urldate = {2023-11-16},
  abstract = {Recent work in mechanistic interpretability has shown that behaviors in language models can be successfully reverse-engineered through circuit analysis. A common criticism, however, is that each circuit is task-specific, and thus such analysis cannot contribute to understanding the models at a higher level. In this work, we present evidence that insights (both low-level findings about specific heads and higher-level findings about general algorithms) can indeed generalize across tasks. Specifically, we study the circuit discovered in Wang et al. (2022) for the Indirect Object Identification (IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that it is mostly reused to solve a seemingly different task: Colored Objects (Ippolito \& Callison-Burch, 2023). We provide evidence that the process underlying both tasks is functionally very similar, and contains about a 78\% overlap in in-circuit attention heads. We further present a proof-of-concept intervention experiment, in which we adjust four attention heads in middle layers in order to 'repair' the Colored Objects circuit and make it behave like the IOI circuit. In doing so, we boost accuracy from 49.6\% to 93.7\% on the Colored Objects task and explain most sources of error. The intervention affects downstream attention heads in specific ways predicted by their interactions in the IOI circuit, indicating that this subcircuit behavior is invariant to the different task inputs. Overall, our results provide evidence that it may yet be possible to explain large language models' behavior in terms of a relatively small number of interpretable task-general algorithmic building blocks and computational components.},
  archiveprefix = {arxiv},
  keywords = {graph,mechinterp,not cited,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/39M6YU4N/Merullo et al. - 2024 - Circuit Component Reuse Across Tasks in Transforme.pdf}
}

@article{merullo_mechanism_2023,
  title = {A Mechanism for Solving Relational Tasks in Transformer Language Models},
  author = {Merullo, Jack and Eickhoff, Carsten and Pavlick, Ellie},
  year = {2023},
  month = may,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/A-Mechanism-for-Solving-Relational-Tasks-in-Models-Merullo-Eickhoff/823fb3163a5600b5be957fc8337a9f7cdd177fef},
  urldate = {2023-12-14},
  abstract = {A primary criticism towards language models (LMs) is their inscrutability. This paper presents evidence that, despite their size and complexity, LMs sometimes exploit a simple computational mechanism to solve one-to-one relational tasks (e.g., capital\_of(Poland)=Warsaw). We investigate a range of language model sizes (from 124M parameters to 176B parameters) in an in-context learning setting, and find that for a variety of tasks (involving capital cities, upper-casing, and past-tensing) a key part of the mechanism reduces to a simple linear update typically applied by the feedforward (FFN) networks. These updates also tend to promote the output of the relation in a content-independent way (e.g., encoding Poland:Warsaw::China:Beijing), revealing a predictable pattern that these models take in solving these tasks. We further show that this mechanism is specific to tasks that require retrieval from pretraining memory, rather than retrieval from local context. Our results contribute to a growing body of work on the mechanistic interpretability of LLMs, and offer reason to be optimistic that, despite the massive and non-linear nature of the models, the strategies they ultimately use to solve tasks can sometimes reduce to familiar and even intuitive algorithms.},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/I2P42X99/Merullo et al. - 2023 - A Mechanism for Solving Relational Tasks in Transf.pdf}
}

@article{michaud_opening_2024,
  title = {Opening the AI black box: program synthesis via mechanistic interpretability},
  shorttitle = {Opening the AI black box},
  author = {Michaud, Eric J. and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli{\'c}, Mateja and Tegmark, Max},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.05110},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.05110},
  url = {http://arxiv.org/abs/2402.05110},
  urldate = {2024-02-13},
  abstract = {We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/WCFVS4V8/Michaud et al. - 2024 - Opening the AI black box program synthesis via me.pdf}
}

@article{michaud_quantization_2023,
  title = {The Quantization Model of Neural Scaling},
  author = {Michaud, Eric J. and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  year = {2023},
  month = mar,
  journal = {CoRR},
  eprint = {2303.13506},
  primaryclass = {cond-mat},
  doi = {10.48550/arXiv.2303.13506},
  url = {http://arxiv.org/abs/2303.13506},
  urldate = {2023-10-30},
  abstract = {We propose the \${\textbackslash}textit\{Quantization Model\}\$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the \${\textbackslash}textit\{Quantization Hypothesis\}\$, where learned network capabilities are quantized into discrete chunks (\${\textbackslash}textit\{quanta\}\$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZBB9GYAC/Michaud et al. - 2023 - The Quantization Model of Neural Scaling.pdf}
}

@article{michel_are_2019,
  title = {Are Sixteen Heads Really Better than One?},
  author = {Michel, Paul and Levy, Omer and Neubig, Graham},
  year = {2019},
  month = nov,
  journal = {NeurIPS},
  doi = {10.48550/arXiv.1905.10650},
  url = {http://arxiv.org/abs/1905.10650},
  urldate = {2023-11-10},
  abstract = {Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention "head" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DPLHLTQ5/Michel et al. - 2019 - Are Sixteen Heads Really Better than One.pdf}
}

@article{micheli_transformers_2023,
  title = {Transformers are Sample-Efficient World Models},
  author = {Micheli, Vincent and Alonso, Eloi and Fleuret, Fran{\c c}ois},
  year = {2023},
  month = mar,
  journal = {ICLR},
  eprint = {2209.00588},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.00588},
  url = {http://arxiv.org/abs/2209.00588},
  urldate = {2024-02-26},
  abstract = {Deep reinforcement learning agents are notoriously sample inefficient, which considerably limits their application to real-world problems. Recently, many model-based methods have been designed to address this issue, with learning in the imagination of a world model being one of the most prominent approaches. However, while virtually unlimited interaction with a simulated environment sounds appealing, the world model has to be accurate over extended periods of time. Motivated by the success of Transformers in sequence modeling tasks, we introduce IRIS, a data-efficient agent that learns in a world model composed of a discrete autoencoder and an autoregressive Transformer. With the equivalent of only two hours of gameplay in the Atari 100k benchmark, IRIS achieves a mean human normalized score of 1.046, and outperforms humans on 10 out of 26 games, setting a new state of the art for methods without lookahead search. To foster future research on Transformers and world models for sample-efficient reinforcement learning, we release our code and models at https://github.com/eloialonso/iris.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/M6ALJEDK/Micheli et al. - 2023 - Transformers are Sample-Efficient World Models.pdf}
}

@article{mickus_how_2022,
  title = {How to Dissect a Muppet: The Structure of Transformer Embedding Spaces},
  shorttitle = {How to Dissect a Muppet},
  author = {Mickus, Timothee and Paperno, Denis and Constant, Mathieu},
  year = {2022},
  month = jun,
  journal = {CoRR},
  eprint = {2206.03529},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.03529},
  url = {http://arxiv.org/abs/2206.03529},
  urldate = {2023-08-29},
  abstract = {Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/H7H35ZNQ/Mickus et al. - 2022 - How to Dissect a Muppet The Structure of Transfor.pdf}
}

@article{mickus_how_2022a,
  title = {How to Dissect a Muppet: The Structure of Transformer Embedding Spaces},
  shorttitle = {How to Dissect a Muppet},
  author = {Mickus, Timothee and Paperno, Denis and Constant, Mathieu},
  year = {2022},
  month = sep,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {981--996},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00501},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00501/112915/How-to-Dissect-a-Muppet-The-Structure-of},
  urldate = {2023-12-14},
  abstract = {Abstract             Pretrained embeddings based on the Transformer architecture have taken the NLP community by storm. We show that they can mathematically be reframed as a sum of vector factors and showcase how to use this reframing to study the impact of each component. We provide evidence that multi-head attentions and feed-forwards are not equally useful in all downstream applications, as well as a quantitative overview of the effects of finetuning on the overall embedding space. This approach allows us to draw connections to a wide range of previous studies, from vector space anisotropy to attention weights.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/V5JGVPBR/Mickus et al. - 2022 - How to Dissect a Muppet The Structure of Transfor.pdf}
}

@article{mikolov_distributed_2013,
  title = {Distributed Representations of Words and Phrases and their Compositionality},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = oct,
  journal = {NeurIPS},
  eprint = {1310.4546},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1310.4546},
  url = {http://arxiv.org/abs/1310.4546},
  urldate = {2023-11-02},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/WZ66E6T3/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@article{mikolov_linguistic_2013,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  editor = {Vanderwende, Lucy and Daum{\'e} III, Hal and Kirchhoff, Katrin},
  year = {2013},
  month = jun,
  journal = {NAACL-HLT},
  pages = {746--751},
  url = {https://aclanthology.org/N13-1090},
  urldate = {2024-03-20},
  file = {/Users/leonardbereska/Zotero/storage/P9NATCG3/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf}
}

@article{miller_explanation_2019,
  title = {Explanation in artificial intelligence: Insights from the social sciences},
  shorttitle = {Explanation in artificial intelligence},
  author = {Miller, Tim},
  year = {2019},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2018.07.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
  urldate = {2023-10-19},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PBF8NT4D/Miller - 2019 - Explanation in artificial intelligence Insights f.pdf}
}

@article{miller_we_2023,
  title = {We Found An Neuron in GPT-2},
  author = {Miller, Joseph and Neo, Clement},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/cgqh99SHsCv3jJYDS/we-found-an-neuron-in-gpt-2},
  urldate = {2023-11-30},
  abstract = {We started out with the question: How does GPT-2 know when to use the word "an" over "a"? The choice depends on whether the word that comes after sta{\dots}},
  language = {en},
  keywords = {empirical,not cited},
  file = {/Users/leonardbereska/Zotero/storage/CZU3B7LD/Miller and Neo - 2023 - We Found An Neuron in GPT-2.html}
}

@article{millidge_singular_2022,
  title = {The Singular Value Decompositions of Transformer Weight Matrices are Highly Interpretable},
  author = {Millidge, Beren and Black, Sid},
  year = {2022},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight},
  urldate = {2023-11-30},
  abstract = {Please go to the colab for interactive viewing and playing with the phenomena. For space reasons, not all results included in the colab are included{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/KF9WL5V6/Millidge and Black - 2022 - The Singular Value Decompositions of Transformer W.html}
}

@article{mitchell_fast_2022,
  title = {Fast Model Editing at Scale},
  author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Finn, Chelsea and Manning, Christopher D.},
  year = {2022},
  month = jun,
  journal = {ICLR},
  eprint = {2110.11309},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.11309},
  url = {http://arxiv.org/abs/2110.11309},
  urldate = {2024-03-19},
  abstract = {While large pre-trained models have enabled impressive results on a variety of downstream tasks, the largest existing models still make errors, and even accurate predictions may become outdated over time. Because detecting all such failures at training time is impossible, enabling both developers and end users of such models to correct inaccurate outputs while leaving the model otherwise intact is desirable. However, the distributed, black-box nature of the representations learned by large neural networks makes producing such targeted edits difficult. If presented with only a single problematic input and new desired output, fine-tuning approaches tend to overfit; other editing algorithms are either computationally infeasible or simply ineffective when applied to very large models. To enable easy post-hoc editing at scale, we propose Model Editor Networks using Gradient Decomposition (MEND), a collection of small auxiliary editing networks that use a single desired input-output pair to make fast, local edits to a pre-trained model's behavior. MEND learns to transform the gradient obtained by standard fine-tuning, using a low-rank decomposition of the gradient to make the parameterization of this transformation tractable. MEND can be trained on a single GPU in less than a day even for 10 billion+ parameter models; once trained MEND enables rapid application of new edits to the pre-trained model. Our experiments with T5, GPT, BERT, and BART models show that MEND is the only approach to model editing that effectively edits the behavior of models with more than 10 billion parameters. Code and data available at https://sites.google.com/view/mend-editing.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/L8Q5GRTW/Mitchell et al. - 2022 - Fast Model Editing at Scale.pdf}
}

@article{mitchell_memorybased_2022,
  title = {Memory-Based Model Editing at Scale},
  author = {Mitchell, Eric and Lin, Charles and Bosselut, Antoine and Manning, Christopher D. and Finn, Chelsea},
  year = {2022},
  month = jun,
  journal = {ICML},
  pages = {15817--15831},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/mitchell22a.html},
  urldate = {2024-03-19},
  abstract = {Even the largest neural networks make errors, and once-correct predictions can become invalid as the world changes. Model editors make local updates to the behavior of base (pre-trained) models to inject updated knowledge or correct undesirable behaviors. Existing model editors have shown promise, but also suffer from insufficient expressiveness: they struggle to accurately model an edit's intended scope (examples affected by the edit), leading to inaccurate predictions for test inputs loosely related to the edit, and they often fail altogether after many edits. As a higher-capacity alternative, we propose Semi-Parametric Editing with a Retrieval-Augmented Counterfactual Model (SERAC), which stores edits in an explicit memory and learns to reason over them to modulate the base model's predictions as needed. To enable more rigorous evaluation of model editors, we introduce three challenging language model editing problems based on question answering, fact-checking, and dialogue generation. We find that only SERAC achieves high performance on all three problems, consistently outperforming existing approaches to model editing by a significant margin. Code, data, and additional project information will be made available at https://sites.google.com/view/serac-editing.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/8KMDCEUB/Mitchell et al. - 2022 - Memory-Based Model Editing at Scale.pdf}
}

@article{molina_traveling_2023,
  title = {Traveling Words: A Geometric Interpretation of Transformers},
  shorttitle = {Traveling Words},
  author = {Molina, Raul},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.07315},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.07315},
  url = {http://arxiv.org/abs/2309.07315},
  urldate = {2023-11-16},
  abstract = {Transformers have significantly advanced the field of natural language processing, but comprehending their internal mechanisms remains a challenge. In this paper, we introduce a novel geometric perspective that elucidates the inner mechanisms of transformer operations. Our primary contribution is illustrating how layer normalization confines the latent features to a hyper-sphere, subsequently enabling attention to mold the semantic representation of words on this surface. This geometric viewpoint seamlessly connects established properties such as iterative refinement and contextual embeddings. We validate our insights by probing a pre-trained 124M parameter GPT-2 model. Our findings reveal clear query-key attention patterns in early layers and build upon prior observations regarding the subject-specific nature of attention heads at deeper layers. Harnessing these geometric insights, we present an intuitive understanding of transformers, depicting them as processes that model the trajectory of word particles along the hyper-sphere.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JBU8YKE5/Molina - 2023 - Traveling Words A Geometric Interpretation of Tra.pdf}
}

@article{molnar_general_2020,
  title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
  author = {Molnar, Christoph and K{\"o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and {Grosse-Wentrup}, Moritz and Bischl, Bernd},
  year = {2020},
  month = jul,
  journal = {CoRR},
  eprint = {2007.04131},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2007.04131},
  url = {http://arxiv.org/abs/2007.04131},
  urldate = {2023-10-20},
  abstract = {An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YXNHEQXC/Molnar et al. - 2020 - General Pitfalls of Model-Agnostic Interpretation .pdf}
}

@article{molnar_general_2022,
  title = {General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models},
  author = {Molnar, Christoph and K{\"o}nig, Gunnar and Herbinger, Julia and Freiesleben, Timo and Dandl, Susanne and Scholbeck, Christian A. and Casalicchio, Giuseppe and {Grosse-Wentrup}, Moritz and Bischl, Bernd},
  editor = {Holzinger, Andreas and Goebel, Randy and Fong, Ruth and Moon, Taesup and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2022},
  journal = {International Workshop on Extending Explainable AI Beyond Deep Models and Classifiers},
  volume = {13200},
  pages = {39--68},
  doi = {10.1007/978-3-031-04083-2_4},
  url = {https://link.springer.com/10.1007/978-3-031-04083-2_4},
  urldate = {2024-01-24},
  abstract = {Abstract             An increasing number of model-agnostic interpretation techniques for machine learning (ML) models such as partial dependence plots (PDP), permutation feature importance (PFI) and Shapley values provide insightful model interpretations, but can lead to wrong conclusions if applied incorrectly. We highlight many general pitfalls of ML model interpretation, such as using interpretation techniques in the wrong context, interpreting models that do not generalize well, ignoring feature dependencies, interactions, uncertainty estimates and issues in high-dimensional settings, or making unjustified causal interpretations, and illustrate them with examples. We focus on pitfalls for global methods that describe the average model behavior, but many pitfalls also apply to local methods that explain individual predictions. Our paper addresses ML practitioners by raising awareness of pitfalls and identifying solutions for correct model interpretation, but also addresses ML researchers by discussing open issues for further research.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/I9R8IQ6F/Molnar et al. - 2022 - General Pitfalls of Model-Agnostic Interpretation .pdf}
}

@book{molnar_interpretable_2022,
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  author = {Molnar, Christoph},
  year = {2022},
  publisher = {Independently published},
  url = {https://christophm.github.io/interpretable-ml-book},
  abstract = {Machine learning has great potential for improving products, processes and research. But computers usually do not explain their predictions which is a barrier to the adoption of machine learning. This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. The focus of the book is on model-agnostic methods for interpreting black box models such as feature importance and accumulated local effects, and explaining individual predictions with Shapley values and LIME. In addition, the book presents methods specific to deep neural networks. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project. Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.},
  keywords = {not cited}
}

@article{molnar_quantifying_2020,
  title = {Quantifying Model Complexity via Functional Decomposition for Better Post-Hoc Interpretability},
  author = {Molnar, Christoph and Casalicchio, Giuseppe and Bischl, Bernd},
  year = {2020},
  journal = {ECML PKDD},
  volume = {1167},
  eprint = {1904.03867},
  primaryclass = {cs, stat},
  pages = {193--204},
  doi = {10.1007/978-3-030-43823-4_17},
  url = {http://arxiv.org/abs/1904.03867},
  urldate = {2024-02-13},
  abstract = {Post-hoc model-agnostic interpretation methods such as partial dependence plots can be employed to interpret complex machine learning models. While these interpretation methods can be applied regardless of model complexity, they can produce misleading and verbose results if the model is too complex, especially w.r.t. feature interactions. To quantify the complexity of arbitrary machine learning models, we propose model-agnostic complexity measures based on functional decomposition: number of features used, interaction strength and main effect complexity. We show that post-hoc interpretation of models that minimize the three measures is more reliable and compact. Furthermore, we demonstrate the application of these measures in a multi-objective optimization approach which simultaneously minimizes loss and complexity.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YGEPEZVV/Molnar et al. - 2020 - Quantifying Model Complexity via Functional Decomp.pdf}
}

@article{momennejad_evaluating_2023,
  title = {Evaluating Cognitive Maps and Planning in Large Language Models with CogEval},
  author = {Momennejad, Ida and Hasanbeig, Hosein and Vieira, Felipe and Sharma, Hiteshi and Ness, Robert Osazuwa and Jojic, Nebojsa and Palangi, Hamid and Larson, Jonathan},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.15129},
  url = {https://arxiv.org/abs/2309.15129},
  urldate = {2024-06-10},
  abstract = {Recently an influx of studies claim emergent cognitive abilities in large language models (LLMs). Yet, most rely on anecdotes, overlook contamination of training sets, or lack systematic Evaluation involving multiple tasks, control conditions, multiple iterations, and statistical robustness tests. Here we make two major contributions. First, we propose CogEval, a cognitive science-inspired protocol for the systematic evaluation of cognitive capacities in Large Language Models. The CogEval protocol can be followed for the evaluation of various abilities. Second, here we follow CogEval to systematically evaluate cognitive maps and planning ability across eight LLMs (OpenAI GPT-4, GPT-3.5-turbo-175B, davinci-003-175B, Google Bard, Cohere-xlarge-52.4B, Anthropic Claude-1-52B, LLaMA-13B, and Alpaca-7B). We base our task prompts on human experiments, which offer both established construct validity for evaluating planning, and are absent from LLM training sets. We find that, while LLMs show apparent competence in a few planning tasks with simpler structures, systematic evaluation reveals striking failure modes in planning tasks, including hallucinations of invalid trajectories and getting trapped in loops. These findings do not support the idea of emergent out-of-the-box planning ability in LLMs. This could be because LLMs do not understand the latent relational structures underlying planning problems, known as cognitive maps, and fail at unrolling goal-directed trajectories based on the underlying structure. Implications for application and future directions are discussed.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/DWRJCH46/Momennejad et al. - 2023 - Evaluating Cognitive Maps and Planning in Large La.pdf}
}

@article{monea_glitch_2023,
  title = {A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia},
  shorttitle = {A Glitch in the Matrix?},
  author = {Monea, Giovanni and Peyrard, Maxime and Josifoski, Martin and Chaudhary, Vishrav and Eisner, Jason and K{\i}c{\i}man, Emre and Palangi, Hamid and Patra, Barun and West, Robert},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.02073},
  url = {https://arxiv.org/abs/2312.02073},
  urldate = {2024-02-11},
  abstract = {Large language models (LLMs) have demonstrated impressive capabilities in storing and recalling factual knowledge, but also in adapting to novel in-context information. Yet, the mechanisms underlying their in-context grounding remain unknown, especially in situations where in-context information contradicts factual knowledge embedded in the parameters. This is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify the outdated parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the parametric knowledge clashes with the in-context information. We benchmark various LLMs with Fakepedia and discover that GPT-4-turbo has a strong preference for its parametric knowledge. Mistral-7B, on the contrary, is the model that most robustly chooses the grounded answer. Then, we conduct causal mediation analysis on LLM components when answering Fakepedia queries. We demonstrate that inspection of the computational graph alone can predict LLM grounding with 92.8\% accuracy, especially because few MLPs in the Transformer can predict non-grounded behavior. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,fact,in-context,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/EDFQMNDW/Monea et al. - 2023 - A Glitch in the Matrix Locating and Detecting Lan.pdf}
}

@article{morcos_importance_2018,
  title = {On the importance of single directions for generalization},
  author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
  year = {2018},
  month = may,
  journal = {ICLR},
  eprint = {1803.06959},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1803.06959},
  url = {http://arxiv.org/abs/1803.06959},
  urldate = {2024-03-19},
  abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/JLJPB4WN/Morcos et al. - 2018 - On the importance of single directions for general.pdf}
}

@article{morwani_feature_2023,
  title = {Feature emergence via margin maximization: case studies in algebraic tasks},
  shorttitle = {Feature emergence via margin maximization},
  author = {Morwani, Depen and Edelman, Benjamin L. and Oncescu, Costin-Andrei and Zhao, Rosie and Kakade, Sham},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2311.07568},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2311.07568},
  urldate = {2024-02-01},
  abstract = {Understanding the internal representations learned by neural networks is a cornerstone challenge in the science of machine learning. While there have been significant recent strides in some cases towards understanding how neural networks implement specific target functions, this paper explores a complementary question -- why do networks arrive at particular computational strategies? Our inquiry focuses on the algebraic learning tasks of modular addition, sparse parities, and finite group operations. Our primary theoretical findings analytically characterize the features learned by stylized neural networks for these algebraic tasks. Notably, our main technique demonstrates how the principle of margin maximization alone can be used to fully specify the features learned by the network. Specifically, we prove that the trained networks utilize Fourier features to perform modular addition and employ features corresponding to irreducible group-theoretic representations to perform compositions in general groups, aligning closely with the empirical observations of Nanda et al. and Chughtai et al. More generally, we hope our techniques can help to foster a deeper understanding of why neural networks adopt specific computational strategies.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,theory,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/9QLRD2R2/Morwani et al. - 2023 - Feature emergence via margin maximization case st.pdf}
}

@article{moschella_relative_2023,
  title = {Relative representations enable zero-shot latent space communication},
  author = {Moschella, Luca and Maiorca, Valentino and Fumero, Marco and Norelli, Antonio and Locatello, Francesco and Rodol{\`a}, Emanuele},
  year = {2023},
  month = mar,
  journal = {ICLR},
  eprint = {2209.15430},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.15430},
  url = {http://arxiv.org/abs/2209.15430},
  urldate = {2024-06-10},
  abstract = {Neural networks embed the geometric structure of a data manifold lying in a high-dimensional space into latent representations. Ideally, the distribution of the data points in the latent space should depend only on the task, the data, the loss, and other architecture-specific constraints. However, factors such as the random weights initialization, training hyperparameters, or other sources of randomness in the training phase may induce incoherent latent spaces that hinder any form of reuse. Nevertheless, we empirically observe that, under the same data and modeling choices, the angles between the encodings within distinct latent spaces do not change. In this work, we propose the latent similarity between each sample and a fixed set of anchors as an alternative data representation, demonstrating that it can enforce the desired invariances without any additional training. We show how neural architectures can leverage these relative representations to guarantee, in practice, invariance to latent isometries and rescalings, effectively enabling latent space communication: from zero-shot model stitching to latent space comparison between diverse settings. We extensively validate the generalization capability of our approach on different datasets, spanning various modalities (images, text, graphs), tasks (e.g., classification, reconstruction) and architectures (e.g., CNNs, GCNs, transformers).},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/BU47KCT7/Moschella et al. - 2023 - Relative representations enable zero-shot latent s.pdf}
}

@article{mousi_can_2023,
  title = {Can LLMs facilitate interpretation of pre-trained language models?},
  author = {Mousi, Basel and Durrani, Nadir and Dalvi, Fahim},
  year = {2023},
  journal = {EMNLP},
  doi = {10.48550/ARXIV.2305.13386},
  url = {https://arxiv.org/abs/2305.13386},
  urldate = {2023-12-14},
  abstract = {Work done to uncover the knowledge encoded within pre-trained language models rely on annotated corpora or human-in-the-loop methods. However, these approaches are limited in terms of scalability and the scope of interpretation. We propose using a large language model, ChatGPT, as an annotator to enable fine-grained interpretation analysis of pre-trained language models. We discover latent concepts within pre-trained language models by applying agglomerative hierarchical clustering over contextualized representations and then annotate these concepts using ChatGPT. Our findings demonstrate that ChatGPT produces accurate and semantically richer annotations compared to human-annotated concepts. Additionally, we showcase how GPT-based annotations empower interpretation analysis methodologies of which we demonstrate two: probing frameworks and neuron interpretation. To facilitate further exploration and experimentation in the field, we make available a substantial ConceptNet dataset (TCN) comprising 39,000 annotated concepts.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GHMGVRMS/Mousi et al. - 2023 - Can LLMs facilitate interpretation of pre-trained .pdf}
}

@article{mu_compositional_2020,
  title = {Compositional Explanations of Neurons},
  author = {Mu, Jesse and Andreas, Jacob},
  year = {2020},
  month = jun,
  journal = {NeurIPS},
  eprint = {2006.14032},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2006.14032},
  url = {http://arxiv.org/abs/2006.14032},
  urldate = {2023-09-18},
  abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/IURDBMPN/Mu and Andreas - 2020 - Compositional Explanations of Neurons.pdf}
}

@article{murdoch_definitions_2019,
  title = {Definitions, methods, and applications in interpretable machine learning},
  author = {Murdoch, W. James and Singh, Chandan and Kumbier, Karl and {Abbasi-Asl}, Reza and Yu, Bin},
  year = {2019},
  month = oct,
  journal = {PNAS},
  url = {https://pnas.org/doi/full/10.1073/pnas.1900654116},
  urldate = {2023-08-27},
  abstract = {The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work.           ,              Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZMV3F8E2/Murdoch et al. - 2019 - Definitions, methods, and applications in interpre.pdf}
}

@article{murphy_interpretability_2022,
  title = {Interpretability's Alignment-Solving Potential: Analysis of 7 Scenarios},
  shorttitle = {Interpretability's Alignment-Solving Potential},
  author = {Murphy, Evan R.},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/FrFZjkdRsmsbnQEm8/interpretability-s-alignment-solving-potential-analysis-of-7},
  urldate = {2023-11-30},
  abstract = {This is the second post in the sequence ``Interpretability Research for the Most Important Century''. The first post, which introduces the sequence, de{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/47JQ3EH2/Murphy - 2022 - Interpretabilitys Alignment-Solving Potential An.html}
}

@article{muttenthaler_human_2023,
  title = {Human alignment of neural network representations},
  author = {Muttenthaler, Lukas and Dippel, Jonas and Linhardt, Lorenz and Vandermeulen, Robert A. and Kornblith, Simon},
  year = {2023},
  month = apr,
  journal = {ICLR},
  eprint = {2211.01201},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2211.01201},
  url = {http://arxiv.org/abs/2211.01201},
  urldate = {2023-11-29},
  abstract = {Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concepts such as food and animals are well-represented by neural networks whereas others such as royal or sports-related objects are not. Overall, although models trained on larger, more diverse datasets achieve better alignment with humans than models trained on ImageNet alone, our results indicate that scaling alone is unlikely to be sufficient to train neural networks with conceptual representations that match those used by humans.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HRTI9TPQ/Muttenthaler et al. - 2023 - Human alignment of neural network representations.pdf}
}

@article{nainani_evaluating_2024,
  title = {Evaluating Brain-Inspired Modular Training in Automated Circuit Discovery for Mechanistic Interpretability},
  author = {Nainani, Jatin},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.03646},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.03646},
  url = {http://arxiv.org/abs/2401.03646},
  urldate = {2024-01-16},
  abstract = {Large Language Models (LLMs) have experienced a rapid rise in AI, changing a wide range of applications with their advanced capabilities. As these models become increasingly integral to decision-making, the need for thorough interpretability has never been more critical. Mechanistic Interpretability offers a pathway to this understanding by identifying and analyzing specific sub-networks or 'circuits' within these complex systems. A crucial aspect of this approach is Automated Circuit Discovery, which facilitates the study of large models like GPT4 or LLAMA in a feasible manner. In this context, our research evaluates a recent method, Brain-Inspired Modular Training (BIMT), designed to enhance the interpretability of neural networks. We demonstrate how BIMT significantly improves the efficiency and quality of Automated Circuit Discovery, overcoming the limitations of manual methods. Our comparative analysis further reveals that BIMT outperforms existing models in terms of circuit quality, discovery time, and sparsity. Additionally, we provide a comprehensive computational analysis of BIMT, including aspects such as training duration, memory allocation requirements, and inference speed. This study advances the larger objective of creating trustworthy and transparent AI systems in addition to demonstrating how well BIMT works to make neural networks easier to understand.},
  archiveprefix = {arxiv},
  keywords = {mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/ZAKGRT5P/Nainani - 2024 - Evaluating Brain-Inspired Modular Training in Auto.pdf}
}

@article{nakkiran_sgd_2019,
  title = {SGD on Neural Networks Learns Functions of Increasing Complexity},
  author = {Nakkiran, Preetum and Kaplun, Gal and Kalimeris, Dimitris and Yang, Tristan and Edelman, Benjamin L. and Zhang, Fred and Barak, Boaz},
  year = {2019},
  month = may,
  journal = {NeurIPS},
  eprint = {1905.11604},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.11604},
  url = {http://arxiv.org/abs/1905.11604},
  urldate = {2023-11-10},
  abstract = {We perform an experimental study of the dynamics of Stochastic Gradient Descent (SGD) in learning deep neural networks for several real and synthetic classification tasks. We show that in the initial epochs, almost all of the performance improvement of the classifier obtained by SGD can be explained by a linear classifier. More generally, we give evidence for the hypothesis that, as iterations progress, SGD learns functions of increasing complexity. This hypothesis can be helpful in explaining why SGD-learned classifiers tend to generalize well even in the over-parameterized regime. We also show that the linear classifier learned in the initial stages is "retained" throughout the execution even if training is continued to the point of zero training error, and complement this with a theoretical result in a simplified model. Key to our work is a new measure of how well one classifier explains the performance of another, based on conditional mutual information.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/TKY7QKT6/Nakkiran et al. - 2019 - SGD on Neural Networks Learns Functions of Increas.pdf}
}

@article{nanda_200algorithm_2023,
  title = {200 COP in MI: Interpreting Algorithmic Problems},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2022},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/ejtFsvyhRkMofKAFy/200-cop-in-mi-interpreting-algorithmic-problems},
  urldate = {2024-03-01},
  abstract = {This is the fourth post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you w{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/FKDCEJPG/Nanda - 2022 - 200 COP in MI Interpreting Algorithmic Problems.html}
}

@article{nanda_200circuits_2022,
  title = {200 COP in MI: Looking for Circuits in the Wild},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2022},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/XNjRwEX9kxbpzWFWd/200-cop-in-mi-looking-for-circuits-in-the-wild},
  urldate = {2024-03-01},
  abstract = {This is the third post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you wa{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/94D2PBPQ/Nanda - 2022 - 200 COP in MI Looking for Circuits in the Wild.html}
}

@article{nanda_200dynamics_2022,
  title = {200 COP in MI: Analysing Training Dynamics},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2022},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/hHaXzJQi6SKkeXzbg/200-cop-in-mi-analysing-training-dynamics},
  urldate = {2024-03-01},
  abstract = {This is the sixth post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you wa{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/Z3GZVG68/Nanda - 2022 - 200 COP in MI Analysing Training Dynamics.html}
}

@article{nanda_200features_2022,
  title = {200 COP in MI: Studying Learned Features in Language Models},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2022},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/Qup9gorqpd9qKAEav/200-cop-in-mi-studying-learned-features-in-language-models},
  urldate = {2024-03-01},
  abstract = {This is the final post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you wa{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/4DUXDWE4/Nanda - 2022 - 200 COP in MI Studying Learned Features in Langua.html}
}

@article{nanda_200intro_2022,
  title = {200 Concrete Open Problems in Mechanistic Interpretability: Introduction},
  shorttitle = {200 Concrete Open Problems in Mechanistic Interpretability},
  author = {Nanda, Neel},
  year = {2022},
  month = dec,
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability},
  urldate = {2024-03-01},
  abstract = {This is the first post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~If you want to learn the basics before you th{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/HHH7JI7F/Nanda - 2022 - 200 Concrete Open Problems in Mechanistic Interpre.html}
}

@article{nanda_200RL_2023,
  title = {200 COP in MI: Interpreting Reinforcement Learning},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2023},
  month = jan,
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/eqvvDM25MXLGqumnf/200-cop-in-mi-interpreting-reinforcement-learning},
  urldate = {2024-03-01},
  abstract = {This is the ninth post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you wa{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/8CXYME65/Nanda - 2023 - 200 COP in MI Interpreting Reinforcement Learning.html}
}

@article{nanda_200superposition_2023,
  title = {200 COP in MI: Exploring Polysemanticity and Superposition},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2023},
  month = jan,
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/o6ptPu7arZrqRCxyz/200-cop-in-mi-exploring-polysemanticity-and-superposition},
  urldate = {2024-03-01},
  abstract = {Important Note: Since writing this, there's been a lot of exciting work on understanding superposition via training sparse autoencoders to take featu{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/JVT33JIQ/Nanda - 2023 - 200 COP in MI Exploring Polysemanticity and Super.html}
}

@article{nanda_200tool_2023,
  title = {200 COP in MI: Techniques, Tooling and Automation},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2023},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/btasQF7wiCYPsr5qw/200-cop-in-mi-techniques-tooling-and-automation},
  urldate = {2024-03-01},
  abstract = {This is the seventh post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/TGVH9J4H/Nanda - 2023 - 200 COP in MI Techniques, Tooling and Automation.html}
}

@article{nanda_200toylanguage_2022,
  title = {200 COP in MI: The Case for Analysing Toy Language Models},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2022},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/GWCgZrzWCZCuzGktv/200-cop-in-mi-the-case-for-analysing-toy-language-models},
  urldate = {2024-03-01},
  abstract = {This is the second post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you w{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/ANL5Y4IP/Nanda - 2022 - 200 COP in MI The Case for Analysing Toy Language.html}
}

@article{nanda_200vision_2023,
  title = {200 COP in MI: Image Model Interpretability},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = {2023},
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/caMoe6yNfXcaCG2u3/200-cop-in-mi-image-model-interpretability},
  urldate = {2024-03-01},
  abstract = {This is the eighth post in a sequence called 200 Concrete Open Problems in Mechanistic Interpretability.~Start here, then read in any order. If you w{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/ZZFRY4NY/Nanda - 2023 - 200 COP in MI Image Model Interpretability.html}
}

@article{nanda_actually_2023,
  title = {Actually, Othello-GPT Has A Linear Emergent World Representation},
  author = {Nanda, Neel},
  year = {2023},
  month = mar,
  journal = {Neel Nanda's Blog},
  url = {https://neelnanda.io/mechanistic-interpretability/othello},
  keywords = {cited,empirical,feature,linearity,mechinterp,to cite,to extract figures,to extract related work,to review in detail,world models},
  file = {/Users/leonardbereska/Zotero/storage/IGMG4Y3D/Nanda - 2023 - Actually, Othello-GPT Has A Linear Emergent World .html}
}

@article{nanda_attribution_2023,
  title = {Attribution Patching: Activation Patching At Industrial Scale},
  shorttitle = {Attribution Patching},
  author = {Nanda, Neel},
  year = {2023},
  month = feb,
  journal = {Neel Nanda's Blog},
  url = {https://www.neelnanda.io/mechanistic-interpretability/attribution-patching},
  urldate = {2023-11-10},
  abstract = {A write-up of an incomplete project I worked on at Anthropic in early 2022, using gradient-based approximation to make activation patching far more scalable},
  language = {en-US},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/RJHIRS7B/Nanda - 2023 - Attribution Patching Activation Patching At Indus.html}
}

@article{nanda_comprehensive_2022,
  title = {A Comprehensive Mechanistic Interpretability Explainer \& Glossary},
  author = {Nanda, Neel},
  year = {2022},
  month = dec,
  journal = {Neel Nanda's Blog},
  url = {https://www.neelnanda.io/mechanistic-interpretability/glossary},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/NNNICWQB/Nanda - 2022 - A Comprehensive Mechanistic Interpretability Expla.html}
}

@article{nanda_emergent_2023,
  title = {Emergent Linear Representations in World Models of Self-Supervised Sequence Models},
  author = {Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  year = {2023},
  month = sep,
  journal = {BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  eprint = {2309.00941},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.00941},
  url = {http://arxiv.org/abs/2309.00941},
  urldate = {2023-11-08},
  abstract = {How do sequence models represent their decision-making process? Prior work suggests that Othello-playing neural network learned nonlinear models of the board state (Li et al., 2023). In this work, we provide evidence of a closely related linear representation of the board. In particular, we show that probing for "my colour" vs. "opponent's colour" may be a simple yet powerful way to interpret the model's internal state. This precise understanding of the internal representations allows us to control the model's behaviour with simple vector arithmetic. Linear representations enable significant interpretability progress, which we demonstrate with further exploration of how the world model is computed.},
  archiveprefix = {arxiv},
  keywords = {cited,linearity,mechinterp,to cite,to extract related work,world models},
  file = {/Users/leonardbereska/Zotero/storage/DNHDAKWG/Nanda et al. - 2023 - Emergent Linear Representations in World Models of.pdf}
}

@article{nanda_fact_2023,
  title = {Fact finding: Attempting to reverse-engineer factual recall on the neuron level},
  shorttitle = {Fact finding},
  author = {Nanda, Neel and Rajamanoharan, S. and Kram{\'a}r, J. and Shah, R.},
  year = {2023},
  journal = {AI Alignment Forum, 2023c. URL https://www. alignmentforum. org/posts/iGuwZTHWb6DFY3sKB/fact-finding-attempting-to-reverse-engineer-factual-recall},
  pages = {19},
  url = {https://scholar.google.com/scholar?cluster=9546235215420404405&hl=en&oi=scholarr},
  urldate = {2024-06-10}
}

@article{nanda_how_2023,
  title = {How to Think About Activation Patching},
  author = {Nanda, Neel},
  year = {2023},
  month = apr,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/xh85KbTFhbCz7taD4/how-to-think-about-activation-patching},
  urldate = {2024-01-22},
  abstract = {This is an excerpt from my post on attribution patching, that I think is of more general interest, around how to think about the technique of activat{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/W2CSJENV/Nanda - 2023 - How to Think About Activation Patching.html}
}

@article{nanda_longlist_2022,
  title = {A Longlist of Theories of Impact for Interpretability},
  author = {Nanda, Neel},
  year = {2022},
  month = mar,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/uK6sQCNMw8WKzJeCQ/a-longlist-of-theories-of-impact-for-interpretability},
  urldate = {2023-11-29},
  abstract = {I hear a lot of different arguments floating around for exactly how mechanistically interpretability research will reduce x-risk. As an interpretabil{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5DACS556/Nanda - 2022 - A Longlist of Theories of Impact for Interpretabil.html}
}

@article{nanda_mechanistic_2023,
  title = {Mechanistic Interpretability Quickstart Guide},
  author = {Nanda, Neel},
  year = {2023},
  month = jan,
  journal = {Neel Nanda's Blog},
  url = {https://www.neelnanda.io/mechanistic-interpretability/quickstart},
  urldate = {2023-11-30},
  abstract = {An intro guide to a mechanistic interpretability weekend hackathon},
  language = {en-US},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/2BILVKNI/Nanda - 2023 - Mechanistic Interpretability Quickstart Guide.html}
}

@article{nanda_progress_2023,
  title = {Progress measures for grokking via mechanistic interpretability},
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  year = {2023},
  month = jan,
  journal = {ICLR},
  eprint = {2301.05217},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2301.05217},
  url = {http://arxiv.org/abs/2301.05217},
  urldate = {2023-08-26},
  abstract = {Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous {\textbackslash}textit\{progress measures\} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.},
  archiveprefix = {arxiv},
  keywords = {algorithms,cited,dynamics,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/BI63JRD9/Nanda et al. - 2023 - Progress measures for grokking via mechanistic int.pdf}
}

@misc{nanda_transformerlens_2022,
  title = {TransformerLens},
  author = {Nanda, Neel},
  year = {2022},
  url = {https://github. com/neelnanda-io/TransformerLens},
  keywords = {cited}
}

@article{nanfack_adversarial_2023,
  title = {Adversarial Attacks on the Interpretation of Neuron Activation Maximization},
  author = {Nanfack, Geraldin and Fulleringer, Alexander and Marty, Jonathan and Eickenberg, Michael and Belilovsky, Eugene},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.07397},
  url = {https://arxiv.org/abs/2306.07397},
  urldate = {2024-02-11},
  abstract = {The internal functional behavior of trained Deep Neural Networks is notoriously difficult to interpret. Activation-maximization approaches are one set of techniques used to interpret and analyze trained deep-learning models. These consist in finding inputs that maximally activate a given neuron or feature map. These inputs can be selected from a data set or obtained by optimization. However, interpretability methods may be subject to being deceived. In this work, we consider the concept of an adversary manipulating a model for the purpose of deceiving the interpretation. We propose an optimization framework for performing this manipulation and demonstrate a number of ways that popular activation-maximization interpretation techniques associated with CNNs can be manipulated to change the interpretations, shedding light on the reliability of these methods.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {adversarial,not cited,to extract related work,visualization},
  file = {/Users/leonardbereska/Zotero/storage/5MBYS2FC/Nanfack et al. - 2023 - Adversarial Attacks on the Interpretation of Neuro.pdf}
}

@article{nanfack_adversarial_2024,
  title = {Adversarial Attacks on the Interpretation of Neuron Activation Maximization},
  author = {Nanfack, Geraldin and Fulleringer, Alexander and Marty, Jonathan and Eickenberg, Michael and Belilovsky, Eugene},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {5},
  pages = {4315--4324},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i5.28228},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/28228},
  urldate = {2024-06-10},
  abstract = {Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/MI379E4Y/Nanfack et al. - 2024 - Adversarial Attacks on the Interpretation of Neuro.pdf}
}

@article{nardo_remarks_2023,
  title = {Remarks 1--18 on GPT (compressed)},
  author = {Nardo, Cleo},
  year = {2023},
  month = mar,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/7qSHKYRnqyrumEfbt/remarks-1-18-on-gpt-compressed},
  urldate = {2023-05-15},
  abstract = {Status: Highly-compressed insights about LLMs. Includes exercises. Remark 3 and Remark 15 are the most important and entirely self-contained. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/6H5GBZFR/Nardo - 2023 - Remarks 118 on GPT (compressed).html}
}

@article{nardo_waluigi_2023,
  title = {The Waluigi Effect (mega-post)},
  author = {Nardo, Cleo},
  year = {2023},
  month = may,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post},
  urldate = {2023-05-15},
  abstract = {Everyone carries a shadow, and the less it is embodied in the individual's conscious life, the blacker and denser it is. --- Carl Jung {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TPDE32UY/Nardo - 2023 - The Waluigi Effect (mega-post).html}
}

@article{nauta_anecdotal_2023,
  title = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
  shorttitle = {From Anecdotal Evidence to Quantitative Evaluation Methods},
  author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl{\"o}tterer, J{\"o}rg and Van Keulen, Maurice and Seifert, Christin},
  year = {2023},
  month = dec,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {13s},
  pages = {1--42},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3583558},
  url = {https://dl.acm.org/doi/10.1145/3583558},
  urldate = {2023-10-22},
  abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HUC7R63G/Nauta et al. - 2023 - From Anecdotal Evidence to Quantitative Evaluation.pdf}
}

@article{neo_interpreting_2024,
  title = {Interpreting Context Look-ups in Transformers: Investigating Attention-MLP Interactions},
  shorttitle = {Interpreting Context Look-ups in Transformers},
  author = {Neo, Clement and Cohen, Shay B. and Barez, Fazl},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.15055},
  url = {https://arxiv.org/abs/2402.15055},
  urldate = {2024-06-10},
  abstract = {In this paper, we investigate the interplay between attention heads and specialized "next-token" neurons in the Multilayer Perceptron that predict specific tokens. By prompting an LLM like GPT-4 to explain these model internals, we can elucidate attention mechanisms that activate certain next-token neurons. Our analysis identifies attention heads that recognize contexts relevant to predicting a particular token, activating the associated neuron through the residual connection. We focus specifically on heads in earlier layers consistently activating the same next-token neuron across similar prompts. Exploring these differential activation patterns reveals that heads that specialize for distinct linguistic contexts are tied to generating certain tokens. Overall, our method combines neural explanations and probing isolated components to illuminate how attention enables context-dependent, specialized processing in LLMs.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/EI8APE8K/Neo et al. - 2024 - Interpreting Context Look-ups in Transformers Inv.pdf}
}

@article{ngo_agi_2020,
  title = {AGI safety from first principles},
  shorttitle = {AGI safety from first principles},
  author = {Ngo, Richard},
  year = {2020},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/8xRSjC76HasLnMGSf/agi-safety-from-first-principles-introduction},
  urldate = {2024-02-13},
  abstract = {This is the first part of a six-part report called AGI safety from first principles, in which I've attempted to put together the most complete and co{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VYVVKLL2/Ngo - 2020 - AGI safety from first principles.html}
}

@article{ngo_alignment_2022,
  title = {The alignment problem from a deep learning perspective},
  author = {Ngo, Richard and Chan, Lawrence and Mindermann, S{\"o}ren},
  year = {2022},
  month = dec,
  journal = {CoRR},
  eprint = {2209.00626},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.00626},
  url = {http://arxiv.org/abs/2209.00626},
  urldate = {2023-02-09},
  abstract = {Within the coming decades, artificial general intelligence (AGI) may surpass human capabilities at a wide range of important tasks. We outline a case for expecting that, without substantial effort to prevent it, AGIs could learn to pursue goals which are very undesirable (in other words, misaligned) from a human perspective. We argue that AGIs trained in similar ways as today's most capable models could learn to act deceptively to receive higher reward; learn internally-represented goals which generalize beyond their training distributions; and pursue those goals using power-seeking strategies. We outline how the deployment of misaligned AGIs might irreversibly undermine human control over the world, and briefly review research directions aimed at preventing these problems.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/7M6LPWQE/Ngo et al. - 2022 - The alignment problem from a deep learning perspec.pdf}
}

@article{ngo_visualizing_2023,
  title = {Visualizing the deep learning revolution},
  author = {Ngo, Richard},
  year = {2023},
  month = may,
  journal = {Medium},
  url = {https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5},
  urldate = {2024-02-13},
  abstract = {The field of AI has undergone a revolution over the last decade, driven by the success of deep learning techniques. This post aims to{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9257PGD6/Ngo - 2023 - Visualizing the deep learning revolution.html}
}

@article{nguyen_origins_2022a,
  title = {On the Origins of the Block Structure Phenomenon in Neural Network Representations},
  author = {Nguyen, Thao and Raghu, Maithra and Kornblith, Simon},
  year = {2022},
  month = feb,
  journal = {CoRR},
  eprint = {2202.07184},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2202.07184},
  urldate = {2024-01-05},
  abstract = {Recent work has uncovered a striking phenomenon in large-capacity neural networks: they contain blocks of contiguous hidden layers with highly similar representations. This block structure has two seemingly contradictory properties: on the one hand, its constituent layers exhibit highly similar dominant first principal components (PCs), but on the other hand, their representations, and their common first PC, are highly dissimilar across different random seeds. Our work seeks to reconcile these discrepant properties by investigating the origin of the block structure in relation to the data and training methods. By analyzing properties of the dominant PCs, we find that the block structure arises from dominant datapoints - a small group of examples that share similar image statistics (e.g. background color). However, the set of dominant datapoints, and the precise shared image statistic, can vary across random seeds. Thus, the block structure reflects meaningful dataset statistics, but is simultaneously unique to each model. Through studying hidden layer activations and creating synthetic datapoints, we demonstrate that these simple image statistics dominate the representational geometry of the layers inside the block structure. We explore how the phenomenon evolves through training, finding that the block structure takes shape early in training, but the underlying representations and the corresponding dominant datapoints continue to change substantially. Finally, we study the interplay between the block structure and different training mechanisms, introducing a targeted intervention to eliminate the block structure, as well as examining the effects of pretraining and Shake-Shake regularization.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FCY2JF3Z/Nguyen et al. - 2022 - On the Origins of the Block Structure Phenomenon i.pdf}
}

@article{nguyen_survey_2022,
  title = {A Survey of Machine Unlearning},
  author = {Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2209.02299},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.02299},
  url = {http://arxiv.org/abs/2209.02299},
  urldate = {2023-11-09},
  abstract = {Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5A7D2VZC/Nguyen et al. - 2022 - A Survey of Machine Unlearning.pdf}
}

@article{nichani_how_2024,
  title = {How Transformers Learn Causal Structure with Gradient Descent},
  author = {Nichani, Eshaan and Damian, Alex and Lee, Jason D.},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.14735},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.2402.14735},
  url = {http://arxiv.org/abs/2402.14735},
  urldate = {2024-03-04},
  abstract = {The incredible success of transformers on sequence modeling tasks can be largely attributed to the self-attention mechanism, which allows information to be transferred between different parts of a sequence. Self-attention allows transformers to encode causal structure which makes them particularly suitable for sequence modeling. However, the process by which transformers learn such causal structure via gradient-based training algorithms remains poorly understood. To better understand this process, we introduce an in-context learning task that requires learning latent causal structure. We prove that gradient descent on a simplified two-layer transformer learns to solve this task by encoding the latent causal graph in the first attention layer. The key insight of our proof is that the gradient of the attention matrix encodes the mutual information between tokens. As a consequence of the data processing inequality, the largest entries of this gradient correspond to edges in the latent causal graph. As a special case, when the sequences are generated from in-context Markov chains, we prove that transformers learn an induction head (Olsson et al., 2022). We confirm our theoretical findings by showing that transformers trained on our in-context learning task are able to recover a wide variety of causal structures.},
  archiveprefix = {arxiv},
  keywords = {graph},
  file = {/Users/leonardbereska/Zotero/storage/DIXMQSM4/Nichani et al. - 2024 - How Transformers Learn Causal Structure with Gradi.pdf}
}

@article{nicholaskees_cyborgism_,
  title = {Cyborgism},
  author = {NicholasKees and {janus}},
  year = {2023},
  month = oct,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/bxt7uCiHam4QXrQAA/cyborgism},
  urldate = {2023-05-17},
  abstract = {Thanks to Garrett Baker, David Udell, Alex Gray, Paul Colognese, Akash Wasil, Jacques Thibodeau, Michael Ivanitskiy, Zach Stein-Perlman, and Anish Upadhayay for feedback on drafts, as well as Scott V{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/524I77CN/NicholasKees and janus - 2023 - Cyborgism.html}
}

@article{nicholaskees_searching_2022,
  title = {Searching for Search},
  author = {NicholasKees and {janus}},
  year = {2022},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/FDjTgDcGPc7B98AES/searching-for-search-4},
  urldate = {2023-11-29},
  abstract = {Thanks to Dan Braun, Ze Shen Chin, Paul Colognese, Michael Ivanitskiy, Sudhanshu Kasewa, and Lucas Teixeira for feedback on drafts. {\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PZS7LHFF/NicholasKees and janus - 2022 - Searching for Search.html}
}

@article{nicholaskross_why_2023,
  title = {Why and When Interpretability Work is Dangerous},
  author = {Kross, Nicholas},
  year = {2023},
  month = may,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/uhMRgEXabYbWeLc6T/why-and-when-interpretability-work-is-dangerous},
  urldate = {2023-12-05},
  abstract = {Many new researchers are going into AI alignment. For a variety of reasons, they may choose to work for organizations such as Anthropic or OpenAI. Chances are good that a new researcher will be interested in "interpretability". A creeping concern for many: "Is my research going to cause AGI ruin? Am I making the most powerful AI systems more powerful, even though I'm trying to make them safer?" Maybe they've even heard someone say that "mechanistic interpretability is capabilities research". This essay dissects the specific case of interpretability research, to figure out when it does more harm than good. What is Interpretability? How do neural networks "think"? When you input some tokens into ChatGPT, how exactly does it decide what the best next-token is? What attributes of human language does it keep track of, and how does it do so? Interpretability is the sub-area of AI research that tackles these sorts of question. Interpretability can be likened to investigating human psychology from a "bottom-up" approach of observing neurons, cortex structures, neurotransmitters, and similar low-level entities. This focus on the mechanics of a mind's "substrate" (whether in biological neurons or in artificial neural networks) has obvious strengths, yet subtler weaknesses we'll explore later. One example of interpretability work is the recent "neurons" work by OpenAI. In their paper, "Language models can explain neurons in language models", they tell GPT-4 to write explanations of the individual neurons within the smaller GPT-2 model. The idea is to gain human-readable understanding of what a large language model (LLM) is thinking by seeing which neurons correspond to which output-components. So one neuron's activation-state may correspond to the presence of fractions, while another codes for times of day. We also have research such as "Progress measures for grokking via mechanistic interpretability". In this paper, the authors first train a neural network to perform a math operation (whose answer is easily checkable). Then, they analyze the resulting network's structure to reverse-engineer the algorithm it "learned" to use. While human mathematicians and engineers have developed their own ways to solve the math problem (addition modulo a prime number), the neural network eventually hit on its own nonstandard method for doing so. When is This Dangerous? I posit that interpretability work is "dangerous" when it enhances the overall capabilities of an AI system, without making that system more aligned with human goals. This tracks well with the increasingly-popular notion of "speeding up capabilities research VS speeding up alignment research". We prefer when our work counterfactually increases AI alignment, while not otherwise speeding up the development of AGI capabilities. The key fact about interpretability research, which determines its safety/usefulness under the above criteria, is whether it enhances human control over an AI system. This suggests a few concrete rules-of-thumb, which a researcher can apply to their interpretability project P: If P gives us a higher-resolution picture of an AI's thought patterns, without giving us a way to reliably change them, then P is dangerous interpretability research. If P is used to make a relied-on AI system less-powerful or less-general, yet safer for humans to use, then P is less dangerous. If P makes it easier/more efficient to train powerful AI models, then P is dangerous. (This would be similar to making every GPU on Earth 10x as energy-efficient or 10x as fast at its computations: clearly speeding up the development of dangerous capabilities.) If P is used in conjunction with, or as, a "steering" mechanism to control an AI's behavior, then P is less dangerous. The Sealed Interpretability Lab One thought experiment can show us the potential dangers of interpretability research in greater detail. This is based on a question I was asked by "woog" on Discord. Imagine a lab whose output is sealed off from the rest of the world. Its researchers can look at other public research, but they can't release anything learned at the lab. This lab's sole focus is AI interpretability, revealing ML systems' "thoughts" to human observers. The guiding question: If somebody works at this lab, are they speeding up capabilities research? One detail that helps answer this question, is what kinds of AI systems the lab is working with. If the lab can only work with existing ML models, such as ChatGPT, then we presume it cannot train its own models. This reduces the computing-power requirements of the lab, which already makes it unlikely to advance capabilities through "just scaling it up". However, its actions create knowledge that in general would speed up capabilities development, either internally or when sharing research with trusted partners. If the lab can create its own ML models, especially of a size comparable to the state-of-the-art LLMs, then it's likely to advance capabilities research. What happened here? If the interpretability-only lab can build large models, it can cause doom... but the same holds for merely working with existing large models? How can that be? If the "rules of thumb" noted above apply to most interpretability research, then interpretability research can easily end up making it easier to develop AI capabilities. This could make weaker models stronger, and strong models even more strong. So to make things truly safe, the interpretability-only lab can't work with the newest models... the ones being used in real life, and which are the most likely to be dangerous and deceptive. Toy-model research could be useless (since it's "easy to interpret" at a glance), large-model research could increase the dangerous capabilities of existing AI systems, and cutting-edge-model research itself speeds up capabilities progress. (As usual, the more powerful an AI system gets, the harder it is to align properly. Interpretability, without the steering mechanisms that are likely the core of AI alignment, doesn't help this.) It gets even worse from here: If the interpretability lab, as stated, never releases research, then it can't provide useful interpretability techniques to the top capabilities-increasing labs. On the other hand, if those labs are doing things besides alignment (which they currently are), they are likely to use the interpretability techniques to use their models more efficiently: If the interpretability work reveals problems with an ML model's thought patterns, we may or may not have easy ways to correct those thought patterns directly, rather than the outputs. If we do find ways to correct an AI's thought patterns, that would be progress on alignment (see "Just Retarget The Search" below). This could be verified (but not necessarily aided) by interpretability. If the interpretability work only reveals surface-level problems, it can leave a model's deeper malignant thought patterns untouched, while increasing the confidence placed in it by human operators. John Wentworth pointed out something sort-of-similar for the technique of Reinforcement Learning from Human Feedback (RLHF); the shallow easy-to-fix problems get fixed faster, while the deeper problems are hidden from view. Basically, interpretability research can get more capabilities out of current state-of-the-art (SOTA) models, and can guide the capabilities-training of future models. Another detail: How "sealed" is this interpretability lab? If the lab never releases any of its interpretability research to anybody, then no other AI developers can benefit from alignment-enhancing interpretability work. If the lab only releases its interpretability work to a few trusted top-level AI labs, those labs are likely to use the work to increase the capabilities of their models, without improving the "steer-ability" (see below) of them. As elaborated before, this can happen despite the intentions of the top-level labs. If the lab publicly releases its interpretability results for all to see, then all the above problems can spread to every other lab. We end up with a "damned if you do, damned if you don't" decision-tree. Each leaf can speed up capabilities through sharing, speed up capabilities through independent model-building, or waste the resources of alignment donors. Any interpretability-only research, can enhance the capabilities of existing models. Interpretability research, when mixed with capabilities research, advances capabilities overall. Progress on interpretability can easily be repurposed to use unaligned models more efficiently. This can be thought of as "increasing capabilities". With no progress on interpretability, the interpretability-only lab has no purpose. Now, given how detail-contingent many of these scenarios are, it's plausible that an organization could fix or avoid all of them. However, unless more of the top capabilities labs have info/exfohazard policies I'm not aware of, there's little evidence that these groups are optimizing against the breadth of failure modes described here. What Would the World Look Like, Otherwise? To get a better sense of whether interpretability work is dangerous, we can imagine conditions that would be true if they weren't dangerous. That is, in a world where interpretability work was accelerating alignment faster than capabilities (or was accelerating neither), what would we expect to see? There should be multiple competing schools of thought, giving different answers to "how do neural networks think?". When new interpretability research is released by a top lab, the results are held up as evidence for/against such answers. As interpretability research progresses, its techniques are adopted for use in the largest/most-important ML models. If OpenAI comes up with an interpretability method, that quickly gets used to make ChatGPT and Bing AI safer for users, even if it makes them less generally-capable. Interpretability tactics slow down, or don't impact the speed of, new advances in capabilities. Do we actually see these things in real life? While there are different research agendas for AI alignment, and multiple schools of thought for "how a mind works", they don't seem to be impacted much by new interpretability research. Some interpretability techniques are used in ML training. However, I am not aware of any time when information uncovered by an interpretability tool has led to a change-of-course or a deeper-alignment solution in a mainstream model. Capabilities continues to advance quickly, despite the growing work in interpretability. Either state-of-the-art models aren't built using new interpretability techniques, or they are (yet keep making mistakes and being hard-to-control), or they're helping in a way that's hard for outsiders to observe and verify. This is more of a point for "little/no impact", which isn't so bad. Overall, it looks like interpretability work is often ignored or not-very-useful in practice. This is a far cry from it being fully-dangerous, at least at present. Maybe it is helping alignment, but work on it is slow. (Interpretability has been around since at least 2018, but that may not be enough time for its work to bear fruit.) When Interpretability is Still Important I generally break down the problem of AI alignment into two subproblems: Steering cognition: Can we control the thought and behavior patterns of a powerful mind at all? This is the question behind the rocket alignment problem analogy. Currently, we have large, inscrutable neural networks that output increasingly-smart answers to given questions. We can't easily or reliably guide a neural network to avoid unwanted behaviors or thought patterns. Deciding/implanting values: Even if we can steer a powerful AI system to think and behave in safe/friendly ways, how do we then point it towards the best values for the future? This vein of research includes the concept of Coherent Extrapolated Volition, the value-loading part of the QACI alignment setup, and (in my view) the idea of moral uncertainty. If the Qualia Research Institute focused consistently on their mission to understand "what makes a being sentient at all?" and "what experiences will be positive or negative for sentient beings", their work would generally be on the values-side as well. It seems that interpretability work would be, not only helpful, but essential for the "steering cognition" subproblem. After all, if you cannot discern a boat's location, you would be hard-pressed to get "better" at steering it. The same is true for the internal mechanisms of artificial minds. If we can't tell what an AI system is "thinking", how do we know if we're really "in control"? However, you'll note that interpretability on its own does not solve either of the two difficult subproblems listed. If you're stuck in a self-driving car that's going to ram into a wall, having a more-accurate prediction of the impact-angle is not going to stop or steer the car out of harm's way. Nevertheless, "knowing when we're steering" could still be centrally-important for "solving steering". Wentworth's "Just Retarget The Search" essay shows us a potential instantiation of this idea. Imagine a day when interpretability tools are good enough to identify higher-level "modules" for general reasoning, search, and goal-directedness in AI systems. If these higher-level modules can be picked out, their data can then be rewritten so "target" what human want. This mostly or entirely solves the "steering cognition" subproblem. Under certain assumptions, such as the "natural abstraction hypothesis" being true (i.e. the aforementioned "modules" existing), this use of alignment would be quite safe and alignment-oriented. But this exception itself demonstrates why interpretability is not enough; some theoretical backing is likely still needed, so we can tell if we're "binding" the AI's behavior in full, or just one part of its cognition. Even if interpretability is essential, that does not preclude it from being dangerous in the ways described earlier. The Implications for P(doom) If AGI is developed by 2070, will it become uncontrollable by humans, in a way that causes an existential catastrophe? On its face, interpretability work is supposed to lower the odds of that occurring. As noted above, interpretability work can help us confirm the viability of alignment solutions for steering cognition. But it doesn't really give us those steering solutions, and it's unlikely to do so before a dangerous AGI system is developed. Reasonably, most interpretability work is at risk of increasing humanity's P(doom). In particular, the following criteria modulate the resulting risk-change: If interpretability research isn't tightly coupled to cognition-steering research, it could increase P(doom). If interpretability research is released to the public and/or top capabilities labs, it could increase P(doom). If interpretability research is either too low-level to help humans steer cognition (due to remaining inscrutability), or too surface-level/outputs-based to detect deeper misalignment with human objectives, it wouldn't decrease P(doom). If interpretability research continues to get more resources and researcher-manpower (or be a more-parallelizable use of those resources) than more-direct alignment research paths, it could increase P(doom) by competing with those paths. In closing, if alignment-conscious researchers continue going into the interpretability subfield, the probability of AGI ruin will tend to increase.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/7B33ZEEH/Kross - 2023 - Why and When Interpretability Work is Dangerous.html}
}

@article{nickyp_llm_2023,
  title = {LLM Modularity: The Separability of Capabilities in Large Language Models},
  shorttitle = {LLM Modularity},
  author = {NickyP},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/j84JhErNezMxyK4dH/llm-modularity-the-separability-of-capabilities-in-large},
  urldate = {2023-11-30},
  abstract = {Separating out different capabilities. Post format: First, a 30-second TL;DR, next a 5-minute summary, and finally the full {\textasciitilde}40-minute full length te{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LR5MJ56K/NickyP - 2023 - LLM Modularity The Separability of Capabilities i.html}
}

@article{nostalgebraist_interpreting_2020,
  title = {interpreting GPT: the logit lens},
  shorttitle = {interpreting GPT},
  author = {{nostalgebraist}},
  year = {2020},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
  urldate = {2023-11-10},
  abstract = {This post relates an observation I've made in my work with GPT-2, which I have not seen made elsewhere. {\dots}},
  language = {en},
  keywords = {cited,mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/7GUP3BIL/nostalgebraist - 2020 - interpreting GPT the logit lens.html}
}

@article{notsawo_predicting_2023,
  title = {Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok},
  shorttitle = {Predicting Grokking Long Before it Happens},
  author = {Notsawo, Pascal Jr. Tikeng and Zhou, Hattie and Pezeshki, Mohammad and Rish, Irina and Dumas, Guillaume},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.13253},
  url = {https://arxiv.org/abs/2306.13253},
  urldate = {2024-06-10},
  abstract = {This paper focuses on predicting the occurrence of grokking in neural networks, a phenomenon in which perfect generalization emerges long after signs of overfitting or memorization are observed. It has been reported that grokking can only be observed with certain hyper-parameters. This makes it critical to identify the parameters that lead to grokking. However, since grokking occurs after a large number of epochs, searching for the hyper-parameters that lead to it is time-consuming. In this paper, we propose a low-cost method to predict grokking without training for a large number of epochs. In essence, by studying the learning curve of the first few epochs, we show that one can predict whether grokking will occur later on. Specifically, if certain oscillations occur in the early epochs, one can expect grokking to occur if the model is trained for a much longer period of time. We propose using the spectral signature of a learning curve derived by applying the Fourier transform to quantify the amplitude of low-frequency components to detect the presence of such oscillations. We also present additional experiments aimed at explaining the cause of these oscillations and characterizing the loss landscape.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/MQKG7XA9/Notsawo et al. - 2023 - Predicting Grokking Long Before it Happens A look.pdf}
}

@article{oikarinen_clipdissect_2022,
  title = {CLIP-Dissect: Automatic Description of Neuron Representations in Deep Vision Networks},
  shorttitle = {CLIP-Dissect},
  author = {Oikarinen, Tuomas and Weng, Tsui-Wei},
  year = {2022},
  journal = {ICLR},
  eprint = {2204.10965},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2204.10965},
  url = {http://arxiv.org/abs/2204.10965},
  urldate = {2023-10-26},
  abstract = {In this paper, we propose CLIP-Dissect, a new technique to automatically describe the function of individual hidden neurons inside vision networks. CLIP-Dissect leverages recent advances in multimodal vision/language models to label internal neurons with open-ended concepts without the need for any labeled data or human examples. We show that CLIP-Dissect provides more accurate descriptions than existing methods for last layer neurons where the ground-truth is available as well as qualitatively good descriptions for hidden layer neurons. In addition, our method is very flexible: it is model agnostic, can easily handle new concepts and can be extended to take advantage of better multimodal models in the future. Finally CLIP-Dissect is computationally efficient and can label all neurons from five layers of ResNet-50 in just 4 minutes, which is more than 10 times faster than existing methods. Our code is available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. Finally, crowdsourced user study results are available at Appendix B to further support the effectiveness of our method.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QA3QJ56L/Oikarinen and Weng - 2022 - CLIP-Dissect Automatic Description of Neuron Repr.pdf}
}

@article{olah_building_2018,
  title = {The Building Blocks of Interpretability},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  year = {2018},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2018/building-blocks},
  urldate = {2023-05-31},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work,to review in detail,tool,vision,visualization},
  file = {/Users/leonardbereska/Zotero/storage/9WMCKGK4/Olah et al. - 2018 - The Building Blocks of Interpretability.html}
}

@article{olah_distributed_2023,
  title = {Distributed Representations: Composition \& Superposition},
  author = {Olah, Chris},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/superposition-composition/index.html},
  keywords = {foundational,mechinterp,not cited,superposition,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/5YMYB5SZ/Olah - 2023 - Distributed Representations Composition & Superpo.html}
}

@article{olah_feature_2017,
  title = {Feature Visualization},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  url = {https://distill.pub/2017/feature-visualization},
  urldate = {2023-05-31},
  keywords = {cited,feature,mechinterp,to cite,to extract figures,to extract related work,to review in detail,visualization},
  file = {/Users/leonardbereska/Zotero/storage/5TWUD884/Olah et al. - 2017 - Feature Visualization.html}
}

@article{olah_mechanistic_2022,
  title = {Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases},
  author = {Olah, Christopher},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/mech-interp-essay/index.html},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/ACFGP82J/Olah - 2022 - Mechanistic Interpretability, Variables, and the I.html}
}

@article{olah_overview_2020,
  title = {An Overview of Early Vision in InceptionV1},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = apr,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/early-vision},
  urldate = {2024-01-09},
  abstract = {An overview of all the neurons in the first five layers of InceptionV1, organized into a taxonomy of 'neuron groups.'},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NVA66ZC8/Olah et al. - 2020 - An Overview of Early Vision in InceptionV1.html}
}

@article{olah_reflections_2024,
  title = {Reflections on Qualitative Research},
  author = {Olah, Chris and Jermyn, Adam},
  year = {2024},
  month = mar,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/qualitative-essay/index.html},
  urldate = {2024-03-13},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/7D2AMR3H/index.html}
}

@article{olah_research_2017,
  title = {Research Debt},
  author = {Olah, Chris and Carter, Shan},
  year = {2017},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2017/research-debt},
  urldate = {2023-11-29},
  abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BMGCV42G/Olah and Carter - 2017 - Research Debt.html}
}

@article{olah_zoom_2020,
  title = {Zoom In: An Introduction to Circuits},
  shorttitle = {Zoom In},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/zoom-in},
  urldate = {2023-05-31},
  keywords = {circuit,cited,feature,foundational,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/NGSD97LT/Olah et al. - 2020 - Zoom In An Introduction to Circuits.html}
}

@article{olshausen_sparse_1997,
  title = {Sparse coding with an overcomplete basis set: a strategy employed by V1?},
  shorttitle = {Sparse coding with an overcomplete basis set},
  author = {Olshausen, B. A. and Field, D. J.},
  year = {1997},
  month = dec,
  journal = {Vision Res},
  volume = {37},
  number = {23},
  pages = {3311--3325},
  issn = {0042-6989},
  doi = {10.1016/s0042-6989(97)00169-7},
  url = {https://pubmed.ncbi.nlm.nih.gov/9425546/},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  language = {eng},
  pmid = {9425546},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/NJH8JATT/Olshausen and Field - 1997 - Sparse coding with an overcomplete basis set a st.pdf}
}

@article{olsson_incontext_2022,
  title = {In-context Learning and Induction Heads},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
  keywords = {circuit,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/9ZXSBRBA/Olsson et al. - 2022 - In-context Learning and Induction Heads.html}
}

@article{omahony_disentangling_2023,
  title = {Disentangling Neuron Representations with Concept Vectors},
  author = {O'Mahony, Laura and Andrearczyk, Vincent and Muller, Henning and Graziani, Mara},
  year = {2023},
  month = apr,
  journal = {CVPR Workshops},
  eprint = {2304.09707},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.09707},
  url = {http://arxiv.org/abs/2304.09707},
  urldate = {2023-09-18},
  abstract = {Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/VLHWMIRP/O'Mahony et al. - 2023 - Disentangling Neuron Representations with Concept .pdf}
}

@article{openai_aligning_2022,
  title = {Aligning language models to follow instructions},
  author = {OpenAI},
  year = {2022},
  journal = {OpenAI Blog},
  url = {https://openai.com/research/instruction-following},
  urldate = {2023-05-15},
  abstract = {We've trained language models that are much better at following user intentions than GPT-3 while also making them more truthful and less toxic, using techniques developed through our alignment research. These~InstructGPT~models, which are trained with humans in the loop, are now deployed as the default language models on our~API.},
  language = {en-US},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RZQCP6ZY/OpenAI - 2022 - Aligning language models to follow instructions.html}
}

@article{openai_introducing_2022,
  title = {Introducing ChatGPT},
  author = {OpenAI},
  year = {2022},
  journal = {OpenAI Blog},
  url = {https://openai.com/blog/chatgpt},
  urldate = {2023-05-17},
  abstract = {We've trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  language = {en-US},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5AHFPKH8/OpenAI - 2022 - Introducing ChatGPT.html}
}

@article{openai_our_2022,
  title = {Our approach to alignment research},
  author = {OpenAI},
  year = {2022},
  journal = {OpenAI Blog},
  url = {https://openai.com/blog/our-approach-to-alignment-research},
  urldate = {2023-05-17},
  abstract = {We are improving our AI systems' ability to learn from human feedback and to assist humans at evaluating AI. Our goal is to build a sufficiently aligned AI system that can help us solve all other alignment~problems.},
  language = {en-US},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/J3YAP993/OpenAI - 2022 - Our approach to alignment research.html}
}

@article{ortu_competition_2024,
  title = {Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals},
  shorttitle = {Competition of Mechanisms},
  author = {Ortu, Francesco and Jin, Zhijing and Doimo, Diego and Sachan, Mrinmaya and Cazzaniga, Alberto and Sch{\"o}lkopf, Bernhard},
  year = {2024},
  month = jun,
  journal = {ACL 2024},
  eprint = {2402.11655},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.11655},
  url = {http://arxiv.org/abs/2402.11655},
  urldate = {2024-06-10},
  abstract = {Interpretability research aims to bridge the gap between empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research focuses on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose a formulation of competition of mechanisms, which focuses on the interplay of multiple mechanisms instead of individual mechanisms and traces how one of them becomes dominant in the final prediction. We uncover how and where mechanisms compete within LLMs using two interpretability methods: logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components and reveal attention positions that effectively control the strength of certain mechanisms. Code: https://github.com/francescortu/comp-mech. Data: https://huggingface.co/datasets/francescortu/comp-mech.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/SXNNSMZW/Ortu et al. - 2024 - Competition of Mechanisms Tracing How Language Mo.pdf}
}

@article{osband_epistemic_2023,
  title = {Epistemic Neural Networks},
  author = {Osband, Ian and Wen, Zheng and Asghari, Seyed Mohammad and Dwaracherla, Vikranth and Ibrahimi, Morteza and Lu, Xiuyuan and Van Roy, Benjamin},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2107.08924},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2107.08924},
  url = {http://arxiv.org/abs/2107.08924},
  urldate = {2023-07-10},
  abstract = {Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. In principle, ensemble-based approaches produce effective joint predictions, but the computational costs of training large ensembles can become prohibitive. We introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks. To accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as an interface for models that produce joint predictions.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/IHDJ5CUN/Osband et al. - 2023 - Epistemic Neural Networks.pdf}
}

@article{ouyang_training_2022,
  title = {Training language models to follow instructions with human feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2203.02155},
  url = {https://arxiv.org/abs/2203.02155},
  urldate = {2023-11-10},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/N82655F8/Ouyang et al. - 2022 - Training language models to follow instructions wi.pdf}
}

@article{pai_facade_2023,
  title = {FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation},
  shorttitle = {FACADE},
  author = {Pai, Dhruv and Carranza, Andres and Schaeffer, Rylan and Tandon, Arnuv and Koyejo, Sanmi},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.10563},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.10563},
  url = {http://arxiv.org/abs/2307.10563},
  urldate = {2023-08-27},
  abstract = {We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/B5QJJ6BB/Pai et al. - 2023 - FACADE A Framework for Adversarial Circuit Anomal.pdf}
}

@article{pal_future_2023,
  title = {Future Lens: Anticipating Subsequent Tokens from a Single Hidden State},
  shorttitle = {Future Lens},
  author = {Pal, Koyena and Sun, Jiuding and Yuan, Andrew and Wallace, Byron C. and Bau, David},
  year = {2023},
  journal = {CoNLL},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2311.04897},
  url = {https://arxiv.org/abs/2311.04897},
  urldate = {2023-12-11},
  abstract = {We conjecture that hidden state vectors corresponding to individual input tokens encode information sufficient to accurately predict several tokens ahead. More concretely, in this paper we ask: Given a hidden (internal) representation of a single token at position \$t\$ in an input, can we reliably anticipate the tokens that will appear at positions \${\textbackslash}geq t + 2\$? To test this, we measure linear approximation and causal intervention methods in GPT-J-6B to evaluate the degree to which individual hidden states in the network contain signal rich enough to predict future hidden states and, ultimately, token outputs. We find that, at some layers, we can approximate a model's output with more than 48\% accuracy with respect to its prediction of subsequent tokens through a single hidden state. Finally we present a "Future Lens" visualization that uses these methods to create a new view of transformer states.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/MVNUX623/Pal et al. - 2023 - Future Lens Anticipating Subsequent Tokens from a.pdf}
}

@article{palit_visionlanguage_2023,
  title = {Towards vision-language mechanistic interpretability: A causal tracing tool for BLIP},
  author = {Palit, Vedant and Pandey, Rohan and Arora, Aryaman and Liang, P.},
  year = {2023},
  journal = {ICCVW},
  volume = {null},
  pages = {2848--2853},
  doi = {10.1109/ICCVW60793.2023.00307},
  url = {https://www.semanticscholar.org/paper/d494727306a375e524c4c4c8cc1a2dc1845cc4b7},
  abstract = {Mechanistic interpretability seeks to understand the neural mechanisms that enable specific behaviors in Large Language Models (LLMs) by leveraging causality-based methods. While these approaches have identified neural circuits that copy spans of text, capture factual knowledge, and more, they remain unusable for multimodal models since adapting these tools to the vision-language domain requires considerable architectural changes. In this work, we adapt a unimodal causal tracing tool to BLIP to enable the study of the neural mechanisms underlying image-conditioned text generation. We demonstrate our approach on a visual question answering dataset, highlighting the causal relevance of later layer representations for all tokens. Furthermore, we release our BLIP causal tracing tool as open source to enable further experimentation in vision-language mechanistic interpretability by the community. Our code is available at this URL.},
  arxivid = {2308.14179},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/9CUNZI3U/Palit et al. - 2023 - Towards vision-language mechanistic interpretabili.pdf}
}

@article{pan_effects_2022,
  title = {The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models},
  shorttitle = {The Effects of Reward Misspecification},
  author = {Pan, Alexander and Bhatia, Kush and Steinhardt, Jacob},
  year = {2022},
  month = feb,
  journal = {ICLR},
  eprint = {2201.03544},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2201.03544},
  url = {http://arxiv.org/abs/2201.03544},
  urldate = {2023-08-26},
  abstract = {Reward hacking -- where RL agents exploit gaps in misspecified reward functions -- has been widely observed, but not yet systematically studied. To understand how reward hacking arises, we construct four RL environments with misspecified rewards. We investigate reward hacking as a function of agent capabilities: model capacity, action space resolution, observation space noise, and training time. More capable agents often exploit reward misspecifications, achieving higher proxy reward and lower true reward than less capable agents. Moreover, we find instances of phase transitions: capability thresholds at which the agent's behavior qualitatively shifts, leading to a sharp decrease in the true reward. Such phase transitions pose challenges to monitoring the safety of ML systems. To address this, we propose an anomaly detection task for aberrant policies and offer several baseline detectors.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2NB5PLXQ/Pan et al. - 2022 - The Effects of Reward Misspecification Mapping an.pdf}
}

@article{pan_rewards_2023,
  title = {Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark},
  shorttitle = {Do the Rewards Justify the Means?},
  author = {Pan, Alexander and Chan, Jun Shern and Zou, Andy and Li, Nathaniel and Basart, Steven and Woodside, Thomas and Ng, Jonathan and Zhang, Hanlin and Emmons, Scott and Hendrycks, Dan},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2304.03279},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.03279},
  url = {http://arxiv.org/abs/2304.03279},
  urldate = {2023-08-26},
  abstract = {Artificial agents have traditionally been trained to maximize reward, which may incentivize power-seeking and deception, analogous to how next-token prediction in language models (LMs) may incentivize toxicity. So do agents naturally learn to be Machiavellian? And how do we measure these behaviors in general-purpose models such as GPT-4? Towards answering these questions, we introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games containing over half a million rich, diverse scenarios that center on social decision-making. Scenario labeling is automated with LMs, which are more performant than human annotators. We mathematize dozens of harmful behaviors and use our annotations to evaluate agents' tendencies to be power-seeking, cause disutility, and commit ethical violations. We observe some tension between maximizing reward and behaving ethically. To improve this trade-off, we investigate LM-based methods to steer agents' towards less harmful behaviors. Our results show that agents can both act competently and morally, so concrete progress can currently be made in machine ethics--designing agents that are Pareto improvements in both safety and capabilities.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HIX29EJC/Pan et al. - 2023 - Do the Rewards Justify the Means Measuring Trade-.pdf}
}

@article{panigrahi_taskspecific_2023,
  title = {Task-Specific Skill Localization in Fine-tuned Language Models},
  author = {Panigrahi, Abhishek and Saunshi, Nikunj and Zhao, Haoyu and Arora, Sanjeev},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2302.06600},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2302.06600},
  url = {http://arxiv.org/abs/2302.06600},
  urldate = {2023-10-25},
  abstract = {Pre-trained language models can be fine-tuned to solve diverse NLP tasks, including in few-shot settings. Thus fine-tuning allows the model to quickly pick up task-specific ``skills,'' but there has been limited study of where these newly-learnt skills reside inside the massive model. This paper introduces the term skill localization for this problem and proposes a solution. Given the downstream task and a model fine-tuned on that task, a simple optimization is used to identify a very small subset of parameters (\${\textbackslash}sim0.01\$\% of model parameters) responsible for (\${$>$}95\$\%) of the model's performance, in the sense that grafting the fine-tuned values for just this tiny subset onto the pre-trained model gives performance almost as well as the fine-tuned model. While reminiscent of recent works on parameter-efficient fine-tuning, the novel aspects here are that: (i) No further re-training is needed on the subset (unlike, say, with lottery tickets). (ii) Notable improvements are seen over vanilla fine-tuning with respect to calibration of predictions in-distribution (\$40\$-\$90\$\% error reduction) as well as the quality of predictions out-of-distribution (OOD). In models trained on multiple tasks, a stronger notion of skill localization is observed, where the sparse regions corresponding to different tasks are almost disjoint, and their overlap (when it happens) is a proxy for task similarity. Experiments suggest that localization via grafting can assist certain forms of continual learning.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FKVQCDXV/Panigrahi et al. - 2023 - Task-Specific Skill Localization in Fine-tuned Lan.pdf}
}

@article{panousis_discover_2023,
  title = {DISCOVER: Making Vision Networks Interpretable via Competition and Dissection},
  shorttitle = {DISCOVER},
  author = {Panousis, Konstantinos P. and Chatzis, Sotirios},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.04929},
  url = {https://arxiv.org/abs/2310.04929},
  urldate = {2023-10-26},
  abstract = {Modern deep networks are highly complex and their inferential outcome very hard to interpret. This is a serious obstacle to their transparent deployment in safety-critical or bias-aware applications. This work contributes to post-hoc interpretability, and specifically Network Dissection. Our goal is to present a framework that makes it easier to discover the individual functionality of each neuron in a network trained on a vision task; discovery is performed in terms of textual description generation. To achieve this objective, we leverage: (i) recent advances in multimodal vision-text models and (ii) network layers founded upon the novel concept of stochastic local competition between linear units. In this setting, only a small subset of layer neurons are activated for a given input, leading to extremely high activation sparsity (as low as only \${\textbackslash}approx 4{\textbackslash}\%\$). Crucially, our proposed method infers (sparse) neuron activation patterns that enables the neurons to activate/specialize to inputs with specific characteristics, diversifying their individual functionality. This capacity of our method supercharges the potential of dissection processes: human understandable descriptions are generated only for the very few active neurons, thus facilitating the direct investigation of the network's decision process. As we experimentally show, our approach: (i) yields Vision Networks that retain or improve classification performance, and (ii) realizes a principled framework for text-based description and examination of the generated neuronal representations.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RNPG5LSB/Panousis and Chatzis - 2023 - DISCOVER Making Vision Networks Interpretable via.pdf}
}

@article{park_ai_2023,
  title = {AI Deception: A Survey of Examples, Risks, and Potential Solutions},
  shorttitle = {AI Deception},
  author = {Park, Peter S. and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2308.14752},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2308.14752},
  url = {http://arxiv.org/abs/2308.14752},
  urldate = {2023-10-13},
  abstract = {This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/LARMVW83/Park et al. - 2023 - AI Deception A Survey of Examples, Risks, and Pot.pdf}
}

@article{park_linear_2023,
  title = {The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  year = {2023},
  month = nov,
  journal = {NeurIPS Workshop on Causal Representation Learning},
  eprint = {2311.03658},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2311.03658},
  urldate = {2023-11-09},
  abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZK7VCT4A/Park et al. - 2023 - The Linear Representation Hypothesis and the Geome.pdf}
}

@article{pasad_layerwise_2022,
  title = {Layer-wise Analysis of a Self-supervised Speech Representation Model},
  author = {Pasad, Ankita and Chou, Ju-Chieh and Livescu, Karen},
  year = {2022},
  month = dec,
  journal = {CoRR},
  eprint = {2107.04734},
  primaryclass = {cs, eess},
  doi = {10.48550/arXiv.2107.04734},
  url = {http://arxiv.org/abs/2107.04734},
  urldate = {2023-08-29},
  abstract = {Recently proposed self-supervised learning approaches have been successful for pre-training speech representation models. The utility of these learned representations has been observed empirically, but not much has been studied about the type or extent of information encoded in the pre-trained representations themselves. Developing such insights can help understand the capabilities and limits of these models and enable the research community to more efficiently develop their usage for downstream applications. In this work, we begin to fill this gap by examining one recent and successful pre-trained model (wav2vec 2.0), via its intermediate representation vectors, using a suite of analysis tools. We use the metrics of canonical correlation, mutual information, and performance on simple downstream tasks with non-parametric probes, in order to (i) query for acoustic and linguistic information content, (ii) characterize the evolution of information across model layers, and (iii) understand how fine-tuning the model for automatic speech recognition (ASR) affects these observations. Our findings motivate modifying the fine-tuning protocol for ASR, which produces improved word error rates in a low-resource setting.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/BA39JB36/Pasad et al. - 2022 - Layer-wise Analysis of a Self-supervised Speech Re.pdf}
}

@article{patel_mapping_2022,
  title = {Mapping Language Models to Grounded Conceptual Spaces},
  author = {Patel, Roma and Pavlick, Ellie},
  year = {2022},
  journal = {ICLR},
  url = {https://openreview.net/forum?id=gJcEM8sxHK},
  urldate = {2024-06-20},
  abstract = {A fundamental criticism of text-only language models (LMs) is their lack of grounding---that is, the ability to tie a word for which they have learned a representation, to its actual use in the world. However, despite this limitation, large pre-trained LMs have been shown to have a remarkable grasp of the conceptual structure of language, as demonstrated by their ability to answer questions, generate fluent text, or make inferences about entities, objects, and properties that they have never physically observed. In this work we investigate the extent to which the rich conceptual structure that LMs learn indeed reflects the conceptual structure of the non-linguistic world---which is something that LMs have never observed. We do this by testing whether the LMs can learn to map an entire conceptual domain (e.g., direction or colour) onto a grounded world representation given only a small number of examples. For example, we show a model what the word ``left" means using a textual depiction of a grid world, and assess how well it can generalise to related concepts, for example, the word ``right", in a similar grid world. We investigate a range of generative language models of varying sizes (including GPT-2 and GPT-3), and see that although the smaller models struggle to perform this mapping, the largest model can not only learn to ground the concepts that it is explicitly taught, but appears to generalise to several instances of unseen concepts as well. Our results suggest an alternative means of building grounded language models: rather than learning grounded representations ``from scratch'', it is possible that large text-only models learn a sufficiently rich conceptual structure that could allow them to be grounded in a data-efficient way.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZEULQP7T/Patel and Pavlick - 2021 - Mapping Language Models to Grounded Conceptual Spa.pdf}
}

@article{patil_can_2023,
  title = {Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks},
  shorttitle = {Can Sensitive Information Be Deleted From LLMs?},
  author = {Patil, Vaidehi and Hase, Peter and Bansal, Mohit},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.17410},
  url = {https://arxiv.org/abs/2309.17410},
  urldate = {2023-12-11},
  abstract = {Pretrained language models sometimes possess knowledge that we do not wish them to, including memorized personal information and knowledge that could be used to harm people. They can also output toxic or harmful text. To mitigate these safety and informational issues, we propose an attack-and-defense framework for studying the task of deleting sensitive information directly from model weights. We study direct edits to model weights because (1) this approach should guarantee that particular deleted information is never extracted by future prompt attacks, and (2) it should protect against whitebox attacks, which is necessary for making claims about safety/privacy in a setting where publicly available model weights could be used to elicit sensitive information. Our threat model assumes that an attack succeeds if the answer to a sensitive question is located among a set of B generated candidates, based on scenarios where the information would be insecure if the answer is among B candidates. Experimentally, we show that even state-of-the-art model editing methods such as ROME struggle to truly delete factual information from models like GPT-J, as our whitebox and blackbox attacks can recover "deleted" information from an edited model 38\% of the time. These attacks leverage two key observations: (1) that traces of deleted information can be found in intermediate model hidden states, and (2) that applying an editing method for one question may not delete information across rephrased versions of the question. Finally, we provide new defense methods that protect against some extraction attacks, but we do not find a single universally effective defense method. Our results suggest that truly deleting sensitive information is a tractable but difficult problem, since even relatively low attack success rates have potentially severe societal implications for real-world deployment of language models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/34YI976V/Patil et al. - 2023 - Can Sensitive Information Be Deleted From LLMs Ob.pdf}
}

@article{payani_learning_2019,
  title = {Learning Algorithms via Neural Logic Networks},
  author = {Payani, Ali and Fekri, Faramarz},
  year = {2019},
  month = apr,
  journal = {ICML},
  eprint = {1904.01554},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1904.01554},
  urldate = {2024-03-19},
  abstract = {We propose a novel learning paradigm for Deep Neural Networks (DNN) by using Boolean logic algebra. We first present the basic differentiable operators of a Boolean system such as conjunction, disjunction and exclusive-OR and show how these elementary operators can be combined in a simple and meaningful way to form Neural Logic Networks (NLNs). We examine the effectiveness of the proposed NLN framework in learning Boolean functions and discrete-algorithmic tasks. We demonstrate that, in contrast to the implicit learning in MLP approach, the proposed neural logic networks can learn the logical functions explicitly that can be verified and interpreted by human. In particular, we propose a new framework for learning the inductive logic programming (ILP) problems by exploiting the explicit representational power of NLN. We show the proposed neural ILP solver is capable of feats such as predicate invention and recursion and can outperform the current state of the art neural ILP solvers using a variety of benchmark tasks such as decimal addition and multiplication, and sorting on ordered list.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/6IG5PRJ2/Payani and Fekri - 2019 - Learning Algorithms via Neural Logic Networks.pdf}
}

@article{pearce_weightbased_2024,
  title = {Weight-based Decomposition: A Case for Bilinear MLPs},
  shorttitle = {Weight-based Decomposition},
  author = {Pearce, Michael T. and Dooms, Thomas and Rigg, Alice},
  year = {2024},
  month = jun,
  url = {https://www.semanticscholar.org/paper/Weight-based-Decomposition%3A-A-Case-for-Bilinear-Pearce-Dooms/7e0f20fa4762370a668f37b030e44f00bfdd48e5},
  urldate = {2024-06-09},
  abstract = {Gated Linear Units (GLUs) have become a common building block in modern foundation models. Bilinear layers drop the non-linearity in the"gate"but still have comparable performance to other GLUs. An attractive quality of bilinear layers is that they can be fully expressed in terms of a third-order tensor and linear operations. Leveraging this, we develop a method to decompose the bilinear tensor into a set of sparsely interacting eigenvectors that show promising interpretability properties in preliminary experiments for shallow image classifiers (MNIST) and small language models (Tiny Stories). Since the decomposition is fully equivalent to the model's original computations, bilinear layers may be an interpretability-friendly architecture that helps connect features to the model weights. Application of our method may not be limited to pretrained bilinear models since we find that language models such as TinyLlama-1.1B can be finetuned into bilinear variants.},
  file = {/Users/leonardbereska/Zotero/storage/REKKW36X/Pearce et al. - 2024 - Weight-based Decomposition A Case for Bilinear ML.pdf}
}

@book{pearl_causality_2009,
  title = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511803161},
  url = {https://www.cambridge.org/core/books/causality/B0046844FAE10CBF274D4ACBDAEB5F5B},
  urldate = {2023-11-10},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. Cited in more than 2,100 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interest to students and professionals in a wide variety of fields. Dr Judea Pearl has received the 2011 Rumelhart Prize for his leading research in Artificial Intelligence (AI) and systems from The Cognitive Science Society.},
  keywords = {cited}
}

@article{perez_attention_2021,
  title = {Attention is Turing Complete},
  author = {Perez, Jorge and Barcelo, Pablo and Marinkovic, Javier},
  year = {2021},
  journal = {JMLR},
  url = {https://jmlr.org/papers/v22/20-302.html},
  abstract = {Alternatives to recurrent neural networks, in particular, architectures based on self-attention, are gaining momentum for processing input sequences. In spite of their relevance, the computational properties of such networks have not yet been fully explored. We study the computational power of the Transformer, one of the most paradigmatic architectures exemplifying self-attention. We show that the Transformer with hard-attention is Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. Our study also reveals some minimal sets of elements needed to obtain this completeness result.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/JLINZ98D/Perez et al. - Attention is Turing Complete.pdf}
}

@article{perez_discovering_2022,
  title = {Discovering Language Model Behaviors with Model-Written Evaluations},
  author = {Perez, Ethan and Ringer, Sam and Luko{\v s}i{\=u}t{\.e}, Kamil{\.e} and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and {Tran-Johnson}, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noem{\'i} and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and {Telleen-Lawton}, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and {Hatfield-Dodds}, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
  year = {2022},
  month = dec,
  journal = {CoRR},
  eprint = {2212.09251},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.09251},
  url = {http://arxiv.org/abs/2212.09251},
  urldate = {2023-08-26},
  abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/CGM496B9/Perez et al. - 2022 - Discovering Language Model Behaviors with Model-Wr.pdf}
}

@article{perez_red_2022,
  title = {Red Teaming Language Models with Language Models},
  author = {Perez, Ethan and Huang, Saffron and Song, Francis and Cai, Trevor and Ring, Roman and Aslanides, John and Glaese, Amelia and McAleese, Nat and Irving, Geoffrey},
  year = {2022},
  month = feb,
  journal = {CoRR},
  eprint = {2202.03286},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2202.03286},
  url = {http://arxiv.org/abs/2202.03286},
  urldate = {2023-08-26},
  abstract = {Language Models (LMs) often cannot be deployed because of their potential to harm users in hard-to-predict ways. Prior work identifies harmful behaviors before deployment by using human annotators to hand-write test cases. However, human annotation is expensive, limiting the number and diversity of test cases. In this work, we automatically find cases where a target LM behaves in a harmful way, by generating test cases ("red teaming") using another LM. We evaluate the target LM's replies to generated test questions using a classifier trained to detect offensive content, uncovering tens of thousands of offensive replies in a 280B parameter LM chatbot. We explore several methods, from zero-shot generation to reinforcement learning, for generating test cases with varying levels of diversity and difficulty. Furthermore, we use prompt engineering to control LM-generated test cases to uncover a variety of other harms, automatically finding groups of people that the chatbot discusses in offensive ways, personal and hospital phone numbers generated as the chatbot's own contact info, leakage of private training data in generated text, and harms that occur over the course of a conversation. Overall, LM-based red teaming is one promising tool (among many needed) for finding and fixing diverse, undesirable LM behaviors before impacting users.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NDJ9QCKC/Perez et al. - 2022 - Red Teaming Language Models with Language Models.pdf}
}

@article{petersen_deep_2022,
  title = {Deep Differentiable Logic Gate Networks},
  author = {Petersen, Felix and Borgelt, Christian and Kuehne, Hilde and Deussen, Oliver},
  year = {2022},
  month = oct,
  journal = {NeurIPS},
  eprint = {2210.08277},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.08277},
  url = {http://arxiv.org/abs/2210.08277},
  urldate = {2024-03-19},
  abstract = {Recently, research has increasingly focused on developing efficient neural network architectures. In this work, we explore logic gate networks for machine learning tasks by learning combinations of logic gates. These networks comprise logic gates such as "AND" and "XOR", which allow for very fast execution. The difficulty in learning logic gate networks is that they are conventionally non-differentiable and therefore do not allow training with gradient descent. Thus, to allow for effective training, we propose differentiable logic gate networks, an architecture that combines real-valued logics and a continuously parameterized relaxation of the network. The resulting discretized logic gate networks achieve fast inference speeds, e.g., beyond a million images of MNIST per second on a single CPU core.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/FN53FLHY/Petersen et al. - 2022 - Deep Differentiable Logic Gate Networks.pdf}
}

@article{pfeiffer_modular_2023,
  title = {Modular Deep Learning},
  author = {Pfeiffer, Jonas and Ruder, Sebastian and Vuli{\'c}, Ivan and Ponti, Edoardo Maria},
  year = {2023},
  month = feb,
  journal = {CoRR},
  eprint = {2302.11529},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2302.11529},
  url = {http://arxiv.org/abs/2302.11529},
  urldate = {2023-10-25},
  abstract = {Transfer learning has recently become the dominant paradigm of machine learning. Pre-trained models fine-tuned for downstream tasks achieve better performance with fewer labelled examples. Nonetheless, it remains unclear how to develop models that specialise towards multiple tasks without incurring negative interference and that generalise systematically to non-identically distributed tasks. Modular deep learning has emerged as a promising solution to these challenges. In this framework, units of computation are often implemented as autonomous parameter-efficient modules. Information is conditionally routed to a subset of modules and subsequently aggregated. These properties enable positive transfer and systematic generalisation by separating computation from routing and updating modules locally. We offer a survey of modular architectures, providing a unified view over several threads of research that evolved independently in the scientific literature. Moreover, we explore various additional purposes of modularity, including scaling language models, causal inference, programme induction, and planning in reinforcement learning. Finally, we report various concrete applications where modularity has been successfully deployed such as cross-lingual and cross-modal knowledge transfer. Related talks and projects to this survey, are available at https://www.modulardeeplearning.com/.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GDWM5CII/Pfeiffer et al. - 2023 - Modular Deep Learning.pdf}
}

@article{plumb_regularizing_2019,
  title = {Regularizing Black-box Models for Improved Interpretability},
  author = {Plumb, Gregory and {Al-Shedivat}, Maruan and Xing, E. and Talwalkar, Ameet},
  year = {2019},
  month = feb,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Regularizing-Black-box-Models-for-Improved-Plumb-Al-Shedivat/2b9fc544602c4793b1d95b77e3c38deea467b41f},
  urldate = {2023-10-26},
  abstract = {Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, ExpO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to define. We demonstrate that post-hoc explanations for ExpO-regularized models have better explanation quality, as measured by the common fidelity and stability metrics. We verify that improving these metrics leads to significantly more useful explanations with a user study on a realistic task.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ISMHBAW9/Plumb et al. - 2019 - Regularizing Black-box Models for Improved Interpr.pdf}
}

@article{pochinkov_machine_2023,
  title = {Machine Unlearning Evaluations as Interpretability Benchmarks},
  author = {Pochinkov, Nicky and , Nandi},
  year = {2023},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/mTi8TQEyP5Pr7oczd/machine-unlearning-evaluations-as-interpretability},
  urldate = {2023-11-16},
  abstract = {Interpreting Models by Ablation. Image generated by DALL-E 3. {\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/55H92K3J/Pochinkov and  - 2023 - Machine Unlearning Evaluations as Interpretability.html}
}

@article{poli_hyena_2023,
  title = {Hyena Hierarchy: Towards Larger Convolutional Language Models},
  shorttitle = {Hyena Hierarchy},
  author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  year = {2023},
  month = apr,
  journal = {ICML},
  eprint = {2302.10866},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2302.10866},
  url = {http://arxiv.org/abs/2302.10866},
  urldate = {2023-10-22},
  abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/T98YKM9B/Poli et al. - 2023 - Hyena Hierarchy Towards Larger Convolutional Lang.pdf}
}

@article{pope_qapr_2023,
  title = {QAPR 5: grokking is maybe not *that* big a deal?},
  shorttitle = {QAPR 5},
  author = {Pope, Quintin},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/GpSzShaaf8po4rcmA/qapr-5-grokking-is-maybe-not-that-big-a-deal},
  urldate = {2024-02-12},
  abstract = {[Thanks to support from Cavendish Labs and a Lightspeed grant, I've been able to restart the Quintin's Alignment Papers Roundup sequence.] {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GLTGBST8/Pope - 2023 - QAPR 5 grokking is maybe not that big a deal.html}
}

@article{poupart_contrastive_2024,
  title = {Contrastive Sparse Autoencoders for Interpreting Planning of Chess-Playing Agents},
  author = {Poupart, Yoann},
  year = {2024},
  month = jun,
  url = {https://www.semanticscholar.org/paper/Contrastive-Sparse-Autoencoders-for-Interpreting-of-Poupart/02f91f064cf9c614284f6d1938f0d2be1fa0a0cd},
  urldate = {2024-06-10},
  abstract = {AI led chess systems to a superhuman level, yet these systems heavily rely on black-box algorithms. This is unsustainable in ensuring transparency to the end-user, particularly when these systems are responsible for sensitive decision-making. Recent interpretability work has shown that the inner representations of Deep Neural Networks (DNNs) were fathomable and contained human-understandable concepts. Yet, these methods are seldom contextualised and are often based on a single hidden state, which makes them unable to interpret multi-step reasoning, e.g. planning. In this respect, we propose contrastive sparse autoencoders (CSAE), a novel framework for studying pairs of game trajectories. Using CSAE, we are able to extract and interpret concepts that are meaningful to the chess-agent plans. We primarily focused on a qualitative analysis of the CSAE features before proposing an automated feature taxonomy. Furthermore, to evaluate the quality of our trained CSAE, we devise sanity checks to wave spurious correlations in our results.},
  file = {/Users/leonardbereska/Zotero/storage/UNUTNJW6/Poupart - 2024 - Contrastive Sparse Autoencoders for Interpreting P.pdf}
}

@article{poursabzi-sangdeh_manipulating_2021,
  title = {Manipulating and Measuring Model Interpretability},
  author = {{Poursabzi-Sangdeh}, Forough and Goldstein, Daniel G and Hofman, Jake M and Wortman Vaughan, Jennifer Wortman and Wallach, Hanna},
  year = {2021},
  month = may,
  journal = {CHI},
  pages = {1--52},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3411764.3445315},
  url = {https://dl.acm.org/doi/10.1145/3411764.3445315},
  urldate = {2023-11-10},
  abstract = {With machine learning models being increasingly used to aid decision making even in high-stakes domains, there has been a growing interest in developing interpretable models. Although many supposedly interpretable models have been proposed, there have been relatively few experimental studies investigating whether these models achieve their intended effects, such as making people more closely follow a model's predictions when it is beneficial for them to do so or enabling them to detect when a model has made a mistake. We present a sequence of pre-registered experiments (N = 3, 800) in which we showed participants functionally identical models that varied only in two factors commonly thought to make machine learning models more or less interpretable: the number of features and the transparency of the model (i.e., whether the model internals are clear or black box). Predictably, participants who saw a clear model with few features could better simulate the model's predictions. However, we did not find that participants more closely followed its predictions. Furthermore, showing participants a clear model meant that they were less able to detect and correct for the model's sizable mistakes, seemingly due to information overload. These counterintuitive findings emphasize the importance of testing over intuition when developing interpretable models.},
  isbn = {9781450380966},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/IDHXYJDS/Poursabzi-Sangdeh et al. - 2021 - Manipulating and Measuring Model Interpretability.pdf}
}

@article{power_grokking_2022,
  title = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  year = {2022},
  month = jan,
  journal = {CoRR},
  eprint = {2201.02177},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2201.02177},
  url = {http://arxiv.org/abs/2201.02177},
  urldate = {2023-11-10},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HAFYPFWN/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf}
}

@article{prakash_finetuning_2024,
  title = {Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking},
  shorttitle = {Fine-Tuning Enhances Existing Mechanisms},
  author = {Prakash, Nikhil and Shaham, Tamar Rott and Haklay, Tal and Belinkov, Yonatan and Bau, David},
  year = {2024},
  month = feb,
  journal = {ICML},
  eprint = {2402.14811},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.14811},
  url = {http://arxiv.org/abs/2402.14811},
  urldate = {2024-05-17},
  abstract = {Fine-tuning on generalized tasks such as instruction following, code generation, and mathematics has been shown to enhance language models' performance on a range of tasks. Nevertheless, explanations of how such fine-tuning influences the internal computations in these models remain elusive. We study how fine-tuning affects the internal mechanisms implemented in language models. As a case study, we explore the property of entity tracking, a crucial facet of language comprehension, where models fine-tuned on mathematics have substantial performance gains. We identify the mechanism that enables entity tracking and show that (i) in both the original model and its fine-tuned versions primarily the same circuit implements entity tracking. In fact, the entity tracking circuit of the original model on the fine-tuned versions performs better than the full original model. (ii) The circuits of all the models implement roughly the same functionality: Entity tracking is performed by tracking the position of the correct entity in both the original model and its fine-tuned versions. (iii) Performance boost in the fine-tuned models is primarily attributed to its improved ability to handle the augmented positional information. To uncover these findings, we employ: Patch Patching, DCM, which automatically detects model components responsible for specific semantics, and CMAP, a new approach for patching activations across models to reveal improved mechanisms. Our findings suggest that fine-tuning enhances, rather than fundamentally alters, the mechanistic operation of the model.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/LZ4MUEKY/Prakash et al. - 2024 - Fine-Tuning Enhances Existing Mechanisms A Case S.pdf}
}

@article{pruthi_evaluating_2021,
  title = {Evaluating Explanations: How much do explanations from the teacher aid students?},
  shorttitle = {Evaluating Explanations},
  author = {Pruthi, Danish and Bansal, Rachit and Dhingra, Bhuwan and Soares, Livio Baldini and Collins, Michael and Lipton, Zachary C. and Neubig, Graham and Cohen, William W.},
  year = {2021},
  month = dec,
  journal = {TACL},
  eprint = {2012.00893},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.00893},
  url = {http://arxiv.org/abs/2012.00893},
  urldate = {2023-08-29},
  abstract = {While many methods purport to explain predictions by highlighting salient features, what aims these explanations serve and how they ought to be evaluated often go unstated. In this work, we introduce a framework to quantify the value of explanations via the accuracy gains that they confer on a student model trained to simulate a teacher model. Crucially, the explanations are available to the student during training, but are not available at test time. Compared to prior proposals, our approach is less easily gamed, enabling principled, automatic, model-agnostic evaluation of attributions. Using our framework, we compare numerous attribution methods for text classification and question answering, and observe quantitative differences that are consistent (to a moderate to high degree) across different student model architectures and learning strategies.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RIGANB6V/Pruthi et al. - 2021 - Evaluating Explanations How much do explanations .pdf}
}

@article{pruthi_learning_2020,
  title = {Learning to Deceive with Attention-Based Explanations},
  author = {Pruthi, Danish and Gupta, Mansi and Dhingra, Bhuwan and Neubig, Graham and Lipton, Zachary C.},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  journal = {ACL},
  pages = {4782--4793},
  doi = {10.18653/v1/2020.acl-main.432},
  url = {https://aclanthology.org/2020.acl-main.432},
  urldate = {2024-03-20},
  abstract = {Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention's reliability as a tool for auditing algorithms in the context of fairness and accountability.},
  file = {/Users/leonardbereska/Zotero/storage/BCWLX7KN/Pruthi et al. - 2020 - Learning to Deceive with Attention-Based Explanati.pdf}
}

@article{quirke_increasing_2024,
  title = {Increasing Trust in Language Models through the Reuse of Verified Circuits},
  author = {Quirke, Philip and Neo, Clement and Barez, Fazl},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.02619},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.02619},
  url = {http://arxiv.org/abs/2402.02619},
  urldate = {2024-02-07},
  abstract = {Language Models (LMs) are increasingly used for a wide range of prediction tasks, but their training can often neglect rare edge cases, reducing their reliability. Here, we define a stringent standard of trustworthiness whereby the task algorithm and circuit implementation must be verified, accounting for edge cases, with no known failure modes. We show that a transformer model can be trained to meet this standard if built using mathematically and logically specified frameworks. In this paper, we fully verify a model for n-digit integer addition. To exhibit the reusability of verified modules, we insert the trained integer addition model into an untrained model and train the combined model to perform both addition and subtraction. We find extensive reuse of the addition circuits for both tasks, easing verification of the more complex subtractor model. We discuss how inserting verified task modules into LMs can leverage model reuse to improve verifiability and trustworthiness of language models built using them. The reuse of verified circuits reduces the effort to verify more complex composite models which we believe to be a significant step towards safety of language models.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/KH4WYLS6/Quirke et al. - 2024 - Increasing Trust in Language Models through the Re.pdf}
}

@article{quirke_training_2023,
  title = {Training dynamics of contextual N-grams in language models},
  author = {Quirke, Lucia and Heindrich, Lovis and Gurnee, Wes and Nanda, Neel},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2311.00863},
  pages = {null},
  doi = {10.48550/arXiv.2311.00863},
  url = {https://www.semanticscholar.org/paper/0490c227115021cea3829afe476f3f94009d2cbe},
  abstract = {Prior work has shown the existence of contextual neurons in language models, including a neuron that activates on German text. We show that this neuron exists within a broader contextual n-gram circuit: we find late layer neurons which recognize and continue n-grams common in German text, but which only activate if the German neuron is active. We investigate the formation of this circuit throughout training and find that it is an example of what we call a second-order circuit. In particular, both the constituent n-gram circuits and the German detection circuit which culminates in the German neuron form with independent functions early in training - the German detection circuit partially through modeling German unigram statistics, and the n-grams by boosting appropriate completions. Only after both circuits have already formed do they fit together into a second-order circuit. Contrary to the hypotheses presented in prior work, we find that the contextual n-gram circuit forms gradually rather than in a sudden phase transition. We further present a range of anomalous observations such as a simultaneous phase transition in many tasks coinciding with the learning rate warm-up, and evidence that many context neurons form simultaneously early in training but are later unlearned.},
  arxivid = {2311.00863},
  keywords = {mechinterp,not cited,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/25HXTXNA/Quirke et al. - 2023 - Training dynamics of contextual N-grams in languag.pdf}
}

@article{quirke_understanding_2023,
  title = {Understanding Addition in Transformers},
  author = {Quirke, Philip and Barez, Fazl},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.13121},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.13121},
  url = {http://arxiv.org/abs/2310.13121},
  urldate = {2023-10-27},
  abstract = {Understanding the inner workings of machine learning models like Transformers is vital for their safe and ethical use. This paper presents an in-depth analysis of a one-layer Transformer model trained for integer addition. We reveal that the model divides the task into parallel, digit-specific streams and employs distinct algorithms for different digit positions. Our study also finds that the model starts calculations late but executes them rapidly. A rare use case with high loss is identified and explained. Overall, the model's algorithm is explained in detail. These findings are validated through rigorous testing and mathematical modeling, contributing to the broader works in Mechanistic Interpretability, AI safety, and alignment. Our approach opens the door for analyzing more complex tasks and multi-layer Transformer models.},
  archiveprefix = {arxiv},
  keywords = {algorithms,circuit,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/ZG4Y2KA6/Quirke and Barez - 2023 - Understanding Addition in Transformers.pdf}
}

@article{radford_learning_2017,
  title = {Learning to Generate Reviews and Discovering Sentiment},
  author = {Radford, Alec and Jozefowicz, Rafal and Sutskever, Ilya},
  year = {2017},
  month = apr,
  journal = {CoRR},
  eprint = {1704.01444},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1704.01444},
  url = {http://arxiv.org/abs/1704.01444},
  urldate = {2024-03-20},
  abstract = {We explore the properties of byte-level recurrent language models. When given sufficient amounts of capacity, training data, and compute time, the representations learned by these models include disentangled features corresponding to high-level concepts. Specifically, we find a single unit which performs sentiment analysis. These representations, learned in an unsupervised manner, achieve state of the art on the binary subset of the Stanford Sentiment Treebank. They are also very data efficient. When using only a handful of labeled examples, our approach matches the performance of strong baselines trained on full datasets. We also demonstrate the sentiment unit has a direct influence on the generative process of the model. Simply fixing its value to be positive or negative generates samples with the corresponding positive or negative sentiment.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/MJADDHW8/Radford et al. - 2017 - Learning to Generate Reviews and Discovering Senti.pdf}
}

@article{radhakrishnan_rlhf_2022,
  title = {RLHF},
  author = {Radhakrishnan, Ansh},
  year = {2022},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/rQH4gRmPMJyjtMpTn/rlhf},
  urldate = {2023-05-15},
  abstract = {I've been thinking about Reinforcement Learning from Human Feedback (RLHF) a lot lately, mostly as a result of~my AGISF capstone project attempting to use it to teach a language model to write better{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5ZAMFYXD/Radhakrishnan - 2022 - RLHF.html}
}

@misc{rai_practical_2024,
  title = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  author = {Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  year = {2024},
  month = jul,
  number = {arXiv:2407.02646},
  eprint = {2407.02646},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.02646},
  url = {http://arxiv.org/abs/2407.02646},
  urldate = {2024-07-09},
  abstract = {Mechanistic interpretability (MI) is an emerging sub-field of interpretability that seeks to understand a neural network model by reverse-engineering its internal computations. Recently, MI has garnered significant attention for interpreting transformer-based language models (LMs), resulting in many novel insights yet introducing new challenges. However, there has not been work that comprehensively reviews these insights and challenges, particularly as a guide for newcomers to this field. To fill this gap, we present a comprehensive survey outlining fundamental objects of study in MI, techniques that have been used for its investigation, approaches for evaluating MI results, and significant findings and applications stemming from the use of MI to understand LMs. In particular, we present a roadmap for beginners to navigate the field and leverage MI for their benefit. Finally, we also identify current gaps in the field and discuss potential future directions.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/B3K88CQY/Rai et al. - 2024 - A Practical Review of Mechanistic Interpretability.pdf;/Users/leonardbereska/Zotero/storage/LMBXICU2/2407.html}
}

@article{rajamanoharan_improving_2024,
  title = {Improving Dictionary Learning with Gated Sparse Autoencoders},
  author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2404.16014},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.16014},
  url = {http://arxiv.org/abs/2404.16014},
  urldate = {2024-06-10},
  abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/JYKKM3XZ/Rajamanoharan et al. - 2024 - Improving Dictionary Learning with Gated Sparse Au.pdf}
}

@article{rajendran_learning_2024,
  title = {Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models},
  shorttitle = {Learning Interpretable Concepts},
  author = {Rajendran, Goutham and Buchholz, Simon and Aragam, Bryon and Sch{\"o}lkopf, Bernhard and Ravikumar, Pradeep},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.09236},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.2402.09236},
  url = {http://arxiv.org/abs/2402.09236},
  urldate = {2024-06-10},
  abstract = {To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/428NRDRW/Rajendran et al. - 2024 - Learning Interpretable Concepts Unifying Causal R.pdf}
}

@article{ram_what_2023,
  title = {What Are You Token About? Dense Retrieval as Distributions Over the Vocabulary},
  shorttitle = {What Are You Token About?},
  author = {Ram, Ori and Bezalel, Liat and Zicher, Adi and Belinkov, Yonatan and Berant, Jonathan and Globerson, Amir},
  year = {2023},
  month = may,
  journal = {ACL},
  eprint = {2212.10380},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.10380},
  url = {http://arxiv.org/abs/2212.10380},
  urldate = {2024-03-19},
  abstract = {Dual encoders are now the dominant architecture for dense retrieval. Yet, we have little understanding of how they represent text, and why this leads to good performance. In this work, we shed light on this question via distributions over the vocabulary. We propose to interpret the vector representations produced by dual encoders by projecting them into the model's vocabulary space. We show that the resulting projections contain rich semantic information, and draw connection between them and sparse retrieval. We find that this view can offer an explanation for some of the failure cases of dense retrievers. For example, we observe that the inability of models to handle tail entities is correlated with a tendency of the token distributions to forget some of the tokens of those entities. We leverage this insight and propose a simple way to enrich query and passage representations with lexical information at inference time, and show that this significantly improves performance compared to the original model in zero-shot settings, and specifically on the BEIR benchmark.},
  archiveprefix = {arxiv},
  keywords = {logit lens},
  file = {/Users/leonardbereska/Zotero/storage/35V4G4GH/Ram et al. - 2023 - What Are You Token About Dense Retrieval as Distr.pdf}
}

@article{ramanujan_what_2020,
  title = {What's Hidden in a Randomly Weighted Neural Network?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  journal = {CVPR},
  pages = {11893--11902},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Ramanujan_Whats_Hidden_in_a_Randomly_Weighted_Neural_Network_CVPR_2020_paper.html},
  urldate = {2024-02-19},
  file = {/Users/leonardbereska/Zotero/storage/L64L4RVW/Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf}
}

@article{rauker_transparent_2023,
  title = {Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks},
  shorttitle = {Toward Transparent AI},
  author = {R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and {Hadfield-Menell}, Dylan},
  year = {2023},
  month = aug,
  journal = {TMLR},
  eprint = {2207.13243},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2207.13243},
  url = {http://arxiv.org/abs/2207.13243},
  urldate = {2023-08-27},
  abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,review,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/2MVPX3C4/Ruker et al. - 2023 - Toward Transparent AI A Survey on Interpreting th.pdf}
}

@article{ravfogel_kernelized_2022,
  title = {Kernelized Concept Erasure},
  author = {Ravfogel, Shauli and Vargas, Francisco and Goldberg, Yoav and Cotterell, Ryan},
  year = {2022},
  journal = {EMNLP},
  eprint = {2201.12191},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2201.12191},
  url = {http://arxiv.org/abs/2201.12191},
  urldate = {2023-11-10},
  abstract = {The representation space of neural models for textual data emerges in an unsupervised manner during training. Understanding how those representations encode human-interpretable concepts is a fundamental problem. One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HNWLCZKP/Ravfogel et al. - 2022 - Kernelized Concept Erasure.pdf}
}

@article{ravfogel_null_2020,
  title = {Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection},
  shorttitle = {Null It Out},
  author = {Ravfogel, Shauli and Elazar, Yanai and Gonen, Hila and Twiton, Michael and Goldberg, Yoav},
  editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
  year = {2020},
  month = jul,
  journal = {ACL},
  pages = {7237--7256},
  doi = {10.18653/v1/2020.acl-main.647},
  url = {https://aclanthology.org/2020.acl-main.647},
  urldate = {2024-03-19},
  abstract = {The ability to control for the kinds of information encoded in neural representation has a variety of use cases, especially in light of the challenge of interpreting these models. We present Iterative Null-space Projection (INLP), a novel method for removing information from neural representations. Our method is based on repeated training of linear classifiers that predict a certain property we aim to remove, followed by projection of the representations on their null-space. By doing so, the classifiers become oblivious to that target property, making it hard to linearly separate the data according to it. While applicable for multiple uses, we evaluate our method on bias and fairness use-cases, and show that our method is able to mitigate bias in word embeddings, as well as to increase fairness in a setting of multi-class classification.},
  file = {/Users/leonardbereska/Zotero/storage/KY83R3EA/Ravfogel et al. - 2020 - Null It Out Guarding Protected Attributes by Iter.pdf}
}

@article{ravichander_probing_2021,
  title = {Probing the Probing Paradigm: Does Probing Accuracy Entail Task Relevance?},
  shorttitle = {Probing the Probing Paradigm},
  author = {Ravichander, Abhilasha and Belinkov, Yonatan and Hovy, Eduard},
  year = {2021},
  journal = {ACL},
  pages = {3363--3377},
  doi = {10.18653/v1/2021.eacl-main.295},
  url = {https://aclanthology.org/2021.eacl-main.295},
  urldate = {2024-01-24},
  abstract = {Although neural models have achieved impressive results on several NLP benchmarks, little is understood about the mechanisms they use to perform language tasks. Thus, much recent attention has been devoted to analyzing the sentence representations learned by neural encoders, through the lens of `probing' tasks. However, to what extent was the information encoded in sentence representations, as discovered through a probe, actually used by the model to perform its task? In this work, we examine this probing paradigm through a case study in Natural Language Inference, showing that models can learn to encode linguistic properties even if they are not needed for the task on which the model was trained. We further identify that pretrained word embeddings play a considerable role in encoding these properties rather than the training task itself, highlighting the importance of careful controls when designing probing experiments. Finally, through a set of controlled synthetic tasks, we demonstrate models can encode these properties considerably above chance-level, even when distributed in the data as random noise, calling into question the interpretation of absolute claims on probing tasks.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PECYRJY5/Ravichander et al. - 2021 - Probing the Probing Paradigm Does Probing Accurac.pdf}
}

@article{reddy_mechanistic_2023a,
  title = {The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
  author = {Reddy, Gautam},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.03002},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.03002},
  url = {http://arxiv.org/abs/2312.03002},
  urldate = {2024-01-03},
  abstract = {Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence. In-context learning contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/L4E6G9HX/Reddy - 2023 - The mechanistic basis of data dependence and abrup.pdf}
}

@article{reed_neural_2016,
  title = {Neural Programmer-Interpreters},
  author = {Reed, Scott and {de Freitas}, Nando},
  year = {2016},
  month = feb,
  journal = {ICLR},
  eprint = {1511.06279},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1511.06279},
  url = {http://arxiv.org/abs/1511.06279},
  urldate = {2024-03-19},
  abstract = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/2PMABDIQ/Reed and de Freitas - 2016 - Neural Programmer-Interpreters.pdf}
}

@article{ren_defining_2023,
  title = {Defining and Quantifying the Emergence of Sparse Concepts in DNNs},
  author = {Ren, Jie and Li, Mingjie and Chen, Qirui and Deng, Huiqi and Zhang, Quanshi},
  year = {2023},
  month = apr,
  journal = {CoRR},
  eprint = {2111.06206},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2111.06206},
  url = {http://arxiv.org/abs/2111.06206},
  urldate = {2024-02-19},
  abstract = {This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/DWLWUGJK/Ren et al. - 2023 - Defining and Quantifying the Emergence of Sparse C.pdf}
}

@article{ribeiro_why_2016,
  title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  shorttitle = {"Why Should I Trust You?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  journal = {NAACL},
  eprint = {1602.04938},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1602.04938},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2023-11-16},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3PXZL4NK/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@article{richens_robust_2024,
  title = {Robust agents learn causal world models},
  author = {Richens, Jonathan and Everitt, Tom},
  year = {2024},
  month = feb,
  journal = {ICLR Oral},
  eprint = {2402.10877},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2402.10877},
  urldate = {2024-02-20},
  abstract = {It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound under a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/B5YRXHI2/Richens and Everitt - 2024 - Robust agents learn causal world models.pdf}
}

@article{riegel_logical_2020,
  title = {Logical Neural Networks},
  author = {Riegel, Ryan and Gray, Alexander and Luus, Francois and Khan, Naweed and Makondo, Ndivhuwo and Akhalwaya, Ismail Yunus and Qian, Haifeng and Fagin, Ronald and Barahona, Francisco and Sharma, Udit and Ikbal, Shajith and Karanam, Hima and Neelam, Sumit and Likhyani, Ankita and Srivastava, Santosh},
  year = {2020},
  month = jun,
  journal = {NeurIPS},
  eprint = {2006.13155},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.13155},
  url = {http://arxiv.org/abs/2006.13155},
  urldate = {2023-11-10},
  abstract = {We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3UKMSTG4/Riegel et al. - 2020 - Logical Neural Networks.pdf}
}

@article{rimsky_steering_2024,
  title = {Steering Llama 2 via Contrastive Activation Addition},
  author = {Rimsky, Nina and Gabrieli, Nick and Schulz, Julian and Tong, Meg and Hubinger, Evan and Turner, Alexander Matt},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2312.06681},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.06681},
  url = {http://arxiv.org/abs/2312.06681},
  urldate = {2024-03-18},
  abstract = {We introduce Contrastive Activation Addition (CAA), an innovative method for steering language models by modifying their activations during forward passes. CAA computes "steering vectors" by averaging the difference in residual stream activations between pairs of positive and negative examples of a particular behavior, such as factual versus hallucinatory responses. During inference, these steering vectors are added at all token positions after the user's prompt with either a positive or negative coefficient, allowing precise control over the degree of the targeted behavior. We evaluate CAA's effectiveness on Llama 2 Chat using multiple-choice behavioral question datasets and open-ended generation tasks. We demonstrate that CAA significantly alters model behavior, is effective over and on top of traditional methods like finetuning and system prompt design, and minimally reduces capabilities. Moreover, we gain deeper insights into CAA's mechanisms by employing various activation space interpretation methods. CAA accurately steers model outputs and sheds light on how high-level concepts are represented in Large Language Models (LLMs).},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/TQ2MH2EN/Rimsky et al. - 2024 - Steering Llama 2 via Contrastive Activation Additi.pdf}
}

@book{roberts_principles_2022,
  title = {The Principles of Deep Learning Theory},
  author = {Roberts, Daniel A. and Yaida, Sho and Hanin, Boris},
  year = {2022},
  month = may,
  eprint = {2106.10165},
  primaryclass = {hep-th, stat},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781009023405},
  url = {http://arxiv.org/abs/2106.10165},
  urldate = {2024-02-26},
  abstract = {This book develops an effective theory approach to understanding deep neural networks of practical relevance. Beginning from a first-principles component-level picture of networks, we explain how to determine an accurate description of the output of trained networks by solving layer-to-layer iteration equations and nonlinear learning dynamics. A main result is that the predictions of networks are described by nearly-Gaussian distributions, with the depth-to-width aspect ratio of the network controlling the deviations from the infinite-width Gaussian description. We explain how these effectively-deep networks learn nontrivial representations from training and more broadly analyze the mechanism of representation learning for nonlinear models. From a nearly-kernel-methods perspective, we find that the dependence of such models' predictions on the underlying learning algorithm can be expressed in a simple and universal way. To obtain these results, we develop the notion of representation group flow (RG flow) to characterize the propagation of signals through the network. By tuning networks to criticality, we give a practical solution to the exploding and vanishing gradient problem. We further explain how RG flow leads to near-universal behavior and lets us categorize networks built from different activation functions into universality classes. Altogether, we show that the depth-to-width ratio governs the effective model complexity of the ensemble of trained networks. By using information-theoretic techniques, we estimate the optimal aspect ratio at which we expect the network to be practically most useful and show how residual connections can be used to push this scale to arbitrary depths. With these tools, we can learn in detail about the inductive bias of architectures, hyperparameters, and optimizers.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/M7BGAYY2/Roberts et al. - 2022 - The Principles of Deep Learning Theory.pdf}
}

@article{robertzk_training_2023,
  title = {Training Process Transparency through Gradient Interpretability: Early experiments on toy language models},
  shorttitle = {Training Process Transparency through Gradient Interpretability},
  author = {{robertzk} and {evhub}},
  year = {2023},
  month = jul,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/DtkA5jysFZGv7W4qP/training-process-transparency-through-gradient},
  urldate = {2023-11-10},
  abstract = {The work presented in this post was conducted during the SERI MATS 3.1 program. Thank you to Evan Hubinger for providing feedback on the outlined exp{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/6YU6NDB3/robertzk and evhub - 2023 - Training Process Transparency through Gradient Int.html}
}

@article{rodis_multimodal_2023,
  title = {Multimodal Explainable Artificial Intelligence: A Comprehensive Review of Methodological Advances and Future Research Directions},
  shorttitle = {Multimodal Explainable Artificial Intelligence},
  author = {Rodis, Nikolaos and Sardianos, Christos and Papadopoulos, Georgios Th. and {Radoglou-Grammatikis}, Panagiotis and Sarigiannidis, Panagiotis and Varlamis, Iraklis},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.05731},
  url = {https://arxiv.org/abs/2306.05731},
  urldate = {2023-10-25},
  abstract = {The current study focuses on systematically analyzing the recent advances in the field of Multimodal eXplainable Artificial Intelligence (MXAI). In particular, the relevant primary prediction tasks and publicly available datasets are initially described. Subsequently, a structured presentation of the MXAI methods of the literature is provided, taking into account the following criteria: a) The number of the involved modalities, b) The stage at which explanations are produced, and c) The type of the adopted methodology (i.e. mathematical formalism). Then, the metrics used for MXAI evaluation are discussed. Finally, a comprehensive analysis of current challenges and future research directions is provided.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9UVQ5UVV/Rodis et al. - 2023 - Multimodal Explainable Artificial Intelligence A .pdf}
}

@article{roger_what_2023,
  title = {What Discovering Latent Knowledge Did and Did Not Find},
  author = {Roger, Fabien},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4},
  urldate = {2024-02-12},
  abstract = {Thanks to Marius Hobbhahn and Oam Patel for helpful feedback on drafts. Thanks to Collin and Haotian for answering many questions about their work. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/WRJRDGD6/Roger - 2023 - What Discovering Latent Knowledge Did and Did Not .html}
}

@article{rogers_primer_2021,
  title = {A Primer in BERTology: What We Know About How BERT Works},
  shorttitle = {A Primer in BERTology},
  author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  year = {2021},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {842--866},
  issn = {2307-387X},
  url = {https://doi.org/10.1162/tacl_a_00349},
  urldate = {2023-07-31},
  abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4D8GBQTU/Rogers et al. - 2021 - A Primer in BERTology What We Know About How BERT.pdf}
}

@article{roth_efficient_2021,
  title = {Efficient population coding depends on stimulus convergence and source of noise},
  author = {R{\"o}th, Kai and Shao, Shuai and Gjorgjieva, Julijana},
  year = {2021},
  month = apr,
  journal = {PLOS Computational Biology},
  volume = {17},
  number = {4},
  pages = {e1008897},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008897},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008897},
  urldate = {2024-01-19},
  abstract = {Sensory organs transmit information to downstream brain circuits using a neural code comprised of spikes from multiple neurons. According to the prominent efficient coding framework, the properties of sensory populations have evolved to encode maximum information about stimuli given biophysical constraints. How information coding depends on the way sensory signals from multiple channels converge downstream is still unknown, especially in the presence of noise which corrupts the signal at different points along the pathway. Here, we calculated the optimal information transfer of a population of nonlinear neurons under two scenarios. First, a lumped-coding channel where the information from different inputs converges to a single channel, thus reducing the number of neurons. Second, an independent-coding channel when different inputs contribute independent information without convergence. In each case, we investigated information loss when the sensory signal was corrupted by two sources of noise. We determined critical noise levels at which the optimal number of distinct thresholds of individual neurons in the population changes. Comparing our system to classical physical systems, these changes correspond to first- or second-order phase transitions for the lumped- or the independent-coding channel, respectively. We relate our theoretical predictions to coding in a population of auditory nerve fibers recorded experimentally, and find signatures of efficient coding. Our results yield important insights into the diverse coding strategies used by neural populations to optimally integrate sensory stimuli in the presence of distinct sources of noise.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9XWYGDLU/Rth et al. - 2021 - Efficient population coding depends on stimulus co.pdf}
}

@article{rozell_sparse_2008,
  title = {Sparse Coding via Thresholding and Local Competition in Neural Circuits},
  author = {Rozell, Christopher J. and Johnson, Don H. and Baraniuk, Richard G. and Olshausen, Bruno A.},
  year = {2008},
  month = oct,
  journal = {Neural Computation},
  volume = {20},
  number = {10},
  pages = {2526--2563},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.03-07-486},
  url = {https://doi.org/10.1162/neco.2008.03-07-486},
  urldate = {2023-09-18},
  abstract = {While evidence indicates that neural systems may be employing sparse approximations to represent sensed stimuli, the mechanisms underlying this ability are not understood. We describe a locally competitive algorithm (LCA) that solves a collection of sparse coding principles minimizing a weighted combination of mean-squared error and a coefficient cost function. LCAs are designed to be implemented in a dynamical system composed of many neuron-like elements operating in parallel. These algorithms use thresholding functions to induce local (usually one-way) inhibitory competitions between nodes to produce sparse representations. LCAs produce coefficients with sparsity levels comparable to the most popular centralized sparse coding algorithms while being readily suited for neural implementation. Additionally, LCA coefficients for video sequences demonstrate inertial properties that are both qualitatively and quantitatively more regular (i.e., smoother and more predictable) than the coefficients produced by greedy algorithms.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/CC6FIN65/Rozell et al. - 2008 - Sparse Coding via Thresholding and Local Competiti.pdf}
}

@article{rubenstein_causal_2017,
  title = {Causal Consistency of Structural Equation Models},
  author = {Rubenstein, Paul K. and Weichwald, Sebastian and Bongers, Stephan and Mooij, Joris M. and Janzing, Dominik and {Grosse-Wentrup}, Moritz and Sch{\"o}lkopf, Bernhard},
  year = {2017},
  month = jul,
  journal = {CoRR},
  eprint = {1707.00819},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1707.00819},
  url = {http://arxiv.org/abs/1707.00819},
  urldate = {2024-03-19},
  abstract = {Complex systems can be modelled at various levels of detail. Ideally, causal models of the same system should be consistent with one another in the sense that they agree in their predictions of the effects of interventions. We formalise this notion of consistency in the case of Structural Equation Models (SEMs) by introducing exact transformations between SEMs. This provides a general language to consider, for instance, the different levels of description in the following three scenarios: (a) models with large numbers of variables versus models in which the `irrelevant' or unobservable variables have been marginalised out; (b) micro-level models versus macro-level models in which the macro-variables are aggregate features of the micro-variables; (c) dynamical time series models versus models of their stationary behaviour. Our analysis stresses the importance of well specified interventions in the causal modelling process and sheds light on the interpretation of cyclic SEMs.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/9BBEV3QG/Rubenstein et al. - 2017 - Causal Consistency of Structural Equation Models.pdf}
}

@article{rudin_interpretable_2021,
  title = {Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges},
  shorttitle = {Interpretable Machine Learning},
  author = {Rudin, Cynthia and Chen, Chaofan and Chen, Zhi and Huang, Haiyang and Semenova, Lesia and Zhong, Chudi},
  year = {2021},
  month = jul,
  journal = {CoRR},
  eprint = {2103.11251},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2103.11251},
  url = {http://arxiv.org/abs/2103.11251},
  urldate = {2023-10-20},
  abstract = {Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the "Rashomon set" of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QIFLI8T3/Rudin et al. - 2021 - Interpretable Machine Learning Fundamental Princi.pdf}
}

@article{rudin_stop_2019,
  title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
  author = {Rudin, Cynthia},
  year = {2019},
  month = may,
  journal = {Nat Mach Intell},
  volume = {1},
  number = {5},
  pages = {206--215},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0048-x},
  url = {https://www.nature.com/articles/s42256-019-0048-x},
  urldate = {2023-10-19},
  abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
  copyright = {2019 Springer Nature Limited},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZF8ZW2T9/Rudin - 2019 - Stop explaining black box machine learning models .pdf}
}

@article{rushing_explorations_2024,
  title = {Explorations of Self-Repair in Language Models},
  author = {Rushing, Cody and Nanda, Neel},
  year = {2024},
  month = may,
  journal = {ICML},
  eprint = {2402.15390},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.15390},
  url = {http://arxiv.org/abs/2402.15390},
  urldate = {2024-06-10},
  abstract = {Prior interpretability research studying narrow distributions has preliminarily identified self-repair, a phenomena where if components in large language models are ablated, later components will change their behavior to compensate. Our work builds off this past literature, demonstrating that self-repair exists on a variety of models families and sizes when ablating individual attention heads on the full training distribution. We further show that on the full training distribution self-repair is imperfect, as the original direct effect of the head is not fully restored, and noisy, since the degree of self-repair varies significantly across different prompts (sometimes overcorrecting beyond the original effect). We highlight two different mechanisms that contribute to self-repair, including changes in the final LayerNorm scaling factor and sparse sets of neurons implementing Anti-Erasure. We additionally discuss the implications of these results for interpretability practitioners and close with a more speculative discussion on the mystery of why self-repair occurs in these models at all, highlighting evidence for the Iterative Inference hypothesis in language models, a framework that predicts self-repair.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/EDBBTTRL/Rushing and Nanda - 2024 - Explorations of Self-Repair in Language Models.pdf}
}

@book{russell_human_2019,
  title = {Human Compatible: Artificial Intelligence and the Problem of Control},
  shorttitle = {Human Compatible},
  author = {Russell, Stuart},
  year = {2019},
  month = oct,
  publisher = {Penguin Books},
  abstract = {"The most important book on AI this year." --The Guardian"Mr. Russell's exciting book goes deep, while sparkling with dry witticisms." --The Wall Street Journal"The most important book I have read in quite some time" (Daniel Kahneman); "A must-read" (Max Tegmark); "The book we've all been waiting for" (Sam Harris)A leading artificial intelligence researcher lays out a new approach to AI that will enable us to coexist successfully with increasingly intelligent machinesIn the popular imagination, superhuman artificial intelligence is an approaching tidal wave that threatens not just jobs and human relationships, but civilization itself. Conflict between humans and machines is seen as inevitable and its outcome all too predictable.In this groundbreaking book, distinguished AI researcher Stuart Russell argues that this scenario can be avoided, but only if we rethink AI from the ground up. Russell begins by exploring the idea of intelligence in humans and in machines. He describes the near-term benefits we can expect, from intelligent personal assistants to vastly accelerated scientific research, and outlines the AI breakthroughs that still have to happen before we reach superhuman AI. He also spells out the ways humans are already finding to misuse AI, from lethal autonomous weapons to viral sabotage.If the predicted breakthroughs occur and superhuman AI emerges, we will have created entities far more powerful than ourselves. How can we ensure they never, ever, have power over us? Russell suggests that we can rebuild AI on a new foundation, according to which machines are designed to be inherently uncertain about the human preferences they are required to satisfy. Such machines would be humble, altruistic, and committed to pursue our objectives, not theirs. This new foundation would allow us to create machines that are provably deferential and provably beneficial.},
  language = {English},
  keywords = {not cited}
}

@article{ruthenis_internal_2022,
  title = {Internal Interfaces Are a High-Priority Interpretability Target},
  author = {Ruthenis, Thane},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/nwLQt4e7bstCyPEXs/internal-interfaces-are-a-high-priority-interpretability},
  urldate = {2024-01-20},
  abstract = {tl;dr: ML models, like all software, and like the NAH would predict, must consist of several specialized "modules". Such modules would form interface{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3596EN4Q/Ruthenis - 2022 - Internal Interfaces Are a High-Priority Interpreta.html}
}

@article{ruthenis_worldmodel_2023,
  title = {World-Model Interpretability Is All We Need},
  author = {Ruthenis, Thane},
  year = {2023},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/HaHcsrDSZ3ZC2b4fK/world-model-interpretability-is-all-we-need},
  urldate = {2024-01-20},
  abstract = {Summary, by sections: {$\bullet$}  1. Perfect world-model interpretability seems both sufficient for robust alignment (via a decent variety of approaches) and{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/QYW46J99/Ruthenis - 2023 - World-Model Interpretability Is All We Need.html}
}

@article{ryan_greenblatt_two_2023,
  title = {Two problems with `Simulators' as a frame},
  author = {Greenblatt, Ryan},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/HD2s4mj4fsx6WtFAR/two-problems-with-simulators-as-a-frame},
  urldate = {2023-05-15},
  abstract = {(Thanks to Lawrence Chan and Buck Shlegeris for comments. Thanks to Nate Thomas for many comments and editing) {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/CPCMUL54/Greenblatt - 2023 - Two problems with Simulators as a frame.html}
}

@article{sajjad_neuronlevel_2022,
  title = {Neuron-level Interpretation of Deep NLP Models: A Survey},
  shorttitle = {Neuron-level Interpretation of Deep NLP Models},
  author = {Sajjad, Hassan and Durrani, Nadir and Dalvi, Fahim},
  year = {2022},
  month = nov,
  journal = {TACL},
  volume = {10},
  pages = {1285--1303},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00519},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00519/113852/Neuron-level-Interpretation-of-Deep-NLP-Models-A},
  urldate = {2023-10-22},
  abstract = {Abstract             The proliferation of Deep Neural Networks in various domains has seen an increased need for interpretability of these models. Preliminary work done along this line, and papers that surveyed such, are focused on high-level representation analysis. However, a recent branch of work has concentrated on interpretability at a more granular level of analyzing neurons within these models. In this paper, we survey the work done on neuron analysis including: i) methods to discover and understand neurons in a network; ii) evaluation methods; iii) major findings including cross architectural comparisons that neuron analysis has unraveled; iv) applications of neuron probing such as: controlling the model, domain adaptation, and so forth; and v) a discussion on open issues and future research directions.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/83PIYLKP/Sajjad et al. - 2022 - Neuron-level Interpretation of Deep NLP Models A .pdf}
}

@article{sakarvadia_attention_2023,
  title = {Attention Lens: A Tool for Mechanistically Interpreting the Attention Head Information Retrieval Mechanism},
  shorttitle = {Attention Lens},
  author = {Sakarvadia, Mansi and Khan, Arham and Ajith, Aswathy and Grzenda, Daniel and Hudson, Nathaniel and Bauer, Andr{\'e} and Chard, Kyle and Foster, Ian},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.16270},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.16270},
  url = {http://arxiv.org/abs/2310.16270},
  urldate = {2023-11-16},
  abstract = {Transformer-based Large Language Models (LLMs) are the state-of-the-art for natural language tasks. Recent work has attempted to decode, by reverse engineering the role of linear layers, the internal mechanisms by which LLMs arrive at their final predictions for text completion tasks. Yet little is known about the specific role of attention heads in producing the final token prediction. We propose Attention Lens, a tool that enables researchers to translate the outputs of attention heads into vocabulary tokens via learned attention-head-specific transformations called lenses. Preliminary findings from our trained lenses indicate that attention heads play highly specialized roles in language models. The code for Attention Lens is available at github.com/msakarvadia/AttentionLens.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/2AZZVNYE/Sakarvadia et al. - 2023 - Attention Lens A Tool for Mechanistically Interpr.pdf}
}

@article{sakarvadia_memory_2023,
  title = {Memory Injections: Correcting Multi-Hop Reasoning Failures during Inference in Transformer-Based Language Models},
  shorttitle = {Memory Injections},
  author = {Sakarvadia, Mansi and Ajith, Aswathy and Khan, Arham and Grzenda, Daniel and Hudson, Nathaniel and Bauer, Andr{\'e} and Chard, Kyle and Foster, Ian},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.05605},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.05605},
  url = {http://arxiv.org/abs/2309.05605},
  urldate = {2023-11-16},
  abstract = {Answering multi-hop reasoning questions requires retrieving and synthesizing information from diverse sources. Large Language Models (LLMs) struggle to perform such reasoning consistently. Here we propose an approach to pinpoint and rectify multi-hop reasoning failures through targeted memory injections on LLM attention heads. First, we analyze the per-layer activations of GPT-2 models in response to single and multi-hop prompts. We then propose a mechanism that allows users to inject pertinent prompt-specific information, which we refer to as "memories," at critical LLM locations during inference. By thus enabling the LLM to incorporate additional relevant information during inference, we enhance the quality of multi-hop prompt completions. We show empirically that a simple, efficient, and targeted memory injection into a key attention layer can often increase the probability of the desired next token in multi-hop tasks, by up to 424\%.},
  archiveprefix = {arxiv},
  keywords = {graph,not cited,to cite,to extract related work,transformer},
  file = {/Users/leonardbereska/Zotero/storage/ZN4Z5XJV/Sakarvadia et al. - 2023 - Memory Injections Correcting Multi-Hop Reasoning .pdf}
}

@article{salin_are_2022,
  title = {Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective},
  shorttitle = {Are Vision-Language Transformers Learning Multimodal Representations?},
  author = {Salin, Emmanuelle and Farah, Badreddine and Ayache, St{\'e}phane and Favre, Benoit},
  year = {2022},
  month = jun,
  journal = {AAAI},
  volume = {36},
  number = {10},
  pages = {11248--11257},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v36i10.21375},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/21375},
  urldate = {2023-10-25},
  abstract = {In recent years, joint text-image embeddings have significantly improved thanks to the development of transformer-based Vision-Language models. Despite these advances, we still need to better understand the representations produced by those models. In this paper, we compare pre-trained and fine-tuned representations at a vision, language and multimodal level. To that end, we use a set of probing tasks to evaluate the performance of state-of-the-art Vision-Language models and introduce new datasets specifically for multimodal probing. These datasets are carefully designed to address a range of multimodal capabilities while minimizing the potential for models to rely on bias. Although the results confirm the ability of Vision-Language models to understand color at a multimodal level, the models seem to prefer relying on bias in text data for object position and size. On semantically adversarial examples, we find that those models are able to pinpoint fine-grained multimodal differences. Finally, we also notice that fine-tuning a Vision-Language model on multimodal tasks does not necessarily improve its multimodal ability. We make all datasets and code available to replicate experiments.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/E6IVZD6L/Salin et al. - 2022 - Are Vision-Language Transformers Learning Multimod.pdf}
}

@article{salvatori_braininspired_2023,
  title = {Brain-Inspired Computational Intelligence via Predictive Coding},
  author = {Salvatori, Tommaso and Mali, Ankur and Buckley, Christopher L. and Lukasiewicz, Thomas and Rao, Rajesh P. N. and Friston, Karl and Ororbia, Alexander},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2308.07870},
  url = {https://arxiv.org/abs/2308.07870},
  urldate = {2024-01-23},
  abstract = {Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a powerful inversion scheme for a specific class of continuous-state generative models. With the hope of foregrounding research in this direction, we survey the literature that has contributed to this perspective, highlighting the many ways that PC might play a role in the future of machine learning and computational intelligence at large.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9462ERF4/Salvatori et al. - 2023 - Brain-Inspired Computational Intelligence via Pred.pdf}
}

@article{samek_explaining_2021,
  title = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  shorttitle = {Explaining Deep Neural Networks and Beyond},
  author = {Samek, Wojciech and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and Anders, Christopher J. and M{\"u}ller, Klaus-Robert},
  year = {2021},
  month = mar,
  journal = {Proc. IEEE},
  volume = {109},
  number = {3},
  pages = {247--278},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2021.3060483},
  url = {https://arxiv.org/abs/2003.07631},
  abstract = {With the broader and highly successful usage of machine learning (ML) in industry and the sciences, there has been a growing demand for explainable artificial intelligence (XAI). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear ML, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on ``post hoc'' explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of ML; and 4) demonstrate successful usage of XAI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of ML.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/B9NFIW4G/Samek et al. - 2021 - Explaining Deep Neural Networks and Beyond A Revi.pdf}
}

@article{samin_smart_2023,
  title = {A smart enough LLM might be deadly if you run it for long enough},
  author = {Samin, Mikhail},
  year = {2023},
  month = may,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/PfoJLZg2e4xXt2Q6a/a-smart-enough-llm-might-be-deadly-if-you-run-it-for-long},
  urldate = {2023-05-15},
  abstract = {TL;DR: I introduce a new potential threat: a smart enough LLM, even if it's myopic, not fine-tuned with any sort of RL, ``non-agentic'', and was prompt-engineered into imitating aligned and helpful sc{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GM7CDHMF/Samin - 2023 - A smart enough LLM might be deadly if you run it f.html}
}

@article{sanchez_stay_2023,
  title = {Stay on topic with Classifier-Free Guidance},
  author = {Sanchez, Guillaume and Fan, Honglu and Spangher, Alexander and Levi, Elad and Ammanamanchi, Pawan Sasanka and Biderman, Stella},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2306.17806},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.17806},
  url = {http://arxiv.org/abs/2306.17806},
  urldate = {2023-10-30},
  abstract = {Classifier-Free Guidance (CFG) has recently emerged in text-to-image generation as a lightweight technique to encourage prompt-adherence in generations. In this work, we demonstrate that CFG can be used broadly as an inference-time technique in pure language modeling. We show that CFG (1) improves the performance of Pythia, GPT-2 and LLaMA-family models across an array of tasks: Q{\textbackslash}\&A, reasoning, code generation, and machine translation, achieving SOTA on LAMBADA with LLaMA-7B over PaLM-540B; (2) brings improvements equivalent to a model with twice the parameter-count; (3) can stack alongside other inference-time methods like Chain-of-Thought and Self-Consistency, yielding further improvements in difficult tasks; (4) can be used to increase the faithfulness and coherence of assistants in challenging form-driven and content-driven prompts: in a human evaluation we show a 75{\textbackslash}\% preference for GPT4All using CFG over baseline.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JW47BRMT/Sanchez et al. - 2023 - Stay on topic with Classifier-Free Guidance.pdf}
}

@article{saphra_interpretability_2023,
  title = {Interpretability Creationism},
  author = {Saphra, Naomi},
  year = {2023},
  journal = {The Gradient},
  url = {https://thegradient.pub/interpretability-creationism},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9MB9GV8I/Saphra - 2023 - Interpretability Creationism.html}
}

@article{sarti_inseq_2023,
  title = {Inseq: An Interpretability Toolkit for Sequence Generation Models},
  shorttitle = {Inseq},
  author = {Sarti, Gabriele and Feldhus, Nils and Sickert, Ludwig and Van Der Wal, Oskar},
  year = {2023},
  journal = {ACL},
  pages = {421--435},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.acl-demo.40},
  url = {https://aclanthology.org/2023.acl-demo.40},
  urldate = {2024-02-11},
  abstract = {Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.},
  language = {en},
  keywords = {not cited,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/3554H463/Sarti et al. - 2023 - Inseq An Interpretability Toolkit for Sequence Ge.pdf}
}

@article{schaeffer_are_2023,
  title = {Are Emergent Abilities of Large Language Models a Mirage?},
  author = {Schaeffer, Rylan and Miranda, Brando and Koyejo, Sanmi},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2304.15004},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.15004},
  url = {http://arxiv.org/abs/2304.15004},
  urldate = {2023-07-03},
  abstract = {Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/42BM4K9C/Schaeffer et al. - 2023 - Are Emergent Abilities of Large Language Models a .pdf}
}

@article{scherlis_inner_2023,
  title = {Inner Misalignment in "Simulator" LLMs},
  author = {Scherlis, Adam},
  year = {2023},
  month = jan,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/FLMyTjuTiGytE6sP2/inner-misalignment-in-simulator-llms},
  urldate = {2023-05-15},
  abstract = {Alternate title: "Somewhat Contra Scott On Simulators". {$\bullet$} Scott Alexander has a recent post up on large language models as simulators. {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4C9EVJ9P/Scherlis - 2023 - Inner Misalignment in Simulator LLMs.html}
}

@article{scherlis_polysemanticity_2023,
  title = {Polysemanticity and Capacity in Neural Networks},
  author = {Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S. and Benton, Joe and Shlegeris, Buck},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2210.01892},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.01892},
  url = {http://arxiv.org/abs/2210.01892},
  urldate = {2023-09-18},
  abstract = {Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature {\textbackslash}emph\{capacity\}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model architecture on the interpretability of its neurons.},
  archiveprefix = {arxiv},
  keywords = {cited,fundamental,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/DASCWAYC/Scherlis et al. - 2023 - Polysemanticity and Capacity in Neural Networks.pdf}
}

@article{scholkopf_causal_2021,
  title = {Towards Causal Representation Learning},
  author = {Sch{\"o}lkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
  year = {2021},
  month = feb,
  journal = {Special Issue of Proceedings of the IEEE - Advances in Machine Learning and Deep Neural Networks},
  eprint = {2102.11107},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2102.11107},
  url = {http://arxiv.org/abs/2102.11107},
  urldate = {2024-06-10},
  abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/6EGNHKIW/Schlkopf et al. - 2021 - Towards Causal Representation Learning.pdf}
}

@article{schrimpf_neural_2021,
  title = {The neural architecture of language: Integrative modeling converges on predictive processing},
  shorttitle = {The neural architecture of language},
  author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2021},
  month = nov,
  journal = {PNAS},
  volume = {118},
  number = {45},
  pages = {e2105646118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2105646118},
  url = {https://www.pnas.org/doi/10.1073/pnas.2105646118},
  urldate = {2023-05-17},
  abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful ``transformer'' models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models' neural fits (``brain score'') and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/G544AXQX/Schrimpf et al. - 2021 - The neural architecture of language Integrative m.pdf}
}

@article{schubert_high_2021,
  title = {High/Low frequency detectors},
  author = {Schubert, Ludwig and Voss, Chelsea and Olah, Chris},
  year = {2021},
  month = jan,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/frequency-edges},
  urldate = {2024-02-14},
  keywords = {not cited,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/IEEAFP39/Schubert et al. - 2021 - HighLow frequency detectors.html}
}

@article{schulz_simulators_2023,
  title = {Simulators Increase the Likelihood of Alignment by Default},
  author = {Schulz, Wuschel},
  year = {2023},
  month = apr,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/yzGPLdEqa2ytT7MY2/simulators-increase-the-likelihood-of-alignment-by-default},
  urldate = {2023-05-08},
  abstract = {Alignment by Default is the idea that achieving alignment in artificial general intelligence (AGI) may be more straightforward than initially anticipated. When an AI possesses a comprehensive and det{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KKNVRTYI/Schulz - 2023 - Simulators Increase the Likelihood of Alignment by.html}
}

@article{schuster_confident_2022,
  title = {Confident Adaptive Language Modeling},
  author = {Schuster, Tal and Fisch, Adam and Gupta, Jai and Dehghani, Mostafa and Bahri, Dara and Tran, Vinh Q. and Tay, Yi and Metzler, Donald},
  year = {2022},
  month = oct,
  journal = {NeurIPS Oral},
  eprint = {2207.07061},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2207.07061},
  url = {http://arxiv.org/abs/2207.07061},
  urldate = {2024-03-14},
  abstract = {Recent advances in Transformer-based large language models (LLMs) have led to significant performance improvements across many tasks. These gains come with a drastic increase in the models' size, potentially leading to slow and costly use at inference time. In practice, however, the series of generations made by LLMs is composed of varying levels of difficulty. While certain predictions truly benefit from the models' full capacity, other continuations are more trivial and can be solved with reduced compute. In this work, we introduce Confident Adaptive Language Modeling (CALM), a framework for dynamically allocating different amounts of compute per input and generation timestep. Early exit decoding involves several challenges that we address here, such as: (1) what confidence measure to use; (2) connecting sequence-level constraints to local per-token exit decisions; and (3) attending back to missing hidden representations due to early exits in previous tokens. Through theoretical analysis and empirical experiments on three diverse text generation tasks, we demonstrate the efficacy of our framework in reducing compute -- potential speedup of up to \${\textbackslash}times 3\$ -- while provably maintaining high performance.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MEQFBTLZ/Schuster et al. - 2022 - Confident Adaptive Language Modeling.pdf}
}

@article{schwalbe_comprehensive_2023,
  title = {A comprehensive taxonomy for explainable artificial intelligence: a systematic survey of surveys on methods and concepts},
  shorttitle = {A comprehensive taxonomy for explainable artificial intelligence},
  author = {Schwalbe, Gesina and Finzel, Bettina},
  year = {2023},
  month = jan,
  journal = {Data Min Knowl Disc},
  issn = {1573-756X},
  doi = {10.1007/s10618-022-00867-8},
  url = {https://doi.org/10.1007/s10618-022-00867-8},
  urldate = {2024-02-11},
  abstract = {In the meantime, a wide variety of terminologies, motivations, approaches, and evaluation criteria have been developed within the research field of explainable artificial intelligence (XAI). With the amount of XAI methods vastly growing, a taxonomy of methods is needed by researchers as well as practitioners: To grasp the breadth of the topic, compare methods, and to select the right XAI method based on traits required by a specific use-case context. Many taxonomies for XAI methods of varying level of detail and depth can be found in the literature. While they often have a different focus, they also exhibit many points of overlap. This paper unifies these efforts and provides a complete taxonomy of XAI methods with respect to notions present in the current state of research. In a structured literature analysis and meta-study, we identified and reviewed more than 50 of the most cited and current surveys on XAI methods, metrics, and method traits. After summarizing them in a survey of surveys, we merge terminologies and concepts of the articles into a unified structured taxonomy. Single concepts therein are illustrated by more than 50 diverse selected example methods in total, which we categorize accordingly. The taxonomy may serve both beginners, researchers, and practitioners as a reference and wide-ranging overview of XAI method traits and aspects. Hence, it provides foundations for targeted, use-case-oriented, and context-sensitive future research.},
  language = {en},
  keywords = {not cited,survey,taxonomy,XAI},
  file = {/Users/leonardbereska/Zotero/storage/SJUYQKWU/Schwalbe and Finzel - 2023 - A comprehensive taxonomy for explainable artificia.pdf}
}

@article{schwettmann_find_2024,
  title = {Find: A function description benchmark for evaluating interpretability methods},
  shorttitle = {Find},
  author = {Schwettmann, Sarah and Shaham, Tamar and Materzynska, Joanna and Chowdhury, Neil and Li, Shuang and Andreas, Jacob and Bau, David and Torralba, Antonio},
  year = {2024},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ef0164c1112f56246224af540857348f-Abstract-Datasets_and_Benchmarks.html},
  urldate = {2024-06-10},
  file = {/Users/leonardbereska/Zotero/storage/ZM94ABLI/Schwettmann et al. - 2024 - Find A function description benchmark for evaluat.pdf}
}

@article{schwettmann_function_2023,
  title = {A Function Interpretation Benchmark for Evaluating Interpretability Methods},
  author = {Schwettmann, Sarah and Shaham, Tamar Rott and Materzynska, Joanna and Chowdhury, Neil and Li, Shuang and Andreas, Jacob and Bau, David and Torralba, Antonio},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.03886},
  url = {https://arxiv.org/abs/2309.03886},
  urldate = {2023-10-26},
  abstract = {Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate new and existing methods that use language models (LMs) to produce code-based and language descriptions of function behavior. We find that an off-the-shelf LM augmented with only black-box access to functions can sometimes infer their structure, acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, LM-based descriptions tend to capture global function behavior and miss local corruptions. These results show that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/XG8K5QFV/Schwettmann et al. - 2023 - A Function Interpretation Benchmark for Evaluating.pdf}
}

@article{selvaraju_gradcam_2016,
  title = {Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization},
  shorttitle = {Grad-CAM},
  author = {Selvaraju, Ramprasaath R. and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  year = {2016},
  journal = {ICCV},
  url = {https://arxiv.org/abs/1610.02391},
  urldate = {2024-01-24},
  abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are "important" for predictions from these models - or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping. Unlike CAM, Grad-CAM requires no re-training and is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into their failure modes showing that seemingly unreasonable predictions have reasonable explanations, and (b) outperform pixel-space gradient visualizations (Guided Backpropagation and Deconvolution) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + LSTM models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a "stronger" deep network from a "weaker" one even when both networks make identical predictions.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/X5ZZG62M/Selvaraju et al. - 2016 - Grad-CAM Why did you say that Visual Explanation.pdf}
}

@article{septon_integrating_2022,
  title = {Integrating Policy Summaries with Reward Decomposition for Explaining Reinforcement Learning Agents},
  author = {Septon, Yael and Huber, Tobias and Andr{\'e}, Elisabeth and Amir, Ofra},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2210.11825},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.11825},
  url = {http://arxiv.org/abs/2210.11825},
  urldate = {2023-06-01},
  abstract = {Explaining the behavior of reinforcement learning agents operating in sequential decision-making settings is challenging, as their behavior is affected by a dynamic environment and delayed rewards. Methods that help users understand the behavior of such agents can roughly be divided into local explanations that analyze specific decisions of the agents and global explanations that convey the general strategy of the agents. In this work, we study a novel combination of local and global explanations for reinforcement learning agents. Specifically, we combine reward decomposition, a local explanation method that exposes which components of the reward function influenced a specific decision, and HIGHLIGHTS, a global explanation method that shows a summary of the agent's behavior in decisive states. We conducted two user studies to evaluate the integration of these explanation methods and their respective benefits. Our results show significant benefits for both methods. In general, we found that the local reward decomposition was more useful for identifying the agents' priorities. However, when there was only a minor difference between the agents' preferences, then the global information provided by HIGHLIGHTS additionally improved participants' understanding.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/E2QEBXUI/Septon et al. - 2022 - Integrating Policy Summaries with Reward Decomposi.pdf}
}

@article{shah_148_2021,
  title = {[AN \#148]: Analyzing generalization across more axes than just accuracy or loss},
  shorttitle = {[AN \#148]},
  author = {Shah, Rohin},
  year = {2021},
  month = apr,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/H79dxa7XXMBhwqZLm/an-148-analyzing-generalization-across-more-axes-than-just},
  urldate = {2023-12-04},
  abstract = {Alignment Newsletter is a weekly publication with recent content relevant to AI alignment around the world. Find all Alignment Newsletter resources h{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/Z98NJZAU/Shah - 2021 - [AN #148] Analyzing generalization across more ax.html}
}

@article{shah_categorizing_2023,
  title = {Categorizing failures as ``outer'' or ``inner'' misalignment is often confused},
  author = {Shah, Rohin},
  year = {2023},
  month = jun,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/JKwrDwsaRiSxTv9ur/categorizing-failures-as-outer-or-inner-misalignment-is},
  urldate = {2023-08-26},
  abstract = {Pop quiz: Are the following failures examples of outer or inner misalignment? Or is it ambiguous? {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9GR555W7/Shah - 2023 - Categorizing failures as outer or inner misali.html}
}

@article{shah_goal_2022,
  title = {Goal Misgeneralization: Why Correct Specifications Aren't Enough For Correct Goals},
  shorttitle = {Goal Misgeneralization},
  author = {Shah, Rohin and Varma, Vikrant and Kumar, Ramana and Phuong, Mary and Krakovna, Victoria and Uesato, Jonathan and Kenton, Zac},
  year = {2022},
  month = nov,
  journal = {CoRR},
  eprint = {2210.01790},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.01790},
  url = {http://arxiv.org/abs/2210.01790},
  urldate = {2023-02-23},
  abstract = {The field of AI alignment is concerned with AI systems that pursue unintended goals. One commonly studied mechanism by which an unintended goal might arise is specification gaming, in which the designer-provided specification is flawed in a way that the designers did not foresee. However, an AI system may pursue an undesired goal even when the specification is correct, in the case of goal misgeneralization. Goal misgeneralization is a specific form of robustness failure for learning algorithms in which the learned program competently pursues an undesired goal that leads to good performance in training situations but bad performance in novel test situations. We demonstrate that goal misgeneralization can occur in practical systems by providing several examples in deep learning systems across a variety of domains. Extrapolating forward to more capable systems, we provide hypotheticals that illustrate how goal misgeneralization could lead to catastrophic risk. We suggest several research directions that could reduce the risk of goal misgeneralization for future systems.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HUMXDHJU/Shah et al. - 2022 - Goal Misgeneralization Why Correct Specifications.pdf}
}

@article{shao_gold_2023,
  title = {Gold Doesn't Always Glitter: Spectral Removal of Linear and Nonlinear Guarded Attribute Information},
  shorttitle = {Gold Doesn't Always Glitter},
  author = {Shao, Shun and Ziser, Yftah and Cohen, Shay B.},
  year = {2023},
  month = apr,
  journal = {EACL},
  eprint = {2203.07893},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2203.07893},
  url = {http://arxiv.org/abs/2203.07893},
  urldate = {2023-11-10},
  abstract = {We describe a simple and effective method (Spectral Attribute removaL; SAL) to remove private or guarded information from neural representations. Our method uses matrix decomposition to project the input representations into directions with reduced covariance with the guarded information rather than maximal covariance as factorization methods normally use. We begin with linear information removal and proceed to generalize our algorithm to the case of nonlinear information removal using kernels. Our experiments demonstrate that our algorithm retains better main task performance after removing the guarded information compared to previous work. In addition, our experiments demonstrate that we need a relatively small amount of guarded attribute data to remove information about these attributes, which lowers the exposure to sensitive data and is more suitable for low-resource scenarios. Code is available at https://github.com/jasonshaoshun/SAL.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/59LF3Z7S/Shao et al. - 2023 - Gold Doesn't Always Glitter Spectral Removal of L.pdf}
}

@article{shapley_value_1988,
  title = {A value for {\emph{n}} -person games},
  author = {Shapley, Lloyd S.},
  editor = {Roth, Alvin E.},
  year = {1988},
  month = oct,
  journal = {Cambridge University Press},
  pages = {31--40},
  url = {https://www.cambridge.org/core/product/identifier/CBO9780511528446A008/type/book_part},
  urldate = {2024-01-24},
  abstract = {Introduction At the foundation of the theory of games is the assumption that the players of a game can evaluate, in their utility scales, every ``prospect'' that might arise as a result of a play. In attempting to apply the theory to any field, one would normally expect to be permitted to include, in the class of ``prospects,'' the prospect of having to play a game. The possibility of evaluating games is therefore of critical importance. So long as the theory is unable to assign values to the games typically found in application, only relatively simple situations---where games do not depend on other games---will be susceptible to analysis and solution. In the finite theory of von Neumann and Morgenstern difficulty in evaluation persists for the ``essential'' games, and for only those. In this note we deduce a value for the ``essential'' case and examine a number of its elementary properties. We proceed from a set of three axioms, having simple intuitive interpretations, which suffice to determine the value uniquely. Our present work, though mathematically self-contained, is founded conceptually on the von Neumann---Morgenstern theory up to their introduction of characteristic functions. We thereby inherit certain important underlying assumptions: (a) that utility is objective and transferable; (b) that games are cooperative affairs; (c) that games, granting (a) and (b), are adequately represented by their characteristic functions.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/L3X3AWHJ/Shapley - 1988 - A value for n -person games.pdf}
}

@article{sharkey_circumventing_2022,
  title = {Circumventing interpretability: How to defeat mind-readers},
  shorttitle = {Circumventing interpretability},
  author = {Sharkey, Lee},
  year = {2022},
  month = dec,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2212.11415},
  url = {https://arxiv.org/abs/2212.11415},
  urldate = {2023-09-18},
  abstract = {The increasing capabilities of artificial intelligence (AI) systems make it ever more important that we interpret their internals to ensure that their intentions are aligned with human values. Yet there is reason to believe that misaligned artificial intelligence will have a convergent instrumental incentive to make its thoughts difficult for us to interpret. In this article, I discuss many ways that a capable AI might circumvent scalable interpretability methods and suggest a framework for thinking about these potential future risks.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/WN43KVNC/Sharkey - 2022 - Circumventing interpretability How to defeat mind.pdf}
}

@article{sharkey_current_2022,
  title = {Current themes in mechanistic interpretability research},
  author = {Sharkey, Lee and Black, Sid and {beren}},
  year = {2022},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research},
  urldate = {2023-11-29},
  abstract = {This post gives an overview of discussions - from the perspective and understanding of the interpretability team at Conjecture - between mechanistic{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/V6HHNRQQ/Sharkey et al. - 2022 - Current themes in mechanistic interpretability res.html}
}

@article{sharkey_fundamental_2023,
  title = {'Fundamental' vs 'applied' mechanistic interpretability research},
  author = {Sharkey, Lee},
  year = {2023},
  month = may,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/uvEyizLAGykH8LwMx/fundamental-vs-applied-mechanistic-interpretability-research},
  urldate = {2023-11-30},
  abstract = {When justifying my mechanistic interpretability research interests to others, I've occasionally found it useful to borrow a distinction from physics{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/LY5HG3NP/Sharkey - 2023 - 'Fundamental' vs 'applied' mechanistic interpretab.html}
}

@article{sharkey_taking_2022,
  title = {Taking features out of superposition with sparse autoencoders},
  author = {Sharkey, Lee and Braun, Dan and Millidge, Beren},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition},
  urldate = {2023-07-31},
  abstract = {Recent results from Anthropic suggest that neural networks represent features in superposition. This motivates the search for a method that can identify those features. Here, we construct a toy dataset of neural activations and see if we can recover the known ground truth features using sparse coding. We show that, contrary to some initial expectations, it turns out that an extremely simple method -- training a single layer autoencoder to reconstruct neural activations with an L1 penalty on hidden activations -- doesn't just identify features that minimize the loss, but actually recovers the ground truth features that generated the data. We're sharing these observations quickly so that others can begin to extract the features used by neural networks as early as possible. We also share some incomplete observations of what happens when we apply this method to a small language model and our reflections on further research directions.},
  language = {en},
  keywords = {added one sentence summary,cited,feature,mechinterp,SAE,superposition,to cite,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/8KE3X8B9/Sharkey et al. - 2022 - [Interim research report] Taking features out of s.html}
}

@article{sharkey_technical_2023,
  title = {A technical note on bilinear layers for interpretability},
  author = {Sharkey, Lee},
  year = {2023},
  month = may,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.03452},
  url = {https://arxiv.org/abs/2305.03452},
  urldate = {2023-07-31},
  abstract = {The ability of neural networks to represent more features than neurons makes interpreting them challenging. This phenomenon, known as superposition, has spurred efforts to find architectures that are more interpretable than standard multilayer perceptrons (MLPs) with elementwise activation functions. In this note, I examine bilinear layers, which are a type of MLP layer that are mathematically much easier to analyze while simultaneously performing better than standard MLPs. Although they are nonlinear functions of their input, I demonstrate that bilinear layers can be expressed using only linear operations and third order tensors. We can integrate this expression for bilinear layers into a mathematical framework for transformer circuits, which was previously limited to attention-only transformers. These results suggest that bilinear layers are easier to analyze mathematically than current architectures and thus may lend themselves to deeper safety insights by allowing us to talk more formally about circuits in neural networks. Additionally, bilinear layers may offer an alternative path for mechanistic interpretability through understanding the mechanisms of feature construction instead of enumerating a (potentially exponentially) large number of features in large models.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,fundamental,intrinsic,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/QU97FZJU/Sharkey - 2023 - A technical note on bilinear layers for interpreta.pdf}
}

@article{sharkey_theories_2023,
  title = {Theories of Change for AI Auditing},
  author = {Sharkey, Lee and {beren} and Hobbhahn, Marius},
  year = {2023},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/LwJwDNFhjurAKFiJm/theories-of-change-for-ai-auditing},
  urldate = {2023-11-30},
  abstract = {Executive summary Our mission at Apollo Research is to reduce catastrophic risks from AI by auditing advanced AI systems for misalignment and dangero{\dots}},
  language = {en},
  keywords = {not cited,relevance,to cite},
  file = {/Users/leonardbereska/Zotero/storage/VE98J2RK/Sharkey et al. - 2023 - Theories of Change for AI Auditing.html}
}

@article{sharma_interpretable_2022,
  title = {Interpretable Deep Reinforcement Learning for Green Security Games with Real-Time Information},
  author = {Sharma, Vishnu Dutt and Dickerson, John P. and Tokekar, Pratap},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2211.04987},
  url = {https://arxiv.org/abs/2211.04987},
  urldate = {2023-05-31},
  abstract = {Green Security Games with real-time information (GSG-I) add the real-time information about the agents' movement to the typical GSG formulation. Prior works on GSG-I have used deep reinforcement learning (DRL) to learn the best policy for the agent in such an environment without any need to store the huge number of state representations for GSG-I. However, the decision-making process of DRL methods is largely opaque, which results in a lack of trust in their predictions. To tackle this issue, we present an interpretable DRL method for GSG-I that generates visualization to explain the decisions taken by the DRL algorithm. We also show that this approach performs better and works well with a simpler training regimen compared to the existing method.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5UUWTWFU/Sharma et al. - 2022 - Interpretable Deep Reinforcement Learning for Gree.pdf}
}

@article{sharma_understanding_2023,
  title = {Towards Understanding Sycophancy in Language Models},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and {Hatfield-Dodds}, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.13548},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2310.13548},
  url = {http://arxiv.org/abs/2310.13548},
  urldate = {2023-10-25},
  abstract = {Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgements favoring sycophantic responses.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/CBSQKCYB/Sharma et al. - 2023 - Towards Understanding Sycophancy in Language Model.pdf}
}

@article{shen_large_2023,
  title = {Large Language Model Alignment: A Survey},
  shorttitle = {Large Language Model Alignment},
  author = {Shen, Tianhao and Jin, Renren and Huang, Yufei and Liu, Chuang and Dong, Weilong and Guo, Zishan and Wu, Xinwei and Liu, Yan and Xiong, Deyi},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.15025},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.15025},
  url = {http://arxiv.org/abs/2309.15025},
  urldate = {2024-01-16},
  abstract = {Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values. This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead. Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.},
  archiveprefix = {arxiv},
  keywords = {alignment,LLMs,not cited,survey,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/AVUHTVKX/Shen et al. - 2023 - Large Language Model Alignment A Survey.pdf}
}

@article{shen_positional_2023,
  title = {Positional Description Matters for Transformers Arithmetic},
  author = {Shen, Ruoqi and Bubeck, S{\'e}bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2311.14737},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2311.14737},
  url = {http://arxiv.org/abs/2311.14737},
  urldate = {2023-11-29},
  abstract = {Transformers, central to the successes in modern Natural Language Processing, often falter on arithmetic tasks despite their vast capabilities --which paradoxically include remarkable coding abilities. We observe that a crucial challenge is their naive reliance on positional information to solve arithmetic problems with a small number of digits, leading to poor performance on larger numbers. Herein, we delve deeper into the role of positional encoding, and propose several ways to fix the issue, either by modifying the positional encoding directly, or by modifying the representation of the arithmetic task to leverage standard positional encoding differently. We investigate the value of these modifications for three tasks: (i) classical multiplication, (ii) length extrapolation in addition, and (iii) addition in natural language context. For (i) we train a small model on a small dataset (100M parameters and 300k samples) with remarkable aptitude in (direct, no scratchpad) 15 digits multiplication and essentially perfect up to 12 digits, while usual training in this context would give a model failing at 4 digits multiplication. In the experiments on addition, we use a mere 120k samples to demonstrate: for (ii) extrapolation from 10 digits to testing on 12 digits numbers while usual training would have no extrapolation, and for (iii) almost perfect accuracy up to 5 digits while usual training would be correct only up to 3 digits (which is essentially memorization with a training set of 120k samples).},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QZ78VWPG/Shen et al. - 2023 - Positional Description Matters for Transformers Ar.pdf}
}

@article{shlegeris_trying_2022,
  title = {Trying to disambiguate different questions about whether RLHF is ``good''},
  author = {Shlegeris, Buck},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/NG6FrXgmqPd5Wn3mh/trying-to-disambiguate-different-questions-about-whether},
  urldate = {2023-05-15},
  abstract = {(A few of the words in this post were written by Ryan Greenblatt and Ajeya Cotra. Thanks to Oliver Habryka and Max Nadeau for particularly helpful comments.) {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YPGICEKT/Shlegeris - 2022 - Trying to disambiguate different questions about w.html}
}

@article{shovelain_riskreward_2023,
  title = {The risk-reward tradeoff of interpretability research},
  author = {Shovelain, Justin and McKernon, Elliot},
  year = {2023},
  month = jul,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/HdqdqNC3MyABHzSqf/the-risk-reward-tradeoff-of-interpretability-research},
  urldate = {2023-12-05},
  abstract = {Interpretability research is conducted to improve our understanding of AI. Many see interpretability as essential for AI safety, but recently some ha{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZQVMSZL9/Shovelain and McKernon - 2023 - The risk-reward tradeoff of interpretability resea.html}
}

@article{shrikumar_learning_2017,
  title = {Learning Important Features Through Propagating Activation Differences},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  year = {2017},
  journal = {ICML},
  eprint = {1704.02685},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1704.02685},
  url = {http://arxiv.org/abs/1704.02685},
  urldate = {2024-01-24},
  abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/8EXRIGLC/Shrikumar et al. - 2017 - Learning Important Features Through Propagating Ac.pdf}
}

@article{shwartz-ziv_compress_2023,
  title = {To Compress or Not to Compress- Self-Supervised Learning and Information Theory: A Review},
  shorttitle = {To Compress or Not to Compress- Self-Supervised Learning and Information Theory},
  author = {{Shwartz-Ziv}, Ravid and LeCun, Yann},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2304.09355},
  primaryclass = {cs, math},
  doi = {10.48550/arXiv.2304.09355},
  url = {http://arxiv.org/abs/2304.09355},
  urldate = {2023-08-26},
  abstract = {{\textbackslash}begin\{abstract\} Deep neural networks excel in supervised learning tasks but are constrained by the need for extensive labeled data. Self-supervised learning emerges as a promising alternative, allowing models to learn without explicit labels. Information theory, and notably the information bottleneck principle, has been pivotal in shaping deep neural networks. This principle focuses on optimizing the trade-off between compression and preserving relevant information, providing a foundation for efficient network design in supervised contexts. However, its precise role and adaptation in self-supervised learning remain unclear. In this work, we scrutinize various self-supervised learning approaches from an information-theoretic perspective, introducing a unified framework that encapsulates the self-supervised information-theoretic learning problem. We weave together existing research into a cohesive narrative, delve into contemporary self-supervised methodologies, and spotlight potential research avenues and inherent challenges. Additionally, we discuss the empirical evaluation of information-theoretic quantities and their estimation methods. Overall, this paper furnishes an exhaustive review of the intersection of information theory, self-supervised learning, and deep neural networks.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DPEQTSSI/Shwartz-Ziv and LeCun - 2023 - To Compress or Not to Compress- Self-Supervised Le.pdf}
}

@article{simon_stepwise_2023,
  title = {On the Stepwise Nature of Self-Supervised Learning},
  author = {Simon, James B. and Knutins, Maksis and Ziyin, Liu and Geisz, Daniel and Fetterman, Abraham J. and Albrecht, Joshua},
  year = {2023},
  month = may,
  journal = {ICML},
  eprint = {2303.15438},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.15438},
  url = {http://arxiv.org/abs/2303.15438},
  urldate = {2023-11-10},
  abstract = {We present a simple picture of the training process of joint embedding self-supervised learning methods. We find that these methods learn their high-dimensional embeddings one dimension at a time in a sequence of discrete, well-separated steps. We arrive at this conclusion via the study of a linearized model of Barlow Twins applicable to the case in which the trained network is infinitely wide. We solve the training dynamics of this model from small initialization, finding that the model learns the top eigenmodes of a certain contrastive kernel in a stepwise fashion, and obtain a closed-form expression for the final learned representations. Remarkably, we then see the same stepwise learning phenomenon when training deep ResNets using the Barlow Twins, SimCLR, and VICReg losses. Our theory suggests that, just as kernel regression can be thought of as a model of supervised learning, kernel PCA may serve as a useful model of self-supervised learning.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/4FK4A53W/Simon et al. - 2023 - On the Stepwise Nature of Self-Supervised Learning.pdf}
}

@article{simonyan_deep_2014,
  title = {Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps},
  shorttitle = {Deep Inside Convolutional Networks},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  month = apr,
  journal = {CoRR},
  eprint = {1312.6034},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1312.6034},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2023-12-13},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/R5PR2IUT/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf}
}

@article{singh_augmenting_2023,
  title = {Augmenting Interpretable Models with LLMs during Training},
  author = {Singh, Chandan and Askari, Armin and Caruana, Rich and Gao, Jianfeng},
  year = {2023},
  month = nov,
  journal = {Nat Commun},
  volume = {14},
  number = {1},
  eprint = {2209.11799},
  primaryclass = {cs, stat},
  pages = {7913},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-43713-1},
  url = {http://arxiv.org/abs/2209.11799},
  urldate = {2024-01-09},
  abstract = {Recent large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their proliferation into high-stakes domains (e.g. medicine) and compute-limited settings has created a burgeoning need for interpretability and efficiency. We address this need by proposing Augmented Interpretable Models (Aug-imodels), a framework for leveraging the knowledge learned by LLMs to build extremely efficient and interpretable models. Aug-imodels use LLMs during fitting but not during inference, allowing complete transparency and often a speed/memory improvement of greater than 1,000x for inference compared to LLMs. We explore two instantiations of Aug-imodels in natural-language processing: (i) Aug-GAM, which augments a generalized additive model with decoupled embeddings from an LLM and (ii) Aug-Tree, which augments a decision tree with LLM feature expansions. Across a variety of text-classification datasets, both outperform their non-augmented counterparts. Aug-GAM can even outperform much larger models (e.g. a 6-billion parameter GPT-J model), despite having 10,000x fewer parameters and being fully transparent. We further explore Aug-imodels in a natural-language fMRI study, where they generate interesting interpretations from scientific data. All code for using Aug-imodels and reproducing results is made available on Github.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/A2DUE58Z/Singh et al. - 2023 - Augmenting Interpretable Models with LLMs during T.pdf}
}

@article{singh_rethinking_2024,
  title = {Rethinking Interpretability in the Era of Large Language Models},
  author = {Singh, Chandan and Inala, Jeevana Priya and Galley, Michel and Caruana, Rich and Gao, Jianfeng},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2402.01761},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.01761},
  url = {http://arxiv.org/abs/2402.01761},
  urldate = {2024-02-07},
  abstract = {Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs. In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.},
  archiveprefix = {arxiv},
  keywords = {not cited,to extract related work,to read},
  file = {/Users/leonardbereska/Zotero/storage/6FIUHZWB/Singh et al. - 2024 - Rethinking Interpretability in the Era of Large La.pdf}
}

@article{sixt_when_2020,
  title = {When Explanations Lie: Why Many Modified BP Attributions Fail},
  shorttitle = {When Explanations Lie},
  author = {Sixt, Leon and Granz, Maximilian and Landgraf, Tim},
  year = {2020},
  journal = {ICML},
  eprint = {1912.09818},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1912.09818},
  url = {http://arxiv.org/abs/1912.09818},
  urldate = {2024-03-05},
  abstract = {Attribution methods aim to explain a neural network's prediction by highlighting the most relevant image areas. A popular approach is to backpropagate (BP) a custom relevance score using modified rules, rather than the gradient. We analyze an extensive set of modified BP methods: Deep Taylor Decomposition, Layer-wise Relevance Propagation (LRP), Excitation BP, PatternAttribution, DeepLIFT, Deconv, RectGrad, and Guided BP. We find empirically that the explanations of all mentioned methods, except for DeepLIFT, are independent of the parameters of later layers. We provide theoretical insights for this surprising behavior and also analyze why DeepLIFT does not suffer from this limitation. Empirically, we measure how information of later layers is ignored by using our new metric, cosine similarity convergence (CSC). The paper provides a framework to assess the faithfulness of new and existing modified BP methods theoretically and empirically. For code see: https://github.com/berleon/when-explanations-lie},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/M6GMTVCD/Sixt et al. - 2020 - When Explanations Lie Why Many Modified BP Attrib.pdf}
}

@article{skalse_my_2023,
  title = {My Criticism of Singular Learning Theory},
  author = {Skalse, Joar},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory},
  urldate = {2024-02-12},
  abstract = {In this post, I will briefly give my criticism of Singular Learning Theory (SLT), and explain why I am skeptical of its significance. I will especial{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YWRHYJP6/Skalse - 2023 - My Criticism of Singular Learning Theory.html}
}

@article{slack_fooling_2020,
  title = {Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods},
  shorttitle = {Fooling LIME and SHAP},
  author = {Slack, Dylan and Hilgard, Sophie and Jia, Emily and Singh, Sameer and Lakkaraju, Himabindu},
  year = {2020},
  month = feb,
  journal = {AIES},
  eprint = {1911.02508},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1911.02508},
  url = {http://arxiv.org/abs/1911.02508},
  urldate = {2023-11-16},
  abstract = {As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real-world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/G6S3EAU2/Slack et al. - 2020 - Fooling LIME and SHAP Adversarial Attacks on Post.pdf}
}

@article{slavachalnev_sparse_2024,
  title = {Sparse MLP Distillation},
  author = {{slavachalnev}},
  year = {2024},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/MXabwqMwo3rkGqEW8/sparse-mlp-distillation},
  urldate = {2024-03-01},
  abstract = {Extract interpretable features from an MLP by distilling it into a sparse student MLP.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/E3CNLQIA/slavachalnev - 2024 - Sparse MLP Distillation.html}
}

@article{slobodkin_curious_2023,
  title = {The curious case of hallucinatory unanswerablity: Finding truths in the hidden states of over-confident large language models},
  author = {Slobodkin, Aviv and Goldman, Omer and Caciularu, Avi and Dagan, Ido and Ravfogel, Shauli},
  year = {2023},
  journal = {CoRR},
  doi = {10.48550/arXiv.2310.11877},
  url = {https://www.semanticscholar.org/paper/a267cfbd6930c8e2c720104ae4a90e39461a6694},
  abstract = {Large language models (LLMs) have been shown to possess impressive capabilities, while also raising crucial concerns about the faithfulness of their responses. A primary issue arising in this context is the management of (un)answerable queries by LLMs, which often results in hallucinatory behavior due to overconfidence. In this paper, we explore the behavior of LLMs when presented with (un)answerable queries. We ask: do models represent the fact that the question is (un)answerable when generating a hallucinatory answer? Our results show strong indications that such models encode the answerability of an input query, with the representation of the first decoded token often being a strong indicator. These findings shed new light on the spatial organization within the latent representations of LLMs, unveiling previously unexplored facets of these models. Moreover, they pave the way for the development of improved decoding techniques with better adherence to factual generation, particularly in scenarios where query (un)answerability is a concern.},
  arxivid = {2310.11877},
  keywords = {not cited,to extract figures},
  file = {/Users/leonardbereska/Zotero/storage/JJEXZHWF/Slobodkin et al. - 2023 - The curious case of hallucinatory unanswerablity .pdf}
}

@article{smilkov_smoothgrad_2017,
  title = {SmoothGrad: removing noise by adding noise},
  shorttitle = {SmoothGrad},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  month = jun,
  journal = {CoRR},
  eprint = {1706.03825},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1706.03825},
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2024-01-24},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/YVN8PK5Q/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf}
}

@article{soares_deep_2023,
  title = {Deep Deceptiveness},
  author = {Soares, Nate},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/XWwvwytieLtEWaFJX/deep-deceptiveness},
  urldate = {2024-02-19},
  abstract = {Meta This post is an attempt to gesture at a class of AI~notkilleveryoneism (alignment) problem that seems to me to go largely unrecognized. E.g., it{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/C8BTQQE9/Soares - 2023 - Deep Deceptiveness.html}
}

@article{soares_four_2015,
  title = {Four Background Claims},
  author = {Soares, Nate},
  year = {2015},
  month = jul,
  journal = {MIRI Blog},
  url = {https://intelligence.org/2015/07/24/four-background-claims/},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/Q4N4GJD6/Soares - 2015 - Four Background Claims.html}
}

@article{soares_if_2023,
  title = {If interpretability research goes well, it may get dangerous},
  author = {Soares, Nate},
  year = {2023},
  month = apr,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous},
  urldate = {2023-12-05},
  abstract = {I've historically been pretty publicly supportive of interpretability research. I'm still supportive of interpretability research. However, I do not necessarily think that all of it should be done in the open indefinitely. Indeed, insofar as interpretability researchers gain understanding of AIs that could significantly advance the capabilities frontier, I encourage interpretability researchers to keep their research closed. I acknowledge that spreading research insights less widely comes with real research costs. I'd endorse building a cross-organization network of people who are committed to not using their understanding to push the capabilities frontier, and sharing freely within that. I acknowledge that public sharing of research insights could, in principle, both shorten timelines and improve our odds of success. I suspect that isn't the case in real life. It's much more important that blatant and direct capabilities research be made private. Anyone fighting for people to keep their AI insights close to the chest, should be focusing on the capabilities work that's happening out in the open, long before they focus on interpretability research. Interpretability research is, I think, some of the best research that can be approached incrementally and by a large number of people, when it comes to improving our odds. (Which is not to say it doesn't require vision and genius; I expect it requires that too.) I simultaneously think it's entirely plausible that a better understanding of the workings of modern AI systems will help capabilities researchers significantly improve capabilities. I acknowledge that this sucks, and puts us in a bind. I don't have good solutions. Reality doesn't have to provide you any outs. There's a tradeoff here. And it's not my tradeoff to make; researchers will have to figure out what they think of the costs and benefits. My guess is that the current field is not close to insights that would significantly improve capabilities, and that growing the field is important (and would be hindered by closure), and also that if the field succeeds to the degree required to move the strategic needle then it's going to start stumbling across serious capabilities improvements before it saves us, and will need to start doing research privately before then. I reiterate that I'd feel {\textasciitilde}pure enthusiasm about a cross-organization network of people trying to understand modern AI systems and committed not to letting their insights push the capabilities frontier. My goal in writing this post, though, is mostly to keep the Overton window open around the claim that there is in fact a tradeoff here, that there are reasons to close even interpretability research. Maybe those reasons should win out, or maybe they shouldn't, but don't let my praise of interpretability research obscure the fact that there are tradeoffs here.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BLREIGWE/Soares - 2023 - If interpretability research goes well, it may get.html}
}

@article{soulos_discovering_2020,
  title = {Discovering the Compositional Structure of Vector Representations with Role Learning Networks},
  author = {Soulos, Paul and McCoy, R. Thomas and Linzen, Tal and Smolensky, Paul},
  editor = {Alishahi, Afra and Belinkov, Yonatan and Chrupa{\l}a, Grzegorz and Hupkes, Dieuwke and Pinter, Yuval and Sajjad, Hassan},
  year = {2020},
  month = nov,
  journal = {BlackboxNLP},
  pages = {238--254},
  doi = {10.18653/v1/2020.blackboxnlp-1.23},
  url = {https://aclanthology.org/2020.blackboxnlp-1.23},
  urldate = {2024-03-18},
  abstract = {How can neural networks perform so well on compositional tasks even though they lack explicit compositional representations? We use a novel analysis technique called ROLE to show that recurrent neural networks perform well on such tasks by converging to solutions which implicitly represent symbolic structure. This method uncovers a symbolic structure which, when properly embedded in vector space, closely approximates the encodings of a standard seq2seq network trained to perform the compositional SCAN task. We verify the causal importance of the discovered symbolic structure by showing that, when we systematically manipulate hidden embeddings based on this symbolic structure, the model's output is changed in the way predicted by our analysis.},
  file = {/Users/leonardbereska/Zotero/storage/RYH6U8BW/Soulos et al. - 2020 - Discovering the Compositional Structure of Vector .pdf}
}

@article{sovrano_objective_2023,
  title = {An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability},
  shorttitle = {An Objective Metric for Explainable AI},
  author = {Sovrano, Francesco and Vitali, Fabio},
  year = {2023},
  month = oct,
  journal = {Knowledge-Based Systems},
  volume = {278},
  eprint = {2109.05327},
  primaryclass = {cs},
  pages = {110866},
  issn = {09507051},
  doi = {10.1016/j.knosys.2023.110866},
  url = {http://arxiv.org/abs/2109.05327},
  urldate = {2023-11-01},
  abstract = {Explainable AI was born as a pathway to allow humans to explore and understand the inner working of complex systems. However, establishing what is an explanation and objectively evaluating explainability are not trivial tasks. This paper presents a new model-agnostic metric to measure the Degree of Explainability of information in an objective way. We exploit a specific theoretical model from Ordinary Language Philosophy called the Achinstein's Theory of Explanations, implemented with an algorithm relying on deep language models for knowledge graph extraction and information retrieval. To understand whether this metric can measure explainability, we devised a few experiments and user studies involving more than 190 participants, evaluating two realistic systems for healthcare and finance using famous AI technology, including Artificial Neural Networks and TreeSHAP. The results we obtained are statistically significant (with P values lower than .01), suggesting that our proposed metric for measuring the Degree of Explainability is robust in several scenarios, and it aligns with concrete expectations.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5U52ZH36/Sovrano and Vitali - 2023 - An Objective Metric for Explainable AI How and Wh.pdf}
}

@article{speith_new_2023,
  title = {A New Perspective on Evaluation Methods for Explainable Artificial Intelligence (XAI)},
  author = {Speith, Timo and Langer, Markus},
  year = {2023},
  month = sep,
  journal = {IEEE REW},
  pages = {325--331},
  publisher = {IEEE},
  address = {Hannover, Germany},
  doi = {10.1109/REW57809.2023.00061},
  url = {https://ieeexplore.ieee.org/document/10260827/},
  urldate = {2023-10-22},
  abstract = {One of the big challenges in the field of explainable artificial intelligence (XAI) is how to evaluate explainability approaches. Many evaluation methods (EMs) have been proposed, but a gold standard has yet to be established. Several authors classified EMs for explainability approaches into categories along aspects of the EMs themselves (e.g., heuristic-based, human-centered, application-grounded, functionally-grounded). In this vision paper, we propose that EMs can also be classified according to aspects of the XAI process they target. Building on models that spell out the main processes in XAI, we propose that there are explanatory information EMs, understanding EMs, and desiderata EMs. This novel perspective is intended to augment the perspective of other authors by focusing less on the EMs themselves but on what explainability approaches intend to achieve (i.e., provide good explanatory information, facilitate understanding, satisfy societal desiderata). We hope that the combination of the two perspectives will allow us to more comprehensively evaluate the advantages and disadvantages of explainability approaches, helping us to make a more informed decision about which approaches to use or how to improve them.},
  isbn = {9798350326918},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/H2SKHJHA/Speith and Langer - 2023 - A New Perspective on Evaluation Methods for Explai.pdf}
}

@article{srivastava_dropout_2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {JMLR},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v15/srivastava14a.html},
  urldate = {2023-11-10},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different thinned networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/IWZJW9VQ/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@article{srivastava_imitation_2022,
  title = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  shorttitle = {Beyond the Imitation Game},
  author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and {Garriga-Alonso}, Adri{\`a} and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders and Madotto, Andrea and Santilli, Andrea and Stuhlm{\"u}ller, Andreas and Dai, Andrew and La, Andrew and Lampinen, Andrew and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karaka{\c s}, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bart{\l}omiej and {\"O}zyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ram{\'i}rez, C{\'e}sar Ferri and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and {Callison-Burch}, Chris and Waites, Chris and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, Daniel and Khashabi, Daniel and Levy, Daniel and Gonz{\'a}lez, Daniel Mosegu{\'i} and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodola, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and {Mart{\'i}nez-Plumed}, Fernando and Happ{\'e}, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and {de Melo}, Gerard and Kruszewski, Germ{\'a}n and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria and {Jaimovitch-L{\'o}pez}, Gonzalo and Betz, Gregor and {Gur-Ari}, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry and Sch{\"u}tze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fern{\'a}ndez and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Koco{\'n}, Jan and Thompson, Jana and Kaplan, Jared and Radom, Jarema and {Sohl-Dickstein}, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Berant, Jonathan and Frohberg, J{\"o}rg and Rozen, Jos and {Hernandez-Orallo}, Jose and Boudeman, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh D. and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and {Contreras-Ochando}, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Col{\'o}n, Luis Oliveros and Metz, Luke and {\c S}enel, L{\"u}tfi Kerem and Bosma, Maarten and Sap, Maarten and {ter Hoeve}, Maartje and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Quintana, Maria Jose Ram{\'i}rez and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, M{\'a}ty{\'a}s and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael A. and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Sw{\k e}drowski, Micha{\l} and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Mi{\l}kowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Delgado, Ram{\'o}n Risco and Milli{\`e}re, Rapha{\"e}l and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and LeBras, Ronan and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Ruslan and Chi, Ryan and Lee, Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel S. and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Shyamolima and Debnath and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven T. and Shieber, Stuart M. and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsu and Wu, Te-Lin and Desbordes, Th{\'e}o and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and {Telleen-Lawton}, Timothy and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay and Prabhu, Vinay Uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Yiding and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
  year = {2022},
  month = jun,
  journal = {CoRR},
  eprint = {2206.04615},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2206.04615},
  url = {http://arxiv.org/abs/2206.04615},
  urldate = {2023-02-21},
  abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/SFVKDFIC/Srivastava et al. - 2022 - Beyond the Imitation Game Quantifying and extrapo.pdf}
}

@article{stander_grokking_2023,
  title = {Grokking Group Multiplication with Cosets},
  author = {Stander, Dashiell and Yu, Qinan and Fan, Honglu and Biderman, Stella},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.06581},
  url = {https://arxiv.org/abs/2312.06581},
  urldate = {2024-02-11},
  abstract = {We use the group Fourier transform over the symmetric group \$S\_n\$ to reverse engineer a 1-layer feedforward network that has "grokked" the multiplication of \$S\_5\$ and \$S\_6\$. Each model discovers the true subgroup structure of the full group and converges on circuits that decompose the group multiplication into the multiplication of the group's conjugate subgroups. We demonstrate the value of using the symmetries of the data and models to understand their mechanisms and hold up the ``coset circuit'' that the model uses as a fascinating example of the way neural networks implement computations. We also draw attention to current challenges in conducting mechanistic interpretability research by comparing our work to Chughtai et al. [6] which alleges to find a different algorithm for this same problem.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,grokking,mechinterp,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/X7T8ZW4J/Stander et al. - 2023 - Grokking Group Multiplication with Cosets.pdf}
}

@article{steiner_take_2022,
  title = {Take 13: RLHF bad, conditioning good.},
  shorttitle = {Take 13},
  author = {Steiner, Charlie},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/AXpXG9oTiucidnqPK/take-13-rlhf-bad-conditioning-good},
  urldate = {2023-05-15},
  abstract = {As a writing exercise, I'm writing an AI Alignment Hot Take Advent Calendar - one new hot take, written every day some days for 25 days. I have now procrastinated enough that I probably have enough h{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PNX5D9GH/Steiner - 2022 - Take 13 RLHF bad, conditioning good..html}
}

@article{steiner_take_2022a,
  title = {Take 12: RLHF's use is evidence that orgs will jam RL at real-world problems.},
  shorttitle = {Take 12},
  author = {Steiner, Charlie},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/QQMzxSJDgWkAhupi5/take-12-rlhf-s-use-is-evidence-that-orgs-will-jam-rl-at-real},
  urldate = {2023-05-15},
  abstract = {As a writing exercise, I'm writing an AI Alignment Hot Take Advent Calendar - one new hot take, written every day some days for 25 days. I have now procrastinated enough that I probably have enough h{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ZM2BI5WW/Steiner - 2022 - Take 12 RLHF's use is evidence that orgs will jam.html}
}

@article{steiner_take_2022b,
  title = {Take 10: Fine-tuning with RLHF is aesthetically unsatisfying.},
  shorttitle = {Take 10},
  author = {Steiner, Charlie},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/QujNmRy3uFyrkfqb7/take-10-fine-tuning-with-rlhf-is-aesthetically-unsatisfying},
  urldate = {2023-05-15},
  abstract = {As a writing exercise, I'm writing an AI Alignment Hot Take Advent Calendar - one new hot take, written every day for 25 days. Or until I run out of hot takes. This take owes a lot to the Simulators{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/U3NL3SIV/Steiner - 2022 - Take 10 Fine-tuning with RLHF is aesthetically un.html}
}

@article{steiner_take_2022c,
  title = {Take 9: No, RLHF/IDA/debate doesn't solve outer alignment.},
  shorttitle = {Take 9},
  author = {Steiner, Charlie},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/6YNZt5xbBT5dJXknC/take-9-no-rlhf-ida-debate-doesn-t-solve-outer-alignment},
  urldate = {2023-05-15},
  abstract = {As a writing exercise, I'm writing an AI Alignment Hot Take Advent Calendar - one new hot take, written every day (ish) for 25 days. Or until I run out of hot takes. And now, time for the week of RLH{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/FNGW477J/Steiner - 2022 - Take 9 No, RLHFIDAdebate doesn't solve outer al.html}
}

@article{steinhardt_future_2022,
  title = {Future ML Systems Will Be Qualitatively Different},
  author = {Steinhardt, Jacob},
  year = {2022},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/pZaPhGg2hmmPwByHc/future-ml-systems-will-be-qualitatively-different},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YE5NLEP2/Steinhardt - 2022 - Future ML Systems Will Be Qualitatively Different.html}
}

@article{stolfo_causal_2023,
  title = {A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models},
  author = {Stolfo, Alessandro and Jin, Zhijing and Shridhar, Kumar and Schoelkopf, Bernhard and Sachan, Mrinmaya},
  editor = {Rogers, Anna and {Boyd-Graber}, Jordan and Okazaki, Naoaki},
  year = {2023},
  month = jul,
  journal = {ACL},
  pages = {545--561},
  doi = {10.18653/v1/2023.acl-long.32},
  url = {https://aclanthology.org/2023.acl-long.32},
  urldate = {2024-03-19},
  abstract = {We have recently witnessed a number of impressive results on hard mathematical reasoning problems with language models. At the same time, the robustness of these models has also been called into question; recent works have shown that models can rely on shallow patterns in the problem description when generating a solution. Building on the idea of behavioral testing, we propose a novel framework, which pins down the causal effect of various factors in the input, e.g., the surface form of the problem text, the operands, and math operators on the output solution. By grounding the behavioral analysis in a causal graph describing an intuitive reasoning process, we study the behavior of language models in terms of robustness and sensitivity to direct interventions in the input space. We apply our framework on a test bed of math word problems. Our analysis shows that robustness does not appear to continuously improve as a function of size, but the GPT-3 Davinci models (175B) achieve a dramatic improvement in both robustness and sensitivity compared to all other GPT variants.},
  file = {/Users/leonardbereska/Zotero/storage/M2HPC4KD/Stolfo et al. - 2023 - A Causal Framework to Quantify the Robustness of M.pdf}
}

@misc{stolfo_confidence_2024,
  title = {Confidence Regulation Neurons in Language Models},
  author = {Stolfo, Alessandro and Wu, Ben and Gurnee, Wes and Belinkov, Yonatan and Song, Xingyi and Sachan, Mrinmaya and Nanda, Neel},
  year = {2024},
  month = jun,
  number = {arXiv:2406.16254},
  eprint = {2406.16254},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.16254},
  url = {http://arxiv.org/abs/2406.16254},
  urldate = {2024-07-09},
  abstract = {Despite their widespread use, the mechanisms by which large language models (LLMs) represent and regulate uncertainty in next-token predictions remain largely unexplored. This study investigates two critical components believed to influence this uncertainty: the recently discovered entropy neurons and a new set of components that we term token frequency neurons. Entropy neurons are characterized by an unusually high weight norm and influence the final layer normalization (LayerNorm) scale to effectively scale down the logits. Our work shows that entropy neurons operate by writing onto an unembedding null space, allowing them to impact the residual stream norm with minimal direct effect on the logits themselves. We observe the presence of entropy neurons across a range of models, up to 7 billion parameters. On the other hand, token frequency neurons, which we discover and describe here for the first time, boost or suppress each token's logit proportionally to its log frequency, thereby shifting the output distribution towards or away from the unigram distribution. Finally, we present a detailed case study where entropy neurons actively manage confidence in the setting of induction, i.e. detecting and continuing repeated subsequences.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/PBG5LZF3/Stolfo et al. - 2024 - Confidence Regulation Neurons in Language Models.pdf;/Users/leonardbereska/Zotero/storage/CMXQYIY4/2406.html}
}

@article{stolfo_mechanistic_2023,
  title = {A Mechanistic Interpretation of Arithmetic Reasoning in Language Models using Causal Mediation Analysis},
  author = {Stolfo, Alessandro and Belinkov, Yonatan and Sachan, Mrinmaya},
  year = {2023},
  month = oct,
  journal = {EMNLP},
  eprint = {2305.15054},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.15054},
  url = {http://arxiv.org/abs/2305.15054},
  urldate = {2023-11-16},
  abstract = {Mathematical reasoning in large language models (LMs) has garnered significant attention in recent work, but there is a limited understanding of how these models process and store information related to arithmetic tasks within their architecture. In order to improve our understanding of this aspect of language models, we present a mechanistic interpretation of Transformer-based LMs on arithmetic questions using a causal mediation analysis framework. By intervening on the activations of specific model components and measuring the resulting changes in predicted probabilities, we identify the subset of parameters responsible for specific predictions. This provides insights into how information related to arithmetic is processed by LMs. Our experimental results indicate that LMs process the input by transmitting the information relevant to the query from mid-sequence early layers to the final token using the attention mechanism. Then, this information is processed by a set of MLP modules, which generate result-related information that is incorporated into the residual stream. To assess the specificity of the observed activation dynamics, we compare the effects of different model components on arithmetic queries with other tasks, including number retrieval from prompts and factual knowledge questions.},
  archiveprefix = {arxiv},
  keywords = {cited,graph},
  file = {/Users/leonardbereska/Zotero/storage/A2ASK3KU/Stolfo et al. - 2023 - A Mechanistic Interpretation of Arithmetic Reasoni.pdf}
}

@article{stoyanovich_imperative_2020,
  title = {The imperative of interpretable machines},
  author = {Stoyanovich, Julia and Van Bavel, Jay J. and West, Tessa V.},
  year = {2020},
  month = apr,
  journal = {Nat Mach Intell},
  volume = {2},
  number = {4},
  pages = {197--199},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-0171-8},
  url = {https://www.nature.com/articles/s42256-020-0171-8},
  urldate = {2024-02-10},
  abstract = {As artificial intelligence becomes prevalent in society, a framework is needed to connect interpretability and trust in algorithm-assisted decisions, for a range of stakeholders.},
  copyright = {2020 Springer Nature Limited},
  language = {en},
  keywords = {not cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/TRVP3DEP/Stoyanovich et al. - 2020 - The imperative of interpretable machines.pdf}
}

@article{strother_emergence_2017,
  title = {The Emergence of Directional Selectivity in the Visual Motion Pathway of Drosophila},
  author = {Strother, James A. and Wu, Shiuan-Tze and Wong, Allan M. and Nern, Aljoscha and Rogers, Edward M. and Le, Jasmine Q. and Rubin, Gerald M. and Reiser, Michael B.},
  year = {2017},
  month = apr,
  journal = {Neuron},
  volume = {94},
  number = {1},
  pages = {168-182.e10},
  publisher = {Elsevier},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.03.010},
  url = {https://www.cell.com/neuron/abstract/S0896-6273(17)30193-9},
  urldate = {2024-01-19},
  language = {English},
  pmid = {28384470},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LB67IHAL/Strother et al. - 2017 - The Emergence of Directional Selectivity in the Vi.pdf}
}

@article{subramani_extracting_2022,
  title = {Extracting Latent Steering Vectors from Pretrained Language Models},
  author = {Subramani, Nishant and Suresh, Nivedita and Peters, Matthew E.},
  year = {2022},
  month = may,
  journal = {ACL},
  eprint = {2205.05124},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.05124},
  url = {http://arxiv.org/abs/2205.05124},
  urldate = {2024-02-26},
  abstract = {Prior work on controllable text generation has focused on learning how to control language models through trainable decoding, smart-prompt design, or fine-tuning based on a desired objective. We hypothesize that the information needed to steer the model to generate a target sentence is already encoded within the model. Accordingly, we explore a different approach altogether: extracting latent vectors directly from pretrained language model decoders without fine-tuning. Experiments show that there exist steering vectors, which, when added to the hidden states of the language model, generate a target sentence nearly perfectly ({$>$} 99 BLEU) for English sentences from a variety of domains. We show that vector arithmetic can be used for unsupervised sentiment transfer on the Yelp sentiment benchmark, with performance comparable to models tailored to this task. We find that distances between steering vectors reflect sentence similarity when evaluated on a textual similarity benchmark (STS-B), outperforming pooled hidden states of models. Finally, we present an analysis of the intrinsic properties of the steering vectors. Taken together, our results suggest that frozen LMs can be effectively controlled through their latent steering space.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/HVMMP4NK/Subramani et al. - 2022 - Extracting Latent Steering Vectors from Pretrained.pdf}
}

@article{sucholutsky_getting_2023,
  title = {Getting aligned on representational alignment},
  author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and M{\"u}ller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2310.13018},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2310.13018},
  url = {http://arxiv.org/abs/2310.13018},
  urldate = {2023-11-26},
  abstract = {Biological and artificial information processing systems form representations that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? How can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in cognitive science, neuroscience, and machine learning. For example, cognitive scientists measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists align fMRI responses from multiple individuals into a shared representational space for group-level analyses, and ML researchers distill knowledge from teacher models into student models by increasing their alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between these fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from all three fields and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EWFA4ZQ2/Sucholutsky et al. - 2023 - Getting aligned on representational alignment.pdf}
}

@article{sundararajan_axiomatic_2017,
  title = {Axiomatic Attribution for Deep Networks},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = jun,
  journal = {ICML},
  eprint = {1703.01365},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1703.01365},
  url = {http://arxiv.org/abs/1703.01365},
  urldate = {2024-01-24},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EH34DJ2E/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf}
}

@article{swaminathan_schemalearning_2023,
  title = {Schema-learning and rebinding as mechanisms of in-context learning and emergence},
  author = {Swaminathan, Sivaramakrishnan and Dedieu, Antoine and Vasudeva Raju, Rajkumar and Shanahan, Murray and {Lazaro-Gredilla}, Miguel and George, Dileep},
  year = {2023},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  pages = {28785--28804},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/5bc3356e0fa1753fff7e8d6628e71b22-Abstract-Conference.html},
  urldate = {2024-06-10},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/RLDB43FD/Swaminathan et al. - 2023 - Schema-learning and rebinding as mechanisms of in-.pdf}
}

@article{swedlow_modelling_2006,
  title = {Modelling data across labs, genomes, space and time},
  author = {Swedlow, Jason R. and Lewis, Suzanna E. and Goldberg, Ilya G.},
  year = {2006},
  month = nov,
  journal = {Nat Cell Biol},
  volume = {8},
  number = {11},
  pages = {1190--1194},
  publisher = {Nature Publishing Group},
  issn = {1476-4679},
  doi = {10.1038/ncb1496},
  url = {https://www.nature.com/articles/ncb1496},
  urldate = {2024-04-30},
  abstract = {Logical models and physical specifications provide the foundation for storage, management and analysis of complex sets of data, and describe the relationships between measured data elements and metadata --- the contextual descriptors that define the primary data. Here, we use imaging applications to illustrate the purpose of the various implementations of data specifications and the requirement for open, standardized, data formats to facilitate the sharing of critical digital data and metadata.},
  copyright = {2006 Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/D8A5ZCVM/Swedlow et al. - 2006 - Modelling data across labs, genomes, space and tim.pdf}
}

@article{syed_attribution_2023,
  title = {Attribution Patching Outperforms Automated Circuit Discovery},
  author = {Syed, Aaquib and Rager, Can and Conmy, Arthur},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.10348},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.10348},
  url = {http://arxiv.org/abs/2310.10348},
  urldate = {2023-10-27},
  abstract = {Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.},
  archiveprefix = {arxiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/XYLZIIVA/Syed et al. - 2023 - Attribution Patching Outperforms Automated Circuit.pdf}
}

@article{szepannek_how_2019,
  title = {How Much Can We See? A Note on Quantifying Explainability of Machine Learning Models},
  shorttitle = {How Much Can We See?},
  author = {Szepannek, G.},
  year = {2019},
  month = oct,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/How-Much-Can-We-See-A-Note-on-Quantifying-of-Models-Szepannek/44020fe72ca46ff3e657c6a72a274b3a15518928},
  urldate = {2023-09-18},
  abstract = {One of the most popular approaches to understanding feature effects of modern black box machine learning models are partial dependence plots (PDP). These plots are easy to understand but only able to visualize low order dependencies. The paper is about the question 'How much can we see?': A framework is developed to quantify the explainability of arbitrary machine learning models, i.e. up to what degree the visualization as given by a PDP is able to explain the predictions of the model. The result allows for a judgement whether an attempt to explain a black box model is sufficient or not.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/TPNYQ3TX/Szepannek - 2019 - How Much Can We See A Note on Quantifying Explain.pdf}
}

@article{tagade_prototype_2023,
  title = {Prototype Generation: Robust Feature Visualisation for Data Independent Interpretability},
  shorttitle = {Prototype Generation},
  author = {Tagade, Arush and Rumbelow, Jessica},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.17144},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2309.17144},
  urldate = {2024-02-11},
  abstract = {We introduce Prototype Generation, a stricter and more robust form of feature visualisation for model-agnostic, data-independent interpretability of image classification models. We demonstrate its ability to generate inputs that result in natural activation paths, countering previous claims that feature visualisation algorithms are untrustworthy due to the unnatural internal activations. We substantiate these claims by quantitatively measuring similarity between the internal activations of our generated prototypes and natural images. We also demonstrate how the interpretation of generated prototypes yields important insights, highlighting spurious correlations and biases learned by models which quantitative methods over test-sets cannot identify.},
  archiveprefix = {arxiv},
  keywords = {feature,not cited,visualization},
  file = {/Users/leonardbereska/Zotero/storage/AM9T53GP/Tagade and Rumbelow - 2023 - Prototype Generation Robust Feature Visualisation.pdf}
}

@article{takizawa_othello_2023,
  title = {Othello is Solved},
  author = {Takizawa, Hiroki},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.19387},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2310.19387},
  urldate = {2023-11-09},
  abstract = {The game of Othello is one of the world's most complex and popular games that has yet to be computationally solved. Othello has roughly ten octodecillion (10 to the 58th power) possible game records and ten octillion (10 to the 28th power) possible game position. The challenge of solving Othello, determining the outcome of a game with no mistake made by either player, has long been a grand challenge in computer science. This paper announces a significant milestone: Othello is now solved, computationally proved that perfect play by both players lead to a draw. Strong Othello software has long been built using heuristically designed search techniques. Solving a game provides the solution which enables software to play the game perfectly.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LEPT2AFQ/Takizawa - 2023 - Othello is Solved.pdf}
}

@article{tamkin_codebook_2023,
  title = {Codebook Features: Sparse and Discrete Interpretability for Neural Networks},
  author = {Tamkin, Alex and Taufeeque, Mohammad and Goodman, Noah D},
  year = {2023},
  journal = {CoRR},
  url = {https://arxiv.org/abs/2310.17230},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/H4G6WIHA/Tamkin et al. - 2023 - Codebook Features Sparse and Discrete Interpretab.pdf}
}

@article{tan_understanding_2023,
  title = {Understanding Grokking Through A Robustness Viewpoint},
  author = {Tan, Zhiquan and Huang, Weiran},
  year = {2023},
  month = nov,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2311.06597v2},
  urldate = {2024-06-10},
  abstract = {Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular \$l\_2\$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that \$l\_2\$ norm correlates with grokking on the test data not in a timely way, we propose new metrics based on robustness and information theory and find that our new metrics correlate well with the grokking phenomenon and may be used to predict grokking.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/DVWEUUTF/Tan and Huang - 2023 - Understanding Grokking Through A Robustness Viewpo.pdf}
}

@article{taylor_artificial_2021,
  title = {Artificial cognition: How experimental psychology can help generate explainable artificial intelligence},
  shorttitle = {Artificial cognition},
  author = {Taylor, J. Eric T. and Taylor, Graham W.},
  year = {2021},
  month = apr,
  journal = {Psychon Bull Rev},
  volume = {28},
  number = {2},
  pages = {454--475},
  issn = {1531-5320},
  doi = {10.3758/s13423-020-01825-5},
  url = {https://doi.org/10.3758/s13423-020-01825-5},
  urldate = {2023-11-24},
  abstract = {Artificial intelligence powered by deep neural networks has reached a level of complexity where it can be difficult or impossible to express how a model makes its decisions. This black-box problem is especially concerning when the model makes decisions with consequences for human well-being. In response, an emerging field called explainable artificial intelligence (XAI) aims to increase the interpretability, fairness, and transparency of machine learning. In this paper, we describe how cognitive psychologists can make contributions to XAI. The human mind is also a black box, and cognitive psychologists have over 150 years of experience modeling it through experimentation. We ought to translate the methods and rigor of cognitive psychology to the study of artificial black boxes in the service of explainability. We provide a review of XAI for psychologists, arguing that current methods possess a blind spot that can be complemented by the experimental cognitive tradition. We also provide a framework for research in XAI, highlight exemplary cases of experimentation within XAI inspired by psychological science, and provide a tutorial on experimenting with machines. We end by noting the advantages of an experimental approach and invite other psychologists to conduct research in this exciting new field.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YYX7GF9W/Taylor and Taylor - 2021 - Artificial cognition How experimental psychology .pdf}
}

@article{taylor_introduction_2024,
  title = {An introduction to graphical tensor notation for mechanistic interpretability},
  author = {Taylor, Jordan K.},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.01790},
  url = {https://arxiv.org/abs/2402.01790},
  urldate = {2024-02-11},
  abstract = {Graphical tensor notation is a simple way of denoting linear operations on tensors, originating from physics. Modern deep learning consists almost entirely of operations on or between tensors, so easily understanding tensor operations is quite important for understanding these systems. This is especially true when attempting to reverse-engineer the algorithms learned by a neural network in order to understand its behavior: a field known as mechanistic interpretability. It's often easy to get confused about which operations are happening between tensors and lose sight of the overall structure, but graphical tensor notation makes it easier to parse things at a glance and see interesting equivalences. The first half of this document introduces the notation and applies it to some decompositions (SVD, CP, Tucker, and tensor network decompositions), while the second half applies it to some existing some foundational approaches for mechanistically understanding language models, loosely following ``A Mathematical Framework for Transformer Circuits'', then constructing an example ``induction head'' circuit in graphical tensor notation.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {graph,mechinterp,not cited,notation,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/UGPXVBBS/Taylor - 2024 - An introduction to graphical tensor notation for m.pdf}
}

@article{technicalities_shallow_2023,
  title = {Shallow review of live agendas in alignment \& safety},
  author = {{technicalities} and Stag},
  year = {2023},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety},
  urldate = {2024-03-07},
  abstract = {Summary You can't optimise an allocation of resources if you don't know what the current one is. Existing maps of alignment research are mostly too o{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/R6SCMYBR/technicalities and Stag - 2023 - Shallow review of live agendas in alignment & safe.html}
}

@article{tegmark_provably_2023,
  title = {Provably safe systems: the only path to controllable AGI},
  shorttitle = {Provably safe systems},
  author = {Tegmark, Max and Omohundro, Steve},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.01933},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.01933},
  url = {http://arxiv.org/abs/2309.01933},
  urldate = {2023-10-27},
  abstract = {We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.},
  archiveprefix = {arxiv},
  keywords = {alignment,cited,opinion,to cite},
  file = {/Users/leonardbereska/Zotero/storage/3UIQKLTL/Tegmark and Omohundro - 2023 - Provably safe systems the only path to controllab.pdf}
}

@article{templeton_scaling_2024,
  title = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
  author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian},
  year = {2024},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
}

@article{tenney_bert_2019,
  title = {BERT Rediscovers the Classical NLP Pipeline},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  month = aug,
  journal = {ACL},
  eprint = {1905.05950},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1905.05950},
  url = {http://arxiv.org/abs/1905.05950},
  urldate = {2024-01-19},
  abstract = {Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EAH7RNL3/Tenney et al. - 2019 - BERT Rediscovers the Classical NLP Pipeline.pdf}
}

@article{tenney_language_2020,
  title = {The Language Interpretability Tool: Extensible, Interactive Visualizations and Analysis for NLP Models},
  shorttitle = {The Language Interpretability Tool},
  author = {Tenney, Ian and Wexler, James and Bastings, Jasmijn and Bolukbasi, Tolga and Coenen, Andy and Gehrmann, Sebastian and Jiang, Ellen and Pushkarna, Mahima and Radebaugh, Carey and Reif, Emily and Yuan, Ann},
  year = {2020},
  journal = {EMNLP},
  pages = {107--118},
  doi = {10.18653/v1/2020.emnlp-demos.15},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.15},
  urldate = {2024-02-11},
  abstract = {We present the Language Interpretability Tool (LIT), an open-source platform for visualization and understanding of NLP models. We focus on core questions about model behavior: Why did my model make this prediction? When does it perform poorly? What happens under a controlled change in the input? LIT integrates local explanations, aggregate analysis, and counterfactual generation into a streamlined, browser-based interface to enable rapid exploration and error analysis. We include case studies for a diverse set of workflows, including exploring counterfactuals for sentiment analysis, measuring gender bias in coreference systems, and exploring local behavior in text generation. LIT supports a wide range of models---including classification, seq2seq, and structured prediction---and is highly extensible through a declarative, framework-agnostic API. LIT is under active development, with code and full documentation available at https://github.com/pair-code/lit.},
  language = {en},
  keywords = {not cited,to cite,tool},
  file = {/Users/leonardbereska/Zotero/storage/KEJSKVM3/Tenney et al. - 2020 - The Language Interpretability Tool Extensible, In.pdf}
}

@article{thilak_slingshot_2022,
  title = {The Slingshot Mechanism: An Empirical Study of Adaptive Optimizers and the Grokking Phenomenon},
  shorttitle = {The Slingshot Mechanism},
  author = {Thilak, Vimal and Littwin, Etai and Zhai, Shuangfei and Saremi, Omid and Paiss, Roni and Susskind, Joshua},
  year = {2022},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2206.04817},
  url = {https://arxiv.org/abs/2206.04817},
  urldate = {2023-11-10},
  abstract = {The grokking phenomenon as reported by Power et al. ( arXiv:2201.02177 ) refers to a regime where a long period of overfitting is followed by a seemingly sudden transition to perfect generalization. In this paper, we attempt to reveal the underpinnings of Grokking via a series of empirical studies. Specifically, we uncover an optimization anomaly plaguing adaptive optimizers at extremely late stages of training, referred to as the Slingshot Mechanism. A prominent artifact of the Slingshot Mechanism can be measured by the cyclic phase transitions between stable and unstable training regimes, and can be easily monitored by the cyclic behavior of the norm of the last layers weights. We empirically observe that without explicit regularization, Grokking as reported in ( arXiv:2201.02177 ) almost exclusively happens at the onset of Slingshots, and is absent without it. While common and easily reproduced in more general settings, the Slingshot Mechanism does not follow from any known optimization theories that we are aware of, and can be easily overlooked without an in depth examination. Our work points to a surprising and useful inductive bias of adaptive gradient optimizers at late stages of training, calling for a revised theoretical analysis of their origin.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/C8UCAKU9/Thilak et al. - 2022 - The Slingshot Mechanism An Empirical Study of Ada.pdf}
}

@article{tian_just_2023,
  title = {Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback},
  shorttitle = {Just Ask for Calibration},
  author = {Tian, Katherine and Mitchell, Eric and Zhou, Allan and Sharma, Archit and Rafailov, Rafael and Yao, Huaxiu and Finn, Chelsea and Manning, Christopher D.},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2305.14975},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.14975},
  url = {http://arxiv.org/abs/2305.14975},
  urldate = {2023-07-10},
  abstract = {A trustworthy real-world prediction system should be well-calibrated; that is, its confidence in an answer is indicative of the likelihood that the answer is correct, enabling deferral to a more expensive expert in cases of low-confidence predictions. While recent studies have shown that unsupervised pre-training produces large language models (LMs) that are remarkably well-calibrated, the most widely-used LMs in practice are fine-tuned with reinforcement learning with human feedback (RLHF-LMs) after the initial unsupervised pre-training stage, and results are mixed as to whether these models preserve the well-calibratedness of their ancestors. In this paper, we conduct a broad evaluation of computationally feasible methods for extracting confidence scores from LLMs fine-tuned with RLHF. We find that with the right prompting strategy, RLHF-LMs verbalize probabilities that are much better calibrated than the model's conditional probabilities, enabling fairly well-calibrated predictions. Through a combination of prompting strategy and temperature scaling, we find that we can reduce the expected calibration error of RLHF-LMs by over 50\%.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9XXUK4N8/Tian et al. - 2023 - Just Ask for Calibration Strategies for Eliciting.pdf}
}

@article{tigges_linear_2023,
  title = {Linear Representations of Sentiment in Large Language Models},
  author = {Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.15154},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.15154},
  url = {http://arxiv.org/abs/2310.15154},
  urldate = {2023-10-31},
  abstract = {Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. Through causal interventions, we isolate this direction and show it is causally relevant in both toy tasks and real world datasets such as Stanford Sentiment Treebank. Through this case study we model a thorough investigation of what a single direction means on a broad data distribution. We further uncover the mechanisms that involve this direction, highlighting the roles of a small subset of attention heads and neurons. Finally, we discover a phenomenon which we term the summarization motif: sentiment is not solely represented on emotionally charged words, but is additionally summarized at intermediate positions without inherent sentiment, such as punctuation and names. We show that in Stanford Sentiment Treebank zero-shot classification, 76\% of above-chance classification accuracy is lost when ablating the sentiment direction, nearly half of which (36\%) is due to ablating the summarized sentiment direction exclusively at comma positions.},
  archiveprefix = {arxiv},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/73I7MGZZ/Tigges et al. - 2023 - Linear Representations of Sentiment in Large Langu.pdf}
}

@article{tishby_information_2000,
  title = {The information bottleneck method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {2000},
  month = apr,
  journal = {CoRR},
  eprint = {physics/0004057},
  doi = {10.48550/arXiv.physics/0004057},
  url = {http://arxiv.org/abs/physics/0004057},
  urldate = {2024-01-18},
  abstract = {We define the relevant information in a signal \$x{\textbackslash}in X\$ as being the information that this signal provides about another signal \$y{\textbackslash}in {\textbackslash}Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \${\textbackslash}X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \${\textbackslash}X\$ that preserves the maximum information about \${\textbackslash}Y\$. That is, we squeeze the information that \${\textbackslash}X\$ provides about \${\textbackslash}Y\$ through a `bottleneck' formed by a limited set of codewords \${\textbackslash}tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,{\textbackslash}x)\$ emerges from the joint statistics of \${\textbackslash}X\$ and \${\textbackslash}Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X {\textbackslash}to {\textbackslash}tX\$ and \${\textbackslash}tX {\textbackslash}to {\textbackslash}Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4LKQN8QT/Tishby et al. - 2000 - The information bottleneck method.pdf}
}

@article{todd_function_2023,
  title = {Function Vectors in Large Language Models},
  author = {Todd, Eric and Li, Millicent L. and Sharma, Arnab Sen and Mueller, Aaron and Wallace, Byron C. and Bau, David},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.15213},
  url = {https://arxiv.org/abs/2310.15213},
  urldate = {2023-12-11},
  abstract = {We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some extent they can be summed to create vectors that trigger new complex tasks. Taken together, our findings suggest that LLMs contain internal abstractions of general-purpose functions that can be invoked in a variety of contexts.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9J9KYYHU/Todd et al. - 2023 - Function Vectors in Large Language Models.pdf}
}

@article{tosches_evolution_2018,
  title = {Evolution of pallium, hippocampus, and cortical cell types revealed by single-cell transcriptomics in reptiles},
  author = {Tosches, Maria Antonietta and Yamawaki, Tracy M. and Naumann, Robert K. and Jacobi, Ariel A. and Tushev, Georgi and Laurent, Gilles},
  year = {2018},
  month = may,
  journal = {Science},
  volume = {360},
  number = {6391},
  pages = {881--888},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.aar4237},
  url = {https://www.science.org/doi/10.1126/science.aar4237},
  urldate = {2024-01-19},
  abstract = {Computations in the mammalian cortex are carried out by glutamatergic and {$\gamma$}-aminobutyric acid--releasing (GABAergic) neurons forming specialized circuits and areas. Here we asked how these neurons and areas evolved in amniotes. We built a gene expression atlas of the pallium of two reptilian species using large-scale single-cell messenger RNA sequencing. The transcriptomic signature of glutamatergic neurons in reptilian cortex suggests that mammalian neocortical layers are made of new cell types generated by diversification of ancestral gene-regulatory programs. By contrast, the diversity of reptilian cortical GABAergic neurons indicates that the interneuron classes known in mammals already existed in the common ancestor of all amniotes.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/RQYG5X9C/Tosches et al. - 2018 - Evolution of pallium, hippocampus, and cortical ce.pdf}
}

@article{trager_linear_2023,
  title = {Linear Spaces of Meanings: Compositional Structures in Vision-Language Models},
  shorttitle = {Linear Spaces of Meanings},
  author = {Trager, Matthew and Perera, Pramuditha and Zancato, Luca and Achille, Alessandro and Bhatia, Parminder and Soatto, Stefano},
  year = {2023},
  month = oct,
  journal = {2023 IEEE/CVF International Conference on Computer Vision (ICCV)},
  pages = {15349--15358},
  publisher = {IEEE},
  address = {Paris, France},
  doi = {10.1109/ICCV51070.2023.01412},
  url = {https://ieeexplore.ieee.org/document/10377972/},
  urldate = {2024-06-10},
  abstract = {We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a preexisting vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic operations on embedding vectors can be used as compositional and interpretable methods for regulating the behavior of VLMs.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350307184}
}

@article{trivedi_learning_2021,
  title = {Learning to Synthesize Programs as Interpretable and Generalizable Policies},
  author = {Trivedi, Dweep and Zhang, Jesse and Sun, Shao-Hua and Lim, Joseph J.},
  year = {2021},
  journal = {NeurIPS},
  eprint = {2108.13643},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2108.13643},
  url = {http://arxiv.org/abs/2108.13643},
  urldate = {2024-02-19},
  abstract = {Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/FR2GHXCA/Trivedi et al. - 2021 - Learning to Synthesize Programs as Interpretable a.pdf}
}

@article{turbe_evaluation_2023,
  title = {Evaluation of post-hoc interpretability methods in time-series classification},
  author = {Turb{\'e}, Hugues and Bjelogrlic, Mina and Lovis, Christian and Mengaldo, Gianmarco},
  year = {2023},
  month = mar,
  journal = {Nat Mach Intell},
  volume = {5},
  number = {3},
  pages = {250--260},
  issn = {2522-5839},
  doi = {10.1038/s42256-023-00620-w},
  url = {https://www.nature.com/articles/s42256-023-00620-w},
  urldate = {2023-08-27},
  abstract = {Abstract             Post-hoc interpretability methods are critical tools to explain neural-network results. Several post-hoc methods have emerged in recent years but they produce different results when applied to a given task, raising the question of which method is the most suitable to provide accurate post-hoc interpretability. To understand the performance of each method, quantitative evaluation of interpretability methods is essential; however, currently available frameworks have several drawbacks that hinder the adoption of post-hoc interpretability methods, especially in high-risk sectors. In this work we propose a framework with quantitative metrics to assess the performance of existing post-hoc interpretability methods, particularly in time-series classification. We show that several drawbacks identified in the literature are addressed, namely, the dependence on human judgement, retraining and the shift in the data distribution when occluding samples. We also design a synthetic dataset with known discriminative features and tunable complexity. The proposed methodology and quantitative metrics can be used to understand the reliability of interpretability methods results obtained in practical applications. In turn, they can be embedded within operational workflows in critical fields that require accurate interpretability results for, example, regulatory policies.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/DNS7SSSB/Turb et al. - 2023 - Evaluation of post-hoc interpretability methods in.pdf}
}

@article{turner_activation_2023,
  title = {Activation Addition: Steering Language Models Without Optimization},
  shorttitle = {Activation Addition},
  author = {Turner, Alexander Matt and Thiergart, Lisa and Udell, David and Leech, Gavin and Mini, Ulisse and MacDiarmid, Monte},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2308.10248},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2308.10248},
  url = {http://arxiv.org/abs/2308.10248},
  urldate = {2023-11-09},
  abstract = {Reliably controlling the behavior of large language models is a pressing open problem. Existing methods include supervised finetuning, reinforcement learning from human feedback, prompt engineering, and guided decoding. We instead investigate activation engineering: modifying activations at inference time to predictably alter model behavior. In particular, we bias the forward pass with an added 'steering vector' implicitly specified through natural language. Unlike past work which learned these steering vectors, our Activation Addition (ActAdd) method computes them by taking the activation differences that result from pairs of prompts. We demonstrate ActAdd on GPT-2 on OpenWebText and ConceptNet. Our inference-time approach yields control over high-level properties of output and preserves off-target model performance. It involves far less compute and implementation effort than finetuning, allows users to provide natural language specifications, and its overhead scales naturally with model size.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MXQVYFQ4/Turner et al. - 2023 - Activation Addition Steering Language Models With.pdf}
}

@article{turner_optimal_2021,
  title = {Optimal Policies Tend to Seek Power},
  author = {Turner, Alexander Matt and Smith, Logan and Shah, Rohin and Critch, Andrew and Tadepalli, Prasad},
  year = {2021},
  journal = {NeurIPS Spotlight},
  eprint = {1912.01683},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1912.01683},
  urldate = {2023-05-17},
  abstract = {Some researchers speculate that intelligent reinforcement learning (RL) agents would be incentivized to seek resources and power in pursuit of their objectives. Other researchers point out that RL agents need not have human-like power-seeking instincts. To clarify this discussion, we develop the first formal theory of the statistical tendencies of optimal policies. In the context of Markov decision processes, we prove that certain environmental symmetries are sufficient for optimal policies to tend to seek power over the environment. These symmetries exist in many environments in which the agent can be shut down or destroyed. We prove that in these environments, most reward functions make it optimal to seek power by keeping a range of options available and, when maximizing average reward, by navigating towards larger sets of potential terminal states.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/3DFW4SYN/Turner et al. - 2023 - Optimal Policies Tend to Seek Power.pdf}
}

@article{turntrout_understanding_2023,
  title = {Understanding and controlling a maze-solving policy network},
  author = {TurnTrout and {peligrietzer} and Mini, Ulisse and {montemac} and Udell, David},
  year = {2023},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network},
  urldate = {2023-04-12},
  abstract = {Previously: Predictions for shard theory mechanistic interpretability results~ {\dots}},
  language = {en},
  keywords = {graph,mechinterp,not cited,RL,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/KXIIZGY8/TurnTrout et al. - 2023 - Understanding and controlling a maze-solving polic.pdf}
}

@article{turpin_language_2023,
  title = {Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting},
  shorttitle = {Language Models Don't Always Say What They Think},
  author = {Turpin, Miles and Michael, Julian and Perez, Ethan and Bowman, Samuel R.},
  year = {2023},
  month = dec,
  journal = {NeurIPS},
  eprint = {2305.04388},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.04388},
  url = {http://arxiv.org/abs/2305.04388},
  urldate = {2024-03-19},
  abstract = {Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs--e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always "(A)"--which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36\% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/4WPK6Z8F/Turpin et al. - 2023 - Language Models Don't Always Say What They Think .pdf}
}

@article{tzachor_responsible_2022,
  title = {Responsible artificial intelligence in agriculture requires systemic understanding of risks and externalities},
  author = {Tzachor, Asaf and Devare, Medha and King, Brian and Avin, Shahar and {{\'O} h{\'E}igeartaigh}, Se{\'a}n},
  year = {2022},
  month = feb,
  journal = {Nat Mach Intell},
  volume = {4},
  number = {2},
  pages = {104--109},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00440-4},
  url = {https://www.nature.com/articles/s42256-022-00440-4},
  urldate = {2023-01-26},
  abstract = {Global agriculture is poised to benefit from the rapid advance and diffusion of artificial intelligence (AI) technologies. AI in agriculture could improve crop management and agricultural productivity through plant phenotyping, rapid diagnosis of plant disease, efficient application of agrochemicals and assistance for growers with location-relevant agronomic advice. However, the ramifications of machine learning (ML) models, expert systems and autonomous machines for farms, farmers and food security are poorly understood and under-appreciated. Here, we consider systemic risk factors of AI in agriculture. Namely, we review risks relating to interoperability, reliability and relevance of agricultural data, unintended socio-ecological consequences resulting from ML models optimized for yields, and safety and security concerns associated with deployment of ML platforms at scale. As a response, we suggest risk-mitigation measures, including inviting rural anthropologists and applied ecologists into the technology design process, applying frameworks for responsible and human-centred innovation, setting data cooperatives for improved data transparency and ownership rights, and initial deployment of agricultural AI in digital sandboxes.},
  copyright = {2022 Springer Nature Limited},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/Z5VB8S9K/Tzachor et al. - 2022 - Responsible artificial intelligence in agriculture.html}
}

@article{vaintrob_mathematical_2024,
  title = {Toward A Mathematical Framework for Computation in Superposition},
  author = {Vaintrob, Dmitry and {jake\_mendel} and Kaarel},
  year = {2024},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/2roZtSr5TGmLjXMnT/toward-a-mathematical-framework-for-computation-in},
  urldate = {2024-02-22},
  abstract = {Author order randomized. Authors contributed roughly equally --- see attribution section for details. {\dots}},
  language = {en},
  keywords = {algorithms,cited,computation,mechinterp,superposition,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/YI4CT4CC/Vaintrob et al. - 2024 - Toward A Mathematical Framework for Computation in.html}
}

@article{vanderweij_extending_2024,
  title = {Extending Activation Steering to Broad Skills and Multiple Behaviours},
  author = {{van der Weij}, Teun and Poesio, Massimo and Schoots, Nandi},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.05767},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.05767},
  url = {http://arxiv.org/abs/2403.05767},
  urldate = {2024-03-15},
  abstract = {Current large language models have dangerous capabilities, which are likely to become more problematic in the future. Activation steering techniques can be used to reduce risks from these capabilities. In this paper, we investigate the efficacy of activation steering for broad skills and multiple behaviours. First, by comparing the effects of reducing performance on general coding ability and Python-specific ability, we find that steering broader skills is competitive to steering narrower skills. Second, we steer models to become more or less myopic and wealth-seeking, among other behaviours. In our experiments, combining steering vectors for multiple different behaviours into one steering vector is largely unsuccessful. On the other hand, injecting individual steering vectors at different places in a model simultaneously is promising.},
  archiveprefix = {arxiv},
  keywords = {mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/KBSYILMW/van der Weij et al. - 2024 - Extending Activation Steering to Broad Skills and .pdf}
}

@article{vanhooser_similarity_2007,
  title = {Similarity and Diversity in Visual Cortex: Is There a Unifying Theory of Cortical Computation?},
  shorttitle = {Similarity and Diversity in Visual Cortex},
  author = {Van Hooser, Stephen D.},
  year = {2007},
  month = dec,
  journal = {Neuroscientist},
  volume = {13},
  number = {6},
  pages = {639--656},
  publisher = {SAGE Publications Inc STM},
  issn = {1073-8584},
  doi = {10.1177/1073858407306597},
  url = {https://doi.org/10.1177/1073858407306597},
  urldate = {2024-01-19},
  abstract = {The cerebral cortex, with its conserved 6-layer structure, has inspired many unifying models of function. However, recent comparative studies of primary visual cortex have revealed considerable structural diversity, raising doubts about the possibility of an all-encompassing theory. This review examines similarities and differences in V1 across mammals. Gross laminar interconnections are relatively conserved. Major functional response classes are found universally or nearly universally. Orientation and spatial frequency tuning bandwidths are quite similar despite an enormous range of visual resolution across species, and orientation tuning is contrast-invariant. Nevertheless, there is considerable diversity in the abundance of different cell classes, laminar organization, functional architecture, and functional connectivity. Orientation-selective responses arise in different layers in different species. Some mammals have elaborate columnar architecture like orientation maps and ocular dominance bands, but others lack this organization with no apparent impact on single cell properties. Finally, local functional connectivity varies according to map structure: similar cells are connected in smooth map regions but dissimilar cells are linked in animals without maps. If there is a single structure/function relation for cortex, it must accommodate significant variations in cortical circuitry. Alternatively, natural selection may craft unique circuits that function differently in each species. NEURO-SCIENTIST 13(6):639---656, 2007. DOI: 10.1177/1073858407306597},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VHFYE9GV/Van Hooser - 2007 - Similarity and Diversity in Visual Cortex Is Ther.pdf}
}

@article{variengien_look_2023,
  title = {Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models},
  shorttitle = {Look Before You Leap},
  author = {Variengien, Alexandre and Winsor, Eric},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.10091},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.10091},
  url = {http://arxiv.org/abs/2312.10091},
  urldate = {2024-01-03},
  abstract = {When solving challenging problems, language models (LMs) are able to identify relevant information from long and complicated contexts. To study how LMs solve retrieval tasks in diverse situations, we introduce ORION, a collection of structured retrieval tasks spanning six domains, from text understanding to coding. Each task in ORION can be represented abstractly by a request (e.g. a question) that retrieves an attribute (e.g. the character name) from a context (e.g. a story). We apply causal analysis on 18 open-source language models with sizes ranging from 125 million to 70 billion parameters. We find that LMs internally decompose retrieval tasks in a modular way: middle layers at the last token position process the request, while late layers retrieve the correct entity from the context. After causally enforcing this decomposition, models are still able to solve the original task, preserving 70\% of the original correct token probability in 98 of the 106 studied model-task pairs. We connect our macroscopic decomposition with a microscopic description by performing a fine-grained case study of a question-answering task on Pythia-2.8b. Building on our high-level understanding, we demonstrate a proof of concept application for scalable internal oversight of LMs to mitigate prompt-injection while requiring human supervision on only a single input. Our solution improves accuracy drastically (from 15.5\% to 97.5\% on Pythia-12b). This work presents evidence of a universal emergent modular processing of tasks across varied domains and models and is a pioneering effort in applying interpretability for scalable internal oversight of LMs.},
  archiveprefix = {arxiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/5SQNQ5A4/Variengien and Winsor - 2023 - Look Before You Leap A Universal Emergent Decompo.pdf}
}

@article{varma_explaining_2023,
  title = {Explaining grokking through circuit efficiency},
  author = {Varma, Vikrant and Shah, Rohin and Kenton, Zachary and Kram{\'a}r, J{\'a}nos and Kumar, Ramana},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.02390},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.02390},
  url = {http://arxiv.org/abs/2309.02390},
  urldate = {2023-10-30},
  abstract = {One of the most surprising puzzles in neural network generalisation is grokking: a network with perfect training accuracy but poor generalisation will, upon further training, transition to perfect generalisation. We propose that grokking occurs when the task admits a generalising solution and a memorising solution, where the generalising solution is slower to learn but more efficient, producing larger logits with the same parameter norm. We hypothesise that memorising circuits become more inefficient with larger training datasets while generalising circuits do not, suggesting there is a critical dataset size at which memorisation and generalisation are equally efficient. We make and confirm four novel predictions about grokking, providing significant evidence in favour of our explanation. Most strikingly, we demonstrate two novel and surprising behaviours: ungrokking, in which a network regresses from perfect to low test accuracy, and semi-grokking, in which a network shows delayed generalisation to partial rather than perfect test accuracy.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/X7FUVZIC/Varma et al. - 2023 - Explaining grokking through circuit efficiency.pdf}
}

@article{vasudeva_simplicity_2024,
  title = {Simplicity Bias of Transformers to Learn Low Sensitivity Functions},
  author = {Vasudeva, Bhavya and Fu, Deqing and Zhou, Tianyi and Kau, Elliott and Huang, Youqi and Sharan, Vatsal},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.06925},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2403.06925},
  url = {http://arxiv.org/abs/2403.06925},
  urldate = {2024-06-10},
  abstract = {Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with improved robustness; furthermore, it can also be used as an efficient intervention to further improve the robustness of transformers.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/STRF34GR/Vasudeva et al. - 2024 - Simplicity Bias of Transformers to Learn Low Sensi.pdf}
}

@article{vaswani_attention_2017,
  title = {Attention is All you Need},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  journal = {NeurIPS},
  volume = {30},
  url = {https://arxiv.org/abs/1706.03762},
  urldate = {2023-11-29},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NH58S8EY/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{velez_whitebox_2021,
  title = {White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems},
  shorttitle = {White-Box Analysis over Machine Learning},
  author = {Velez, Miguel and Jamshidi, Pooyan and Siegmund, Norbert and Apel, Sven and K{\"a}stner, Christian},
  year = {2021},
  month = jan,
  journal = {CoRR},
  eprint = {2101.05362},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2101.05362},
  url = {http://arxiv.org/abs/2101.05362},
  urldate = {2023-11-06},
  abstract = {Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/LQ7KYHRY/Velez et al. - 2021 - White-Box Analysis over Machine Learning Modeling.pdf}
}

@article{verma_imitation-projected_2019,
  title = {Imitation-Projected Programmatic Reinforcement Learning},
  author = {Verma, Abhinav and Le, Hoang M. and Yue, Yisong and Chaudhuri, Swarat},
  year = {2019},
  journal = {NeurIPS},
  eprint = {1907.05431},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1907.05431},
  url = {http://arxiv.org/abs/1907.05431},
  urldate = {2024-02-19},
  abstract = {We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge -- a meta-algorithm called PROPEL -- is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space. Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches. Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5IAKNTWV/Verma et al. - 2019 - Imitation-Projected Programmatic Reinforcement Lea.pdf}
}

@article{verma_programmatically_2019,
  title = {Programmatically Interpretable Reinforcement Learning},
  author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  year = {2019},
  month = apr,
  journal = {CoRR},
  eprint = {1804.02477},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1804.02477},
  url = {http://arxiv.org/abs/1804.02477},
  urldate = {2023-11-02},
  abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9N8D72YK/Verma et al. - 2019 - Programmatically Interpretable Reinforcement Learn.pdf}
}

@article{vig_bertology_2020,
  title = {BERTology Meets Biology: Interpreting Attention in Protein Language Models},
  shorttitle = {BERTology Meets Biology},
  author = {Vig, Jesse and Madani, Ali and Varshney, Lav R. and Xiong, Caiming and Socher, Richard and Rajani, Nazneen Fatema},
  year = {2020},
  month = jun,
  journal = {bioRxiv},
  url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.26.174417},
  urldate = {2024-02-11},
  abstract = {Abstract                        Transformer architectures have proven to learn useful representations for protein classification and generation tasks. However, these representations present challenges in interpretability. Through the lens of attention, we analyze the inner workings of the Transformer and explore how the model discerns structural and functional properties of proteins. We show that attention (1) captures the folding structure of proteins, connecting amino acids that are far apart in the underlying sequence, but spatially close in the three-dimensional structure, (2) targets binding sites, a key functional component of proteins, and (3) focuses on progressively more complex biophysical properties with increasing layer depth. We also present a three-dimensional visualization of the interaction between attention and protein structure. Our findings align with known biological processes and provide a tool to aid discovery in protein engineering and synthetic biology. The code for visualization and analysis is available at             https://github.com/salesforce/provis             .},
  language = {en},
  keywords = {cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/58V59F82/Vig et al. - 2020 - BERTology Meets Biology Interpreting Attention in.pdf}
}

@article{vig_investigating_2020,
  title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  year = {2020},
  journal = {NeurIPS},
  volume = {33},
  pages = {12388--12401},
  url = {https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html},
  urldate = {2023-12-20},
  abstract = {Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model.  We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms that facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/T3DVSRNW/Vig et al. - 2020 - Investigating Gender Bias in Language Models Using.pdf}
}

@article{vig_visualizing_2019,
  title = {Visualizing Attention in Transformer-Based Language Representation Models},
  author = {Vig, Jesse},
  year = {2019},
  month = apr,
  journal = {CoRR},
  eprint = {1904.02679},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1904.02679},
  url = {http://arxiv.org/abs/1904.02679},
  urldate = {2024-03-20},
  abstract = {We present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/ESX22GW8/Vig - 2019 - Visualizing Attention in Transformer-Based Languag.pdf}
}

@article{vijayakumar_interpretability_2023,
  title = {Interpretability in Activation Space Analysis of Transformers: A Focused Survey},
  shorttitle = {Interpretability in Activation Space Analysis of Transformers},
  author = {Vijayakumar, Soniya},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.09304},
  url = {https://arxiv.org/abs/2302.09304},
  urldate = {2023-10-22},
  abstract = {The field of natural language processing has reached breakthroughs with the advent of transformers. They have remained state-of-the-art since then, and there also has been much research in analyzing, interpreting, and evaluating the attention layers and the underlying embedding space. In addition to the self-attention layers, the feed-forward layers in the transformer are a prominent architectural component. From extensive research, we observe that its role is under-explored. We focus on the latent space, known as the Activation Space, that consists of the neuron activations from these feed-forward layers. In this survey paper, we review interpretability methods that examine the learnings that occurred in this activation space. Since there exists only limited research in this direction, we conduct a detailed examination of each work and point out potential future directions of research. We hope our work provides a step towards strengthening activation space analysis.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5CB3AVH7/Vijayakumar - 2023 - Interpretability in Activation Space Analysis of T.pdf}
}

@article{vilas_analyzing_2023,
  title = {Analyzing vision transformers for image classification in class embedding space},
  author = {Vilas, M. and Schauml{\"o}ffel, Timothy and Roig, Gemma},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2310.18969},
  pages = {null},
  doi = {10.48550/arXiv.2310.18969},
  url = {https://www.semanticscholar.org/paper/7c57530318ae6c6e49c835f23e75affd9cf0827d},
  abstract = {Despite the growing use of transformer models in computer vision, a mechanistic understanding of these networks is still needed. This work introduces a method to reverse-engineer Vision Transformers trained to solve image classification tasks. Inspired by previous research in NLP, we demonstrate how the inner representations at any level of the hierarchy can be projected onto the learned class embedding space to uncover how these networks build categorical representations for their predictions. We use our framework to show how image tokens develop class-specific representations that depend on attention mechanisms and contextual information, and give insights on how self-attention and MLP layers differentially contribute to this categorical composition. We additionally demonstrate that this method (1) can be used to determine the parts of an image that would be important for detecting the class of interest, and (2) exhibits significant advantages over traditional linear probing approaches. Taken together, our results position our proposed framework as a powerful tool for mechanistic interpretability and explainability research.},
  arxivid = {2310.18969},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/AGZXLYFH/Vilas et al. - 2023 - Analyzing vision transformers for image classifica.pdf}
}

@article{voita_informationtheoretic_2020,
  title = {Information-Theoretic Probing with Minimum Description Length},
  author = {Voita, Elena and Titov, Ivan},
  year = {2020},
  month = mar,
  journal = {EMNLP},
  eprint = {2003.12298},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2003.12298},
  url = {http://arxiv.org/abs/2003.12298},
  urldate = {2023-11-16},
  abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HCA5FIAM/Voita and Titov - 2020 - Information-Theoretic Probing with Minimum Descrip.pdf}
}

@article{voita_neurons_2023,
  title = {Neurons in Large Language Models: Dead, N-gram, Positional},
  shorttitle = {Neurons in Large Language Models},
  author = {Voita, Elena and Ferrando, Javier and Nalmpantis, Christoforos},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.04827},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.04827},
  url = {http://arxiv.org/abs/2309.04827},
  urldate = {2023-10-30},
  abstract = {We analyze a family of large language models in such a lightweight manner that can be done on a single GPU. Specifically, we focus on the OPT family of models ranging from 125m to 66b parameters and rely only on whether an FFN neuron is activated or not. First, we find that the early part of the network is sparse and represents many discrete features. Here, many neurons (more than 70\% in some layers of the 66b model) are "dead", i.e. they never activate on a large collection of diverse data. At the same time, many of the alive neurons are reserved for discrete features and act as token and n-gram detectors. Interestingly, their corresponding FFN updates not only promote next token candidates as could be expected, but also explicitly focus on removing the information about triggering them tokens, i.e., current input. To the best of our knowledge, this is the first example of mechanisms specialized at removing (rather than adding) information from the residual stream. With scale, models become more sparse in a sense that they have more dead neurons and token detectors. Finally, some neurons are positional: them being activated or not depends largely (or solely) on position and less so (or not at all) on textual data. We find that smaller models have sets of neurons acting as position range indicators while larger models operate in a less explicit manner.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/R5ZN4VVT/Voita et al. - 2023 - Neurons in Large Language Models Dead, N-gram, Po.pdf}
}

@article{vonoswald_uncovering_2023,
  title = {Uncovering mesa-optimization algorithms in Transformers},
  author = {{von Oswald}, Johannes and Niklasson, Eyvind and Schlegel, Maximilian and Kobayashi, Seijin and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and y Arcas, Blaise Ag{\"u}era and Vladymyrov, Max and Pascanu, Razvan and Sacramento, Jo{\~a}o},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.05858},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.05858},
  url = {http://arxiv.org/abs/2309.05858},
  urldate = {2023-10-30},
  abstract = {Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/I823QG8H/von Oswald et al. - 2023 - Uncovering mesa-optimization algorithms in Transfo.pdf}
}

@article{voss_branch_2021,
  title = {Branch Specialization},
  author = {Voss, Chelsea and Goh, Gabriel and Cammarata, Nick and Petrov, Michael and Schubert, Ludwig and Olah, Chris},
  year = {2021},
  month = apr,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/branch-specialization},
  urldate = {2023-07-31},
  abstract = {When a neural network layer is divided into multiple branches, neurons self-organize into coherent groupings.},
  language = {en},
  keywords = {mechinterp,not cited,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/PURMFVMY/Voss et al. - 2021 - Branch Specialization.html}
}

@article{wallace_universal_2021,
  title = {Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author = {Wallace, Eric and Feng, Shi and Kandpal, Nikhil and Gardner, Matt and Singh, Sameer},
  year = {2021},
  month = jan,
  journal = {CoRR},
  eprint = {1908.07125},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1908.07125},
  url = {http://arxiv.org/abs/1908.07125},
  urldate = {2023-08-26},
  abstract = {Adversarial examples highlight model vulnerabilities and are useful for evaluation and interpretation. We define universal adversarial triggers: input-agnostic sequences of tokens that trigger a model to produce a specific prediction when concatenated to any input from a dataset. We propose a gradient-guided search over tokens which finds short trigger sequences (e.g., one word for classification and four words for language modeling) that successfully trigger the target prediction. For example, triggers cause SNLI entailment accuracy to drop from 89.94\% to 0.55\%, 72\% of "why" questions in SQuAD to be answered "to kill american people", and the GPT-2 language model to spew racist output even when conditioned on non-racial contexts. Furthermore, although the triggers are optimized using white-box access to a specific model, they transfer to other models for all tasks we consider. Finally, since triggers are input-agnostic, they provide an analysis of global model behavior. For instance, they confirm that SNLI models exploit dataset biases and help to diagnose heuristics learned by reading comprehension models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/R9PGG85D/Wallace et al. - 2021 - Universal Adversarial Triggers for Attacking and A.pdf}
}

@article{wang_easyedit_2023,
  title = {EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models},
  shorttitle = {EasyEdit},
  author = {Wang, Peng and Zhang, Ningyu and Xie, Xin and Yao, Yunzhi and Tian, Bozhong and Wang, Mengru and Xi, Zekun and Cheng, Siyuan and Liu, Kangwei and Zheng, Guozhou and Chen, Huajun},
  year = {2023},
  month = aug,
  journal = {CoRR},
  eprint = {2308.07269},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2308.07269},
  url = {http://arxiv.org/abs/2308.07269},
  urldate = {2023-11-16},
  abstract = {Large Language Models (LLMs) usually suffer from knowledge cutoff or fallacy issues, which means they are unaware of unseen events or generate text with incorrect facts owing to the outdated/noisy data. To this end, many knowledge editing approaches for LLMs have emerged -- aiming to subtly inject/edit updated knowledge or adjust undesired behavior while minimizing the impact on unrelated inputs. Nevertheless, due to significant differences among various knowledge editing methods and the variations in task setups, there is no standard implementation framework available for the community, which hinders practitioners to apply knowledge editing to applications. To address these issues, we propose EasyEdit, an easy-to-use knowledge editing framework for LLMs. It supports various cutting-edge knowledge editing approaches and can be readily apply to many well-known LLMs such as T5, GPT-J, LlaMA, etc. Empirically, we report the knowledge editing results on LlaMA-2 with EasyEdit, demonstrating that knowledge editing surpasses traditional fine-tuning in terms of reliability and generalization. We have released the source code on GitHub at https://github.com/zjunlp/EasyEdit, along with Google Colab tutorials and comprehensive documentation for beginners to get started. Besides, we present an online system for real-time knowledge editing, and a demo video at http://knowlm.zjukg.cn/easyedit.mp4.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/PBBERQ5T/Wang et al. - 2023 - EasyEdit An Easy-to-use Knowledge Editing Framewo.pdf}
}

@article{wang_essence_2024,
  title = {On the Essence and Prospect: An Investigation of Alignment Approaches for Big Models},
  shorttitle = {On the Essence and Prospect},
  author = {Wang, Xinpeng and Duan, Shitong and Yi, Xiaoyuan and Yao, Jing and Zhou, Shanlin and Wei, Zhihua and Zhang, Peng and Xu, Dongkuan and Sun, Maosong and Xie, Xing},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.04204},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.04204},
  url = {http://arxiv.org/abs/2403.04204},
  urldate = {2024-03-18},
  abstract = {Big models have achieved revolutionary breakthroughs in the field of AI, but they might also pose potential concerns. Addressing such concerns, alignment technologies were introduced to make these models conform to human preferences and values. Despite considerable advancements in the past year, various challenges lie in establishing the optimal alignment strategy, such as data cost and scalable oversight, and how to align remains an open question. In this survey paper, we comprehensively investigate value alignment approaches. We first unpack the historical context of alignment tracing back to the 1920s (where it comes from), then delve into the mathematical essence of alignment (what it is), shedding light on the inherent challenges. Following this foundation, we provide a detailed examination of existing alignment methods, which fall into three categories: Reinforcement Learning, Supervised Fine-Tuning, and In-context Learning, and demonstrate their intrinsic connections, strengths, and limitations, helping readers better understand this research area. In addition, two emerging topics, personal alignment, and multimodal alignment, are also discussed as novel frontiers in this field. Looking forward, we discuss potential alignment paradigms and how they could handle remaining challenges, prospecting where future alignment will go.},
  archiveprefix = {arxiv},
  keywords = {alignment,review},
  file = {/Users/leonardbereska/Zotero/storage/I687WAXK/Wang et al. - 2024 - On the Essence and Prospect An Investigation of A.pdf}
}

@article{wang_falling_2015,
  title = {Falling Rule Lists},
  author = {Wang, Fulton and Rudin, Cynthia},
  year = {2015},
  month = feb,
  journal = {AISTATS},
  eprint = {1411.5899},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1411.5899},
  url = {http://arxiv.org/abs/1411.5899},
  urldate = {2024-03-20},
  abstract = {Falling rule lists are classification models consisting of an ordered list of if-then rules, where (i) the order of rules determines which example should be classified by each rule, and (ii) the estimated probability of success decreases monotonically down the list. These kinds of rule lists are inspired by healthcare applications where patients would be stratified into risk sets and the highest at-risk patients should be considered first. We provide a Bayesian framework for learning falling rule lists that does not rely on traditional greedy decision tree learning methods.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/RSSJC5GG/Wang and Rudin - 2015 - Falling Rule Lists.pdf}
}

@article{wang_finding_2022,
  title = {Finding Skill Neurons in Pre-trained Transformer-based Language Models},
  author = {Wang, Xiaozhi and Wen, Kaiyue and Zhang, Zhengyan and Hou, Lei and Liu, Zhiyuan and Li, Juanzi},
  year = {2022},
  month = nov,
  journal = {EMNLP},
  eprint = {2211.07349},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.07349},
  url = {http://arxiv.org/abs/2211.07349},
  urldate = {2024-01-23},
  abstract = {Transformer-based pre-trained language models have demonstrated superior performance on various natural language processing tasks. However, it remains unclear how the skills required to handle these tasks distribute among model parameters. In this paper, we find that after prompt tuning for specific tasks, the activations of some neurons within pre-trained Transformers are highly predictive of the task labels. We dub these neurons skill neurons and confirm they encode task-specific skills by finding that: (1) Skill neurons are crucial for handling tasks. Performances of pre-trained Transformers on a task significantly drop when corresponding skill neurons are perturbed. (2) Skill neurons are task-specific. Similar tasks tend to have similar distributions of skill neurons. Furthermore, we demonstrate the skill neurons are most likely generated in pre-training rather than fine-tuning by showing that the skill neurons found with prompt tuning are also crucial for other fine-tuning methods freezing neuron weights, such as the adapter-based tuning and BitFit. We also explore the applications of skill neurons, including accelerating Transformers with network pruning and building better transferability indicators. These findings may promote further research on understanding Transformers. The source code can be obtained from https://github.com/THU-KEG/Skill-Neuron.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/M2IX3DJX/Wang et al. - 2022 - Finding Skill Neurons in Pre-trained Transformer-b.pdf}
}

@article{wang_forbidden_2023,
  title = {Forbidden facts: An investigation of competing objectives in llama-2},
  author = {Wang, Tony T. and Wang, Miles and Hariharan, Kaivu and Shavit, N.},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2312.08793},
  pages = {null},
  doi = {10.48550/arXiv.2312.08793},
  url = {https://www.semanticscholar.org/paper/a8f0207dfae566bdc798a18198d1c914a5e0aed7},
  abstract = {LLMs often face competing pressures (for example helpfulness vs. harmlessness). To understand how models resolve such conflicts, we study Llama-2-chat models on the forbidden fact task. Specifically, we instruct Llama-2 to truthfully complete a factual recall statement while forbidding it from saying the correct answer. This often makes the model give incorrect answers. We decompose Llama-2 into 1000+ components, and rank each one with respect to how useful it is for forbidding the correct answer. We find that in aggregate, around 35 components are enough to reliably implement the full suppression behavior. However, these components are fairly heterogeneous and many operate using faulty heuristics. We discover that one of these heuristics can be exploited via a manually designed adversarial attack which we call The California Attack. Our results highlight some roadblocks standing in the way of being able to successfully interpret advanced ML systems. Project website available at https://forbiddenfacts.github.io .},
  arxivid = {2312.08793},
  keywords = {mechinterp,not cited,to cite,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/7XM6PMZ7/Wang et al. - 2023 - Forbidden facts An investigation of competing obj.pdf}
}

@article{wang_gaussian_2023,
  title = {Gaussian Process Probes (GPP) for Uncertainty-Aware Probing},
  author = {Wang, Zi and Ku, Alexander and Baldridge, Jason and Griffiths, Thomas L. and Kim, Been},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2305.18213},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.18213},
  url = {http://arxiv.org/abs/2305.18213},
  urldate = {2024-01-23},
  abstract = {Understanding which concepts models can and cannot represent has been fundamental to many tasks: from effective and responsible use of models to detecting out of distribution data. We introduce Gaussian process probes (GPP), a unified and simple framework for probing and measuring uncertainty about concepts represented by models. As a Bayesian extension of linear probing methods, GPP asks what kind of distribution over classifiers (of concepts) is induced by the model. This distribution can be used to measure both what the model represents and how confident the probe is about what the model represents. GPP can be applied to any pre-trained model with vector representations of inputs (e.g., activations). It does not require access to training data, gradients, or the architecture. We validate GPP on datasets containing both synthetic and real images. Our experiments show it can (1) probe a model's representations of concepts even with a very small number of examples, (2) accurately measure both epistemic uncertainty (how confident the probe is) and aleatory uncertainty (how fuzzy the concepts are to the model), and (3) detect out of distribution data using those uncertainty measures as well as classic methods do. By using Gaussian processes to expand what probing can offer, GPP provides a data-efficient, versatile and uncertainty-aware tool for understanding and evaluating the capabilities of machine learning models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NN7TIENA/Wang et al. - 2023 - Gaussian Process Probes (GPP) for Uncertainty-Awar.pdf}
}

@article{wang_interpretability_2023,
  title = {Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  shorttitle = {Interpretability in the Wild},
  author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  year = {2023},
  journal = {ICLR},
  eprint = {2211.00593},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.00593},
  url = {http://arxiv.org/abs/2211.00593},
  urldate = {2023-08-27},
  abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
  archiveprefix = {arxiv},
  keywords = {behavior,circuit,cited,empirical,graph,mechinterp,scale,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/8JN38PFS/Wang et al. - 2023 - Interpretability in the Wild a Circuit for Indire.pdf}
}

@article{wang_knowledge_2023,
  title = {Knowledge Editing for Large Language Models: A Survey},
  shorttitle = {Knowledge Editing for Large Language Models},
  author = {Wang, Song and Zhu, Yaochen and Liu, Haochen and Zheng, Zaiyi and Chen, Chen and Li, Jundong},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.16218},
  url = {https://arxiv.org/abs/2310.16218},
  urldate = {2023-11-10},
  abstract = {Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2BVQQI9V/Wang et al. - 2023 - Knowledge Editing for Large Language Models A Sur.pdf}
}

@article{wang_scalable_2021,
  title = {Scalable Rule-Based Representation Learning for Interpretable Classification},
  author = {Wang, Zhuo and Zhang, Wei and Liu, Ning and Wang, Jianyong},
  year = {2021},
  month = sep,
  journal = {NeurIPS},
  eprint = {2109.15103},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.15103},
  url = {http://arxiv.org/abs/2109.15103},
  urldate = {2024-03-19},
  abstract = {Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. An improved design of logical activation functions is also devised to increase the scalability of RRL and enable it to discretize the continuous features end-to-end. Exhaustive experiments on nine small and four large data sets show that RRL outperforms the competitive interpretable approaches and can be easily adjusted to obtain a trade-off between classification accuracy and model complexity for different scenarios. Our code is available at: https://github.com/12wang3/rrl.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/R243FXTI/Wang et al. - 2021 - Scalable Rule-Based Representation Learning for In.pdf}
}

@article{ward_honesty_2023,
  title = {Honesty Is the Best Policy: Defining and Mitigating AI Deception},
  shorttitle = {Honesty Is the Best Policy},
  author = {Ward, Francis Rhys and Belardinelli, Francesco and Toni, Francesca and Everitt, Tom},
  year = {2023},
  month = dec,
  journal = {NeurIPS Spotlight},
  eprint = {2312.01350},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.01350},
  url = {http://arxiv.org/abs/2312.01350},
  urldate = {2024-02-12},
  abstract = {Deceptive agents are a challenge for the safety, trustworthiness, and cooperation of AI systems. We focus on the problem that agents might deceive in order to achieve their goals (for instance, in our experiments with language models, the goal of being evaluated as truthful). There are a number of existing definitions of deception in the literature on game theory and symbolic AI, but there is no overarching theory of deception for learning agents in games. We introduce a formal definition of deception in structural causal games, grounded in the philosophy literature, and applicable to real-world machine learning systems. Several examples and results illustrate that our formal definition aligns with the philosophical and commonsense meaning of deception. Our main technical result is to provide graphical criteria for deception. We show, experimentally, that these results can be used to mitigate deception in reinforcement learning agents and language models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/DJDKQ2FS/Ward et al. - 2023 - Honesty Is the Best Policy Defining and Mitigatin.pdf}
}

@article{warstadt_blimp_2020,
  title = {BLiMP: The Benchmark of Linguistic Minimal Pairs for English},
  shorttitle = {BLiMP},
  author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  editor = {Johnson, Mark and Roark, Brian and Nenkova, Ani},
  year = {2020},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {377--392},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  doi = {10.1162/tacl_a_00321},
  url = {https://aclanthology.org/2020.tacl-1.25},
  urldate = {2024-01-24},
  abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs---that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BB7AMCCW/Warstadt et al. - 2020 - BLiMP The Benchmark of Linguistic Minimal Pairs f.pdf}
}

@book{watanabe_algebraic_2009,
  title = {Algebraic Geometry and Statistical Learning Theory},
  author = {Watanabe, Sumio},
  year = {2009},
  series = {Cambridge Monographs on Applied and Computational Mathematics},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511800474},
  url = {https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A},
  urldate = {2023-11-10},
  abstract = {Sure to be influential, this book lays the foundations for the use of algebraic geometry in statistical learning theory. Many widely used statistical models and learning machines applied to information science have a parameter space that is singular: mixture models, neural networks, HMMs, Bayesian networks, and stochastic context-free grammars are major examples. Algebraic geometry and singularity theory provide the necessary tools for studying such non-smooth models. Four main formulas are established: 1. the log likelihood function can be given a common standard form using resolution of singularities, even applied to more complex models; 2. the asymptotic behaviour of the marginal likelihood or 'the evidence' is derived based on zeta function theory; 3. new methods are derived to estimate the generalization errors in Bayes and Gibbs estimations from training errors; 4. the generalization errors of maximum likelihood and a posteriori methods are clarified by empirical process theory on algebraic varieties.},
  keywords = {cited}
}

@book{watanabe_mathematical_2018,
  title = {Mathematical Theory of Bayesian Statistics},
  author = {Watanabe, Sumio},
  year = {2018},
  month = apr,
  edition = {1},
  publisher = {{Chapman and Hall}},
  doi = {10.1201/9781315373010},
  url = {https://www.taylorfrancis.com/books/9781482238082},
  urldate = {2023-11-10},
  abstract = {Semantic Scholar extracted view of "Mathematical Theory of Bayesian Statistics" by Sumio Watanabe},
  language = {en},
  keywords = {not cited}
}

@article{wei_chainofthought_2023,
  title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  year = {2023},
  month = jan,
  journal = {CoRR},
  eprint = {2201.11903},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2201.11903},
  url = {http://arxiv.org/abs/2201.11903},
  urldate = {2023-05-17},
  abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NDUBCB5R/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf}
}

@article{wei_emergent_2022,
  title = {Emergent Abilities of Large Language Models},
  author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
  year = {2022},
  month = oct,
  journal = {TMLR},
  eprint = {2206.07682},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.07682},
  url = {http://arxiv.org/abs/2206.07682},
  urldate = {2023-10-25},
  abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/38WU8Q5P/Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf}
}

@article{weiss_thinking_2021,
  title = {Thinking Like Transformers},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  year = {2021},
  month = jul,
  journal = {CoRR},
  eprint = {2106.06981},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.06981},
  url = {http://arxiv.org/abs/2106.06981},
  urldate = {2023-06-23},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YCKN78ZS/Weiss et al. - 2021 - Thinking Like Transformers.pdf}
}

@article{wen_transformers_2023,
  title = {Transformers are uninterpretable with myopic methods: a case study with bounded Dyck grammars},
  shorttitle = {Transformers are uninterpretable with myopic methods},
  author = {Wen, Kaiyue and Li, Yuchen and Liu, Bingbin and Risteski, Andrej},
  year = {2023},
  month = dec,
  journal = {CoRR},
  eprint = {2312.01429},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2312.01429},
  url = {http://arxiv.org/abs/2312.01429},
  urldate = {2024-01-09},
  abstract = {Interpretability methods aim to understand the algorithm implemented by a trained model (e.g., a Transofmer) by examining various aspects of the model, such as the weight matrices or the attention patterns. In this work, through a combination of theoretical results and carefully controlled experiments on synthetic data, we take a critical view of methods that exclusively focus on individual parts of the model, rather than consider the network as a whole. We consider a simple synthetic setup of learning a (bounded) Dyck language. Theoretically, we show that the set of models that (exactly or approximately) solve this task satisfy a structural characterization derived from ideas in formal languages (the pumping lemma). We use this characterization to show that the set of optima is qualitatively rich; in particular, the attention pattern of a single layer can be ``nearly randomized'', while preserving the functionality of the network. We also show via extensive experiments that these constructions are not merely a theoretical artifact: even after severely constraining the architecture of the model, vastly different solutions can be reached via standard training. Thus, interpretability claims based on inspecting individual heads or weight matrices in the Transformer can be misleading.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5RTCACXH/Wen et al. - 2023 - Transformers are uninterpretable with myopic metho.pdf;/Users/leonardbereska/Zotero/storage/V4CZR5SN/Wen et al. - 2023 - Transformers are uninterpretable with myopic metho.pdf}
}

@article{wentworth_how_2022,
  title = {How To Go From Interpretability To Alignment: Just Retarget The Search},
  shorttitle = {How To Go From Interpretability To Alignment},
  author = {Wentworth, John},
  year = {2022},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/w4aeAFzSAguvqA5qu/how-to-go-from-interpretability-to-alignment-just-retarget},
  urldate = {2023-11-30},
  abstract = {When people talk about~prosaic alignment proposals, there's a common pattern: they'll be outlining some overcomplicated scheme, and then they'll say{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MKZ43FWZ/Wentworth - 2022 - How To Go From Interpretability To Alignment Just.html}
}

@article{wentworth_public_2020,
  title = {Public Static: What is Abstraction?},
  shorttitle = {Public Static},
  author = {Wentworth, John},
  year = {2020},
  month = jun,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/vDGvHBDuMtcPd8Lks/public-static-what-is-abstraction},
  urldate = {2023-12-04},
  abstract = {Author's Note: Most of the posts in this sequence are essentially a log of work-in-progress. This post is intended as a more presentable (``public'') a{\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JRKVYELA/Wentworth - 2020 - Public Static What is Abstraction.html}
}

@article{wentworth_testing_2021,
  title = {Testing The Natural Abstraction Hypothesis: Project Intro},
  shorttitle = {Testing The Natural Abstraction Hypothesis},
  author = {Wentworth, John},
  year = {2021},
  month = apr,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/cy3BhHrGinZCp3LXE/testing-the-natural-abstraction-hypothesis-project-intro},
  urldate = {2023-12-04},
  abstract = {The natural abstraction hypothesis says that {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/N75QIWXU/Wentworth - 2021 - Testing The Natural Abstraction Hypothesis Projec.html}
}

@article{whittington_disentanglement_2023a,
  title = {Disentanglement with Biological Constraints: A Theory of Functional Cell Types},
  shorttitle = {Disentanglement with Biological Constraints},
  author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy E. J.},
  year = {2023},
  month = mar,
  journal = {CoRR},
  eprint = {2210.01768},
  primaryclass = {cs, q-bio},
  doi = {10.48550/arXiv.2210.01768},
  url = {http://arxiv.org/abs/2210.01768},
  urldate = {2023-07-31},
  abstract = {Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentanglement in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why single neurons in the brain often represent single human-interpretable factors, and steps towards an understanding task structure shapes the structure of brain representation.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QHJQEVCV/Whittington et al. - 2023 - Disentanglement with Biological Constraints A The.pdf}
}

@article{whittington_disentangling_2022,
  title = {Disentangling with Biological Constraints: A Theory of Functional Cell Types},
  shorttitle = {Disentangling with Biological Constraints},
  author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy E. J.},
  year = {2022},
  month = sep,
  journal = {CoRR},
  eprint = {2210.01768},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2210.01768},
  urldate = {2023-01-09},
  abstract = {Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentangling in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why, when, and how neurons represent factors in both brains and machines, and is a first step towards understanding of how task demands structure neural representations.},
  archiveprefix = {arxiv},
  language = {en},
  keywords = {bioinspired,cited,disentangling,grid cells,read},
  file = {/Users/leonardbereska/Zotero/storage/6QD5J4UH/Whittington et al. - 2022 - Disentangling with Biological Constraints A Theor.pdf}
}

@article{wiegreffe_attention_2019,
  title = {Attention is not not Explanation},
  author = {Wiegreffe, Sarah and Pinter, Yuval},
  editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  year = {2019},
  month = nov,
  journal = {EMNLP-IJCNLP},
  pages = {11--20},
  doi = {10.18653/v1/D19-1002},
  url = {https://aclanthology.org/D19-1002},
  urldate = {2024-03-20},
  abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
  file = {/Users/leonardbereska/Zotero/storage/8QPPYCPF/Wiegreffe and Pinter - 2019 - Attention is not not Explanation.pdf}
}

@article{wolf_fundamental_2023,
  title = {Fundamental Limitations of Alignment in Large Language Models},
  author = {Wolf, Yotam and Wies, Noam and Levine, Yoav and Shashua, Amnon},
  year = {2023},
  month = apr,
  journal = {CoRR},
  eprint = {2304.11082},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.11082},
  url = {http://arxiv.org/abs/2304.11082},
  urldate = {2023-05-08},
  abstract = {An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading alignment approaches such as reinforcement learning from human feedback increase the LLM's proneness to being prompted into the undesired behaviors. Moreover, we include the notion of personas in our BEB framework, and find that behaviors which are generally very unlikely to be exhibited by the model can be brought to the front by prompting the model to behave as specific persona. This theoretical result is being experimentally demonstrated in large scale by the so called contemporary "chatGPT jailbreaks", where adversarial users trick the LLM into breaking its alignment guardrails by triggering it into acting as a malicious persona. Our results expose fundamental limitations in alignment of LLMs and bring to the forefront the need to devise reliable mechanisms for ensuring AI safety.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/NY9EGJ5Z/Wolf et al. - 2023 - Fundamental Limitations of Alignment in Large Lang.pdf}
}

@article{wong_word_2023,
  title = {From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought},
  shorttitle = {From Word Models to World Models},
  author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2306.12672},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.12672},
  url = {http://arxiv.org/abs/2306.12672},
  urldate = {2024-02-13},
  abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.},
  archiveprefix = {arxiv},
  keywords = {cited,to cite,world models},
  file = {/Users/leonardbereska/Zotero/storage/Q5MUX4PV/Wong et al. - 2023 - From Word Models to World Models Translating from.pdf}
}

@article{wortsman_supermasks_2020,
  title = {Supermasks in superposition},
  author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
  year = {2020},
  journal = {NeurIPS},
  url = {https://arxiv.org/abs/2006.14769},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/Q629Y7YB/Wortsman et al. - 2020 - Supermasks in superposition.pdf}
}

@book{wright_highdimensional_2022,
  title = {High-dimensional data analysis with low-dimensional models: Principles, computation, and applications},
  author = {Wright, John and Ma, Yi},
  year = {2022},
  publisher = {Cambridge University Press}
}

@article{wu_backdoorbench_2022,
  title = {BackdoorBench: A Comprehensive Benchmark of Backdoor Learning},
  shorttitle = {BackdoorBench},
  author = {Wu, Baoyuan and Chen, Hongrui and Zhang, Mingda and Zhu, Zihao and Wei, Shaokui and Yuan, Danni and Shen, Chao},
  year = {2022},
  month = oct,
  journal = {NeurIPS Datasets and Benchmarks},
  eprint = {2206.12654},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.12654},
  url = {http://arxiv.org/abs/2206.12654},
  urldate = {2024-02-19},
  abstract = {Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility. Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning. All codes and evaluations of BackdoorBench are publicly available at {\textbackslash}url\{https://backdoorbench.github.io\}.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/JIVLJK6J/Wu et al. - 2022 - BackdoorBench A Comprehensive Benchmark of Backdo.pdf}
}

@article{wu_causal_2022,
  title = {Causal Distillation for Language Models},
  author = {Wu, Zhengxuan and Geiger, Atticus and Rozner, Joshua and Kreiss, Elisa and Lu, Hanson and Icard, Thomas and Potts, Christopher and Goodman, Noah},
  editor = {Carpuat, Marine and {de Marneffe}, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
  year = {2022},
  month = jul,
  journal = {NAACL-HLT},
  pages = {4288--4295},
  doi = {10.18653/v1/2022.naacl-main.318},
  url = {https://aclanthology.org/2022.naacl-main.318},
  urldate = {2024-03-19},
  abstract = {Distillation efforts have led to language models that are more compact and efficient without serious drops in performance. The standard approach to distillation trains a student model against two objectives: a task-specific objective (e.g., language modeling) and an imitation objective that encourages the hidden states of the student model to be similar to those of the larger teacher model. In this paper, we show that it is beneficial to augment distillation with a third objective that encourages the student to imitate the causal dynamics of the teacher through a distillation interchange intervention training objective (DIITO). DIITO pushes the student model to become a causal abstraction of the teacher model -- a faithful model with simpler causal structure. DIITO is fully differentiable, easily implemented, and combines flexibly with other objectives. Compared against standard distillation with the same setting, DIITO results in lower perplexity on the WikiText-103M corpus (masked language modeling) and marked improvements on the GLUE benchmark (natural language understanding), SQuAD (question answering), and CoNLL-2003 (named entity recognition).},
  file = {/Users/leonardbereska/Zotero/storage/BLCVCDNL/Wu et al. - 2022 - Causal Distillation for Language Models.pdf}
}

@article{wu_causal_2023,
  title = {Causal Proxy Models for Concept-Based Model Explanations},
  author = {Wu, Zhengxuan and D'Oosterlinck, Karel and Geiger, Atticus and Zur, Amir and Potts, Christopher},
  year = {2023},
  journal = {ICML},
  eprint = {2209.14279},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.14279},
  url = {http://arxiv.org/abs/2209.14279},
  urldate = {2024-03-19},
  abstract = {Explainability methods for NLP systems encounter a version of the fundamental problem of causal inference: for a given ground-truth input text, we never truly observe the counterfactual texts necessary for isolating the causal effects of model representations on outputs. In response, many explainability methods make no use of counterfactual texts, assuming they will be unavailable. In this paper, we show that robust causal explainability methods can be created using approximate counterfactuals, which can be written by humans to approximate a specific counterfactual or simply sampled using metadata-guided heuristics. The core of our proposal is the Causal Proxy Model (CPM). A CPM explains a black-box model \${\textbackslash}mathcal\{N\}\$ because it is trained to have the same actual input/output behavior as \${\textbackslash}mathcal\{N\}\$ while creating neural representations that can be intervened upon to simulate the counterfactual input/output behavior of \${\textbackslash}mathcal\{N\}\$. Furthermore, we show that the best CPM for \${\textbackslash}mathcal\{N\}\$ performs comparably to \${\textbackslash}mathcal\{N\}\$ in making factual predictions, which means that the CPM can simply replace \${\textbackslash}mathcal\{N\}\$, leading to more explainable deployed models. Our code is available at https://github.com/frankaging/Causal-Proxy-Model.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/TT29SAIN/Wu et al. - 2022 - Causal Proxy Models for Concept-Based Model Explan.pdf}
}

@article{wu_interpretability_2023,
  title = {Interpretability at Scale: Identifying Causal Mechanisms in Alpaca},
  shorttitle = {Interpretability at Scale},
  author = {Wu, Zhengxuan and Geiger, Atticus and Potts, Christopher and Goodman, Noah D.},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2305.08809},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.08809},
  url = {http://arxiv.org/abs/2305.08809},
  urldate = {2023-08-27},
  abstract = {Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.},
  archiveprefix = {arxiv},
  keywords = {causal,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/KEDMWB53/Wu et al. - 2023 - Interpretability at Scale Identifying Causal Mech.pdf}
}

@article{wu_pyvene_2024,
  title = {pyvene: A Library for Understanding and Improving PyTorch Models via Interventions},
  shorttitle = {pyvene},
  author = {Wu, Zhengxuan and Geiger, Atticus and Arora, Aryaman and Huang, Jing and Wang, Zheng and Goodman, Noah D. and Manning, Christopher D. and Potts, Christopher},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2403.07809},
  url = {https://arxiv.org/abs/2403.07809},
  urldate = {2024-06-10},
  abstract = {Interventions on model-internal states are fundamental operations in many areas of AI, including model editing, steering, robustness, and interpretability. To facilitate such research, we introduce \${\textbackslash}textbf\{pyvene\}\$, an open-source Python library that supports customizable interventions on a range of different PyTorch modules. \${\textbackslash}textbf\{pyvene\}\$ supports complex intervention schemes with an intuitive configuration format, and its interventions can be static or include trainable parameters. We show how \${\textbackslash}textbf\{pyvene\}\$ provides a unified and extensible framework for performing interventions on neural models and sharing the intervened upon models with others. We illustrate the power of the library via interpretability analyses using causal abstraction and knowledge localization. We publish our library through Python Package Index (PyPI) and provide code, documentation, and tutorials at https://github.com/stanfordnlp/pyvene.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/UN4LFKY3/Wu et al. - 2024 - pyvene A Library for Understanding and Improving .pdf}
}

@article{wu_reft_2024,
  title = {ReFT: Representation Finetuning for Language Models},
  shorttitle = {ReFT},
  author = {Wu, Zhengxuan and Arora, Aryaman and Wang, Zheng and Geiger, Atticus and Jurafsky, Dan and Manning, Christopher D. and Potts, Christopher},
  year = {2024},
  month = may,
  journal = {CoRR},
  eprint = {2404.03592},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.03592},
  url = {http://arxiv.org/abs/2404.03592},
  urldate = {2024-06-21},
  abstract = {Parameter-efficient finetuning (PEFT) methods seek to adapt large neural models via updates to a small number of weights. However, much prior interpretability work has shown that representations encode rich semantic information, suggesting that editing representations might be a more powerful alternative. We pursue this hypothesis by developing a family of Representation Finetuning (ReFT) methods. ReFT methods operate on a frozen base model and learn task-specific interventions on hidden representations. We define a strong instance of the ReFT family, Low-rank Linear Subspace ReFT (LoReFT), and we identify an ablation of this method that trades some performance for increased efficiency. Both are drop-in replacements for existing PEFTs and learn interventions that are 15x--65x more parameter-efficient than LoRA. We showcase LoReFT on eight commonsense reasoning tasks, four arithmetic reasoning tasks, instruction-tuning, and GLUE. In all these evaluations, our ReFTs deliver the best balance of efficiency and performance, and almost always outperform state-of-the-art PEFTs. We release a generic ReFT training library publicly at https://github.com/stanfordnlp/pyreft.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/89XT8A26/Wu et al. - 2024 - ReFT Representation Finetuning for Language Model.pdf;/Users/leonardbereska/Zotero/storage/RV625326/2404.html}
}

@article{wu_reply_2024,
  title = {A Reply to Makelov et al. (2023)'s "Interpretability Illusion" Arguments},
  author = {Wu, Zhengxuan and Geiger, Atticus and Huang, Jing and Arora, Aryaman and Icard, Thomas and Potts, Christopher and Goodman, Noah D.},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.12631},
  url = {https://arxiv.org/abs/2401.12631},
  urldate = {2024-02-11},
  abstract = {We respond to the recent paper by Makelov et al. (2023), which reviews subspace interchange intervention methods like distributed alignment search (DAS; Geiger et al. 2023) and claims that these methods potentially cause "interpretability illusions". We first review Makelov et al. (2023)'s technical notion of what an "interpretability illusion" is, and then we show that even intuitive and desirable explanations can qualify as illusions in this sense. As a result, their method of discovering "illusions" can reject explanations they consider "non-illusory". We then argue that the illusions Makelov et al. (2023) see in practice are artifacts of their training and evaluation paradigms. We close by emphasizing that, though we disagree with their core characterization, Makelov et al. (2023)'s examples and discussion have undoubtedly pushed the field of interpretability forward.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {mechinterp,not cited,to cite,to extract related work,to read},
  file = {/Users/leonardbereska/Zotero/storage/HTNXVT67/Wu et al. - 2024 - A Reply to Makelov et al. (2023)'s Interpretabili.pdf}
}

@article{wu_sparsity_2017,
  title = {Beyond Sparsity: Tree Regularization of Deep Models for Interpretability},
  shorttitle = {Beyond Sparsity},
  author = {Wu, Mike and Hughes, Michael C. and Parbhoo, Sonali and Zazzi, Maurizio and Roth, Volker and {Doshi-Velez}, Finale},
  year = {2017},
  month = nov,
  journal = {CoRR},
  eprint = {1711.06178},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1711.06178},
  url = {http://arxiv.org/abs/1711.06178},
  urldate = {2023-10-22},
  abstract = {The lack of interpretability remains a key barrier to the adoption of deep models in many applications. In this work, we explicitly regularize deep models so human users might step through the process behind their predictions in little time. Specifically, we train deep time-series models so their class-probability predictions have high accuracy while being closely modeled by decision trees with few nodes. Using intuitive toy examples as well as medical tasks for treating sepsis and HIV, we demonstrate that this new tree regularization yields models that are easier for humans to simulate than simpler L1 or L2 penalties without sacrificing predictive power.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/4KNDBBHY/Wu et al. - 2017 - Beyond Sparsity Tree Regularization of Deep Model.pdf}
}

@article{xie_protolm_2023,
  title = {Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models},
  shorttitle = {Proto-lm},
  author = {Xie, Sean and Vosoughi, Soroush and Hassanpour, Saeed},
  year = {2023},
  journal = {EMNLP},
  doi = {10.48550/ARXIV.2311.01732},
  url = {https://arxiv.org/abs/2311.01732},
  urldate = {2024-02-11},
  abstract = {Large Language Models (LLMs) have significantly advanced the field of Natural Language Processing (NLP), but their lack of interpretability has been a major concern. Current methods for interpreting LLMs are post hoc, applied after inference time, and have limitations such as their focus on low-level features and lack of explainability at higher level text units. In this work, we introduce proto-lm, a prototypical network-based white-box framework that allows LLMs to learn immediately interpretable embeddings during the fine-tuning stage while maintaining competitive performance. Our method's applicability and interpretability are demonstrated through experiments on a wide range of NLP tasks, and our results indicate a new possibility of creating interpretable models without sacrificing performance. This novel approach to interpretability in LLMs can pave the way for more interpretable models without the need to sacrifice performance.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {intrinsic,LLMs,not cited,prototype},
  file = {/Users/leonardbereska/Zotero/storage/HEX2Z7BU/Xie et al. - 2023 - Proto-lm A Prototypical Network-Based Framework f.pdf}
}

@article{xu_benign_2023,
  title = {Benign overfitting and grokking in ReLU networks for XOR cluster data},
  author = {Xu, Zhiwei and Wang, Yutong and Frei, Spencer and Vardi, Gal and Hu, Wei},
  year = {2023},
  journal = {CoRR},
  volume = {abs/2310.02541},
  pages = {null},
  doi = {10.48550/arXiv.2310.02541},
  url = {https://www.semanticscholar.org/paper/72b18667ad609c1fcb1df66a8995369b84dedfd7},
  abstract = {Neural networks trained by gradient descent (GD) have exhibited a number of surprising generalization behaviors. First, they can achieve a perfect fit to noisy training data and still generalize near-optimally, showing that overfitting can sometimes be benign. Second, they can undergo a period of classical, harmful overfitting -- achieving a perfect fit to training data with near-random performance on test data -- before transitioning ("grokking") to near-optimal generalization later in training. In this work, we show that both of these phenomena provably occur in two-layer ReLU networks trained by GD on XOR cluster data where a constant fraction of the training labels are flipped. In this setting, we show that after the first step of GD, the network achieves 100\% training accuracy, perfectly fitting the noisy labels in the training data, but achieves near-random test accuracy. At a later training step, the network achieves near-optimal test accuracy while still fitting the random labels in the training data, exhibiting a"grokking"phenomenon. This provides the first theoretical result of benign overfitting in neural network classification when the data distribution is not linearly separable. Our proofs rely on analyzing the feature learning process under GD, which reveals that the network implements a non-generalizable linear classifier after one step and gradually learns generalizable features in later steps.},
  arxivid = {2310.02541},
  keywords = {grokking,not cited,toy models},
  file = {/Users/leonardbereska/Zotero/storage/WUWNC5Z8/Xu et al. - 2023 - Benign overfitting and grokking in ReLU networks f.pdf}
}

@article{xuanyuan_global_2023,
  title = {Global Concept-Based Interpretability for Graph Neural Networks via Neuron Analysis},
  author = {Xuanyuan, Han and Barbiero, Pietro and Georgiev, Dobrik and Magister, Lucie Charlotte and Li{\'o}, Pietro},
  year = {2023},
  journal = {AAAI},
  doi = {10.48550/ARXIV.2208.10609},
  url = {https://arxiv.org/abs/2208.10609},
  urldate = {2023-08-24},
  abstract = {Graph neural networks (GNNs) are highly effective on a variety of graph-related tasks; however, they lack interpretability and transparency. Current explainability approaches are typically local and treat GNNs as black-boxes. They do not look inside the model, inhibiting human trust in the model and explanations. Motivated by the ability of neurons to detect high-level semantic concepts in vision models, we perform a novel analysis on the behaviour of individual GNN neurons to answer questions about GNN interpretability, and propose new metrics for evaluating the interpretability of GNN neurons. We propose a novel approach for producing global explanations for GNNs using neuron-level concepts to enable practitioners to have a high-level view of the model. Specifically, (i) to the best of our knowledge, this is the first work which shows that GNN neurons act as concept detectors and have strong alignment with concepts formulated as logical compositions of node degree and neighbourhood properties; (ii) we quantitatively assess the importance of detected concepts, and identify a trade-off between training duration and neuron-level interpretability; (iii) we demonstrate that our global explainability approach has advantages over the current state-of-the-art -- we can disentangle the explanation into individual interpretable concepts backed by logical descriptions, which reduces potential for bias and improves user-friendliness.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/MG2CI489/Xuanyuan et al. - 2023 - Global Concept-Based Interpretability for Graph Ne.pdf}
}

@article{yadlowsky_pretraining_2023,
  title = {Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in Transformer Models},
  author = {Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2311.00871},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2311.00871},
  url = {http://arxiv.org/abs/2311.00871},
  urldate = {2023-11-08},
  abstract = {Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of \$(x, f(x))\$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/WBDFKFQK/Yadlowsky et al. - 2023 - Pretraining Data Mixtures Enable Narrow Model Sele.pdf}
}

@article{yang_large_2024,
  title = {Do Large Language Models Latently Perform Multi-Hop Reasoning?},
  author = {Yang, Sohee and Gribovskaya, Elena and Kassner, Nora and Geva, Mor and Riedel, Sebastian},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.16837},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.16837},
  url = {http://arxiv.org/abs/2402.16837},
  urldate = {2024-06-10},
  abstract = {We study whether Large Language Models (LLMs) latently perform multi-hop reasoning with complex prompts such as "The mother of the singer of 'Superstition' is". We look for evidence of a latent reasoning pathway where an LLM (1) latently identifies "the singer of 'Superstition'" as Stevie Wonder, the bridge entity, and (2) uses its knowledge of Stevie Wonder's mother to complete the prompt. We analyze these two hops individually and consider their co-occurrence as indicative of latent multi-hop reasoning. For the first hop, we test if changing the prompt to indirectly mention the bridge entity instead of any other entity increases the LLM's internal recall of the bridge entity. For the second hop, we test if increasing this recall causes the LLM to better utilize what it knows about the bridge entity. We find strong evidence of latent multi-hop reasoning for the prompts of certain relation types, with the reasoning pathway used in more than 80\% of the prompts. However, the utilization is highly contextual, varying across different types of prompts. Also, on average, the evidence for the second hop and the full multi-hop traversal is rather moderate and only substantial for the first hop. Moreover, we find a clear scaling trend with increasing model size for the first hop of reasoning but not for the second hop. Our experimental findings suggest potential challenges and opportunities for future development and applications of LLMs.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/NLV6MDP9/Yang et al. - 2024 - Do Large Language Models Latently Perform Multi-Ho.pdf}
}

@article{yao_editing_2023,
  title = {Editing Large Language Models: Problems, Methods, and Opportunities},
  shorttitle = {Editing Large Language Models},
  author = {Yao, Yunzhi and Wang, Peng and Tian, Bozhong and Cheng, Siyuan and Li, Zhoubo and Deng, Shumin and Chen, Huajun and Zhang, Ningyu},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2305.13172},
  url = {https://arxiv.org/abs/2305.13172},
  urldate = {2023-11-03},
  abstract = {Despite the ability to train capable LLMs, the methodology for maintaining their relevancy and rectifying errors remains elusive. To this end, the past few years have witnessed a surge in techniques for editing LLMs, the objective of which is to efficiently alter the behavior of LLMs within a specific domain without negatively impacting performance across other inputs. This paper embarks on a deep exploration of the problems, methods, and opportunities related to model editing for LLMs. In particular, we provide an exhaustive overview of the task definition and challenges associated with model editing, along with an in-depth empirical analysis of the most progressive methods currently at our disposal. We also build a new benchmark dataset to facilitate a more robust evaluation and pinpoint enduring issues intrinsic to existing techniques. Our objective is to provide valuable insights into the effectiveness and feasibility of each editing technique, thereby assisting the community in making informed decisions on the selection of the most appropriate method for a specific task or context. Code and datasets are available at https://github.com/zjunlp/EasyEdit.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/MC55WEE4/Yao et al. - 2023 - Editing Large Language Models Problems, Methods, .pdf}
}

@article{ye_sparse_2022,
  title = {Sparse Distillation: Speeding Up Text Classification by Using Bigger Student Models},
  shorttitle = {Sparse Distillation},
  author = {Ye, Qinyuan and Khabsa, Madian and Lewis, Mike and Wang, Sinong and Ren, Xiang and Jaech, Aaron},
  editor = {Carpuat, Marine and {de Marneffe}, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
  year = {2022},
  month = jul,
  journal = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {2361--2375},
  doi = {10.18653/v1/2022.naacl-main.169},
  url = {https://aclanthology.org/2022.naacl-main.169},
  urldate = {2024-04-25},
  abstract = {Distilling state-of-the-art transformer models into lightweight student models is an effective way to reduce computation cost at inference time. The student models are typically compact transformers with fewer parameters, while expensive operations such as self-attention persist. Therefore, the improved inference speed may still be unsatisfactory for real-time or high-volume use cases. In this paper, we aim to further push the limit of inference speed by distilling teacher models into bigger, sparser student models -- bigger in that they scale up to billions of parameters; sparser in that most of the model parameters are n-gram embeddings. Our experiments on six single-sentence text classification tasks show that these student models retain 97\% of the RoBERTa-Large teacher performance on average, and meanwhile achieve up to 600x speed-up on both GPUs and CPUs at inference time. Further investigation reveals that our pipeline is also helpful for sentence-pair classification tasks, and in domain generalization settings.},
  file = {/Users/leonardbereska/Zotero/storage/S4L6WFTU/Ye et al. - 2022 - Sparse Distillation Speeding Up Text Classificati.pdf}
}

@article{yeom_pruning_2021,
  title = {Pruning by Explaining: A Novel Criterion for Deep Neural Network Pruning},
  shorttitle = {Pruning by Explaining},
  author = {Yeom, Seul-Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  year = {2021},
  month = jul,
  journal = {Pattern Recognition},
  volume = {115},
  eprint = {1912.08881},
  primaryclass = {cs, stat},
  pages = {107899},
  issn = {00313203},
  doi = {10.1016/j.patcog.2021.107899},
  url = {http://arxiv.org/abs/1912.08881},
  urldate = {2023-09-18},
  abstract = {The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/J8RS3LP7/Yeom et al. - 2021 - Pruning by Explaining A Novel Criterion for Deep .pdf}
}

@article{yildirim_task_2023,
  title = {From task structures to world models: What do LLMs know?},
  shorttitle = {From task structures to world models},
  author = {Yildirim, Ilker and Paul, L. A.},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.04276},
  url = {https://arxiv.org/abs/2310.04276},
  urldate = {2024-02-11},
  abstract = {In what sense does a large language model have knowledge? The answer to this question extends beyond the capabilities of a particular AI system, and challenges our assumptions about the nature of knowledge and intelligence. We answer by granting LLMs "instrumental knowledge"; knowledge defined by a certain set of abilities. We then ask how such knowledge is related to the more ordinary, "worldly" knowledge exhibited by human agents, and explore this in terms of the degree to which instrumental knowledge can be said to incorporate the structured world models of cognitive science. We discuss ways LLMs could recover degrees of worldly knowledge, and suggest such recovery will be governed by an implicit, resource-rational tradeoff between world models and task demands.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited,to cite,world models},
  file = {/Users/leonardbereska/Zotero/storage/64MIG9GP/Yildirim and Paul - 2023 - From task structures to world models What do LLMs.pdf}
}

@article{yin_interpreting_2022,
  title = {Interpreting Language Models with Contrastive Explanations},
  author = {Yin, Kayo and Neubig, Graham},
  year = {2022},
  month = may,
  journal = {CoRR},
  eprint = {2202.10419},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2202.10419},
  url = {http://arxiv.org/abs/2202.10419},
  urldate = {2024-01-24},
  abstract = {Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics. Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding. To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/GIJP55Y2/Yin and Neubig - 2022 - Interpreting Language Models with Contrastive Expl.pdf}
}

@article{yousefi_decoding_2024,
  title = {Decoding In-Context Learning: Neuroscience-inspired Analysis of Representations in Large Language Models},
  shorttitle = {Decoding In-Context Learning},
  author = {Yousefi, Safoora and Betthauser, Leo and Hasanbeig, Hosein and Milli{\`e}re, Rapha{\"e}l and Momennejad, Ida},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2310.00313},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.00313},
  url = {http://arxiv.org/abs/2310.00313},
  urldate = {2024-06-10},
  abstract = {Large language models (LLMs) exhibit remarkable performance improvement through in-context learning (ICL) by leveraging task-specific examples in the input. However, the mechanisms behind this improvement remain elusive. In this work, we investigate how LLM embeddings and attention representations change following in-context-learning, and how these changes mediate improvement in behavior. We employ neuroscience-inspired techniques such as representational similarity analysis (RSA) and propose novel methods for parameterized probing and measuring ratio of attention to relevant vs. irrelevant information in Llama-2 70B and Vicuna 13B. We designed two tasks with a priori relationships among their conditions: linear regression and reading comprehension. We formed hypotheses about expected similarities in task representations and measured hypothesis alignment of LLM representations before and after ICL as well as changes in attention. Our analyses revealed a meaningful correlation between improvements in behavior after ICL and changes in both embeddings and attention weights across LLM layers. This empirical framework empowers a nuanced understanding of how latent representations shape LLM behavior, offering valuable tools and insights for future research and practical applications.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/22973FGU/Yousefi et al. - 2024 - Decoding In-Context Learning Neuroscience-inspire.pdf}
}

@article{yu_characterizing_2023,
  title = {Characterizing Mechanisms for Factual Recall in Language Models},
  author = {Yu, Qinan and Merullo, Jack and Pavlick, Ellie},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.15910},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.15910},
  url = {http://arxiv.org/abs/2310.15910},
  urldate = {2023-11-16},
  abstract = {Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g., "The capital of Poland is London") to overwrite what it learned in pretraining ("Warsaw"). On Pythia and GPT2, the training frequency of both the query country ("Poland") and the in-context city ("London") highly affect the models' likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88{\textbackslash}\% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/CEILTIN7/Yu et al. - 2023 - Characterizing Mechanisms for Factual Recall in La.pdf}
}

@article{yu_whitebox_2023,
  title = {White-Box Transformers via Sparse Rate Reduction},
  author = {Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe and Wu, Ziyang and Tong, Shengbang and Haeffele, Benjamin D. and Ma, Yi},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.01129},
  url = {https://arxiv.org/abs/2306.01129},
  urldate = {2023-11-10},
  abstract = {In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at {\textbackslash}url\{https://github.com/Ma-Lab-Berkeley/CRATE\}.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/W67WAT78/Yu et al. - 2023 - White-Box Transformers via Sparse Rate Reduction.pdf}
}

@article{yudkowsky_agi_2022,
  title = {AGI Ruin: A List of Lethalities},
  shorttitle = {AGI Ruin},
  author = {Yudkowsky, Eliezer},
  year = {2022},
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities},
  urldate = {2024-02-26},
  abstract = {Preamble: (If you're already familiar with all basics and don't want any preamble, skip ahead to~Section B for technical difficulties of alignment pr{\dots}},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/SZZZ3XV6/Yudkowsky - 2022 - AGI Ruin A List of Lethalities.html}
}

@article{yudkowsky_ai_2016,
  title = {AI Alignment: Why It's Hard, and Where to Start},
  shorttitle = {AI Alignment},
  author = {Yudkowsky, Eliezer},
  year = {2016},
  month = dec,
  journal = {MIRI Blog},
  url = {https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/},
  urldate = {2023-05-17},
  abstract = {Back in May, I gave a talk at Stanford University for the Symbolic Systems Distinguished Speaker series, titled ``The AI Alignment Problem: Why It's Hard, And Where To Start.'' The video for this talk is now available on Youtube: ~ ~ We have an approximately complete transcript of the talk and Q\&A session here, slides... Read more >>},
  language = {en-US},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/YEHLIYPW/Yudkowsky - 2016 - AI Alignment Why It's Hard, and Where to Start.html}
}

@article{yudkowsky_gpts_2023,
  title = {GPTs are Predictors, not Imitators},
  author = {Yudkowsky, Eliezer},
  year = {2023},
  month = apr,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators},
  urldate = {2023-05-15},
  abstract = {(Related text posted to Twitter; this version is edited and has a more advanced final section.) {\dots}},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/G99A9KFE/Yudkowsky - 2023 - GPTs are Predictors, not Imitators.html}
}

@article{yun_transformer_2021,
  title = {Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  shorttitle = {Transformer visualization via dictionary learning},
  author = {Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A. and LeCun, Yann},
  year = {2021},
  journal = {NAACL Workshop DeeLIO},
  eprint = {2103.15949},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2103.15949},
  url = {http://arxiv.org/abs/2103.15949},
  urldate = {2023-11-16},
  abstract = {Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/zeyuyun1/TransformerVis},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/TDV636A2/Yun et al. - 2021 - Transformer visualization via dictionary learning.pdf}
}

@article{zaken_bitfit_2022,
  title = {BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  shorttitle = {BitFit},
  author = {Zaken, Elad Ben and Ravfogel, Shauli and Goldberg, Yoav},
  year = {2022},
  month = sep,
  journal = {CoRR},
  eprint = {2106.10199},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.10199},
  url = {http://arxiv.org/abs/2106.10199},
  urldate = {2023-08-29},
  abstract = {We introduce BitFit, a sparse-finetuning method where only the bias-terms of the model (or a subset of them) are being modified. We show that with small-to-medium training data, applying BitFit on pre-trained BERT models is competitive with (and sometimes better than) fine-tuning the entire model. For larger data, the method is competitive with other sparse fine-tuning methods. Besides their practical utility, these findings are relevant for the question of understanding the commonly-used process of finetuning: they support the hypothesis that finetuning is mainly about exposing knowledge induced by language-modeling training, rather than learning new task-specific linguistic knowledge.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/ANKBH3AM/Zaken et al. - 2022 - BitFit Simple Parameter-efficient Fine-tuning for.pdf}
}

@article{zaman_multilingual_2023,
  title = {A Multilingual Perspective Towards the Evaluation of Attribution Methods in Natural Language Inference},
  author = {Zaman, Kerem and Belinkov, Yonatan},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2204.05428},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2204.05428},
  url = {http://arxiv.org/abs/2204.05428},
  urldate = {2023-08-29},
  abstract = {Most evaluations of attribution methods focus on the English language. In this work, we present a multilingual approach for evaluating attribution methods for the Natural Language Inference (NLI) task in terms of faithfulness and plausibility. First, we introduce a novel cross-lingual strategy to measure faithfulness based on word alignments, which eliminates the drawbacks of erasure-based evaluations.We then perform a comprehensive evaluation of attribution methods, considering different output mechanisms and aggregation methods. Finally, we augment the XNLI dataset with highlight-based explanations, providing a multilingual NLI dataset with highlights, to support future exNLP studies. Our results show that attribution methods performing best for plausibility and faithfulness are different.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/VW8XV3GQ/Zaman and Belinkov - 2023 - A Multilingual Perspective Towards the Evaluation .pdf}
}

@article{zeiler_visualizing_2014,
  title = {Visualizing and Understanding Convolutional Networks},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2014},
  journal = {ECCV},
  eprint = {1311.2901},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1311.2901},
  url = {http://arxiv.org/abs/1311.2901},
  urldate = {2023-11-29},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/AAK8WRDC/Zeiler and Fergus - 2014 - Visualizing and Understanding Convolutional Networ.pdf}
}

@article{zhang_best_2023,
  title = {Towards Best Practices of Activation Patching in Language Models: Metrics and Methods},
  shorttitle = {Towards Best Practices of Activation Patching in Language Models},
  author = {Zhang, Fred and Nanda, Neel},
  year = {2023},
  journal = {ICLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2309.16042},
  url = {https://arxiv.org/abs/2309.16042},
  urldate = {2023-10-25},
  abstract = {Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/CRIXG3DS/Zhang and Nanda - 2023 - Towards Best Practices of Activation Patching in L.pdf}
}

@article{zhang_can_2023,
  title = {Can Transformers Learn to Solve Problems Recursively?},
  author = {Zhang, Shizhuo Dylan and Tigges, Curt and Biderman, Stella and Raginsky, Maxim and Ringer, Talia},
  year = {2023},
  month = jun,
  journal = {CoRR},
  eprint = {2305.14699},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.14699},
  url = {http://arxiv.org/abs/2305.14699},
  urldate = {2023-10-30},
  abstract = {Neural networks have in recent years shown promise for helping software engineers write programs and even formally verify them. While semantic information plays a crucial part in these processes, it remains unclear to what degree popular neural architectures like transformers are capable of modeling that information. This paper examines the behavior of neural networks learning algorithms relevant to programs and formal verification proofs through the lens of mechanistic interpretability, focusing in particular on structural recursion. Structural recursion is at the heart of tasks on which symbolic tools currently outperform neural models, like inferring semantic relations between datatypes and emulating program behavior. We evaluate the ability of transformer models to learn to emulate the behavior of structurally recursive functions from input-output examples. Our evaluation includes empirical and conceptual analyses of the limitations and capabilities of transformer models in approximating these functions, as well as reconstructions of the ``shortcut" algorithms the model learns. By reconstructing these algorithms, we are able to correctly predict 91 percent of failure cases for one of the approximated functions. Our work provides a new foundation for understanding the behavior of neural networks that fail to solve the very tasks they are trained for.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MSZHIUG7/Zhang et al. - 2023 - Can Transformers Learn to Solve Problems Recursive.pdf}
}

@article{zhang_emergent_2022,
  title = {Emergent Modularity in Pre-trained Transformers},
  author = {Zhang, Zhengyan and Zeng, Zhiyuan and Lin, Yankai and Xiao, Chaojun and Han, Xu and Liu, Zhiyuan and Sun, Maosong and Zhou, Jie},
  year = {2022},
  month = sep,
  journal = {OpenReview},
  url = {https://openreview.net/forum?id=XHuQacT6sa6},
  urldate = {2023-10-25},
  abstract = {Pre-trained Transformers have shown the potential to realize the dream of general intelligence, encouraging researchers to explore the analogy between Transformers and human brains. These advances raise the question of whether Transformers have a modular structure similar to brain regions, where neurons are closely related and specialized in a certain function. In this work, we analyze the modularity of Transformers by studying the expert networks, which are clusters of neurons, in Mixture-of-Experts (MoE) Transformers. To evaluate the functional specialization of experts, we propose a novel framework to identify the functionality of both neurons and experts. We conduct empirical analyses on two representative pre-trained Transformers and find that (1) Transformer neurons are functionally specialized, which provides the necessary condition of modularity. (2) Transformer experts are modularized. There are functional experts, where clustered are the neurons specialized in a certain function. (3) The modular structure is stabilized at the early stage of pre-training, which is faster than the neuron stabilization. It reveals the coarse-to-fine mechanism of pre-training, which first constructs the coarse modular structure and then improves the fine-grained neuron functions. In summary, we explore the emergent modularity in pre-trained Transformers and hope to help the community better understand the working mechanism of Transformers. Our code and data will be released to facilitate future research.},
  language = {en},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/96QXA6P6/Zhang et al. - 2022 - Emergent Modularity in Pre-trained Transformers.pdf}
}

@article{zhang_instilling_2023,
  title = {Instilling Inductive Biases with Subnetworks},
  author = {Zhang, Enyan and Lepori, Michael A. and Pavlick, Ellie},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.10899},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2310.10899},
  urldate = {2024-01-03},
  abstract = {Despite the recent success of artificial neural networks on a variety of tasks, we have little knowledge or control over the exact solutions these models implement. Instilling inductive biases -- preferences for some solutions over others -- into these models is one promising path toward understanding and controlling their behavior. Much work has been done to study the inherent inductive biases of models and instill different inductive biases through hand-designed architectures or carefully curated training regimens. In this work, we explore a more mechanistic approach: Subtask Induction. Our method discovers a functional subnetwork that implements a particular subtask within a trained model and uses it to instill inductive biases towards solutions utilizing that subtask. Subtask Induction is flexible and efficient, and we demonstrate its effectiveness with two experiments. First, we show that Subtask Induction significantly reduces the amount of training data required for a model to adopt a specific, generalizable solution to a modular arithmetic task. Second, we demonstrate that Subtask Induction successfully induces a human-like shape bias while increasing data efficiency for convolutional and transformer-based image classification models.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2LS88QG2/Zhang et al. - 2024 - Instilling Inductive Biases with Subnetworks.pdf;/Users/leonardbereska/Zotero/storage/7MZYLDNY/Zhang et al. - 2023 - Instilling Inductive Biases with Subnetworks.pdf}
}

@article{zhang_interpreting_2019,
  title = {Interpreting CNNs via Decision Trees},
  author = {Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian},
  year = {2019},
  journal = {CVPR},
  pages = {6261--6270},
  url = {https://arxiv.org/abs/1802.00121},
  urldate = {2024-02-19},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3XJGKKNF/Zhang et al. - 2019 - Interpreting CNNs via Decision Trees.pdf}
}

@article{zhang_moefication_2022,
  title = {MoEfication: Transformer Feed-forward Layers are Mixtures of Experts},
  shorttitle = {MoEfication},
  author = {Zhang, Zhengyan and Lin, Yankai and Liu, Zhiyuan and Li, Peng and Sun, Maosong and Zhou, Jie},
  year = {2022},
  month = apr,
  journal = {ACL},
  eprint = {2110.01786},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.01786},
  url = {http://arxiv.org/abs/2110.01786},
  urldate = {2023-11-30},
  abstract = {Recent work has shown that feed-forward networks (FFNs) in pre-trained Transformers are a key component, storing various linguistic and factual knowledge. However, the computational patterns of FFNs are still unclear. In this work, we study the computational patterns of FFNs and observe that most inputs only activate a tiny ratio of neurons of FFNs. This phenomenon is similar to the sparsity of the human brain, which drives research on functional partitions of the human brain. To verify whether functional partitions also emerge in FFNs, we propose to convert a model into its MoE version with the same parameters, namely MoEfication. Specifically, MoEfication consists of two phases: (1) splitting the parameters of FFNs into multiple functional partitions as experts, and (2) building expert routers to decide which experts will be used for each input. Experimental results show that MoEfication can conditionally use 10\% to 30\% of FFN parameters while maintaining over 95\% original performance for different models on various downstream tasks. Besides, MoEfication brings two advantages: (1) it significantly reduces the FLOPS of inference, i.e., 2x speedup with 25\% of FFN parameters, and (2) it provides a fine-grained perspective to study the inner mechanism of FFNs. The source code of this paper can be obtained from https://github.com/thunlp/MoEfication.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/48ZGA26G/Zhang et al. - 2022 - MoEfication Transformer Feed-forward Layers are M.pdf}
}

@article{zhang_sparse_2021,
  title = {Sparse Attention with Linear Units},
  author = {Zhang, Biao and Titov, Ivan and Sennrich, Rico},
  year = {2021},
  month = oct,
  journal = {EMNLP},
  eprint = {2104.07012},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2104.07012},
  url = {http://arxiv.org/abs/2104.07012},
  urldate = {2024-01-29},
  abstract = {Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off') for some queries, which is not possible with sparsified softmax alternatives.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/5NNHMMLQ/Zhang et al. - 2021 - Sparse Attention with Linear Units.pdf}
}

@article{zhang_survey_2021,
  title = {A Survey on Neural Network Interpretability},
  author = {Zhang, Yu and Tino, Peter and Leonardis, Ales and Tang, Ke},
  year = {2021},
  month = oct,
  journal = {IEEE Trans. Emerg. Top. Comput. Intell.},
  volume = {5},
  number = {5},
  pages = {726--742},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2021.3100641},
  url = {https://ieeexplore.ieee.org/document/9521221/},
  urldate = {2023-08-27},
  abstract = {Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/336QN435/Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf}
}

@article{zhang_trained_2023,
  title = {Trained Transformers Learn Linear Models In-Context},
  author = {Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L.},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2306.09927},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2306.09927},
  url = {http://arxiv.org/abs/2306.09927},
  urldate = {2024-03-19},
  abstract = {Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares. Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this global minimum, when given a test prompt of labeled examples from a new prediction task, the transformer achieves prediction error competitive with the best linear predictor over the test prompt distribution. We additionally characterize the robustness of the trained transformer to a variety of distribution shifts and show that although a number of shifts are tolerated, shifts in the covariate distribution of the prompts are not. Motivated by this, we consider a generalized ICL setting where the covariate distributions can vary across prompts. We show that although gradient flow succeeds at finding a global minimum in this setting, the trained transformer is still brittle under mild covariate shifts. We complement this finding with experiments on large, nonlinear transformer architectures which we show are more robust under covariate shifts.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/3IBUBNZ8/Zhang et al. - 2023 - Trained Transformers Learn Linear Models In-Contex.pdf}
}

@article{zhang_understanding_2017,
  title = {Understanding deep learning requires rethinking generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  month = feb,
  journal = {ICLR},
  eprint = {1611.03530},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1611.03530},
  url = {http://arxiv.org/abs/1611.03530},
  urldate = {2023-02-27},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/S9RTVA9B/Zhang et al. - 2017 - Understanding deep learning requires rethinking ge.pdf}
}

@article{zhang_unsupervised_2023,
  title = {Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models},
  author = {Zhang, Zijian and Lin, Luping Liu Zhijie and Zhu, Yichen and Zhao, Zhou},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.09912},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.09912},
  url = {http://arxiv.org/abs/2310.09912},
  urldate = {2023-10-27},
  abstract = {We propose the first unsupervised and learning-based method to identify interpretable directions in the h-space of pre-trained diffusion models. Our method is derived from an existing technique that operates on the GAN latent space. In a nutshell, we employ a shift control module for pre-trained diffusion models to manipulate a sample into a shifted version of itself, followed by a reconstructor to reproduce both the type and the strength of the manipulation. By jointly optimizing them, the model will spontaneously discover disentangled and interpretable directions. To prevent the discovery of meaningless and destructive directions, we employ a discriminator to maintain the fidelity of shifted sample. Due to the iterative generative process of diffusion models, our training requires a substantial amount of GPU VRAM to store numerous intermediate tensors for back-propagating gradient. To address this issue, we first propose a general VRAM-efficient training algorithm based on gradient checkpointing technique to back-propagate any gradient through the whole generative process, with acceptable occupancy of VRAM and sacrifice of training efficiency. Compared with existing related works on diffusion models, our method inherently identifies global and scalable directions, without necessitating any other complicated procedures. Extensive experiments on various datasets demonstrate the effectiveness of our method.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/R3YJDM9N/Zhang et al. - 2023 - Unsupervised Discovery of Interpretable Directions.pdf}
}

@article{zhang_visual_2018,
  title = {Visual Interpretability for Deep Learning: a Survey},
  shorttitle = {Visual Interpretability for Deep Learning},
  author = {Zhang, Quanshi and Zhu, Song-Chun},
  year = {2018},
  month = feb,
  journal = {Frontiers of Information Technology \& Electronic Engineering},
  eprint = {1802.00614},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1802.00614},
  url = {http://arxiv.org/abs/1802.00614},
  urldate = {2023-10-20},
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/KJNZ6E5N/Zhang and Zhu - 2018 - Visual Interpretability for Deep Learning a Surve.pdf}
}

@article{zhao_gender_2018,
  title = {Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods},
  shorttitle = {Gender Bias in Coreference Resolution},
  author = {Zhao, Jieyu and Wang, Tianlu and Yatskar, Mark and Ordonez, Vicente and Chang, Kai-Wei},
  editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
  year = {2018},
  month = jun,
  journal = {NAACL HLT},
  pages = {15--20},
  doi = {10.18653/v1/N18-2003},
  url = {https://aclanthology.org/N18-2003},
  urldate = {2024-01-24},
  abstract = {In this paper, we introduce a new benchmark for co-reference resolution focused on gender bias, WinoBias. Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter). We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score. Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing datasets.},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/QPVXLP7V/Zhao et al. - 2018 - Gender Bias in Coreference Resolution Evaluation .pdf}
}

@article{zhao_uncovering_2024,
  title = {Towards Uncovering How Large Language Model Works: An Explainability Perspective},
  shorttitle = {Towards Uncovering How Large Language Model Works},
  author = {Zhao, Haiyan and Yang, Fan and Shen, Bo and Lakkaraju, Himabindu and Du, Mengnan},
  year = {2024},
  month = feb,
  url = {https://www.semanticscholar.org/paper/Towards-Uncovering-How-Large-Language-Model-Works%3A-Zhao-Yang/4f60009234e76f9f8969f6cca23b3b07e944e984},
  urldate = {2024-06-10},
  abstract = {Large language models (LLMs) have led to breakthroughs in language tasks, yet the internal mechanisms that enable their remarkable generalization and reasoning abilities remain opaque. This lack of transparency presents challenges such as hallucinations, toxicity, and misalignment with human values, hindering the safe and beneficial deployment of LLMs. This paper aims to uncover the mechanisms underlying LLM functionality through the lens of explainability. First, we review how knowledge is architecturally composed within LLMs and encoded in their internal parameters via mechanistic interpretability techniques. Then, we summarize how knowledge is embedded in LLM representations by leveraging probing techniques and representation engineering. Additionally, we investigate the training dynamics through a mechanistic perspective to explain phenomena such as grokking and memorization. Lastly, we explore how the insights gained from these explanations can enhance LLM performance through model editing, improve efficiency through pruning, and better align with human values.},
  file = {/Users/leonardbereska/Zotero/storage/IMCDHTRZ/Zhao et al. - 2024 - Towards Uncovering How Large Language Model Works.pdf}
}

@article{zheng_large_2024,
  title = {Large Language Models Are Not Robust Multiple Choice Selectors},
  author = {Zheng, Chujie and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  year = {2024},
  month = feb,
  journal = {ICLR Spotlight},
  eprint = {2309.03882},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.03882},
  url = {http://arxiv.org/abs/2309.03882},
  urldate = {2024-03-14},
  abstract = {Multiple choice questions (MCQs) serve as a common yet important task format in the evaluation of large language models (LLMs). This work shows that modern LLMs are vulnerable to option position changes in MCQs due to their inherent "selection bias", namely, they prefer to select specific option IDs as answers (like "Option A"). Through extensive empirical analyses with 20 LLMs on three benchmarks, we pinpoint that this behavioral bias primarily stems from LLMs' token bias, where the model a priori assigns more probabilistic mass to specific option ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mitigate selection bias, we propose a label-free, inference-time debiasing method, called PriDe, which separates the model's prior bias for option IDs from the overall prediction distribution. PriDe first estimates the prior by permutating option contents on a small number of test samples, and then applies the estimated prior to debias the remaining samples. We demonstrate that it achieves interpretable and transferable debiasing with high computational efficiency. We hope this work can draw broader research attention to the bias and robustness of modern LLMs.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/C2FA2DMG/Zheng et al. - 2024 - Large Language Models Are Not Robust Multiple Choi.pdf}
}

@article{zhong_clock_2023,
  title = {The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks},
  shorttitle = {The Clock and the Pizza},
  author = {Zhong, Ziqian and Liu, Ziming and Tegmark, Max and Andreas, Jacob},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.17844},
  url = {https://arxiv.org/abs/2306.17844},
  urldate = {2023-07-31},
  abstract = {Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {algorithms,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/EXY8J3DP/Zhong et al. - 2023 - The Clock and the Pizza Two Stories in Mechanisti.pdf}
}

@article{zhong_mquake_2023,
  title = {MQuAKE: Assessing Knowledge Editing in Language Models via Multi-Hop Questions},
  shorttitle = {MQuAKE},
  author = {Zhong, Zexuan and Wu, Zhengxuan and Manning, Christopher D. and Potts, Christopher and Chen, Danqi},
  year = {2023},
  month = oct,
  journal = {EMNLP},
  eprint = {2305.14795},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.14795},
  url = {http://arxiv.org/abs/2305.14795},
  urldate = {2023-11-10},
  abstract = {The information stored in large language models (LLMs) falls out of date quickly, and retraining from scratch is often not an option. This has recently given rise to a range of techniques for injecting new facts through updating model weights. Current evaluation paradigms are extremely limited, mainly validating the recall of edited facts, but changing one fact should cause rippling changes to the model's related beliefs. If we edit the UK Prime Minister to now be Rishi Sunak, then we should get a different answer to Who is married to the British Prime Minister? In this work, we present a benchmark, MQuAKE (Multi-hop Question Answering for Knowledge Editing), comprising multi-hop questions that assess whether edited models correctly answer questions where the answer should change as an entailed consequence of edited facts. While we find that current knowledge-editing approaches can recall edited facts accurately, they fail catastrophically on the constructed multi-hop questions. We thus propose a simple memory-based approach, MeLLo, which stores all edited facts externally while prompting the language model iteratively to generate answers that are consistent with the edited facts. While MQuAKE remains challenging, we show that MeLLo scales well with LLMs (up to 175B) and outperforms previous model editors by a large margin.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/579VYJDR/Zhong et al. - 2023 - MQuAKE Assessing Knowledge Editing in Language Mo.pdf}
}

@article{zhou_leasttomost_2022,
  title = {Least-to-Most Prompting Enables Complex Reasoning in Large Language Models},
  author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2205.10625},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.10625},
  url = {http://arxiv.org/abs/2205.10625},
  urldate = {2023-03-20},
  abstract = {Although chain-of-thought prompting has shown impressive results on many natural language reasoning tasks, it often performs poorly on tasks which need to solve problems harder than the demonstration examples. To tackle such easy-to-hard generalization issues, we propose a novel prompting strategy, least-to-most prompting. It reduces a complex problem into a list of subproblems, and then sequentially solve these subproblems, whereby solving a given subproblem is facilitated by the answers to previously solved subproblems. Experiments on symbolic manipulation, compositional generalization and math reasoning show that least-to-most prompting can generalize to the examples that are harder than those seen in the prompt, and outperform chain-of-thought prompting by a large margin. A notable result is that the GPT-3 code-davinci-002 model with least-to-most-prompting solves the SCAN benchmark regardless of splits (such as length split) with an accuracy of 99.7\% using 14 examples versus an accuracy of 16.2\% by chain-of-thought prompting, and neural-symbolic models in the literature specialized for solving SCAN are trained with the full training set of more than 15,000 examples.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/3UUC2RY7/Zhou et al. - 2022 - Least-to-Most Prompting Enables Complex Reasoning .pdf}
}

@article{zhou_mystery_2023,
  title = {The Mystery and Fascination of LLMs: A Comprehensive Survey on the Interpretation and Analysis of Emergent Abilities},
  shorttitle = {The Mystery and Fascination of LLMs},
  author = {Zhou, Yuxiang and Li, Jiazheng and Xiang, Yanzheng and Yan, Hanqi and Gui, Lin and He, Yulan},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2311.00237},
  url = {https://arxiv.org/abs/2311.00237},
  urldate = {2024-02-11},
  abstract = {Understanding emergent abilities, such as in-context learning (ICL) and chain-of-thought (CoT) prompting in large language models (LLMs), is of utmost importance. This importance stems not only from the better utilization of these capabilities across various tasks, but also from the proactive identification and mitigation of potential risks, including concerns of truthfulness, bias, and toxicity, that may arise alongside these capabilities. In this paper, we present a thorough survey on the interpretation and analysis of emergent abilities of LLMs. First, we provide a concise introduction to the background and definition of emergent abilities. Then, we give an overview of advancements from two perspectives: 1) a macro perspective, emphasizing studies on the mechanistic interpretability and delving into the mathematical foundations behind emergent abilities; and 2) a micro-perspective, concerning studies that focus on empirical interpretability by examining factors associated with these abilities. We conclude by highlighting the challenges encountered and suggesting potential avenues for future research. We believe that our work establishes the basis for further exploration into the interpretation of emergent abilities.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {not cited,to cite,to extract related work,to read},
  file = {/Users/leonardbereska/Zotero/storage/K2AN4AL6/Zhou et al. - 2023 - The Mystery and Fascination of LLMs A Comprehensi.pdf}
}

@article{zhou_object_2015,
  title = {Object Detectors Emerge in Deep Scene CNNs},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2015},
  month = apr,
  journal = {ICLR},
  eprint = {1412.6856},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1412.6856},
  url = {http://arxiv.org/abs/1412.6856},
  urldate = {2024-03-20},
  abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/KBT8XX8J/Zhou et al. - 2015 - Object Detectors Emerge in Deep Scene CNNs.pdf}
}

@article{zhou_what_2023,
  title = {What Algorithms can Transformers Learn? A Study in Length Generalization},
  shorttitle = {What Algorithms can Transformers Learn?},
  author = {Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.16028},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2310.16028},
  url = {http://arxiv.org/abs/2310.16028},
  urldate = {2024-02-13},
  abstract = {Large language models exhibit surprising emergent generalization properties, yet also struggle on many simple reasoning tasks such as arithmetic and parity. This raises the question of if and when Transformer models can learn the true algorithm for solving a task. We study the scope of Transformers' abilities in the specific setting of length generalization on algorithmic tasks. Here, we propose a unifying framework to understand when and how Transformers can exhibit strong length generalization on a given task. Specifically, we leverage RASP (Weiss et al., 2021) -- a programming language designed for the computational model of a Transformer -- and introduce the RASP-Generalization Conjecture: Transformers tend to length generalize on a task if the task can be solved by a short RASP program which works for all input lengths. This simple conjecture remarkably captures most known instances of length generalization on algorithmic tasks. Moreover, we leverage our insights to drastically improve generalization performance on traditionally hard tasks (such as parity and addition). On the theoretical side, we give a simple example where the "min-degree-interpolator" model of learning from Abbe et al. (2023) does not correctly predict Transformers' out-of-distribution behavior, but our conjecture does. Overall, our work provides a novel perspective on the mechanisms of compositional generalization and the algorithmic capabilities of Transformers.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/4EXZ4EET/Zhou et al. - 2023 - What Algorithms can Transformers Learn A Study in.pdf}
}

@article{ziegler_adversarial_2022,
  title = {Adversarial Training for High-Stakes Reliability},
  author = {Ziegler, Daniel M. and Nix, Seraphina and Chan, Lawrence and Bauman, Tim and {Schmidt-Nielsen}, Peter and Lin, Tao and Scherlis, Adam and Nabeshima, Noa and {Weinstein-Raun}, Ben and {de Haas}, Daniel and Shlegeris, Buck and Thomas, Nate},
  year = {2022},
  month = nov,
  journal = {NeurIPS},
  eprint = {2205.01663},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.01663},
  url = {http://arxiv.org/abs/2205.01663},
  urldate = {2023-10-26},
  abstract = {In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance. In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.},
  archiveprefix = {arxiv},
  keywords = {to cite},
  file = {/Users/leonardbereska/Zotero/storage/FRCY9S2B/Ziegler et al. - 2022 - Adversarial Training for High-Stakes Reliability.pdf}
}

@article{ziegler_finetuning_2020,
  title = {Fine-Tuning Language Models from Human Preferences},
  author = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B. and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
  year = {2020},
  month = jan,
  journal = {CoRR},
  eprint = {1909.08593},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1909.08593},
  url = {http://arxiv.org/abs/1909.08593},
  urldate = {2023-02-22},
  abstract = {Reward learning enables the application of reinforcement learning (RL) to tasks where reward is defined by human judgment, building a model of reward by asking humans questions. Most work on reward learning has used simulated environments, but complex information about values is often expressed in natural language, and we believe reward learning for language is a key to making RL practical and safe for real-world tasks. In this paper, we build on advances in generative pretraining of language models to apply reward learning to four natural language tasks: continuing text with positive sentiment or physically descriptive language, and summarization tasks on the TL;DR and CNN/Daily Mail datasets. For stylistic continuation we achieve good results with only 5,000 comparisons evaluated by humans. For summarization, models trained with 60,000 comparisons copy whole sentences from the input but skip irrelevant preamble; this leads to reasonable ROUGE scores and very good performance according to our human labelers, but may be exploiting the fact that labelers rely on simple heuristics.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7EW6BL53/Ziegler et al. - 2020 - Fine-Tuning Language Models from Human Preferences.pdf}
}

@article{zimmermann_how_2021,
  title = {How Well do Feature Visualizations Support Causal Understanding of CNN Activations?},
  author = {Zimmermann, Roland S. and Borowski, Judy and Geirhos, Robert and Bethge, Matthias and Wallis, Thomas S. A. and Brendel, Wieland},
  year = {2021},
  month = nov,
  journal = {NeurIPS},
  eprint = {2106.12447},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.12447},
  url = {http://arxiv.org/abs/2106.12447},
  urldate = {2024-06-12},
  abstract = {A precise understanding of why units in an artificial network respond to certain stimuli would constitute a big step towards explainable artificial intelligence. One widely used approach towards this goal is to visualize unit responses via activation maximization. These synthetic feature visualizations are purported to provide humans with precise information about the image features that cause a unit to be activated - an advantage over other alternatives like strongly activating natural dataset samples. If humans indeed gain causal insight from visualizations, this should enable them to predict the effect of an intervention, such as how occluding a certain patch of the image (say, a dog's head) changes a unit's activation. Here, we test this hypothesis by asking humans to decide which of two square occlusions causes a larger change to a unit's activation. Both a large-scale crowdsourced experiment and measurements with experts show that on average the extremely activating feature visualizations by Olah et al. (2017) indeed help humans on this task (\$68 {\textbackslash}pm 4\$\% accuracy; baseline performance without any visualizations is \$60 {\textbackslash}pm 3\$\%). However, they do not provide any substantial advantage over other visualizations (such as e.g. dataset samples), which yield similar performance (\$66{\textbackslash}pm3\$\% to \$67 {\textbackslash}pm3\$\% accuracy). Taken together, we propose an objective psychophysical task to quantify the benefit of unit-level interpretability methods for humans, and find no evidence that a widely-used feature visualization method provides humans with better "causal understanding" of unit activations than simple alternative visualizations.},
  archiveprefix = {arxiv},
  file = {/Users/leonardbereska/Zotero/storage/3DA272YL/Zimmermann et al. - 2021 - How Well do Feature Visualizations Support Causal .pdf}
}

@article{zimmermann_measuring_2024,
  title = {Measuring Mechanistic Interpretability at Scale Without Humans},
  author = {Zimmermann, Roland S. and Klindt, David A. and Brendel, Wieland},
  year = {2024},
  month = mar,
  url = {https://openreview.net/forum?id=M8yBcvRvwn&referrer=%5Bthe%20profile%20of%20Wieland%20Brendel%5D(%2Fprofile%3Fid%3D~Wieland_Brendel1)},
  urldate = {2024-06-10},
  abstract = {In today's era, whatever we can measure at scale, we can optimize. So far, measuring the interpretability of units in deep neural networks (DNNs) for computer vision still requires direct human evaluation and is not scalable. As a result, the inner workings of DNNs remain a mystery despite the remarkable progress we have seen in their applications. In this work, we introduce the first scalable method to measure the per-unit interpretability in vision DNNs. This method does not require any human evaluations, yet its prediction correlates well with existing human interpretability measurements. We validate its predictive power through an interventional human psychophysics study. We demonstrate the usefulness of this measure by performing previously infeasible experiments: (1) A large-scale interpretability analysis across more than 70 million units from 835 computer vision models, and (2) an extensive analysis of how units transform during training. We find an anticorrelation between a model's downstream classification performance and per-unit interpretability, which is also observable during model training. Furthermore, we see that a layer's location and width influence its interpretability.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/E223M3ZI/Zimmermann et al. - 2024 - Measuring Mechanistic Interpretability at Scale Wi.pdf}
}

@article{zimmermann_scale_2023,
  title = {Scale Alone Does not Improve Mechanistic Interpretability in Vision Models},
  author = {Zimmermann, Roland S. and Klein, Thomas and Brendel, Wieland},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.05471},
  url = {https://arxiv.org/abs/2307.05471},
  urldate = {2023-07-31},
  abstract = {In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We here use a psychophysical paradigm to quantify mechanistic interpretability for a diverse suite of models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the nine investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 120'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset is meant to facilitate research on automated instead of human-based interpretability evaluations that can ultimately be leveraged to directly optimize the mechanistic interpretability of models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {interpretability tax,not cited,scale,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/S8WX8GG7/Zimmermann et al. - 2023 - Scale Alone Does not Improve Mechanistic Interpret.pdf}
}

@article{zou_representation_2023,
  title = {Representation Engineering: A Top-Down Approach to AI Transparency},
  shorttitle = {Representation Engineering},
  author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.01405},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.01405},
  url = {http://arxiv.org/abs/2310.01405},
  urldate = {2023-11-08},
  abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
  archiveprefix = {arxiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/J9ARICI9/Zou et al. - 2023 - Representation Engineering A Top-Down Approach to.pdf}
}

@article{zou_universal_2023,
  title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author = {Zou, Andy and Wang, Zifan and Kolter, J. Zico and Fredrikson, Matt},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.15043},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.15043},
  url = {http://arxiv.org/abs/2307.15043},
  urldate = {2023-10-27},
  abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
  archiveprefix = {arxiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7EWJYTXS/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on .pdf}
}
