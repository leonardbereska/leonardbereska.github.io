@article{adler_complexity_2024,
  title = {On the Complexity of Neural Computation in Superposition},
  author = {Adler, Micah and Shavit, Nir},
  year = 2024,
  month = sep,
  journal = {CoRR},
  eprint = {2409.15318},
  doi = {10.48550/arXiv.2409.15318},
  url = {http://arxiv.org/abs/2409.15318},
  urldate = {2024-11-08},
  abstract = {Recent advances in the understanding of neural networks suggest that superposition, the ability of a single neuron to represent multiple features simultaneously, is a key mechanism underlying the computational efficiency of large-scale networks. This paper explores the theoretical foundations of computing in superposition, focusing on explicit, provably correct algorithms and their efficiency. We present the first lower bounds showing that for a broad class of problems, including permutations and pairwise logical operations, a neural network computing in superposition requires at least \$\textbackslash Omega(m' \textbackslash log m')\$ parameters and \$\textbackslash Omega(\textbackslash sqrt\textbraceleft m' \textbackslash log m'\textbraceright )\$ neurons, where \$m'\$ is the number of output features being computed. This implies that any ``lottery ticket'' sparse sub-network must have at least \$\textbackslash Omega(m' \textbackslash log m')\$ parameters no matter what the initial dense network size. Conversely, we show a nearly tight upper bound: logical operations like pairwise AND can be computed using \$O(\textbackslash sqrt\textbraceleft m'\textbraceright{} \textbackslash log m')\$ neurons and \$O(m' \textbackslash log\textasciicircum 2 m')\$ parameters. There is thus an exponential gap between computing in superposition, the subject of this work, and representing features in superposition, which can require as little as \$O(\textbackslash log m'\$) neurons based on the Johnson-Lindenstrauss Lemma. Our hope is that our results open a path for using complexity theoretic techniques in neural network interpretability research.},
  archiveprefix = {arXiv},
  keywords = {computation in superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/AWPTTGZX/Adler and Shavit - 2024 - On the Complexity of Neural Computation in Superpo.pdf}
}

@article{ahuja_learning_2024,
  title = {Learning Syntax Without Planting Trees: Understanding When and Why Transformers Generalize Hierarchically},
  shorttitle = {Learning Syntax Without Planting Trees},
  author = {Ahuja, Kabir and Balachandran, Vidhisha and Panwar, Madhur and He, Tianxing and Smith, Noah A. and Goyal, Navin and Tsvetkov, Yulia},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=YwLgSimUIT},
  urldate = {2024-08-16},
  abstract = {Transformers trained on natural language data have been shown to exhibit hierarchical generalization without explicitly encoding any structural bias. In this work, we investigate sources of inductive bias in transformer models and their training that could cause such preference for hierarchical generalization. We extensively experiment with transformers trained on five synthetic, controlled datasets using several training objectives and show that while objectives such as sequence-to-sequence modeling, classification, etc., often failed to lead to hierarchical generalization, language modeling objective consistently led to transformers generalizing hierarchically. We then study how different generalization behaviors emerge during the training by conducting pruning experiments that reveal joint existence of subnetworks within the model implementing different generalizations. Finally, we take a Bayesian perspective to understand transformers' preference for hierarchical generalization: We establish a correlation between whether transformers generalize hierarchically on a dataset and if the simplest explanation of that dataset is provided by a hierarchical grammar compared to regular grammars exhibiting linear generalization. Overall, our work presents new insights on the origins of hierarchical generalization in transformers and provides a theoretical framework for studying generalization in language models.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/52TK8DJB/Ahuja et al. - 2024 - Learning Syntax Without Planting Trees Understand.pdf}
}

@article{anand_shannon_2011,
  title = {The Shannon and the Von Neumann entropy of random networks with heterogeneous expected degree},
  author = {Anand, Kartik and Bianconi, Ginestra and Severini, Simone},
  year = 2011,
  month = mar,
  journal = {Phys. Rev. E},
  volume = {83},
  number = {3},
  eprint = {1011.1565},
  primaryclass = {cond-mat},
  pages = {036109},
  issn = {1539-3755, 1550-2376},
  doi = {10.1103/PhysRevE.83.036109},
  url = {http://arxiv.org/abs/1011.1565},
  urldate = {2025-06-05},
  abstract = {Entropic measures of complexity are able to quantify the information encoded in complex network structures. Several entropic measures have been proposed in this respect. Here we study the relation between the Shannon entropy and the Von Neumann entropy of networks with a given expected degree sequence. We find in different examples of network topologies that when the degree distribution contains some heterogeneity, an intriguing correlation emerges between the two entropies. This result seems to suggest that this kind of heterogeneity is implying an equivalence between a quantum and a classical description of networks, which respectively correspond to the Von Neumann and the Shannon entropy.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/9WYC4BB5/Anand et al. - 2011 - The Shannon and the Von Neumann entropy of random .pdf}
}

@article{anders_crafting_2024,
  title = {Crafting Polysemantic Transformer Benchmarks with Known Circuits},
  author = {Anders, Evan and Garriga-Alonso, Adri{\`a}},
  year = 2024,
  month = aug,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/jeoSoJQLuK4JWqtyy/crafting-polysemantic-transformer-benchmarks-with-known},
  urldate = {2024-12-18},
  abstract = {Notes:~ {$\bullet$}  * This research was performed as part of Adri\`a Garriga-Alonso's~MATS 6.0 stream.  * If an opinion is stated in this post saying that "we"\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/GNAGDBE5/Anders and Garriga-alonso - 2024 - Crafting Polysemantic Transformer Benchmarks with .html}
}

@article{anders_sparse_2024,
  title = {Sparse autoencoders find composed features in small toy models},
  author = {Anders, Evan and Neo, Clement and Hoelscher-Obermaier, Jason and Howard, Jessica N.},
  year = 2024,
  month = mar,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy},
  urldate = {2024-12-18},
  abstract = {Summary  * Context:~Sparse Autoencoders (SAEs) reveal interpretable features in the activation spaces of language models. They achieve sparse, interp\dots},
  language = {en}
}

@article{ansuini_intrinsic_2019,
  title = {Intrinsic dimension of data representations in deep neural networks},
  author = {Ansuini, Alessio and Laio, Alessandro and Macke, Jakob and Zoccolan, Davide},
  year = 2019,
  month = may,
  journal = {NeurIPS},
  url = {https://www.semanticscholar.org/paper/Intrinsic-dimension-of-data-representations-in-deep-Ansuini-Laio/5f0bc5273dec29fa6d3bbd743e0190f8de58d582},
  urldate = {2025-05-07},
  abstract = {Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data-representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.},
  file = {/Users/leonardbereska/Zotero/storage/F6RXZNBW/Ansuini et al. - 2019 - Intrinsic dimension of data representations in deep neural networks.pdf}
}

@article{aoyagi_consideration_2024,
  title = {Consideration on the learning efficiency of multiple-layered neural networks with linear units},
  author = {Aoyagi, Miki},
  year = 2024,
  month = apr,
  journal = {Neural Networks},
  volume = {172},
  pages = {106132},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2024.106132},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608024000480},
  urldate = {2024-08-21},
  abstract = {In the last two decades, remarkable progress has been done in singular learning machine theories on the basis of algebraic geometry. These theories reveal that we need to find resolution maps of singularities for analyzing asymptotic behavior of state probability functions when the number of data increases. In particular, it is essential to construct normal crossing divisors of average log loss functions. However, there are few examples for obtaining these for singular models. In this paper, we determine the resolution map and normal crossing divisors for multiple-layered neural networks with linear units. Moreover, we have the exact values for the learning efficiency, which is so called learning coefficients. Multiple-layered neural networks with linear units are simple, however, very important models because these models give the essential information from data of input--output pairs. Moreover, these models are very close to multiple-layered neural networks with rectified linear units (ReLU). We show the learning coefficients of multiple-layered neural networks with linear units are bounded even though the number of layers goes to infinity, which means that the main term of asymptotic expansion of the free energy and generalization error of singular models are much smaller than the dimension of its parameter space.}
}

@article{augustin_adversarial_2020,
  title = {Adversarial Robustness on In- and Out-Distribution Improves Explainability},
  author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
  year = 2020,
  month = jul,
  journal = {ECCV},
  eprint = {2003.09461},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2003.09461},
  url = {http://arxiv.org/abs/2003.09461},
  urldate = {2024-08-03},
  abstract = {Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art \$l\_2\$-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/VMFCEVKH/Augustin et al. - 2020 - Adversarial Robustness on In- and Out-Distribution.pdf}
}

@article{ayonrinde_adaptive_2024,
  title = {Adaptive Sparse Allocation with Mutual Choice \& Feature Choice Sparse Autoencoders},
  author = {Ayonrinde, Kola},
  year = 2024,
  month = nov,
  journal = {CoRR},
  eprint = {2411.02124},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2411.02124},
  url = {http://arxiv.org/abs/2411.02124},
  urldate = {2024-12-20},
  abstract = {Sparse autoencoders (SAEs) are a promising approach to extracting features from neural networks, enabling model interpretability as well as causal interventions on model internals. SAEs generate sparse feature representations using a sparsifying activation function that implicitly defines a set of token-feature matches. We frame the token-feature matching as a resource allocation problem constrained by a total sparsity upper bound. For example, TopK SAEs solve this allocation problem with the additional constraint that each token matches with at most \$k\$ features. In TopK SAEs, the \$k\$ active features per token constraint is the same across tokens, despite some tokens being more difficult to reconstruct than others. To address this limitation, we propose two novel SAE variants, Feature Choice SAEs and Mutual Choice SAEs, which each allow for a variable number of active features per token. Feature Choice SAEs solve the sparsity allocation problem under the additional constraint that each feature matches with at most \$m\$ tokens. Mutual Choice SAEs solve the unrestricted allocation problem where the total sparsity budget can be allocated freely between tokens and features. Additionally, we introduce a new auxiliary loss function, \$\textbackslash mathtt\textbraceleft aux\textbackslash\_zipf\textbackslash\_loss\textbraceright\$, which generalises the \$\textbackslash mathtt\textbraceleft aux\textbackslash\_k\textbackslash\_loss\textbraceright\$ to mitigate dead and underutilised features. Our methods result in SAEs with fewer dead features and improved reconstruction loss at equivalent sparsity levels as a result of the inherent adaptive computation. More accurate and scalable feature extraction methods provide a path towards better understanding and more precise control of foundation models.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/TCTVX89T/Ayonrinde - 2024 - Adaptive Sparse Allocation with Mutual Choice & Fe.pdf}
}

@article{ayonrinde_interpretability_2024,
  title = {Interpretability as Compression: Reconsidering SAE Explanations of Neural Activations with MDL-SAEs},
  shorttitle = {Interpretability as Compression},
  author = {Ayonrinde, Kola and Pearce, Michael T. and Sharkey, Lee},
  year = 2024,
  month = oct,
  journal = {CoRR},
  eprint = {2410.11179},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.11179},
  url = {http://arxiv.org/abs/2410.11179},
  urldate = {2024-12-20},
  abstract = {Sparse Autoencoders (SAEs) have emerged as a useful tool for interpreting the internal representations of neural networks. However, naively optimising SAEs for reconstruction loss and sparsity results in a preference for SAEs that are extremely wide and sparse. We present an information-theoretic framework for interpreting SAEs as lossy compression algorithms for communicating explanations of neural activations. We appeal to the Minimal Description Length (MDL) principle to motivate explanations of activations which are both accurate and concise. We further argue that interpretable SAEs require an additional property, "independent additivity": features should be able to be understood separately. We demonstrate an example of applying our MDL-inspired framework by training SAEs on MNIST handwritten digits and find that SAE features representing significant line segments are optimal, as opposed to SAEs with features for memorised digits from the dataset or small digit fragments. We argue that using MDL rather than sparsity may avoid potential pitfalls with naively maximising sparsity such as undesirable feature splitting and that this framework naturally suggests new hierarchical SAE architectures which provide more concise explanations.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/U22KRUMF/Ayonrinde et al. - 2024 - Interpretability as Compression Reconsidering SAE.pdf}
}

@article{bai_describeanddissect_2024,
  title = {Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models},
  shorttitle = {Describe-and-Dissect},
  author = {Bai, Nicholas and Iyer, Rahul Ajay and Oikarinen, Tuomas and Weng, Tsui-Wei},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=50SMcZ8QQf},
  urldate = {2024-08-16},
  abstract = {In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don't train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2 times as likely to be selected as the best explanation for a neuron than the best baseline.},
  language = {en}
}

@article{balasubramanian_decomposing_2024,
  title = {Decomposing and Interpreting Image Representations via Text in ViTs Beyond CLIP},
  author = {Balasubramanian, Sriram and Basu, Samyadeep and Feizi, Soheil},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=DwhvppIZsD},
  urldate = {2024-08-16},
  abstract = {Recent works have explored how individual components of the CLIP-ViT model contribute to the final representation by leveraging the shared image-text representation space of CLIP. Components like attention heads and MLPs have been found to capture distinct image features such as shape, color, or texture. However, understanding the role of these components in arbitrary vision transformers (ViTs) is challenging. Thus, we introduce a general framework which can identify the roles of various components in ViTs beyond CLIP. Specifically, we (a) automate the decomposition of the final representation into contributions from different model components, and (b) linearly map these contributions to CLIP space to interpret them via text. We also introduce a novel scoring function to rank components by their importance with respect to specific features. Applying our framework to various ViTs (e.g. DeiT, DINO, DINOv2, Swin, MaxViT), we gain insights into the roles of different components concerning particular image features.These insights facilitate applications such as image retrieval, visualizing token importance heatmaps, and mitigating spurious correlations.},
  language = {en}
}

@article{balcells_evolution_2024,
  title = {Evolution of SAE Features Across Layers in LLMs},
  author = {Balcells, Daniel and Lerner, Benjamin and Oesterle, Michael and Ucar, Ediz and Heimersheim, Stefan},
  year = 2024,
  month = nov,
  journal = {NeurIPS Attributing Model Behavior at Scale (ATTRIB) Workshop},
  eprint = {2410.08869},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.08869},
  url = {http://arxiv.org/abs/2410.08869},
  urldate = {2024-12-20},
  abstract = {Sparse Autoencoders for transformer-based language models are typically defined independently per layer. In this work we analyze statistical relationships between features in adjacent layers to understand how features evolve through a forward pass. We provide a graph visualization interface for features and their most similar next-layer neighbors (https://stefanhex.com/spar-2024/feature-browser/), and build communities of related features across layers. We find that a considerable amount of features are passed through from a previous layer, some features can be expressed as quasi-boolean combinations of previous features, and some features become more specialized in later layers.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/WEYX94P8/Balcells et al. - 2024 - Evolution of SAE Features Across Layers in LLMs.pdf}
}

@article{bereska_mechanistic_2024,
  title = {Mechanistic Interpretability for AI Safety --- A Review},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = 2024,
  month = aug,
  journal = {TMLR},
  url = {https://openreview.net/forum?id=ePUVetPKu6},
  urldate = {2024-08-24},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  file = {/Users/leonardbereska/Zotero/storage/9ACIYI7V/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety — A Rev.pdf}
}

@article{black_interpreting_2022,
  title = {Interpreting Neural Networks through the Polytope Lens},
  author = {Black, Sid and Sharkey, Lee and Grinsztajn, Leo and Winsor, Eric and Braun, Dan and Merizian, Jacob and Parker, Kip and Guevara, Carlos Ram{\'o}n and Millidge, Beren and Alfour, Gabriel and Leahy, Connor},
  year = 2022,
  month = nov,
  journal = {CoRR},
  publisher = {arXiv},
  url = {https://arxiv.org/abs/2211.12312},
  urldate = {2023-07-31},
  abstract = {Mechanistic interpretability aims to explain what a neural network has learned at a nuts-and-bolts level. What are the fundamental primitives of neural network representations? Previous mechanistic descriptions have used individual neurons or their linear combinations to understand the representations a network has learned. But there are clues that neurons and their linear combinations are not the correct fundamental units of description: directions cannot describe how neural networks use nonlinearities to structure their representations. Moreover, many instances of individual neurons and their combinations are polysemantic (i.e. they have multiple unrelated meanings). Polysemanticity makes interpreting the network in terms of neurons or directions challenging since we can no longer assign a specific feature to a neural unit. In order to find a basic unit of description that does not suffer from these problems, we zoom in beyond just directions to study the way that piecewise linear activation functions (such as ReLU) partition the activation space into numerous discrete polytopes. We call this perspective the polytope lens. The polytope lens makes concrete predictions about the behavior of neural networks, which we evaluate through experiments on both convolutional image classifiers and language models. Specifically, we show that polytopes can be used to identify monosemantic regions of activation space (while directions are not in general monosemantic) and that the density of polytope boundaries reflect semantic boundaries. We also outline a vision for what mechanistic interpretability might look like through the polytope lens.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {beyond superposition,feature,fundamental},
  file = {/Users/leonardbereska/Zotero/storage/2DDCWY73/Black et al. - 2022 - Interpreting Neural Networks through the Polytope .pdf}
}

@article{boopathy_proper_2020,
  title = {Proper Network Interpretability Helps Adversarial Robustness in Classification},
  author = {Boopathy, Akhilan and Liu, Sijia and Zhang, Gaoyuan and Liu, Cynthia and Chen, Pin-Yu and Chang, Shiyu and Daniel, Luca},
  year = 2020,
  month = oct,
  journal = {ICML},
  doi = {10.48550/arXiv.2006.14748},
  url = {http://arxiv.org/abs/2006.14748},
  urldate = {2024-08-04},
  abstract = {Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against attacks of large perturbation in particular.},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/JKZWCB3A/Boopathy et al. - 2020 - Proper Network Interpretability Helps Adversarial .pdf}
}

@article{braun_identifying_2024,
  title = {Identifying Functionally Important Features with End-to-End Sparse Dictionary Learning},
  author = {Braun, Dan and Taylor, Jordan and Goldowsky-Dill, Nicholas and Sharkey, Lee},
  year = 2024,
  month = may,
  journal = {ICML MI Workshop},
  eprint = {2405.12241},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.12241},
  url = {http://arxiv.org/abs/2405.12241},
  urldate = {2024-06-10},
  abstract = {Identifying the features learned by neural networks is a core challenge in mechanistic interpretability. Sparse autoencoders (SAEs), which learn a sparse, overcomplete dictionary that reconstructs a network's internal activations, have been used to identify these features. However, SAEs may learn more about the structure of the datatset than the computational structure of the network. There is therefore only indirect reason to believe that the directions found in these dictionaries are functionally important to the network. We propose end-to-end (e2e) sparse dictionary learning, a method for training SAEs that ensures the features learned are functionally important by minimizing the KL divergence between the output distributions of the original model and the model with SAE activations inserted. Compared to standard SAEs, e2e SAEs offer a Pareto improvement: They explain more network performance, require fewer total features, and require fewer simultaneously active features per datapoint, all with no cost to interpretability. We explore geometric and qualitative differences between e2e SAE features and standard SAE features. E2e dictionary learning brings us closer to methods that can explain network behavior concisely and accurately. We release our library for training e2e SAEs and reproducing our analysis at https://github.com/ApolloResearch/e2e\_sae},
  archiveprefix = {arXiv},
  keywords = {icml,sparse autoencoder,sparse feature circuits,spotlight},
  file = {/Users/leonardbereska/Zotero/storage/IWRSBULZ/Braun et al. - 2024 - Identifying Functionally Important Features with E.pdf}
}

@article{bricken_monosemanticity_2023,
  title = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nicholas L. and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E. and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Chris},
  year = 2023,
  month = oct,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/monosemantic-features/index.html},
  abstract = {Mechanistic interpretability seeks to understand neural networks by breaking them into components that are more easily understood than the whole. By understanding the function of each component, and how they interact, we hope to be able to reason about the behavior of the entire network. The first step in that program is to identify the correct components to analyze.},
  keywords = {fundamental,sparse autoencoder,superposition,toy models},
  file = {/Users/leonardbereska/Zotero/storage/Q6RCQ5EH/Bricken et al. - 2023 - Towards Monosemanticity Decomposing Language Mode.html}
}

@article{bushnaq_local_2024,
  title = {The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks},
  shorttitle = {The Local Interaction Basis},
  author = {Bushnaq, Lucius and Heimersheim, Stefan and Goldowsky-Dill, Nicholas and Braun, Dan and Mendel, Jake and Hanni, Kaarel and Griffin, Avery and Stohler, Jorn and Wache, Magdalena and Hobbhahn, Marius},
  year = 2024,
  month = may,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2405.10928},
  urldate = {2024-06-09},
  abstract = {Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.},
  keywords = {sparse autoencoder,sparse feature circuits},
  file = {/Users/leonardbereska/Zotero/storage/2Y3B28Z4/Bushnaq et al. - 2024 - The Local Interaction Basis Identifying Computati.pdf}
}

@article{bushnaq_using_2024,
  title = {Using Degeneracy in the Loss Landscape for Mechanistic Interpretability},
  author = {Bushnaq, Lucius and Mendel, Jake and Heimersheim, Stefan and Braun, Dan and Goldowsky-Dill, Nicholas and H{\"a}nni, Kaarel and Wu, Cindy and Hobbhahn, Marius},
  year = 2024,
  month = may,
  journal = {ICML MI Workshop},
  eprint = {2405.10927},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.10927},
  url = {http://arxiv.org/abs/2405.10927},
  urldate = {2024-06-09},
  abstract = {Mechanistic Interpretability aims to reverse engineer the algorithms implemented by neural networks by studying their weights and activations. An obstacle to reverse engineering neural networks is that many of the parameters inside a network are not involved in the computation being implemented by the network. These degenerate parameters may obfuscate internal structure. Singular learning theory teaches us that neural network parameterizations are biased towards being more degenerate, and parameterizations with more degeneracy are likely to generalize further. We identify 3 ways that network parameters can be degenerate: linear dependence between activations in a layer; linear dependence between gradients passed back to a layer; ReLUs which fire on the same subset of datapoints. We also present a heuristic argument that modular networks are likely to be more degenerate, and we develop a metric for identifying modules in a network that is based on this argument. We propose that if we can represent a neural network in a way that is invariant to reparameterizations that exploit the degeneracies, then this representation is likely to be more interpretable, and we provide some evidence that such a representation is likely to have sparser interactions. We introduce the Interaction Basis, a tractable technique to obtain a representation that is invariant to degeneracies from linear dependence of activations or Jacobians.},
  archiveprefix = {arXiv},
  keywords = {icml},
  file = {/Users/leonardbereska/Zotero/storage/HIXB7TVI/Bushnaq et al. - 2024 - Using Degeneracy in the Loss Landscape for Mechani.pdf}
}

@article{bussmann_batchtopk_2024,
  title = {BatchTopK Sparse Autoencoders},
  author = {Bussmann, Bart and Leask, Patrick and Nanda, Neel},
  year = 2024,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2412.06410},
  url = {https://arxiv.org/abs/2412.06410},
  urldate = {2025-06-05},
  abstract = {Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting language model activations by decomposing them into sparse, interpretable features. A popular approach is the TopK SAE, that uses a fixed number of the most active latents per sample to reconstruct the model activations. We introduce BatchTopK SAEs, a training method that improves upon TopK SAEs by relaxing the top-k constraint to the batch-level, allowing for a variable number of latents to be active per sample. As a result, BatchTopK adaptively allocates more or fewer latents depending on the sample, improving reconstruction without sacrificing average sparsity. We show that BatchTopK SAEs consistently outperform TopK SAEs in reconstructing activations from GPT-2 Small and Gemma 2 2B, and achieve comparable performance to state-of-the-art JumpReLU SAEs. However, an advantage of BatchTopK is that the average number of latents can be directly specified, rather than approximately tuned through a costly hyperparameter sweep. We provide code for training and evaluating BatchTopK SAEs at https://github.com/bartbussmann/BatchTopK},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/YTPY8DH2/Bussmann et al. - 2024 - BatchTopK Sparse Autoencoders.pdf}
}

@article{bussmann_learning_2025,
  title = {Learning Multi-Level Features with Matryoshka Sparse Autoencoders},
  author = {Bussmann, Bart and Nabeshima, Noa and Karvonen, Adam and Nanda, Neel},
  year = 2025,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2503.17547},
  url = {https://arxiv.org/abs/2503.17547},
  urldate = {2025-06-05},
  abstract = {Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. number of learned concepts) creates a tension: as dictionary size increases to capture more relevant concepts, sparsity incentivizes features to be split or absorbed into more specific features, leaving high-level features missing or warped. We introduce Matryoshka SAEs, a novel variant that addresses these issues by simultaneously training multiple nested dictionaries of increasing size, forcing the smaller dictionaries to independently reconstruct the inputs without using the larger dictionaries. This organizes features hierarchically - the smaller dictionaries learn general concepts, while the larger dictionaries learn more specific concepts, without incentive to absorb the high-level features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find superior performance on sparse probing and targeted concept erasure tasks, more disentangled concept representations, and reduced feature absorption. While there is a minor tradeoff with reconstruction performance, we believe Matryoshka SAEs are a superior alternative for practical tasks, as they enable training arbitrarily large SAEs while retaining interpretable features at different levels of abstraction.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/RFPCZMS6/Bussmann et al. - 2025 - Learning Multi-Level Features with Matryoshka Sparse Autoencoders.pdf}
}

@article{bussmann_showing_2024,
  title = {Showing SAE Latents Are Not Atomic Using Meta-SAEs},
  author = {Bussmann, Bart and Pearce, Michael and Leask, Patrick and Bloom, Joseph and Sharkey, Lee and Nanda, Neel},
  year = 2024,
  month = aug,
  url = {https://www.alignmentforum.org/posts/TMAmHh4DdMr4nCSr5/showing-sae-latents-are-not-atomic-using-meta-saes},
  urldate = {2025-03-03},
  abstract = {Bart, Michael and Patrick are joint first authors. ~Research conducted as part of MATS 6.0 in Lee Sharkey and Neel Nanda's streams. Thanks to Mckenna\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/3J8GFFPS/Bussmann et al. - 2024 - Showing SAE Latents Are Not Atomic Using Meta-SAEs.html}
}

@article{can_saebench_2024,
  title = {SAEBench: A Comprehensive Benchmark for Sparse Autoencoders},
  shorttitle = {SAEBench},
  author = {Can and Karvonen, Adam and Lin, Johnny and Tigges, Curt and Bloom, Joseph and {chanind} and Lau, Yeu-Tong and Farrell, Eoin and Conmy, Arthur and CallumMcDougall and Ayonrinde, Kola and Wearden, Matthew and Marks, Sam and Nanda, Neel},
  year = 2024,
  month = dec,
  url = {https://www.alignmentforum.org/posts/jGG24BzLdYvi9dugm/saebench-a-comprehensive-benchmark-for-sparse-autoencoders},
  urldate = {2025-03-03},
  abstract = {Adam Karvonen*, Can Rager*, Johnny Lin*, Curt Tigges*, Joseph Bloom*, David Chanin, Yeu-Tong Lau, Eoin Farrell, Arthur Conmy, Callum McDougall, Kola\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/EQ8WAPMF/Can et al. - 2024 - SAEBench A Comprehensive Benchmark for Sparse Aut.html}
}

@article{carbonneau_measuring_2022,
  title = {Measuring Disentanglement: A Review of Metrics},
  shorttitle = {Measuring Disentanglement},
  author = {Carbonneau, Marc-Andr{\'e} and Zaidi, Julian and Boilard, Jonathan and Gagnon, Ghyslain},
  year = 2022,
  month = may,
  journal = {IEEE Trans. Neural Netw. Learn. Syst.},
  eprint = {2012.09276},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.09276},
  url = {http://arxiv.org/abs/2012.09276},
  urldate = {2024-01-04},
  abstract = {Learning to disentangle and represent factors of variation in data is an important problem in AI. While many advances have been made to learn these representations, it is still unclear how to quantify disentanglement. While several metrics exist, little is known on their implicit assumptions, what they truly measure, and their limits. In consequence, it is difficult to interpret results when comparing different representations. In this work, we survey supervised disentanglement metrics and thoroughly analyze them. We propose a new taxonomy in which all metrics fall into one of three families: intervention-based, predictor-based and information-based. We conduct extensive experiments in which we isolate properties of disentangled representations, allowing stratified comparison along several axes. From our experiment results and analysis, we provide insights on relations between disentangled representation properties. Finally, we share guidelines on how to measure disentanglement.},
  archiveprefix = {arXiv},
  keywords = {disentangling,measuring superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/XVU689TV/Carbonneau et al. - 2022 - Measuring Disentanglement A Review of Metrics.pdf}
}

@article{casper_diagnostics_2023,
  title = {Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks},
  author = {Casper, Stephen and Hariharan, Kaivalya and Hadfield-Menell, Dylan},
  year = 2023,
  month = may,
  journal = {NeurIPS 2022 ML Safety Workshop (Best paper award)},
  eprint = {2211.10024},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.10024},
  url = {http://arxiv.org/abs/2211.10024},
  urldate = {2024-08-04},
  abstract = {This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/GQ8U4624/Casper et al. - 2023 - Diagnostics for Deep Neural Networks with Automate.pdf}
}

@article{casper_engineer_2023,
  title = {The Engineer's Interpretability Sequence},
  author = {Casper, Stephen},
  year = 2023,
  month = sep,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7},
  abstract = {Interpretability research is popular, and interpretability tools play a role in almost every agenda for making AI safe. However, for all the interpretability work that exists, there is a significant gap between the research and engineering applications. If one of our main goals for interpretability research is to help us with aligning highly intelligent AI systems in high stakes settings, shouldn't we be seeing tools that are more helpful on real world problems? This 12-post sequence argues for taking an engineering approach to interpretability research. And from this lens, it analyzes existing work and proposes directions for moving forward.},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/6GF4ATE7/Casper - 2023 - The Engineer’s Interpretability Sequence.html}
}

@article{casper_robust_2021,
  title = {Robust Feature-Level Adversaries are Interpretability Tools},
  author = {Casper, Stephen and Nadeau, Max and Hadfield-Menell, Dylan and Kreiman, Gabriel},
  year = 2021,
  month = oct,
  journal = {NeurIPS},
  eprint = {2110.03605},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.03605},
  url = {http://arxiv.org/abs/2110.03605},
  urldate = {2023-10-26},
  abstract = {The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "copy/paste" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at https://github.com/thestephencasper/feature\_level\_adv},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/9XJPJWEH/Casper et al. - 2021 - Robust Feature-Level Adversaries are Interpretabil.pdf}
}

@article{chanin_absorption_2024,
  title = {A is for Absorption: Studying Feature Splitting and Absorption in Sparse Autoencoders},
  shorttitle = {A is for Absorption},
  author = {Chanin, David and Wilken-Smith, James and Dulka, Tom{\'a}{\v s} and Bhatnagar, Hardik and Golechha, Satvik and Bloom, Joseph},
  year = 2024,
  month = sep,
  journal = {CoRR},
  eprint = {2409.14507},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2409.14507},
  url = {http://arxiv.org/abs/2409.14507},
  urldate = {2024-12-20},
  abstract = {Sparse Autoencoders (SAEs) have emerged as a promising approach to decompose the activations of Large Language Models (LLMs) into human-interpretable latents. In this paper, we pose two questions. First, to what extent do SAEs extract monosemantic and interpretable latents? Second, to what extent does varying the sparsity or the size of the SAE affect monosemanticity / interpretability? By investigating these questions in the context of a simple first-letter identification task where we have complete access to ground truth labels for all tokens in the vocabulary, we are able to provide more detail than prior investigations. Critically, we identify a problematic form of feature-splitting we call feature absorption where seemingly monosemantic latents fail to fire in cases where they clearly should. Our investigation suggests that varying SAE size or sparsity is insufficient to solve this issue, and that there are deeper conceptual issues in need of resolution.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/SJI6ZQYW/Chanin et al. - 2024 - A is for Absorption Studying Feature Splitting an.pdf}
}

@article{chanind_toy_2024,
  title = {Toy Models of Feature Absorption in SAEs},
  author = {Chanin, David and Dulka, Tom{\'a}{\v s} and Bhatnagar, Hardik and Bloom, Joseph},
  year = 2024,
  month = oct,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/kcg58WhRxFA9hv9vN/toy-models-of-feature-absorption-in-saes},
  urldate = {2024-12-18},
  abstract = {TLDR; In previous work, we found a problematic form of feature splitting called "feature absorption" when analyzing Gemma Scope SAEs. We hypothesized\dots},
  language = {en}
}

@article{chen_dynamical_2023,
  title = {Dynamical versus Bayesian Phase Transitions in a Toy Model of Superposition},
  author = {Chen, Zhongtian and Lau, Edmund and Mendel, Jake and Wei, Susan and Murfet, Daniel},
  year = 2023,
  month = oct,
  journal = {CoRR},
  doi = {10.48550/arXiv.2310.06301},
  url = {http://arxiv.org/abs/2310.06301},
  urldate = {2023-11-09},
  abstract = {We investigate phase transitions in a Toy Model of Superposition (TMS) using Singular Learning Theory (SLT). We derive a closed formula for the theoretical loss and, in the case of two hidden dimensions, discover that regular \$k\$-gons are critical points. We present supporting theory indicating that the local learning coefficient (a geometric invariant) of these \$k\$-gons determines phase transitions in the Bayesian posterior as a function of training sample size. We then show empirically that the same \$k\$-gon critical points also determine the behavior of SGD training. The picture that emerges adds evidence to the conjecture that the SGD learning trajectory is subject to a sequential learning mechanism. Specifically, we find that the learning process in TMS, be it through SGD or Bayesian learning, can be characterized by a journey through parameter space from regions of high loss and low complexity to regions of low loss and high complexity.},
  keywords = {local learning coefficient,phase transition,toy model of superposition},
  file = {/Users/leonardbereska/Zotero/storage/QR6DQFZT/Chen et al. - 2023 - Dynamical versus Bayesian Phase Transitions in a T.pdf}
}

@article{cheng_bridging_2023,
  title = {Bridging Information-Theoretic and Geometric Compression in Language Models},
  author = {Cheng, Emily and Kervadec, Corentin and Baroni, Marco},
  year = 2023,
  journal = {EMNLP},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.13620},
  url = {https://arxiv.org/abs/2310.13620},
  urldate = {2025-05-07},
  abstract = {For a language model (LM) to faithfully model human language, it must compress vast, potentially infinite information into relatively few dimensions. We propose analyzing compression in (pre-trained) LMs from two points of view: geometric and information-theoretic. We demonstrate that the two views are highly correlated, such that the intrinsic geometric dimension of linguistic data predicts their coding length under the LM. We then show that, in turn, high compression of a linguistic dataset predicts rapid adaptation to that dataset, confirming that being able to compress linguistic information is an important part of successful LM performance. As a practical byproduct of our analysis, we evaluate a battery of intrinsic dimension estimators for the first time on linguistic data, showing that only some encapsulate the relationship between information-theoretic compression, geometric compression, and ease-of-adaptation.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/MY7TJY9T/Cheng et al. - 2023 - Bridging Information-Theoretic and Geometric Compression in Language Models.pdf}
}

@article{cheng_emergence_2024,
  title = {Emergence of a High-Dimensional Abstraction Phase in Language Transformers},
  author = {Cheng, Emily and Doimo, Diego and Kervadec, Corentin and Macocco, Iuri and Yu, Jade and Laio, Alessandro and Baroni, Marco},
  year = 2024,
  journal = {ICLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2405.15471},
  url = {https://arxiv.org/abs/2405.15471},
  urldate = {2025-05-07},
  abstract = {A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/T3JRBL3U/Cheng et al. - 2024 - Emergence of a High-Dimensional Abstraction Phase in Language Transformers.pdf}
}

@article{cheung_superposition_2019,
  title = {Superposition of many models into one},
  author = {Cheung, Brian and Terekhov, Alex and Chen, Yubei and Agrawal, Pulkit and Olshausen, Bruno},
  year = 2019,
  month = jun,
  journal = {CoRR},
  eprint = {1902.05522},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1902.05522},
  url = {http://arxiv.org/abs/1902.05522},
  urldate = {2024-03-18},
  abstract = {We present a method for storing multiple models within a single set of parameters. Models can coexist in superposition and still be retrieved individually. In experiments with neural networks, we show that a surprisingly large number of models can be effectively stored within a single parameter instance. Furthermore, each of these models can undergo thousands of training steps without significantly interfering with other models within the superposition. This approach may be viewed as the online complement of compression: rather than reducing the size of a network after training, we make use of the unrealized capacity of a network during training.},
  archiveprefix = {arXiv},
  keywords = {empirical findings in superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/RGX94VEV/Cheung et al. - 2019 - Superposition of many models into one.pdf}
}

@article{conmy_my_2023,
  title = {My best guess at the important tricks for training 1L SAEs},
  author = {Conmy, Arthur},
  year = 2023,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/fifPCos6ddsmJYahD/my-best-guess-at-the-important-tricks-for-training-1l-saes},
  urldate = {2024-02-16},
  abstract = {TL;DR: this quickly-written post gives a list of my guesses of the most important parts of training a~Sparse Autoencoder on a~1L Transformer, with op\dots},
  language = {en},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/PQ22LNUW/Conmy - 2023 - My best guess at the important tricks for training.html}
}

@misc{cooney_sparse_2024,
  title = {Sparse Autoencoder Library},
  author = {Cooney, Alan},
  year = 2024,
  month = feb,
  url = {https://github.com/ai-safety-foundation/sparse_autoencoder},
  urldate = {2024-02-16},
  abstract = {Sparse Autoencoder for Mechanistic Interpretability},
  copyright = {MIT},
  howpublished = {ai-safety-foundation},
  keywords = {sparse autoencoder}
}

@book{cover_elements_2005,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = 1999,
  edition = {1},
  publisher = {John Wiley \& Sons},
  doi = {10.1002/047174882X},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/047174882X},
  urldate = {2025-06-05},
  abstract = {Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index.},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  language = {en}
}

@article{cowsik_persian_2024,
  title = {The Persian Rug: solving toy models of superposition using large-scale symmetries},
  shorttitle = {The Persian Rug},
  author = {Cowsik, Aditya and Dolev, Kfir and Infanger, Alex},
  year = 2024,
  month = oct,
  journal = {CoRR},
  eprint = {2410.12101},
  primaryclass = {cond-mat},
  url = {http://arxiv.org/abs/2410.12101},
  urldate = {2024-10-25},
  abstract = {We present a complete mechanistic description of the algorithm learned by a minimal non-linear sparse data autoencoder in the limit of large input dimension. The model, originally presented in Elhage et al. (2022), compresses sparse data vectors through a linear layer and decompresses using another linear layer followed by a ReLU activation. We notice that when the data is permutation symmetric (no input feature is privileged) large models reliably learn an algorithm that is sensitive to individual weights only through their large-scale statistics. For these models, the loss function becomes analytically tractable. Using this understanding, we give the explicit scalings of the loss at high sparsity, and show that the model is near-optimal among recently proposed architectures. In particular, changing or adding to the activation function any elementwise or filtering operation can at best improve the model's performance by a constant factor. Finally, we forward-engineer a model with the requisite symmetries and show that its loss precisely matches that of the trained models. Unlike the trained model weights, the low randomness in the artificial weights results in miraculous fractal structures resembling a Persian rug, to which the algorithm is oblivious. Our work contributes to neural network interpretability by introducing techniques for understanding the structure of autoencoders. Code to reproduce our results can be found at https://github.com/KfirD/PersianRug.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {toy model of superposition},
  file = {/Users/leonardbereska/Zotero/storage/Y6SH3AXJ/Cowsik et al. - 2024 - The Persian Rug solving toy models of superpositi.pdf}
}

@misc{cunningham_sae_2024,
  title = {SAE Training Guide v1},
  author = {Cunningham, Hoagy},
  year = 2024,
  url = {https://docs.google.com/document/d/18bxKmrBN4rhhY6vwhpYDf5XC4bHt0eRy3Le9Aco2-1Y/edit?usp=embed_facebook},
  urldate = {2024-02-16},
  language = {en},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/U6MKIDEI/Cunningham - 2024 - SAE Training Guide v1.html}
}

@article{cunningham_sparse_2024,
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = 2024,
  month = jan,
  journal = {ICLR},
  eprint = {2309.08600},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.08600},
  url = {http://arxiv.org/abs/2309.08600},
  urldate = {2023-10-30},
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is \textbackslash textit\textbraceleft polysemanticity\textbraceright, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \textbackslash textit\textbraceleft superposition\textbraceright, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \textbackslash citep\textbraceleft wang2022interpretability\textbraceright{} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/DEJUHG9L/Cunningham et al. - 2024 - Sparse Autoencoders Find Highly Interpretable Feat.pdf}
}

@article{cywinski_saeuron_2025,
  title = {SAeUron: Interpretable Concept Unlearning in Diffusion Models with Sparse Autoencoders},
  shorttitle = {SAeUron},
  author = {Cywi{\'n}ski, Bartosz and Deja, Kamil},
  year = 2025,
  month = jan,
  journal = {CoRR},
  eprint = {2501.18052},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2501.18052},
  url = {http://arxiv.org/abs/2501.18052},
  urldate = {2025-03-03},
  abstract = {Diffusion models, while powerful, can inadvertently generate harmful or undesirable content, raising significant ethical and safety concerns. Recent machine unlearning approaches offer potential solutions but often lack transparency, making it difficult to understand the changes they introduce to the base model. In this work, we introduce SAeUron, a novel method leveraging features learned by sparse autoencoders (SAEs) to remove unwanted concepts in text-to-image diffusion models. First, we demonstrate that SAEs, trained in an unsupervised manner on activations from multiple denoising timesteps of the diffusion model, capture sparse and interpretable features corresponding to specific concepts. Building on this, we propose a feature selection method that enables precise interventions on model activations to block targeted content while preserving overall performance. Evaluation with the competitive UnlearnCanvas benchmark on object and style unlearning highlights SAeUron's state-of-the-art performance. Moreover, we show that with a single SAE, we can remove multiple concepts simultaneously and that in contrast to other methods, SAeUron mitigates the possibility of generating unwanted content, even under adversarial attack. Code and checkpoints are available at: https://github.com/cywinski/SAeUron.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/AVHND4YK/Cywiński and Deja - 2025 - SAeUron Interpretable Concept Unlearning in Diffu.pdf}
}

@article{delgiudice_effective_2021,
  title = {Effective Dimensionality: A Tutorial},
  shorttitle = {Effective Dimensionality},
  author = {Del Giudice, Marco},
  year = 2021,
  journal = {Multivariate behavioral research},
  issn = {0027-3171},
  url = {https://iris.unito.it/handle/2318/1853350},
  urldate = {2024-12-09},
  abstract = {open},
  language = {eng},
  keywords = {matrix effective dimensionality measures},
  annotation = {Accepted: 2022-04-12T07:29:31Z},
  file = {/Users/leonardbereska/Zotero/storage/6FGPPUGM/Del Giudice - 2021 - Effective Dimensionality A Tutorial.pdf}
}

@article{deng_measuring_2023,
  title = {Measuring Feature Sparsity in Language Models},
  author = {Deng, Mingyang and Tao, Lucas and Benton, Joe},
  year = 2023,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2310.07837},
  url = {https://arxiv.org/abs/2310.07837},
  urldate = {2023-10-26},
  abstract = {Recent works have proposed that activations in language models can be modelled as sparse linear combinations of vectors corresponding to features of input text. Under this assumption, these works aimed to reconstruct feature directions using sparse coding. We develop metrics to assess the success of these sparse coding techniques and test the validity of the linearity and sparsity assumptions. We show our metrics can predict the level of sparsity on synthetic sparse linear activations, and can distinguish between sparse linear data and several other distributions. We use our metrics to measure levels of sparsity in several language models. We find evidence that language model activations can be accurately modelled by sparse linear combinations of features, significantly more so than control datasets. We also show that model activations appear to be sparsest in the first and final layers.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {measuring superposition,sparse autoencoder,superposition},
  file = {/Users/leonardbereska/Zotero/storage/86TETC2V/Deng et al. - 2023 - Measuring Feature Sparsity in Language Models.pdf}
}

@article{dorrell_modularity_2024,
  title = {Modularity in Biologically Inspired Representations Depends on Task Variable Range Independence},
  author = {Dorrell, Will and Hsu, Kyle and Hollingsworth, Luke and Lee, Jin Hwa and Wu, Jiajun and Finn, Chelsea and Latham, Peter E. and Behrens, Timothy Edward John and Whittington, James C. R.},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=KSvWtLOOjL},
  urldate = {2024-08-16},
  abstract = {Artificial and biological neurons sometimes modularise into disjoint groups each encoding a single meaningful variable; at other times they entangle the representation of many variables. Understanding why and when this happens would both help machine learning practitioners build interpretable representations and increase our understanding of neural wetware. In this work, we study optimal neural representations under the biologically inspired constraints of nonnegativity and energy efficiency. We develop a theory of the necessary and sufficient conditions on task structure that induce neural modularisation of task-relevant variables in both linear and partially nonlinear settings. Our theory shows that modularisation is governed not by statistical independence of underlying variables as previously thought, but rather by the independence of the ranges of these variables. We corroborate our theoretical predictions in a variety of empirical studies training feedforward and recurrent neural networks on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data, providing an explanation of why prefrontal working memory representations sometimes encode different memories in orthogonal subspaces, and sometimes don't, depending on task structure. Lastly, we suggest a suite of surprising settings in which neurons might be or appear mixed selective without requiring complex nonlinear readouts, as in traditional theories. In summary, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in machines and brains.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/N945DIVK/Dorrell et al. - 2024 - Modularity in Biologically Inspired Representation.pdf}
}

@article{dumas_how_2024,
  title = {How do Llamas process multilingual text? A latent exploration through activation patching},
  shorttitle = {How do Llamas process multilingual text?},
  author = {Dumas, Cl{\'e}ment and Veselovsky, Veniamin and Monea, Giovanni and West, Robert and Wendler, Chris},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=0ku2hIm4BS},
  urldate = {2024-08-16},
  abstract = {A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing Llama-2's forward pass during a word translation task. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean over latents across different language pairs does not impair the model's performance in translating the concept. Our results provide evidence for the existence of language-agnostic concept representations within the model.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/MRKMKUQ5/Dumas et al. - 2024 - How do Llamas process multilingual text A latent .pdf}
}

@article{dunefsky_transcoders_2024,
  title = {Transcoders find interpretable LLM feature circuits},
  author = {Dunefsky, Jacob and Chlenski, Philippe and Nanda, Neel},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=GWqzUR2dOX},
  urldate = {2024-08-16},
  abstract = {A key goal in mechanistic interpretability is circuit analysis: finding sparse subgraphs of models corresponding to specific behaviors or capabilities. However, MLP sublayers make fine-grained circuit analysis on transformer-based language models difficult. In particular, interpretable features---such as those found by sparse autoencoders (SAEs)---are typically linear combinations of extremely many neurons, each with its own nonlinearity to account for. Circuit analysis in this setting thus either yields intractably large circuits or fails to disentangle local and global behavior. To address this we explore **transcoders**, which seek to faithfully approximate a densely activating MLP layer with a wider, sparsely-activating MLP layer. We successfully train transcoders on language models with 120M, 410M, and 1.4B parameters, and find them to perform at least on par with SAEs in terms of sparsity, faithfulness, and human-interpretability. We then introduce a novel method for using transcoders to perform weights-based circuit analysis through MLP sublayers. The resulting circuits neatly factorize into input-dependent and input-invariant terms. Finally, we apply transcoders to reverse-engineer unknown circuits in the model, and we obtain novel insights regarding the "greater-than circuit" in GPT2-small. Our results suggest that transcoders can prove effective in decomposing model computations involving MLPs into interpretable circuits. Code is available at https://github.com/jacobdunefsky/transcoder\_circuits.},
  language = {en},
  keywords = {icml,sparse autoencoder,sparse feature circuits},
  file = {/Users/leonardbereska/Zotero/storage/CFLL5SCH/Dunefsky et al. - 2024 - Transcoders find interpretable LLM feature circuit.pdf}
}

@article{eastwood_framework_2018,
  title = {A Framework for the Quantitative Evaluation of Disentangled Representations},
  author = {Eastwood, Cian and Williams, Christopher K. I.},
  year = 2018,
  month = feb,
  journal = {ICLR},
  url = {https://www.semanticscholar.org/paper/A-Framework-for-the-Quantitative-Evaluation-of-Eastwood-Williams/adf2ac6b99b7d48b6a9c908532ca249de2cec3ae},
  urldate = {2024-12-09},
  abstract = {Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by recent state-of-the-art models.},
  file = {/Users/leonardbereska/Zotero/storage/LAUZA5YT/Eastwood and Williams - 2018 - A Framework for the Quantitative Evaluation of Dis.pdf}
}

@article{eigen_topkconv_2021,
  title = {TopKConv: Increased Adversarial Robustness Through Deeper Interpretability},
  shorttitle = {TopKConv},
  author = {Eigen, Henry and Sadovnik, Amir},
  year = 2021,
  month = dec,
  journal = {ICMLA},
  pages = {15--22},
  doi = {10.1109/ICMLA52953.2021.00011},
  url = {https://ieeexplore.ieee.org/document/9680089},
  urldate = {2024-08-04},
  abstract = {Vulnerability to adversarial inputs remains an issue for deep neural networks. Attackers can slightly modify inputs in order to cause adverse behavior in otherwise highly accurate networks. In addition to making these networks less secure for real world applications, this also emphasizes a misalignment between the features the network uses to make decisions and the ones humans use. In this work we propose that more interpretable networks should yield more robust ones since they are able to rely on features that are more understandable to humans. More specifically, we take inspiration from interpretability based approaches to adversarial robustness, and propose a sparsity based defense to counter the impact of overparameterization on adversarial vulnerability. Building off of the work of the Dynamic-K algorithm, which introduces dynamic routing to fully connected layers in order to encourage sparse, interpretable predictions, we propose TopKConv, a novel method of reducing the number of activation channels used to construct each convolutional feature map. The incorporation of TopKConv alongside Dynamic-k results in a significant increase in adversarial accuracy at no cost to benign accuracy. Further, this is achieved with no fine tuning of or adversarial training.},
  file = {/Users/leonardbereska/Zotero/storage/F2X7FIG6/Eigen and Sadovnik - 2021 - TopKConv Increased Adversarial Robustness Through.html}
}

@article{elhage_privileged_2023,
  title = {Privileged Bases in the Transformer Residual Stream},
  author = {Elhage, Nelson and Lasenby, Robert and Olah, Christopher},
  year = 2023,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/privileged-basis/index.html},
  abstract = {Our mathematical theories of the Transformer architecture suggest that individual coordinates in the residual stream should have no special significance (that is, the basis directions should be in some sense "arbitrary" and no more likely to encode information than random directions). Recent work has shown that this observation is false in practice. We investigate this phenomenon and provisionally conclude that the per-dimension normalizers in the Adam optimizer are to blame for the effect. We explore two other obvious sources of basis dependency in a Transformer: Layer normalization, and finite-precision floating-point calculations. We confidently rule these out as being the source of the observed basis-alignment.},
  keywords = {explaining superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/RWKQ453E/Elhage et al. - 2023 - Privileged Bases in the Transformer Residual Strea.html}
}

@article{elhage_softmax_2022,
  title = {Softmax Linear Units},
  author = {Elhage, Nelson and Hume, Tristan and Catherine, Olsson and Neel, Nanda and Henighan, Tom and Johnston, Scott and ElShowk, Sheer and Joseph, Nicholas and DasSarma, Nova and Mann, Ben and Hernandez, Danny and Askell, Amanda and Ndousse, Kamal and Drain, Dawn and Chen, Anna and Bai, Yuntao and Ganguli, Deep and Lovitt, Liane and Hatfield-Dodds, Zac and Kernion, Jackson and Conerly, Tom and Kravec, Shauna and Fort, Stanislav and Kadavath, Saurav and Jacobson, Josh and Tran-Johnson, Eli and Kaplan, Jared and Clark, Jack and Brown, Tom and McCandlish, Sam and Amodei, Dario and Olah, Christopher},
  year = 2022,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/solu/index.html},
  urldate = {2023-07-31},
  abstract = {As Transformer generative models continue to gain real-world adoption  [1, 2, 3, 4, 5] , it becomes ever more important to ensure they behave predictably and safely, in both the short and long run.  Mechanistic interpretability -- the project of attempting to reverse engineer neural networks into understandable computer programs -- offers one possible avenue for addressing these safety issues: by understanding the internal structures that cause neural networks to produce the outputs they do, it may be possible to address current safety problems more systematically as well as anticipating future safety problems. Until recently mechanistic interpretability has focused primarily on CNN vision models  [6] , but some recent efforts have begun to explore mechanistic interpretability for transformer language models  [7, 8] .  Notably, we were able to reverse-engineer 1 and 2 layer attention-only transformers  [7]  and we used empirical evidence to draw indirect conclusions about in-context learning in arbitrarily large models  [8] . Unfortunately, it has so far been difficult to mechanistically understand large models due to the difficulty of understanding their MLP (feedforward) layers.  This failure to understand and interpret MLP layers appears to be a major blocker to further progress. The underlying issue is that many neurons appear to be polysemantic  [9, 10] , responding to multiple unrelated features. Polysemanticity has been observed before in vision models, but seems especially severe in standard transformer language models.  One plausible explanation for polysemanticity is the superposition hypothesis  [10] , which suggests that neural network layers have more features than neurons as part of a ``sparse coding'' strategy to simulate a much larger layer.  If true, this would make polysmenticity a functionally important property and thus especially difficult to remove without damaging ML performance. In this paper, we report an architectural change which appears to substantially increase the fraction of MLP neurons which appear to be "interpretable" (i.e. respond to an articulable property of the input), at little to no cost to ML performance. Specifically, we replace the activation function with a softmax linear unit (which we term SoLU) and show that this significantly increases the fraction of neurons in the MLP layers which seem to correspond to readily human-understandable concepts, phrases, or categories on quick investigation, as measured by randomized and blinded experiments. We then study our SoLU models and use them to gain several new insights about how information is processed in transformers.  However, we also discover some evidence that the superposition hypothesis is true and there is no free lunch: SoLU may be making some features more interpretable by ``hiding'' others and thus making them even more deeply uninterpretable.  Despite this, SoLU still seems like a net win, as in practical terms it substantially increases the fraction of neurons we are able to understand. Although preliminary, we argue that these results show the potential for a general approach of designing architectures for mechanistic interpretability: there may exist many different models or architectures which all achieve roughly state-of-the-art performance, but which differ greatly in how easy they are to reverse engineer. Put another way, we are in the curious position of being both reverse engineers trying to understand the algorithms neural network parameters implement, and also the hardware designers deciding the network architecture they must run on: perhaps we can exploit this second role to support the first. If so, it may be possible to move the field in a positive direction by discovering (and advocating for) those architectures which are most amenable to reverse engineering.},
  keywords = {engineering monosemanticity,intrinsic interpretability,superposition},
  file = {/Users/leonardbereska/Zotero/storage/YNHFFS3C/Elhage et al. - 2022 - Softmax Linear Units.html}
}

@article{elhage_toy_2022,
  title = {Toy Models of Superposition},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  year = 2022,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/toy_model/index.html},
  abstract = {It would be very convenient if the individual neurons of artificial neural networks corresponded to cleanly interpretable features of the input. For example, in an ``ideal'' ImageNet classifier, each neuron would fire only in the presence of a specific visual feature, such as the color red, a left-facing curve, or a dog snout. Empirically, in models we have studied, some of the neurons do cleanly map to features. But it isn't always the case that features correspond so cleanly to neurons, especially in large language models where it actually seems rare for neurons to correspond to clean features. This brings up many questions. Why is it that neurons sometimes align with features and sometimes don't? Why do some models and tasks have many of these clean neurons, while they're vanishingly rare in others? In this paper, we use toy models --- small ReLU networks trained on synthetic data with sparse input features --- to investigate how and when models represent more features than they have dimensions. We call this phenomenon superposition  [1, 2, 3] . When features are sparse, superposition allows compression beyond what a linear model would do, at the cost of "interference" that requires nonlinear filtering.},
  keywords = {adversarial,fundamental,measuring superposition,superposition,toy model of superposition,toy models},
  file = {/Users/leonardbereska/Zotero/storage/8K24TPD5/Elhage et al. - 2022 - Toy Models of Superposition.html}
}

@article{engels_decomposing_2024,
  title = {Decomposing The Dark Matter of Sparse Autoencoders},
  author = {Engels, Joshua and Riggs, Logan and Tegmark, Max},
  year = 2024,
  journal = {TMLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2410.14670},
  url = {https://arxiv.org/abs/2410.14670},
  urldate = {2025-05-07},
  abstract = {Sparse autoencoders (SAEs) are a promising technique for decomposing language model activations into interpretable linear features. However, current SAEs fall short of completely explaining model performance, resulting in "dark matter": unexplained variance in activations. This work investigates dark matter as an object of study in its own right. Surprisingly, we find that much of SAE dark matter -- about half of the error vector itself and \&gt;90\% of its norm -- can be linearly predicted from the initial activation vector. Additionally, we find that the scaling behavior of SAE error norms at a per token level is remarkably predictable: larger SAEs mostly struggle to reconstruct the same contexts as smaller SAEs. We build on the linear representation hypothesis to propose models of activations that might lead to these observations. These insights imply that the part of the SAE error vector that cannot be linearly predicted ("nonlinear" error) might be fundamentally different from the linearly predictable component. To validate this hypothesis, we empirically analyze nonlinear SAE error and show that 1) it contains fewer not yet learned features, 2) SAEs trained on it are quantitatively worse, and 3) it is responsible for a proportional amount of the downstream increase in cross entropy loss when SAE activations are inserted into the model. Finally, we examine two methods to reduce nonlinear SAE error: inference time gradient pursuit, which leads to a very slight decrease in nonlinear error, and linear transformations from earlier layer SAE outputs, which leads to a larger reduction.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/XV3GF9PY/Engels et al. - 2024 - Decomposing The Dark Matter of Sparse Autoencoders.pdf}
}

@article{engels_not_2024,
  title = {Not All Language Model Features Are Linear},
  author = {Engels, Joshua and Liao, Isaac and Michaud, Eric J. and Gurnee, Wes and Tegmark, Max},
  year = 2024,
  month = may,
  journal = {CoRR},
  eprint = {2405.14860},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.14860},
  url = {http://arxiv.org/abs/2405.14860},
  urldate = {2024-06-06},
  abstract = {Recent work has proposed the linear representation hypothesis: that language models perform computation by manipulating one-dimensional representations of concepts ("features") in activation space. In contrast, we explore whether some language model representations may be inherently multi-dimensional. We begin by developing a rigorous definition of irreducible multi-dimensional features based on whether they can be decomposed into either independent or non-co-occurring lower-dimensional features. Motivated by these definitions, we design a scalable method that uses sparse autoencoders to automatically find multi-dimensional features in GPT-2 and Mistral 7B. These auto-discovered features include strikingly interpretable examples, e.g. circular features representing days of the week and months of the year. We identify tasks where these exact circles are used to solve computational problems involving modular arithmetic in days of the week and months of the year. Finally, we provide evidence that these circular features are indeed the fundamental unit of computation in these tasks with intervention experiments on Mistral 7B and Llama 3 8B, and we find further circular representations by breaking down the hidden states for these tasks into interpretable components.},
  archiveprefix = {arXiv},
  keywords = {beyond superposition},
  file = {/Users/leonardbereska/Zotero/storage/XN5ZG2EY/Engels et al. - 2024 - Not All Language Model Features Are Linear.pdf;/Users/leonardbereska/Zotero/storage/PK9YTKZX/Engels et al. - 2024 - Not All Language Model Features Are Linear.html}
}

@article{engstrom_adversarial_2019,
  title = {Adversarial Robustness as a Prior for Learned Representations},
  author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
  year = 2019,
  month = sep,
  journal = {CoRR},
  eprint = {1906.00945},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1906.00945},
  url = {http://arxiv.org/abs/1906.00945},
  urldate = {2024-08-03},
  abstract = {An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/WNMTUM7F/Engstrom et al. - 2019 - Adversarial Robustness as a Prior for Learned Repr.pdf}
}

@article{etmann_connection_2019,
  title = {On the Connection Between Adversarial Robustness and Saliency Map Interpretability},
  author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Sch{\"o}nlieb, Carola-Bibiane},
  year = 2019,
  month = may,
  journal = {ICML},
  eprint = {1905.04172},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.04172},
  url = {http://arxiv.org/abs/1905.04172},
  urldate = {2024-08-04},
  abstract = {Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/CPVBG9JI/Etmann et al. - 2019 - On the Connection Between Adversarial Robustness a.pdf}
}

@article{farnik_jacobian_2025,
  title = {Jacobian Sparse Autoencoders: Sparsify Computations, Not Just Activations},
  shorttitle = {Jacobian Sparse Autoencoders},
  author = {Farnik, Lucy and Lawson, Tim and Houghton, Conor and Aitchison, Laurence},
  year = 2025,
  month = feb,
  journal = {CoRR},
  eprint = {2502.18147},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2502.18147},
  url = {http://arxiv.org/abs/2502.18147},
  urldate = {2025-03-03},
  abstract = {Sparse autoencoders (SAEs) have been successfully used to discover sparse and human-interpretable representations of the latent activations of LLMs. However, we would ultimately like to understand the computations performed by LLMs and not just their representations. The extent to which SAEs can help us understand computations is unclear because they are not designed to "sparsify" computations in any sense, only latent activations. To solve this, we propose Jacobian SAEs (JSAEs), which yield not only sparsity in the input and output activations of a given model component but also sparsity in the computation (formally, the Jacobian) connecting them. With a na\textbackslash "ive implementation, the Jacobians in LLMs would be computationally intractable due to their size. One key technical contribution is thus finding an efficient way of computing Jacobians in this setup. We find that JSAEs extract a relatively large degree of computational sparsity while preserving downstream LLM performance approximately as well as traditional SAEs. We also show that Jacobians are a reasonable proxy for computational sparsity because MLPs are approximately linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a greater degree of computational sparsity on pre-trained LLMs than on the equivalent randomized LLM. This shows that the sparsity of the computational graph appears to be a property that LLMs learn through training, and suggests that JSAEs might be more suitable for understanding learned transformer computations than standard SAEs.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/BKPNAQ4J/Farnik et al. - 2025 - Jacobian Sparse Autoencoders Sparsify Computation.pdf}
}

@article{fontanari_portfolio_2019,
  title = {Portfolio Risk and the Quantum Majorization of Correlation Matrices},
  author = {Fontanari, Andrea and Eliazar, Iddo and Cirillo, Pasquale and Oosterlee, Cornelis W.},
  year = 2021,
  journal = {IMA Journal of Management Mathematics},
  doi = {10.2139/ssrn.3309585},
  url = {https://papers.ssrn.com/abstract=3309585},
  urldate = {2024-12-09},
  abstract = {We propose Quantum Majorization as a way of comparing and ranking correlation matrices, with the aim of assessing portfolio risk in a unified framework. Quantum majorization is a partial order in the space of correlation matrices, which are evaluated through their spectra. We discuss the connections between Quantum Majorization and an important class of risk functionals, and we define two new risk measures able to capture interesting characteristics of portfolio risk.},
  language = {en},
  keywords = {matrix effective dimensionality measures},
  file = {/Users/leonardbereska/Zotero/storage/LGW7UAXB/Fontanari et al. - 2021 - Portfolio Risk and the Quantum Majorization of Correlation Matrices.pdf}
}

@article{friedman_learning_2023,
  title = {Learning Transformer Programs},
  author = {Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  year = 2023,
  month = jun,
  journal = {NeurIPS},
  eprint = {2306.01128},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2306.01128},
  url = {http://arxiv.org/abs/2306.01128},
  urldate = {2023-07-03},
  abstract = {Recent research in mechanistic interpretability has attempted to reverse-engineer Transformer models by carefully inspecting network weights and activations. However, these approaches require considerable manual effort and still fall short of providing complete, faithful descriptions of the underlying algorithms. In this work, we introduce a procedure for training Transformers that are mechanistically interpretable by design. We build on RASP [Weiss et al., 2021], a programming language that can be compiled into Transformer weights. Instead of compiling human-written programs into Transformers, we design a modified Transformer that can be trained using gradient-based optimization and then be automatically converted into a discrete, human-readable program. We refer to these models as Transformer Programs. To validate our approach, we learn Transformer Programs for a variety of problems, including an in-context learning task, a suite of algorithmic problems (e.g. sorting, recognizing Dyck-languages), and NLP tasks including named entity recognition and text classification. The Transformer Programs can automatically find reasonable solutions, performing on par with standard Transformers of comparable size; and, more importantly, they are easy to interpret. To demonstrate these advantages, we convert Transformers into Python programs and use off-the-shelf code analysis tools to debug model errors and identify the ``circuits'' used to solve different sub-problems. We hope that Transformer Programs open a new path toward the goal of intrinsically interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {algorithms,compiled transformers,computation in superposition,intrinsic interpretability,superposition},
  file = {/Users/leonardbereska/Zotero/storage/I8LH2J8K/Friedman et al. - 2023 - Learning Transformer Programs.pdf}
}

@article{furman_estimating_2024,
  title = {Estimating the Local Learning Coefficient at Scale},
  author = {Furman, Zach and Lau, Edmund},
  year = 2024,
  month = feb,
  journal = {CoRR},
  eprint = {2402.03698},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2402.03698},
  url = {http://arxiv.org/abs/2402.03698},
  urldate = {2024-08-21},
  abstract = {The \textbackslash textit\textbraceleft local learning coefficient\textbraceright{} (LLC) is a principled way of quantifying model complexity, originally derived in the context of Bayesian statistics using singular learning theory (SLT). Several methods are known for numerically estimating the local learning coefficient, but so far these methods have not been extended to the scale of modern deep learning architectures or data sets. Using a method developed in \textbraceleft\textbackslash tt arXiv:2308.12108 [stat.ML]\textbraceright{} we empirically show how the LLC may be measured accurately and self-consistently for deep linear networks (DLNs) up to 100M parameters. We also show that the estimated LLC has the rescaling invariance that holds for the theoretical quantity.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/7NZG8R4L/Furman and Lau - 2024 - Estimating the Local Learning Coefficient at Scale.pdf}
}

@article{gao_pile_2020,
  title = {The Pile: An 800GB Dataset of Diverse Text for Language Modeling},
  shorttitle = {The Pile},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = 2020,
  month = dec,
  journal = {CoRR},
  eprint = {2101.00027},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2101.00027},
  url = {http://arxiv.org/abs/2101.00027},
  urldate = {2025-07-07},
  abstract = {Recent work has demonstrated that increased training dataset diversity improves general cross-domain knowledge and downstream generalization capability for large-scale language models. With this in mind, we present \textbackslash textit\textbraceleft the Pile\textbraceright : an 825 GiB English text corpus targeted at training large-scale language models. The Pile is constructed from 22 diverse high-quality subsets -- both existing and newly constructed -- many of which derive from academic or professional sources. Our evaluation of the untuned performance of GPT-2 and GPT-3 on the Pile shows that these models struggle on many of its components, such as academic writing. Conversely, models trained on the Pile improve significantly over both Raw CC and CC-100 on all components of the Pile, while improving performance on downstream evaluations. Through an in-depth exploratory analysis, we document potentially concerning aspects of the data for prospective users. We make publicly available the code used in its construction.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/DKTVQ8FV/Gao et al. - 2020 - The Pile An 800GB Dataset of Diverse Text for Lan.pdf;/Users/leonardbereska/Zotero/storage/AY4CSK3A/2101.html}
}

@article{gao_scaling_2024,
  title = {Scaling and evaluating sparse autoencoders},
  author = {Gao, Leo and la Tour, Tom Dupr{\'e} and Tillman, Henk and Goh, Gabriel and Troll, Rajan and Radford, Alec and Sutskever, Ilya and Leike, Jan and Wu, Jeffrey},
  year = 2024,
  month = jun,
  journal = {CoRR},
  eprint = {2406.04093},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.04093},
  url = {http://arxiv.org/abs/2406.04093},
  urldate = {2025-03-03},
  abstract = {Sparse autoencoders provide a promising unsupervised approach for extracting interpretable features from a language model by reconstructing activations from a sparse bottleneck layer. Since language models learn many concepts, autoencoders need to be very large to recover all relevant features. However, studying the properties of autoencoder scaling is difficult due to the need to balance reconstruction and sparsity objectives and the presence of dead latents. We propose using k-sparse autoencoders [Makhzani and Frey, 2013] to directly control sparsity, simplifying tuning and improving the reconstruction-sparsity frontier. Additionally, we find modifications that result in few dead latents, even at the largest scales we tried. Using these techniques, we find clean scaling laws with respect to autoencoder size and sparsity. We also introduce several new metrics for evaluating feature quality based on the recovery of hypothesized features, the explainability of activation patterns, and the sparsity of downstream effects. These metrics all generally improve with autoencoder size. To demonstrate the scalability of our approach, we train a 16 million latent autoencoder on GPT-4 activations for 40 billion tokens. We release training code and autoencoders for open-source models, as well as a visualizer.},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/BGNI7H36/Gao et al. - 2024 - Scaling and evaluating sparse autoencoders.pdf}
}

@article{gao_theory_2017,
  title = {A theory of multineuronal dimensionality, dynamics and measurement},
  author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  year = 2017,
  month = nov,
  journal = {bioRxiv},
  publisher = {Neuroscience},
  doi = {10.1101/214262},
  url = {http://biorxiv.org/lookup/doi/10.1101/214262},
  urldate = {2025-06-05},
  abstract = {Abstract           In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/KSGM9YRT/Gao et al. - 2017 - A theory of multineuronal dimensionality, dynamics.pdf}
}

@article{garfinkle_uniqueness_2019,
  title = {On the Uniqueness and Stability of Dictionaries for Sparse Representation of Noisy Signals},
  author = {Garfinkle, Charles J. and Hillar, Christopher J.},
  year = 2019,
  month = dec,
  journal = {IEEE Transactions on Signal Processing},
  volume = {67},
  number = {23},
  pages = {5884--5892},
  issn = {1941-0476},
  doi = {10.1109/TSP.2019.2935914},
  url = {https://ieeexplore.ieee.org/abstract/document/8805108},
  urldate = {2024-06-12},
  abstract = {Learning optimal dictionaries for sparse coding has exposed characteristic sparse features of many natural signals. However, universal guarantees of the stability of such features in the presence of noise are lacking. Here, we provide very general conditions guaranteeing when dictionaries yielding the sparsest encodings are unique and stable with respect to measurement or modeling error. We demonstrate that some or all original dictionary elements are recoverable from noisy data even if the dictionary fails to satisfy the spark condition, its size is overestimated, or only a polynomial number of distinct sparse supports appear in the data. Importantly, we derive these guarantees without requiring any constraints on the recovered dictionary beyond a natural upper bound on its size. Our results yield an effective procedure sufficient to affirm if a proposed solution to the dictionary learning problem is unique within bounds commensurate with the noise. We suggest applications to data analysis, engineering, and neuroscience and close with some remaining challenges left open by our work.},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/3B5V25IA/Garfinkle and Hillar - 2019 - On the Uniqueness and Stability of Dictionaries fo.pdf}
}

@article{ge_automatically_2024a,
  title = {Automatically Identifying Local and Global Circuits with Linear Computation Graphs},
  author = {Ge, Xuyang and Zhu, Fukang and Shu, Wentao and Wang, Junxuan and He, Zhengfu and Qiu, Xipeng},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=b8sq8Y5VFo},
  urldate = {2024-11-07},
  abstract = {Circuit analysis of any certain model behavior is a central task in mechanistic interpretability. We introduce our circuit discovery pipeline with sparse autoencoders (SAEs) and a variant called transcoders. With these two modules inserted into the model, the model's computation graph with respect to OV and MLP circuits becomes strictly linear. Our methods do not require linear approximation to compute the causal effect of each node. This fine-grained graph identifies end-to-end and local circuits accounting for either logits or intermediate features. We can scalably apply this pipeline with a technique called Hierarchical Attribution. We analyze three kinds of circuits in GPT2-Small, namely bracket, induction, and Indirect Object Identification circuits. Our results reveal new findings underlying existing discoveries.},
  language = {en},
  keywords = {icml,sparse autoencoder,sparse feature circuits},
  file = {/Users/leonardbereska/Zotero/storage/KJVQHJBS/Ge et al. - 2024 - Automatically Identifying Local and Global Circuit.pdf}
}

@article{geva_transformer_2021,
  title = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  year = 2021,
  month = sep,
  journal = {CoRR},
  eprint = {2012.14913},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.14913},
  url = {http://arxiv.org/abs/2012.14913},
  urldate = {2023-07-31},
  abstract = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformer-based language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
  archiveprefix = {arXiv},
  keywords = {fundamental,memory,transformer},
  file = {/Users/leonardbereska/Zotero/storage/7RMASLG5/Geva et al. - 2021 - Transformer Feed-Forward Layers Are Key-Value Memo.pdf}
}

@article{ghilardi_efficient_2024,
  title = {Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups},
  author = {Ghilardi, Davide and Belotti, Federico and Molinari, Marco},
  year = 2024,
  month = oct,
  journal = {CoRR},
  eprint = {2410.21508},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.21508},
  url = {http://arxiv.org/abs/2410.21508},
  urldate = {2025-03-03},
  abstract = {Sparse AutoEnocders (SAEs) have recently been employed as an unsupervised approach for understanding the inner workings of Large Language Models (LLMs). They reconstruct the model's activations with a sparse linear combination of interpretable features. However, training SAEs is computationally intensive, especially as models grow in size and complexity. To address this challenge, we propose a novel training strategy that reduces the number of trained SAEs from one per layer to one for a given group of contiguous layers. Our experimental results on Pythia 160M highlight a speedup of up to 6x without compromising the reconstruction quality and performance on downstream tasks. Therefore, layer clustering presents an efficient approach to train SAEs in modern LLMs.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/7GAVLC26/Ghilardi et al. - 2024 - Efficient Training of Sparse Autoencoders for Larg.pdf}
}

@article{goldfeld_estimating_2019,
  title = {Estimating Information Flow in Deep Neural Networks},
  author = {Goldfeld, Ziv and van den Berg, Ewout and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
  year = 2019,
  month = may,
  journal = {ICML},
  eprint = {1810.05728},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1810.05728},
  url = {http://arxiv.org/abs/1810.05728},
  urldate = {2025-06-18},
  abstract = {We study the flow of information and the evolution of internal representations during deep neural network (DNN) training, aiming to demystify the compression aspect of the information bottleneck theory. The theory suggests that DNN training comprises a rapid fitting phase followed by a slower compression phase, in which the mutual information \$I(X;T)\$ between the input \$X\$ and internal representations \$T\$ decreases. Several papers observe compression of estimated mutual information on different DNN models, but the true \$I(X;T)\$ over these networks is provably either constant (discrete \$X\$) or infinite (continuous \$X\$). This work explains the discrepancy between theory and experiments, and clarifies what was actually measured by these past works. To this end, we introduce an auxiliary (noisy) DNN framework for which \$I(X;T)\$ is a meaningful quantity that depends on the network's parameters. This noisy framework is shown to be a good proxy for the original (deterministic) DNN both in terms of performance and the learned representations. We then develop a rigorous estimator for \$I(X;T)\$ in noisy DNNs and observe compression in various models. By relating \$I(X;T)\$ in the noisy DNN to an information-theoretic communication problem, we show that compression is driven by the progressive clustering of hidden representations of inputs from the same class. Several methods to directly monitor clustering of hidden representations, both in noisy and deterministic DNNs, are used to show that meaningful clusters form in the \$T\$ space. Finally, we return to the estimator of \$I(X;T)\$ employed in past works, and demonstrate that while it fails to capture the true (vacuous) mutual information, it does serve as a measure for clustering. This clarifies the past observations of compression and isolates the geometric clustering of hidden representations as the true phenomenon of interest.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/J65PJ6PC/Goldfeld et al. - 2019 - Estimating Information Flow in Deep Neural Network.pdf}
}

@article{gorton_missing_2024,
  title = {The Missing Curve Detectors of InceptionV1: Applying Sparse Autoencoders to InceptionV1 Early Vision},
  shorttitle = {The Missing Curve Detectors of InceptionV1},
  author = {Gorton, Liv},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=IGnoozsfj1},
  urldate = {2024-08-16},
  abstract = {Recent work on sparse autoencoders (SAEs) has shown promise in extracting interpretable features from neural networks and addressing challenges with polysemantic neurons caused by superposition. In this paper, we apply SAEs to the early vision layers of InceptionV1, a well-studied convolutional neural network, with a focus on curve detectors. Our results demonstrate that SAEs can uncover new interpretable features not apparent from examining individual neurons, including additional curve detectors that fill in previous gaps. We also find that SAEs can decompose some polysemantic neurons into more monosemantic constituent features. These findings suggest SAEs are a valuable tool for understanding InceptionV1, and convolutional neural networks more generally.},
  language = {en},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/8UMTIL5S/Gorton - 2024 - The Missing Curve Detectors of InceptionV1 Applyi.pdf}
}

@article{gross_compact_2024,
  title = {Compact Proofs of Model Performance via Mechanistic Interpretability},
  author = {Gross, Jason and Agrawal, Rajashree and Kwa, Thomas and Ong, Euan and Yip, Chun Hei and Gibson, Alex and Noubir, Soufiane and Chan, Lawrence},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  eprint = {2406.11779},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.11779},
  url = {http://arxiv.org/abs/2406.11779},
  urldate = {2024-06-25},
  abstract = {In this work, we propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving lower bounds on the accuracy of 151 small transformers trained on a Max-of-\$K\$ task. We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless noise as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/CACL64ET/Gross et al. - 2024 - Compact Proofs of Model Performance via Mechanisti.pdf}
}

@article{guo_robust_2024,
  title = {Robust Unlearning via Mechanistic Localizations},
  author = {Guo, Phillip Huang and Syed, Aaquib and Sheshadri, Abhay and Ewart, Aidan and Dziugaite, Gintare Karolina},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=06pNzrEjnH},
  urldate = {2024-08-16},
  abstract = {Methods for machine unlearning in large language models seek to remove undesirable knowledge or capabilities without compromising general language modeling performance. This work investigates the use of mechanistic interpretability to improve the precision and effectiveness of unlearning. We demonstrate that localizing unlearning to components with particular mechanisms in factual recall leads to more robust unlearning across different input/output formats, relearning, and latent knowledge, and reduces unintended side effects compared to nonlocalized unlearning. Additionally, we analyze the strengths and weaknesses of different automated (rather than manual) interpretability methods for guiding unlearning, finding that their corresponding unlearned models require smaller edit sizes to achieve unlearning but are much less robust.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/RMFXA3YF/Guo et al. - 2024 - Robust Unlearning via Mechanistic Localizations.pdf}
}

@article{gupta_interpbench_2024a,
  title = {InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques},
  shorttitle = {InterpBench},
  author = {Gupta, Rohan and Arcuschin, Iv{\'a}n and Kwa, Thomas and Garriga-Alonso, Adri{\`a}},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=YXhVojPivQ},
  urldate = {2024-11-07},
  abstract = {Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train these neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.},
  language = {en},
  keywords = {compiled transformers,icml,interpbench,spotlight},
  file = {/Users/leonardbereska/Zotero/storage/Z6WQPAQC/Gupta et al. - 2024 - InterpBench Semi-Synthetic Transformers for Evalu.pdf}
}

@article{gurnee_finding_2023,
  title = {Finding Neurons in a Haystack: Case Studies with Sparse Probing},
  shorttitle = {Finding Neurons in a Haystack},
  author = {Gurnee, Wes and Nanda, Neel and Pauly, Matthew and Harvey, Katherine and Troitskii, Dmitrii and Bertsimas, Dimitris},
  year = 2023,
  journal = {TMLR},
  publisher = {arXiv},
  url = {https://arxiv.org/abs/2305.01610},
  urldate = {2023-07-31},
  abstract = {Despite rapid adoption and deployment of large language models (LLMs), the internal computations of these models remain opaque and poorly understood. In this work, we seek to understand how high-level human-interpretable features are represented within the internal neuron activations of LLMs. We train \$k\$-sparse linear classifiers (probes) on these internal activations to predict the presence of features in the input; by varying the value of \$k\$ we study the sparsity of learned representations and how this varies with model scale. With \$k=1\$, we localize individual neurons which are highly relevant for a particular feature, and perform a number of case studies to illustrate general properties of LLMs. In particular, we show that early layers make use of sparse combinations of neurons to represent many features in superposition, that middle layers have seemingly dedicated neurons to represent higher-level contextual features, and that increasing scale causes representational sparsity to increase on average, but there are multiple types of scaling dynamics. In all, we probe for over 100 unique features comprising 10 different categories in 7 different models spanning 70 million to 6.9 billion parameters.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {empirical findings in superposition,neuron,probing,superposition},
  file = {/Users/leonardbereska/Zotero/storage/LBWU8LZZ/Gurnee et al. - 2023 - Finding Neurons in a Haystack Case Studies with S.pdf}
}

@article{hanna_have_2024a,
  title = {Have Faith in Faithfulness: Going Beyond Circuit Overlap When Finding Model Mechanisms},
  shorttitle = {Have Faith in Faithfulness},
  author = {Hanna, Michael and Pezzelle, Sandro and Belinkov, Yonatan},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=grXgesr5dT},
  urldate = {2024-11-07},
  abstract = {Many recent language model (LM) interpretability studies have adopted the circuits framework, which aims to find the minimal computational subgraph, or circuit, that explains LM behavior on a given task. Most studies determine which edges belong in a LM's circuit for a task by performing causal interventions on each edge independently, but this scales poorly with model size. As a solution, recent work has proposed edge attribution patching (EAP), a scalable gradient-based approximation to interventions. In this paper, we introduce a new method - EAP with integrated gradients (EAP-IG) - that aims to efficiently find circuits while better maintaining one of their core properties: faithfulness. A circuit is faithful if all model edges outside the circuit can be ablated without changing the model's behavior on the task; faithfulness is what justifies studying circuits, rather than the full model. Our experiments demonstrate that circuits found using EAP-IG are more faithful than those found using EAP, even though both have high node overlap with reference circuits found using causal interventions. We conclude more generally that when comparing circuits, measuring overlap is no substitute for measuring faithfulness.},
  language = {en},
  keywords = {icml},
  file = {/Users/leonardbereska/Zotero/storage/NEGAEDMB/Hanna et al. - 2024 - Have Faith in Faithfulness Going Beyond Circuit O.pdf}
}

@misc{hanni_decomposing_2023,
  title = {Decomposing Activations into Features: How Many and How do we Find Them? --- A Survey},
  author = {H{\"a}nni, Kaarel and Mendel, Jake and Kozaronek, Kay},
  year = 2023,
  publisher = {Cambridge University Press},
  url = {https://kaarelh.github.io/doc/decomposition.pdf},
  abstract = {One of the main goals of interpretability is to reverse-engineer neural networks. For this, a reasonable subgoal is to determine the ``variables in terms of which a neural network thinks''. In this paper, we run with the assumption that these variables correspond to particular vectors in activation space, which we call `features', whose linear combinations are the activation vectors on various inputs. We provide an exposition of certain techniques from the literature for reconstructing these features from activation data, discussing both approaches grounded in the assumption of there being many features, few of which are active on any one input; as well as approaches grounded in the assumption that there are relatively few features. We finish by suggesting experiments that might let us figure out which of these two options is closer to the truth.},
  language = {en},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/ZJY2ISDJ/Research and Mats - Decomposing Activations into Features How Many an.pdf}
}

@article{hanni_mathematical_2024,
  title = {Mathematical Models of Computation in Superposition},
  author = {H{\"a}nni, Kaarel and Mendel, Jake and Vaintrob, Dmitry and Chan, Lawrence},
  year = 2024,
  month = aug,
  journal = {ICML MI Workshop},
  eprint = {2408.05451},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2408.05451},
  url = {http://arxiv.org/abs/2408.05451},
  urldate = {2024-08-16},
  abstract = {Superposition -- when a neural network represents more ``features'' than it has dimensions -- seems to pose a serious challenge to mechanistically interpreting current AI systems. Existing theory work studies \textbackslash emph\textbraceleft representational\textbraceright{} superposition, where superposition is only used when passing information through bottlenecks. In this work, we present mathematical models of \textbackslash emph\textbraceleft computation\textbraceright{} in superposition, where superposition is actively helpful for efficiently accomplishing the task. We first construct a task of efficiently emulating a circuit that takes the AND of the \$\textbackslash binom\textbraceleft m\textbraceright\textbraceleft 2\textbraceright\$ pairs of each of \$m\$ features. We construct a 1-layer MLP that uses superposition to perform this task up to \$\textbackslash varepsilon\$-error, where the network only requires \$\textbackslash tilde\textbraceleft O\textbraceright (m\textasciicircum\textbraceleft\textbackslash frac\textbraceleft 2\textbraceright\textbraceleft 3\textbraceright\textbraceright )\$ neurons, even when the input features are \textbackslash emph\textbraceleft themselves in superposition\textbraceright. We generalize this construction to arbitrary sparse boolean circuits of low depth, and then construct ``error correction'' layers that allow deep fully-connected networks of width \$d\$ to emulate circuits of width \$\textbackslash tilde\textbraceleft O\textbraceright (d\textasciicircum\textbraceleft 1.5\textbraceright )\$ and \textbackslash emph\textbraceleft any\textbraceright{} polynomial depth. We conclude by providing some potential applications of our work for interpreting neural networks that implement computation in superposition.},
  archiveprefix = {arXiv},
  keywords = {computation in superposition,icml,sparse autoencoder,sparse feature circuits,spotlight,superposition},
  file = {/Users/leonardbereska/Zotero/storage/69BDC64S/Hänni et al. - 2024 - Mathematical Models of Computation in Superpositio.pdf}
}

@article{he_dictionary_2024,
  title = {Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT},
  shorttitle = {Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability},
  author = {He, Zhengfu and Ge, Xuyang and Tang, Qiong and Sun, Tianxiang and Cheng, Qinyuan and Qiu, Xipeng},
  year = 2024,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2402.12201},
  url = {https://arxiv.org/abs/2402.12201},
  urldate = {2024-06-10},
  abstract = {Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to attack superposition and extract more human-understandable features from model activations. We ask a further question based on the extracted more monosemantic features: How do we recognize circuits connecting the enormous amount of dictionary features? We propose a circuit discovery framework alternative to activation patching. Our framework suffers less from out-of-distribution and proves to be more efficient in terms of asymptotic complexity. The basic unit in our framework is dictionary features decomposed from all modules writing to the residual stream, including embedding, attention output and MLP output. Starting from any logit, dictionary feature or attention score, we manage to trace down to lower-level dictionary features of all tokens and compute their contribution to these more interpretable and local model behaviors. We dig in a small transformer trained on a synthetic task named Othello and find a number of human-understandable fine-grained circuits inside of it.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {circuit,othello,patching,sparse autoencoder,sparsity},
  file = {/Users/leonardbereska/Zotero/storage/88D27J2W/He et al. - 2024 - Dictionary Learning Improves Patch-Free Circuit Di.pdf}
}

@article{he_learning_2024,
  title = {Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks},
  shorttitle = {Learning to grok},
  author = {He, Tianyu and Doshi, Darshil and Das, Aritra and Gromov, Andrey},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=gz0r3w71zQ},
  urldate = {2024-08-16},
  abstract = {Large language models can solve tasks that were not present in the training set. This capability is believed to be due to in-context learning and skill composition. In this work, we study the emergence of in-context learning and skill composition in a collection of modular arithmetic tasks. Specifically, we consider a finite collection of linear modular functions \$z = a x + b y \textbackslash text\textbraceleft{} mod \textbraceright{} p\$ labeled by the vector \$(a, b) \textbackslash in \textbackslash mathbb\textbraceleft Z\textbraceright\_p\textasciicircum 2\$. We use some of these tasks for pre-training and the rest for out-of-distribution testing. We empirically show that a GPT-style transformer exhibits a transition from in-distribution to out-of-distribution generalization as the number of pre-training tasks increases. We find that the smallest model capable of out-of-distribution generalization requires two transformer blocks, while for deeper models, the out-of-distribution generalization phase is transient, necessitating early stopping. Finally, we perform an interpretability study of the pre-trained models, revealing the highly structured representations in both phases; and discuss the learnt algorithm.},
  language = {en},
  keywords = {grokking},
  file = {/Users/leonardbereska/Zotero/storage/655BLDTG/He et al. - 2024 - Learning to grok Emergence of in-context learning.pdf}
}

@article{henighan_superposition_2023,
  title = {Superposition, Memorization, and Double Descent},
  author = {Henighan, Tom and Carter, Shan and Hume, Tristan and Elhage, Nelson and Lasenby, Robert and Fort, Stanislav and Schiefer, Nicholas and Olah, Christopher},
  year = 2023,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/toy-double-descent/index.html},
  keywords = {explaining superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/KHZUIWJ6/Henighan et al. - 2023 - Superposition, Memorization, and Double Descent.html}
}

@article{higgins_betavae_2017,
  title = {beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework},
  shorttitle = {beta-VAE},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  year = 2017,
  journal = {ICLR},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  urldate = {2023-07-31},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial intelligence that is able to learn and reason in the same way that humans do. We introduce beta-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modification of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter beta that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that beta-VAE with appropriately tuned beta {$>$} 1 qualitatively outperforms VAE (beta = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also significantly outperforms all baselines quantitatively. Unlike InfoGAN, beta-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter, which can be directly optimised through a hyper parameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
  language = {en},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/QFGQ8XQC/Higgins et al. - 2017 - beta-VAE Learning Basic Visual Concepts with a Co.pdf}
}

@article{hill_diversity_1973,
  title = {Diversity and Evenness: A Unifying Notation and Its Consequences},
  shorttitle = {Diversity and Evenness},
  author = {Hill, Mark O.},
  year = 1973,
  month = mar,
  journal = {Ecology},
  volume = {54},
  number = {2},
  pages = {427--432},
  issn = {0012-9658, 1939-9170},
  doi = {10.2307/1934352},
  url = {https://esajournals.onlinelibrary.wiley.com/doi/10.2307/1934352},
  urldate = {2025-05-08},
  abstract = {Three commonly used measures of diversity, Simpson's index, Shannon's entropy, and the total number of species, are related to Renyi's definition of a generalized entropy. A unified concept of diversity is presented, according to which there is a continuum of possible diversity measures. In a sense which becomes apparent, these measures provide estimates of the effective number of species present, and differ only in their tendency to include or to ignore the relatively rarer species. The notion of the diversity of a community as opposed to that of a sample is examined, and is related to the asymptotic form of the species---abundance curve. A new and plausible definition of evenness is derived.},
  copyright = {http://onlinelibrary.wiley.com/termsAndConditions\#vor},
  language = {en}
}

@article{hindupur_projecting_2025,
  title = {Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry},
  shorttitle = {Projecting Assumptions},
  author = {Hindupur, Sai Sumedh R. and Lubana, Ekdeep Singh and Fel, Thomas and Ba, Demba},
  year = 2025,
  month = mar,
  journal = {CoRR},
  eprint = {2503.01822},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2503.01822},
  url = {http://arxiv.org/abs/2503.01822},
  urldate = {2025-05-09},
  abstract = {Sparse Autoencoders (SAEs) are widely used to interpret neural networks by identifying meaningful concepts from their representations. However, do SAEs truly uncover all concepts a model relies on, or are they inherently biased toward certain kinds of concepts? We introduce a unified framework that recasts SAEs as solutions to a bilevel optimization problem, revealing a fundamental challenge: each SAE imposes structural assumptions about how concepts are encoded in model representations, which in turn shapes what it can and cannot detect. This means different SAEs are not interchangeable -- switching architectures can expose entirely new concepts or obscure existing ones. To systematically probe this effect, we evaluate SAEs across a spectrum of settings: from controlled toy models that isolate key variables, to semi-synthetic experiments on real model activations and finally to large-scale, naturalistic datasets. Across this progression, we examine two fundamental properties that real-world concepts often exhibit: heterogeneity in intrinsic dimensionality (some concepts are inherently low-dimensional, others are not) and nonlinear separability. We show that SAEs fail to recover concepts when these properties are ignored, and we design a new SAE that explicitly incorporates both, enabling the discovery of previously hidden concepts and reinforcing our theoretical insights. Our findings challenge the idea of a universal SAE and underscores the need for architecture-specific choices in model interpretability. Overall, we argue an SAE does not just reveal concepts -- it determines what can be seen at all.},
  archiveprefix = {arXiv},
  keywords = {to cite},
  file = {/Users/leonardbereska/Zotero/storage/KD2KWRGD/Hindupur et al. - 2025 - Projecting Assumptions The Duality Between Sparse.pdf}
}

@article{hinton_distributed_1984,
  title = {Distributed representations},
  author = {Hinton, Geoffrey E},
  year = 1984,
  journal = {Carnegie Mellon University},
  publisher = {Carnegie Mellon University},
  url = {https://www.cs.toronto.edu/~hinton/absps/pdp3.pdf},
  abstract = {Given a\^  network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements. Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind. \textasciicircum This research was supported by a grant from the System Development Foundation. I thank Jim Anderson, Dave Ackley Dana Ballard, Francis Crick, Scott Fahlman, Jerry Feldman, Christopher Longuet-Higgins, Don Norman, Terry Sejnowski, and Tim Shallice for helpful discussions. Jay McClelland and Dave Rumelhart helped me refine and rewrite many of the ideas presented here A substantially revised version of this report will appear as a chapter by Hinton, McClelland and Rumelhart in Parallel Distributed Processing: Explorations in the micro-structure of cognition, edited by McClelland and Rumelhart)},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/4B5AVHFS/Hinton - 1984 - Distributed representations.pdf}
}

@article{hoagy_additional_2024,
  title = {Some additional SAE thoughts},
  author = {Hoagy},
  year = 2024,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/fqgn56tS5AgjmDpnX/some-additional-sae-thoughts},
  urldate = {2024-02-16},
  abstract = {Thanks to Lee Sharkey for feedback on the first section and Lee Sharkey, Jake Mendel, Kaarel H\"anni and others at the LISA office for conversations ar\dots},
  language = {en},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/XNVTFL7G/Hoagy - 2024 - Some additional SAE thoughts.html}
}

@article{hoogland_developmental_2024,
  title = {The Developmental Landscape of In-Context Learning},
  author = {Hoogland, Jesse and Wang, George and Farrugia-Roberts, Matthew and Carroll, Liam and Wei, Susan and Murfet, Daniel},
  year = 2024,
  month = feb,
  journal = {CoRR},
  doi = {10.48550/arXiv.2402.02364},
  url = {http://arxiv.org/abs/2402.02364},
  urldate = {2024-03-19},
  abstract = {We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.},
  keywords = {developmental},
  file = {/Users/leonardbereska/Zotero/storage/42SIA2XT/Hoogland et al. - 2024 - The Developmental Landscape of In-Context Learning.pdf}
}

@article{hoogland_stagewise_2024,
  title = {Stagewise Development in Neural Networks},
  author = {Hoogland, Jesse and Carroll, Liam and Murfet, Daniel},
  year = 2024,
  month = mar,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/Zza9MNA7YtHkzAtit/stagewise-development-in-neural-networks},
  urldate = {2024-08-21},
  abstract = {{$>$} TLDR: This post accompanies The Developmental Landscape of In-Context Learning by Jesse Hoogland, George Wang, Matthew Farrugia-Roberts, Liam Carro\dots},
  language = {en}
}

@article{huang_inversionview_2024,
  title = {InversionView: A General-Purpose Method for Reading Information from Neural Activations},
  shorttitle = {InversionView},
  author = {Huang, Xinting and Panwar, Madhur and Goyal, Navin and Hahn, Michael},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop (Oral)},
  url = {https://openreview.net/forum?id=P7MW0FahEq},
  urldate = {2024-08-16},
  abstract = {The inner workings of neural networks can be better understood if we can fully decipher the information encoded in neural activations. In this paper, we argue that this information is embodied by the subset of inputs that give rise to similar activations. Computing such subsets is nontrivial as the input space is exponentially large. We propose InversionView, which allows us to practically inspect this subset by sampling from a trained decoder model conditioned on activations. This helps uncover the information content of activation vectors, and facilitates understanding of the algorithms implemented by transformer models. We present four case studies where we investigate models ranging from small transformers to GPT-2. In these studies, we demonstrate the characteristics of our method, show the distinctive advantages it offers, and provide causally verified circuits.},
  language = {en},
  keywords = {icml,oral},
  file = {/Users/leonardbereska/Zotero/storage/YMUGMAMN/Huang et al. - 2024 - InversionView A General-Purpose Method for Readin.pdf}
}

@article{ilyas_adversarial_2019,
  title = {Adversarial Examples Are Not Bugs, They Are Features},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  year = 2019,
  month = aug,
  journal = {NeurIPS},
  eprint = {1905.02175},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.02175},
  url = {http://arxiv.org/abs/1905.02175},
  urldate = {2023-11-29},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/9KITH3U2/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf}
}

@article{jain_what_2024,
  title = {What Makes and Breaks Safety Fine-tuning? A Mechanistic Study},
  shorttitle = {What Makes and Breaks Safety Fine-tuning?},
  author = {Jain, Samyak and Lubana, Ekdeep Singh and Oksuz, Kemal and Joy, Tom and Torr, Philip and Sanyal, Amartya and Dokania, Puneet K.},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=BS2CbUkJpy},
  urldate = {2024-08-16},
  abstract = {Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., "design") versus the specific concepts the task is asked to be performed upon (e.g., a "cycle" vs. a "bomb"). Using this, we investigate three well-known safety fine-tuning methods---supervised safety fine-tuning, direct preference optimization, and unlearning---and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights' null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. We validate our findings, wherever possible, on real-world models---specifically, Llama-2 7B and Llama-3 8B.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/ZFM69P7K/Jain et al. - 2024 - What Makes and Breaks Safety Fine-tuning A Mechan.pdf}
}

@article{jake_mendel_sae_2024,
  title = {SAE feature geometry is outside the superposition hypothesis},
  author = {Mendel, Jake},
  year = 2024,
  month = jun,
  url = {https://www.alignmentforum.org/posts/MFBTjb2qf3ziWmzz6/sae-feature-geometry-is-outside-the-superposition-hypothesis},
  urldate = {2024-12-03},
  abstract = {Summary: Superposition-based interpretations of neural network activation spaces are incomplete. The specific locations of feature vectors contain crucial structural information beyond superposition, as seen in circular arrangements of day-of-the-week features and in the rich structures of feature UMAPs. We don't currently have good concepts for talking about this structure in feature geometry, but it is likely very important for model computation. An eventual understanding of feature geometry might look like a hodgepodge of case-specific explanations, or supplementing superposition with additional concepts, or plausibly an entirely new theory that supersedes superposition. To develop this understanding, it may be valuable to study toy models in depth and do theoretical or conceptual work in addition to studying frontier models.  Epistemic status: Decently confident that the ideas here are directionally correct. I've been thinking these thoughts for a while, and recently got round to writing them up at a high level. Lots of people (including both SAE stans and SAE skeptics) have thought very similar things before and some of them have written about it in various places too. Some of my views, especially the merit of certain research approaches to tackle the problems I highlight, have been presented here without my best attempt to argue for them.},
  language = {en},
  keywords = {beyond superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/Z8VH7QL3/Mendel - 2024 - SAE feature geometry is outside the superposition .html}
}

@article{jake_mendel_sae_2024a,
  title = {SAE feature geometry is outside the superposition hypothesis},
  author = {Mendel, Jake},
  year = 2024,
  month = jun,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/MFBTjb2qf3ziWmzz6/sae-feature-geometry-is-outside-the-superposition-hypothesis},
  urldate = {2024-12-20},
  abstract = {Written at Apollo Research {$\bullet$} Summary: Superposition-based interpretations of neural network activation spaces are incomplete. The specific locations\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/4YNUJF7A/jake_mendel - 2024 - SAE feature geometry is outside the superposition .html}
}

@article{jaynes_information_1957,
  title = {Information Theory and Statistical Mechanics},
  author = {Jaynes, Edwin T.},
  year = 1957,
  month = may,
  journal = {Phys. Rev.},
  volume = {106},
  number = {4},
  pages = {620--630},
  issn = {0031-899X},
  doi = {10.1103/PhysRev.106.620},
  url = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
  urldate = {2025-05-08},
  abstract = {Treatment of the predictive aspect of statistical mechanics as a form of statistical inference is extended to the density-matrix formalism and applied to a discussion of the relation between irreversibility and information loss. A principle of "statistical complementarity" is pointed out, according to which the empirically verifiable probabilities of statistical mechanics necessarily correspond to incomplete predictions. A preliminary discussion is given of the second law of thermodynamics and of a certain class of irreversible processes, in an approximation equivalent to that of the semiclassical theory of radiation.},
  copyright = {http://link.aps.org/licenses/aps-default-license},
  language = {en}
}

@article{jelinek_perplexity_1977,
  title = {Perplexity---a measure of the difficulty of speech recognition tasks},
  author = {Jelinek, Frederick and Mercer, Robert L. and Bahl, Lalit R. and Baker, {\relax Janet}. K.},
  year = 1977,
  month = dec,
  journal = {The Journal of the Acoustical Society of America},
  volume = {62},
  number = {S1},
  pages = {S63-S63},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.2016299},
  url = {https://pubs.aip.org/jasa/article/62/S1/S63/642598/Perplexity-a-measure-of-the-difficulty-of-speech},
  urldate = {2025-05-08},
  abstract = {Using counterexamples, we show that vocabulary size and static and dynamic branching factors are all inadequate as measures of speech recognition complexity of finite state grammars. Information theoretic arguments show that perplexity (the logarithm of which is the familiar entropy) is a more appropriate measure of equivalent choice. It too has certain weaknesses which we discuss. We show that perplexity can also be applied to languages having no obvious statistical description, since an entropy-maximizing probability assignment can be found for any finite-state grammar. Table I shows perplexity values for some well-known speech recognition tasks.             Perplexity Vocabulary Dynamic             Phone Word size branching factor             IBM-Lasers 2.14 21.11 1000 1000             IBM-Raleigh 1.69 7.74 250 7.32             CMU-AIX05 1.52 6.41 1011 35},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/935BUVLR/Jelinek et al. - 1977 - Perplexity—a measure of the difficulty of speech r.pdf}
}

@article{jermyn_circuits_2023,
  title = {Circuits updates - May 2023: Attention Head Superposition},
  author = {Jermyn, Adam and Olah, Chris and Henighan, T},
  year = 2023,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/may-update/index.html},
  keywords = {empirical findings in superposition,superposition}
}

@article{jermyn_engineering_2022,
  title = {Engineering Monosemanticity in Toy Models},
  author = {Jermyn, Adam S. and Schiefer, Nicholas and Hubinger, Evan},
  year = 2022,
  month = nov,
  journal = {CoRR},
  eprint = {2211.09169},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2211.09169},
  urldate = {2023-07-25},
  abstract = {In some neural networks, individual neurons correspond to natural ``features'' in the input. Such \textbackslash emph\textbraceleft monosemantic\textbraceright{} neurons are of great help in interpretability studies, as they can be cleanly understood. In this work we report preliminary attempts to engineer monosemanticity in toy models. We find that models can be made more monosemantic without increasing the loss by just changing which local minimum the training process finds. More monosemantic loss minima have moderate negative biases, and we are able to use this fact to engineer highly monosemantic models. We are able to mechanistically interpret these models, including the residual polysemantic neurons, and uncover a simple yet surprising algorithm. Finally, we find that providing models with more neurons per layer makes the models more monosemantic, albeit at increased computational cost. These findings point to a number of new questions and avenues for engineering monosemanticity, which we intend to study these in future work.},
  archiveprefix = {arXiv},
  keywords = {fundamental,intrinsic interpretability,superposition,toy models},
  file = {/Users/leonardbereska/Zotero/storage/XXRR9944/Jermyn et al. - 2022 - Engineering Monosemanticity in Toy Models.pdf}
}

@article{jost_entropy_2006,
  title = {Entropy and diversity},
  author = {Jost, Lou},
  year = 2006,
  month = may,
  journal = {Oikos},
  volume = {113},
  number = {2},
  pages = {363--375},
  issn = {0030-1299, 1600-0706},
  doi = {10.1111/j.2006.0030-1299.14714.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.2006.0030-1299.14714.x},
  urldate = {2025-05-08},
  abstract = {Entropies such as the Shannon--Wiener and Gini--Simpson indices are not themselves diversities. Conversion of these to effective number of species is the key to a unified and intuitive interpretation of diversity. Effective numbers of species derived from standard diversity indices share a common set of intuitive mathematical properties and behave as one would expect of a diversity, while raw indices do not. Contrary to Keylock, the lack of concavity of effective numbers of species is irrelevant as long as they are used as transformations of concave alpha, beta, and gamma entropies. The practical importance of this transformation is demonstrated by applying it to a popular community similarity measure based on raw diversity indices or entropies. The standard similarity measure based on untransformed indices is shown to give misleading results, but transforming the indices or entropies to effective numbers of species produces a stable, easily interpreted, sensitive general similarity measure. General overlap measures derived from this transformed similarity measure yield the Jaccard index, S\o rensen index, Horn index of overlap, and the Morisita--Horn index as special cases.},
  language = {en}
}

@article{kangaslahti_loss_2024,
  title = {Loss in the Crowd: Hidden Breakthroughs in Language Model Training},
  shorttitle = {Loss in the Crowd},
  author = {Kangaslahti, Sara and Rosenfeld, Elan and Saphra, Naomi},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=Os3z6Oczuu},
  urldate = {2024-08-16},
  abstract = {The training loss curves of a neural network are typically smooth. Any visible discontinuities draw attention as discrete conceptual breakthroughs, while the rest of training is less carefully studied. In this work we hypothesize that similar breakthroughs actually occur frequently throughout training, though their presence is obscured when monitoring the aggregate train loss. To find these hidden transitions, we introduce POLCA, a method for decomposing changes in loss along an arbitrary basis of the low rank training subspace. We use our method to identify clusters of samples that exhibit similar changes in loss through training, disaggregating the overall loss into that of smaller groups of conceptually similar datapoints. We validate our method on synthetic arithmetic, showing that POLCA recovers clusters which represent easily interpretable breakthroughs in the model's capabilities whose existence would otherwise be lost in the crowd.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/YFB2V2II/Kangaslahti et al. - 2024 - Loss in the Crowd Hidden Breakthroughs in Languag.pdf}
}

@article{kantamneni_are_2025,
  title = {Are Sparse Autoencoders Useful? A Case Study in Sparse Probing},
  shorttitle = {Are Sparse Autoencoders Useful?},
  author = {Kantamneni, Subhash and Engels, Joshua and Rajamanoharan, Senthooran and Tegmark, Max and Nanda, Neel},
  year = 2025,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2502.16681},
  url = {https://arxiv.org/abs/2502.16681},
  urldate = {2025-05-07},
  abstract = {Sparse autoencoders (SAEs) are a popular method for interpreting concepts represented in large language model (LLM) activations. However, there is a lack of evidence regarding the validity of their interpretations due to the lack of a ground truth for the concepts used by an LLM, and a growing number of works have presented problems with current SAEs. One alternative source of evidence would be demonstrating that SAEs improve performance on downstream tasks beyond existing baselines. We test this by applying SAEs to the real-world task of LLM activation probing in four regimes: data scarcity, class imbalance, label noise, and covariate shift. Due to the difficulty of detecting concepts in these challenging settings, we hypothesize that SAEs' basis of interpretable, concept-level latents should provide a useful inductive bias. However, although SAEs occasionally perform better than baselines on individual datasets, we are unable to design ensemble methods combining SAEs with baselines that consistently outperform ensemble methods solely using baselines. Additionally, although SAEs initially appear promising for identifying spurious correlations, detecting poor dataset quality, and training multi-token probes, we are able to achieve similar results with simple non-SAE baselines as well. Though we cannot discount SAEs' utility on other tasks, our findings highlight the shortcomings of current SAEs and the need to rigorously evaluate interpretability methods on downstream tasks with strong baselines.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/EM3LWR6J/Kantamneni et al. - 2025 - Are Sparse Autoencoders Useful A Case Study in Sparse Probing.pdf}
}

@article{karvonen_measuring_2024,
  title = {Measuring Progress in Dictionary Learning for Language Model Interpretability with Board Game Models},
  author = {Karvonen, Adam and Wright, Benjamin and Rager, Can and Angell, Rico and Brinkmann, Jannik and Smith, Logan and Verdun, C. M. and Bau, David and Marks, Samuel},
  year = 2024,
  month = jul,
  journal = {NeurIPS, ICML MI Workshop (Oral)},
  url = {https://www.semanticscholar.org/paper/Measuring-Progress-in-Dictionary-Learning-for-Model-Karvonen-Wright/b244b80d0a2d36412b56c0156532d9cbeb298ffa},
  urldate = {2024-08-08},
  abstract = {What latent features are encoded in language model (LM) representations? Recent work on training sparse autoencoders (SAEs) to disentangle interpretable features in LM representations has shown significant promise. However, evaluating the quality of these SAEs is difficult because we lack a ground-truth collection of interpretable features that we expect good SAEs to recover. We thus propose to measure progress in interpretable dictionary learning by working in the setting of LMs trained on chess and Othello transcripts. These settings carry natural collections of interpretable features -- for example,"there is a knight on F3"-- which we leverage into \$\textbackslash textit\textbraceleft supervised\textbraceright\$ metrics for SAE quality. To guide progress in interpretable dictionary learning, we introduce a new SAE training technique, \$\textbackslash textit\textbraceleft p-annealing\textbraceright\$, which improves performance on prior unsupervised metrics as well as our new metrics.},
  keywords = {icml,metrics,oral,sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/Z8PR7SZ7/Karvonen et al. - 2024 - Measuring Progress in Dictionary Learning for Lang.pdf}
}

@article{kim_disentangling_2018,
  title = {Disentangling by Factorising},
  author = {Kim, Hyunjik and Mnih, Andriy},
  year = 2018,
  journal = {ICML},
  eprint = {1802.05983},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1802.05983},
  url = {http://arxiv.org/abs/1802.05983},
  urldate = {2023-07-31},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \$\textbackslash beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  archiveprefix = {arXiv},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/PTHF2YBH/Kim and Mnih - 2018 - Disentangling by Factorising.pdf}
}

@article{kissane_interpreting_2024,
  title = {Interpreting Attention Layer Outputs with Sparse Autoencoders},
  author = {Kissane, Connor and Krzyzanowski, Robert and Bloom, Joseph Isaac and Conmy, Arthur and Nanda, Neel},
  year = 2024,
  month = jun,
  journal = {CoRR},
  eprint = {2406.17759},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.17759},
  url = {http://arxiv.org/abs/2406.17759},
  urldate = {2025-03-03},
  abstract = {Decomposing model activations into interpretable components is a key open problem in mechanistic interpretability. Sparse autoencoders (SAEs) are a popular method for decomposing the internal activations of trained transformers into sparse, interpretable features, and have been applied to MLP layers and the residual stream. In this work we train SAEs on attention layer outputs and show that also here SAEs find a sparse, interpretable decomposition. We demonstrate this on transformers from several model families and up to 2B parameters. We perform a qualitative study of the features computed by attention layers, and find multiple families: long-range context, short-range context and induction features. We qualitatively study the role of every head in GPT-2 Small, and estimate that at least 90\% of the heads are polysemantic, i.e. have multiple unrelated roles. Further, we show that Sparse Autoencoders are a useful tool that enable researchers to explain model behavior in greater detail than prior work. For example, we explore the mystery of why models have so many seemingly redundant induction heads, use SAEs to motivate the hypothesis that some are long-prefix whereas others are short-prefix, and confirm this with more rigorous analysis. We use our SAEs to analyze the computation performed by the Indirect Object Identification circuit (Wang et al.), validating that the SAEs find causally meaningful intermediate variables, and deepening our understanding of the semantics of the circuit. We open-source the trained SAEs and a tool for exploring arbitrary prompts through the lens of Attention Output SAEs.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/LL8TAT6R/Kissane et al. - 2024 - Interpreting Attention Layer Outputs with Sparse A.pdf}
}

@article{koh_faithful_2024,
  title = {Faithful and Fast Influence Function via Advanced Sampling},
  author = {Koh, Jungyeon and Lyu, Hyeonsu and Jang, Jonggyu and Yang, Hyun Jong},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=TTVPbaxXjR},
  urldate = {2024-08-16},
  abstract = {How can we explain the influence of training data on black-box models? Influence functions (IFs) offer a post-hoc solution by utilizing gradients and Hessians. However, computing the Hessian for an entire dataset is resource-intensive, necessitating a feasible alternative. A common approach involves randomly sampling a small subset of the training data, but this method often results in highly inconsistent IF estimates due to the high variance in sample configurations. To address this, we propose two advanced sampling techniques based on features and logits. These samplers select a small yet representative subset of the entire dataset by considering the stochastic distribution of features or logits, thereby enhancing the accuracy of IF estimations. We validate our approach through class removal experiments, a typical application of IFs, using the F1-score to measure how effectively the model forgets the removed class while maintaining inference consistency on the remaining classes. Our method reduces computation time by 30.1\% and memory usage by 42.2\%, or improves the F1-score by 2.5\% compared to the baseline. Our code will be available at https://github.com/jungyeonkoh/samplingIF.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/YNM7G4Y3/Koh et al. - 2024 - Faithful and Fast Influence Function via Advanced .pdf}
}

@article{kovacevic_life_2007,
  title = {Life Beyond Bases: The Advent of Frames (Part I)},
  shorttitle = {Life Beyond Bases},
  author = {Kovacevic, Jelena and Chebira, Amina},
  year = 2007,
  month = jul,
  journal = {IEEE Signal Process. Mag.},
  volume = {24},
  number = {4},
  pages = {86--104},
  issn = {1053-5888},
  doi = {10.1109/MSP.2007.4286567},
  url = {http://ieeexplore.ieee.org/document/4286567/},
  urldate = {2024-12-06},
  abstract = {Coming to the end of Part 1, we hope you have a different picture of a frame in your mind from a "picture frame." While necessarily colored by our personal bias, we intended this tutorial as a basic introduction to frames, geared primarily toward engineering students and those without extensive mathematical training. Frames are here to stay; as wavelet bases before them, they are becoming a standard tool in the signal processing toolbox, spurred by a host of recent applications requiring some level of redundancy.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  keywords = {matrix effective dimensionality measures}
}

@article{lai_sae_2025,
  title = {SAE regularization produces more interpretable models},
  author = {Lai, Peter and StefanHex},
  year = 2025,
  month = jan,
  url = {https://www.lesswrong.com/posts/sYFNGRdDQYQrSJAd8/sae-regularization-produces-more-interpretable-models},
  urldate = {2025-01-29},
  abstract = {Sparse Autoencoders (SAEs) are useful for providing insight into how a model processes and represents information. A key goal is to represent languag\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/C53A4IDF/Lai and StefanHex - 2025 - SAE regularization produces more interpretable mod.html}
}

@article{lan_sparse_2025,
  title = {Sparse Autoencoders Reveal Universal Feature Spaces Across Large Language Models},
  author = {Lan, Michael and Torr, Philip and Meek, Austin and Khakzar, Ashkan and Krueger, David and Barez, Fazl},
  year = 2025,
  month = jan,
  journal = {CoRR},
  eprint = {2410.06981},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.06981},
  url = {http://arxiv.org/abs/2410.06981},
  urldate = {2025-03-03},
  abstract = {We investigate feature universality in large language models (LLMs), a research field that aims to understand how different models similarly represent concepts in the latent spaces of their intermediate layers. Demonstrating feature universality allows discoveries about latent representations to generalize across several models. However, comparing features across LLMs is challenging due to polysemanticity, in which individual neurons often correspond to multiple features rather than distinct ones, making it difficult to disentangle and match features across different models. To address this issue, we employ a method known as dictionary learning by using sparse autoencoders (SAEs) to transform LLM activations into more interpretable spaces spanned by neurons corresponding to individual features. After matching feature neurons across models via activation correlation, we apply representational space similarity metrics on SAE feature spaces across different LLMs. Our experiments reveal significant similarities in SAE feature spaces across various LLMs, providing new evidence for feature universality.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/T7PUGTB6/Lan et al. - 2025 - Sparse Autoencoders Reveal Universal Feature Space.pdf}
}

@article{lau_local_2024,
  title = {The Local Learning Coefficient: A Singularity-Aware Complexity Measure},
  shorttitle = {The Local Learning Coefficient},
  author = {Lau, Edmund and Furman, Zach and Wang, George and Murfet, Daniel and Wei, Susan},
  year = 2024,
  month = sep,
  journal = {CoRR},
  eprint = {2308.12108},
  doi = {10.48550/arXiv.2308.12108},
  url = {http://arxiv.org/abs/2308.12108},
  urldate = {2024-11-25},
  abstract = {The Local Learning Coefficient (LLC) is introduced as a novel complexity measure for deep neural networks (DNNs). Recognizing the limitations of traditional complexity measures, the LLC leverages Singular Learning Theory (SLT), which has long recognized the significance of singularities in the loss landscape geometry. This paper provides an extensive exploration of the LLC's theoretical underpinnings, offering both a clear definition and intuitive insights into its application. Moreover, we propose a new scalable estimator for the LLC, which is then effectively applied across diverse architectures including deep linear networks up to 100M parameters, ResNet image models, and transformer language models. Empirical evidence suggests that the LLC provides valuable insights into how training heuristics might influence the effective complexity of DNNs. Ultimately, the LLC emerges as a crucial tool for reconciling the apparent contradiction between deep learning's complexity and the principle of parsimony.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/KCMUMDBB/Lau et al. - 2023 - Quantifying degeneracy in singular models via the .pdf}
}

@article{leask_sparse_2025,
  title = {Sparse Autoencoders Do Not Find Canonical Units of Analysis},
  author = {Leask, Patrick and Bussmann, Bart and Pearce, Michael and Bloom, Joseph and Tigges, Curt and Moubayed, Noura Al and Sharkey, Lee and Nanda, Neel},
  year = 2025,
  journal = {ICLR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2502.04878},
  url = {https://arxiv.org/abs/2502.04878},
  urldate = {2025-05-07},
  abstract = {A common goal of mechanistic interpretability is to decompose the activations of neural networks into features: interpretable properties of the input computed by the model. Sparse autoencoders (SAEs) are a popular method for finding these features in LLMs, and it has been postulated that they can be used to find a \textbackslash textit\textbraceleft canonical\textbraceright{} set of units: a unique and complete list of atomic features. We cast doubt on this belief using two novel techniques: SAE stitching to show they are incomplete, and meta-SAEs to show they are not atomic. SAE stitching involves inserting or swapping latents from a larger SAE into a smaller one. Latents from the larger SAE can be divided into two categories: \textbackslash emph\textbraceleft novel latents\textbraceright, which improve performance when added to the smaller SAE, indicating they capture novel information, and \textbackslash emph\textbraceleft reconstruction latents\textbraceright, which can replace corresponding latents in the smaller SAE that have similar behavior. The existence of novel features indicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on the decoder matrix of another SAE -- we find that latents in SAEs often decompose into combinations of latents from a smaller SAE, showing that larger SAE latents are not atomic. The resulting decompositions are often interpretable; e.g. a latent representing ``Einstein'' decomposes into ``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find canonical units of analysis, they may still be useful tools. We suggest that future research should either pursue different approaches for identifying such units, or pragmatically choose the SAE size suited to their task. We provide an interactive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/CVXMH6EV/Leask et al. - 2025 - Sparse Autoencoders Do Not Find Canonical Units of Analysis.pdf}
}

@article{lecomte_incidental_2023,
  title = {Incidental Polysemanticity},
  author = {Lecomte, Victor and Thaman, Kushal and Chow, Trevor and Schaeffer, Rylan and Koyejo, Sanmi},
  year = 2023,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.03096},
  url = {https://arxiv.org/abs/2312.03096},
  urldate = {2024-02-11},
  abstract = {Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more "features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments. This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Due to its origin, we term this \textbackslash textit\textbraceleft incidental polysemanticity\textbraceright.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {polysemanticity,understanding polysemanticity},
  file = {/Users/leonardbereska/Zotero/storage/9BTYRJTG/Lecomte et al. - 2023 - Incidental Polysemanticity.pdf}
}

@article{lee_efficient_2006,
  title = {Efficient sparse coding algorithms},
  author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew},
  year = 2006,
  journal = {NeurIPS},
  volume = {19},
  url = {https://proceedings.neurips.cc/paper_files/paper/2006/hash/2d71b2ae158c7c5912cc0bbde2bb9d95-Abstract.html},
  urldate = {2024-01-23},
  abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1 -regularized least squares problem and an L2 -constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/CSF9UJI8/Lee et al. - 2006 - Efficient sparse coding algorithms.pdf}
}

@article{lee_estimating_2023,
  title = {Estimating Entanglement Entropy via Variational Quantum Circuits with Classical Neural Networks},
  author = {Lee, Sangyun and Kwon, Hyukjoon and Lee, Jae Sung},
  year = 2023,
  month = dec,
  journal = {CoRR},
  eprint = {2307.13511},
  primaryclass = {quant-ph},
  doi = {10.48550/arXiv.2307.13511},
  url = {http://arxiv.org/abs/2307.13511},
  urldate = {2025-06-05},
  abstract = {Entropy plays a crucial role in both physics and information science, encompassing classical and quantum domains. In this work, we present the Quantum Neural Entropy Estimator (QNEE), a novel approach that combines classical neural network (NN) with variational quantum circuits to estimate the von Neumann and Renyi entropies of a quantum state. QNEE provides accurate estimates of entropy while also yielding the eigenvalues and eigenstates of the input density matrix. Leveraging the capabilities of classical NN, QNEE can classify different phases of quantum systems that accompany the changes of entanglement entropy. Our numerical simulation demonstrates the effectiveness of QNEE by applying it to the 1D XXZ Heisenberg model. In particular, QNEE exhibits high sensitivity in estimating entanglement entropy near the phase transition point. We expect that QNEE will serve as a valuable tool for quantum entropy estimation and phase classification.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/EJN2IQXP/Lee et al. - 2023 - Estimating Entanglement Entropy via Variational Qu.pdf}
}

@article{li_geometry_2024,
  title = {The Geometry of Concepts: Sparse Autoencoder Feature Structure},
  shorttitle = {The Geometry of Concepts},
  author = {Li, Yuxiao and Michaud, Eric J. and Baek, David D. and Engels, Joshua and Sun, Xiaoqing and Tegmark, Max},
  year = 2024,
  month = oct,
  journal = {CoRR},
  eprint = {2410.19750},
  primaryclass = {q-bio},
  doi = {10.48550/arXiv.2410.19750},
  url = {http://arxiv.org/abs/2410.19750},
  urldate = {2024-12-20},
  abstract = {Sparse autoencoders have recently produced dictionaries of high-dimensional vectors corresponding to the universe of concepts represented by large language models. We find that this concept universe has interesting structure at three levels: 1) The "atomic" small-scale structure contains "crystals" whose faces are parallelograms or trapezoids, generalizing well-known examples such as (man-woman-king-queen). We find that the quality of such parallelograms and associated function vectors improves greatly when projecting out global distractor directions such as word length, which is efficiently done with linear discriminant analysis. 2) The "brain" intermediate-scale structure has significant spatial modularity; for example, math and code features form a "lobe" akin to functional lobes seen in neural fMRI images. We quantify the spatial locality of these lobes with multiple metrics and find that clusters of co-occurring features, at coarse enough scale, also cluster together spatially far more than one would expect if feature geometry were random. 3) The "galaxy" scale large-scale structure of the feature point cloud is not isotropic, but instead has a power law of eigenvalues with steepest slope in middle layers. We also quantify how the clustering entropy depends on the layer.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/92VL3C2Q/Li et al. - 2024 - The Geometry of Concepts Sparse Autoencoder Featu.pdf}
}

@article{liao_assessing_2023,
  title = {Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy},
  author = {Liao, Danqi and Liu, Chen and Christensen, Benjamin W. and Tong, Alexander and Huguet, Guillaume and Wolf, Guy and Nickel, Maximilian and Adelstein, Ian and Krishnaswamy, Smita},
  year = 2023,
  month = dec,
  journal = {CoRR},
  eprint = {2312.04823},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.04823},
  url = {http://arxiv.org/abs/2312.04823},
  urldate = {2025-06-05},
  abstract = {Entropy and mutual information in neural networks provide rich information on the learning process, but they have proven difficult to compute reliably in high dimensions. Indeed, in noisy and high-dimensional data, traditional estimates in ambient dimensions approach a fixed entropy and are prohibitively hard to compute. To address these issues, we leverage data geometry to access the underlying manifold and reliably compute these information-theoretic measures. Specifically, we define diffusion spectral entropy (DSE) in neural representations of a dataset as well as diffusion spectral mutual information (DSMI) between different variables representing data. First, we show that they form noise-resistant measures of intrinsic dimensionality and relationship strength in high-dimensional simulated data that outperform classic Shannon entropy, nonparametric estimation, and mutual information neural estimation (MINE). We then study the evolution of representations in classification networks with supervised learning, self-supervision, or overfitting. We observe that (1) DSE of neural representations increases during training; (2) DSMI with the class label increases during generalizable learning but stays stagnant during overfitting; (3) DSMI with the input signal shows differing trends: on MNIST it increases, while on CIFAR-10 and STL-10 it decreases. Finally, we show that DSE can be used to guide better network initialization and that DSMI can be used to predict downstream classification accuracy across 962 models on ImageNet. The official implementation is available at https://github.com/ChenLiu-1996/DiffusionSpectralEntropy.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/JBNTBLBS/Liao et al. - 2023 - Assessing Neural Network Representations During Tr.pdf}
}

@article{lindner_tracr_2023,
  title = {Tracr: Compiled Transformers as a Laboratory for Interpretability},
  shorttitle = {Tracr},
  author = {Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
  year = 2023,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.05062},
  url = {https://arxiv.org/abs/2301.05062},
  urldate = {2023-07-31},
  abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {algorithms,compiled transformers,computation in superposition,interpbench,superposition,tool,toy models},
  file = {/Users/leonardbereska/Zotero/storage/QBYGQR36/Lindner et al. - 2023 - Tracr Compiled Transformers as a Laboratory for I.pdf}
}

@article{liu_delving_2017,
  title = {Delving into Transferable Adversarial Examples and Black-box Attacks},
  author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
  year = 2017,
  month = feb,
  journal = {CoRR},
  eprint = {1611.02770},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1611.02770},
  url = {http://arxiv.org/abs/1611.02770},
  urldate = {2024-08-03},
  abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/QSK4QTJ8/Liu et al. - 2017 - Delving into Transferable Adversarial Examples and.pdf}
}

@article{liu_seeing_2023,
  title = {Seeing is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability},
  shorttitle = {Seeing is Believing},
  author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
  year = 2023,
  month = jun,
  journal = {Entropy},
  eprint = {2305.08746},
  primaryclass = {cond-mat, q-bio},
  doi = {10.48550/arXiv.2305.08746},
  url = {http://arxiv.org/abs/2305.08746},
  urldate = {2023-07-31},
  abstract = {We introduce Brain-Inspired Modular Training (BIMT), a method for making neural networks more modular and interpretable. Inspired by brains, BIMT embeds neurons in a geometric space and augments the loss function with a cost proportional to the length of each neuron connection. We demonstrate that BIMT discovers useful modular neural networks for many simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries and features for classification, and mathematical structure in algorithmic datasets. The ability to directly see modules with the naked eye can complement current mechanistic interpretability strategies such as probes, interventions or staring at all weights.},
  archiveprefix = {arXiv},
  keywords = {algorithms,intrinsic interpretability,modularity,toy models},
  file = {/Users/leonardbereska/Zotero/storage/JMY5W8XU/Liu et al. - 2023 - Seeing is Believing Brain-Inspired Modular Traini.pdf}
}

@article{locatello_challenging_2019,
  title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = 2019,
  month = jun,
  journal = {CoRR},
  eprint = {1811.12359},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1811.12359},
  url = {http://arxiv.org/abs/1811.12359},
  urldate = {2024-01-04},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  archiveprefix = {arXiv},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/9X8IU9MV/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf}
}

@article{louizos_learning_2018,
  title = {Learning Sparse Neural Networks through \$L\_0\$ Regularization},
  author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  year = 2018,
  month = jun,
  journal = {CoRR},
  eprint = {1712.01312},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1712.01312},
  url = {http://arxiv.org/abs/1712.01312},
  urldate = {2023-08-27},
  abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the \textbackslash emph\textbraceleft hard concrete\textbraceright{} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/P34UA36U/Louizos et al. - 2018 - Learning Sparse Neural Networks through $L_0$ Regu.pdf}
}

@article{madry_deep_2018,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = 2018,
  journal = {CoRR},
  eprint = {1706.06083},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1706.06083},
  url = {http://arxiv.org/abs/1706.06083},
  urldate = {2023-07-03},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/3EVF2D7V/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf}
}

@article{makelov_principled_2024,
  title = {Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control},
  author = {Makelov, Aleksandar and Lange, George and Nanda, Neel},
  year = 2024,
  month = may,
  journal = {CoRR},
  eprint = {2405.08366},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2405.08366},
  urldate = {2024-06-10},
  abstract = {Disentangling model activations into meaningful features is a central problem in interpretability. However, the absence of ground-truth for these features in realistic scenarios makes validating recent approaches, such as sparse dictionary learning, elusive. To address this challenge, we propose a framework for evaluating feature dictionaries in the context of specific tasks, by comparing them against \textbackslash emph\textbraceleft supervised\textbraceright{} feature dictionaries. First, we demonstrate that supervised dictionaries achieve excellent approximation, control, and interpretability of model computations on the task. Second, we use the supervised dictionaries to develop and contextualize evaluations of unsupervised dictionaries along the same three axes. We apply this framework to the indirect object identification (IOI) task using GPT-2 Small, with sparse autoencoders (SAEs) trained on either the IOI or OpenWebText datasets. We find that these SAEs capture interpretable features for the IOI task, but they are less successful than supervised features in controlling the model. Finally, we observe two qualitative phenomena in SAE training: feature occlusion (where a causally relevant concept is robustly overshadowed by even slightly higher-magnitude ones in the learned features), and feature over-splitting (where binary features split into many smaller, less interpretable features). We hope that our framework will provide a useful step towards more objective and grounded evaluations of sparse dictionary learning methods.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/SQER5XUS/Makelov et al. - 2024 - Towards Principled Evaluations of Sparse Autoencod.pdf}
}

@article{makelov_sparse_2024,
  title = {Sparse Autoencoders Match Supervised Features for Model Steering on the IOI Task},
  author = {Makelov, Aleksandar},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=JdrVuEQih5},
  urldate = {2024-08-16},
  abstract = {Sparse autoencoders (SAEs) have attracted atten- tion as a way towards unsupervised disentangling of hidden LLM activations into meaningful fea- tures. However, evaluations of SAE architectures and training algorithms have so far been indi- rect due to the difficulty -- both conceptual and technical -- of obtaining `ground truth' features to compare against. To overcome this, recent work (Makelov et al., 2024) has proposed a suite of SAE evaluations that compare SAE features against feature dictionaries learned with super- vision for a specific model capability. However, the evaluations were implemented in a mostly ex- ploratory way, and did not optimize for eliciting best SAE performance across different SAE vari- ants. While initial results are promising, they rely on qualitative and/or indirect evaluation of the learned features such as proxies for the `true' features, non-trivial assumptions about SAE learning or success in toy models (Elhage et al., 2022; Bricken et al., 2023; Sharkey et al., 2023). As a step to- wards more objective SAE evaluations, recently Makelov et al. (2024) proposed to use sparse feature dictionaries learned with supervision in the context of a given model capability (specifically, the IOI task (Wang et al., 2023)) as a `skyline' for achievable SAE performance w.r.t. this capa- bility. They developed several evaluations that (1) confirm the supervised features provide a high-quality decomposi- tion of model computations w.r.t the capability and (2) use these supervised features to contextualize SAE results, for SAEs trained on distributions of either capability-specific or internet text. In this work, we improve upon this by running a systematic and thorough study of using SAEs for steering on the IOI task, comparing several recently proposed SAE variants: `vanilla' SAEs (Bricken et al., 2023), gated SAEs (Rajamanoha- ran et al., 2024) and topK SAEs (Gao et al., 2024). We find that, even by employing a simple and cheap heuristic for choosing good SAEs for edit- ing, we are able to greatly improve upon the re- sults of prior work, and demonstrate that SAE features are able to perform on par with super- vised feature dictionaries. Further, we find that topK SAEs and gated SAEs generally outperform other variants on this test, and topK SAEs can almost match supervised features in terms of edit quality.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/2PICI89L/Makelov - 2024 - Sparse Autoencoders Match Supervised Features for .pdf}
}

@article{mandelbrot_how_1967,
  title = {How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension},
  shorttitle = {How Long Is the Coast of Britain?},
  author = {Mandelbrot, Benoit},
  year = 1967,
  month = may,
  journal = {Science},
  volume = {156},
  number = {3775},
  pages = {636--638},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.156.3775.636},
  url = {https://www.science.org/doi/10.1126/science.156.3775.636},
  urldate = {2025-02-01},
  abstract = {Geographical curves are so involved in their detail that their lengths are often infinite or, rather, undefinable. However, many are statistically "self-similar," meaning that each portion can be considered a reduced-scale image of the whole. In that case, the degree of complication can be described by a quantity D that has many properties of a "dimension," though it is fractional; that is, it exceeds the value unity associated with the ordinary, rectifiable, curves.}
}

@article{marks_enhancing_2024,
  title = {Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders},
  author = {Marks, Luke and Paren, Alisdair and Krueger, David and Barez, Fazl},
  year = 2024,
  month = nov,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Enhancing-Neural-Network-Interpretability-with-Marks-Paren/c626f4c2ad1ff8501de1fd930deb887e6358c9ba},
  urldate = {2024-11-08},
  abstract = {Sparse Autoencoders (SAEs) have shown promise in improving the interpretability of neural network activations, but can learn features that are not features of the input, limiting their effectiveness. We propose \textbackslash textsc\textbraceleft Mutual Feature Regularization\textbraceright{} \textbackslash textbf\textbraceleft (MFR)\textbraceright, a regularization technique for improving feature learning by encouraging SAEs trained in parallel to learn similar features. We motivate \textbackslash textsc\textbraceleft MFR\textbraceright{} by showing that features learned by multiple SAEs are more likely to correlate with features of the input. By training on synthetic data with known features of the input, we show that \textbackslash textsc\textbraceleft MFR\textbraceright{} can help SAEs learn those features, as we can directly compare the features learned by the SAE with the input features for the synthetic data. We then scale \textbackslash textsc\textbraceleft MFR\textbraceright{} to SAEs that are trained to denoise electroencephalography (EEG) data and SAEs that are trained to reconstruct GPT-2 Small activations. We show that \textbackslash textsc\textbraceleft MFR\textbraceright{} can improve the reconstruction loss of SAEs by up to 21.21\textbackslash\% on GPT-2 Small, and 6.67\textbackslash\% on EEG data. Our results suggest that the similarity between features learned by different SAEs can be leveraged to improve SAE training, thereby enhancing performance and the usefulness of SAEs for model interpretability.},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/DPRY4K6J/Marks et al. - 2024 - Enhancing Neural Network Interpretability with Feature-Aligned Sparse Autoencoders.pdf}
}

@article{marks_interpreting_2023,
  title = {Interpreting Reward Models in RLHF-Tuned Language Models Using Sparse Autoencoders},
  author = {Marks, Luke and Abdullah, Amir and Mendez, Luna and Arike, Rauno and Torr, Philip and Barez, Fazl},
  year = 2023,
  month = oct,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Interpreting-Reward-Models-in-RLHF-Tuned-Language-Marks-Abdullah/53b9f8c62837b12d3955778f737678aabea39a83},
  urldate = {2023-10-25},
  abstract = {Large language models (LLMs) aligned to human preferences via reinforcement learning from human feedback (RLHF) underpin many commercial applications. However, how RLHF impacts LLM internals remains opaque. We propose a novel method to interpret learned reward functions in RLHF-tuned LLMs using sparse autoencoders. Our approach trains autoencoder sets on activations from a base LLM and its RLHF-tuned version. By comparing autoencoder hidden spaces, we identify unique features that reflect the accuracy of the learned reward model. To quantify this, we construct a scenario where the tuned LLM learns token-reward mappings to maximize reward. This is the first application of sparse autoencoders for interpreting learned rewards and broadly inspecting reward learning in LLMs. Our method provides an abstract approximation of reward integrity. This presents a promising technique for ensuring alignment between specified objectives and model behaviors.},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/KTQAWUYL/Marks et al. - 2023 - Interpreting Reward Models in RLHF-Tuned Language .pdf}
}

@article{marks_opensource_2023,
  title = {Some open-source dictionaries and dictionary learning infrastructure},
  author = {Marks, Sam},
  year = 2023,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/AaoWLcmpY3LKvtdyq/some-open-source-dictionaries-and-dictionary-learning},
  urldate = {2024-02-16},
  abstract = {As more people begin work on interpretability projects which incorporate dictionary learning, it will be valuable to have high-quality dictionaries p\dots},
  language = {en},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/HL4WISTE/Marks - 2023 - Some open-source dictionaries and dictionary learn.html}
}

@article{marks_sparse_2024,
  title = {Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models},
  shorttitle = {Sparse Feature Circuits},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = 2024,
  month = mar,
  journal = {CoRR},
  eprint = {2403.19647},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.19647},
  url = {http://arxiv.org/abs/2403.19647},
  urldate = {2024-06-10},
  abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder,sparse feature circuits},
  file = {/Users/leonardbereska/Zotero/storage/PAXI7A5W/Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf}
}

@article{marshall_understanding_2024,
  title = {Understanding polysemanticity in neural networks through coding theory},
  author = {Marshall, Simon C. and Kirchner, Jan H.},
  year = 2024,
  month = jan,
  journal = {CoRR},
  eprint = {2401.17975},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.17975},
  url = {http://arxiv.org/abs/2401.17975},
  urldate = {2024-02-13},
  abstract = {Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al.\textasciitilde (2022). Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability.},
  archiveprefix = {arXiv},
  keywords = {polysemanticity,understanding polysemanticity},
  file = {/Users/leonardbereska/Zotero/storage/JL2PTXI7/Marshall and Kirchner - 2024 - Understanding polysemanticity in neural networks t.pdf}
}

@article{menon_analyzing_2025,
  title = {Analyzing (In)Abilities of SAEs via Formal Languages},
  author = {Menon, Abhinav and Shrivastava, Manish and Krueger, David and Lubana, Ekdeep Singh},
  year = 2025,
  month = feb,
  journal = {NAACL, NeurIPS workshop on Foundation Model Interventions (best paper)},
  eprint = {2410.11767},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.11767},
  url = {http://arxiv.org/abs/2410.11767},
  urldate = {2025-03-03},
  abstract = {Autoencoders have been used for finding interpretable and disentangled features underlying neural network representations in both image and text domains. While the efficacy and pitfalls of such methods are well-studied in vision, there is a lack of corresponding results, both qualitative and quantitative, for the text domain. We aim to address this gap by training sparse autoencoders (SAEs) on a synthetic testbed of formal languages. Specifically, we train SAEs on the hidden representations of models trained on formal languages (Dyck-2, Expr, and English PCFG) under a wide variety of hyperparameter settings, finding interpretable latents often emerge in the features learned by our SAEs. However, similar to vision, we find performance turns out to be highly sensitive to inductive biases of the training pipeline. Moreover, we show latents correlating to certain features of the input do not always induce a causal impact on model's computation. We thus argue that causality has to become a central target in SAE training: learning of causal features should be incentivized from the ground-up. Motivated by this, we propose and perform preliminary investigations for an approach that promotes learning of causally relevant features in our formal language setting.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/6L7JUMTS/Menon et al. - 2025 - Analyzing (In)Abilities of SAEs via Formal Languag.pdf}
}

@article{michaud_quantization_2023,
  title = {The Quantization Model of Neural Scaling},
  author = {Michaud, Eric J. and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  year = 2023,
  month = mar,
  journal = {NeurIPS},
  eprint = {2303.13506},
  primaryclass = {cond-mat},
  doi = {10.48550/arXiv.2303.13506},
  url = {http://arxiv.org/abs/2303.13506},
  urldate = {2023-10-30},
  abstract = {We propose the \$\textbackslash textit\textbraceleft Quantization Model\textbraceright\$ of neural scaling laws, explaining both the observed power law dropoff of loss with model and data size, and also the sudden emergence of new capabilities with scale. We derive this model from what we call the \$\textbackslash textit\textbraceleft Quantization Hypothesis\textbraceright\$, where learned network capabilities are quantized into discrete chunks (\$\textbackslash textit\textbraceleft quanta\textbraceright\$). We show that when quanta are learned in order of decreasing use frequency, then a power law in use frequencies explains observed power law scaling of loss. We validate this prediction on toy datasets, then study how scaling curves decompose for large language models. Using language model internals, we auto-discover diverse model capabilities (quanta) and find tentative evidence that the distribution over corresponding subproblems in the prediction of natural text is compatible with the power law predicted from the neural scaling exponent as predicted from our theory.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/ZBB9GYAC/Michaud et al. - 2023 - The Quantization Model of Neural Scaling.pdf}
}

@article{mikolov_distributed_2013,
  title = {Distributed Representations of Words and Phrases and their Compositionality},
  author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = 2013,
  month = oct,
  journal = {NeurIPS},
  eprint = {1310.4546},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1310.4546},
  url = {http://arxiv.org/abs/1310.4546},
  urldate = {2023-11-02},
  abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  archiveprefix = {arXiv},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/WZ66E6T3/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf}
}

@article{minegishi_rethinking_2025,
  title = {Rethinking Evaluation of Sparse Autoencoders through the Representation of Polysemous Words},
  author = {Minegishi, Gouki and Furuta, Hiroki and Iwasawa, Yusuke and Matsuo, Yutaka},
  year = 2025,
  month = feb,
  journal = {ICLR},
  eprint = {2501.06254},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2501.06254},
  url = {http://arxiv.org/abs/2501.06254},
  urldate = {2025-03-03},
  abstract = {Sparse autoencoders (SAEs) have gained a lot of attention as a promising tool to improve the interpretability of large language models (LLMs) by mapping the complex superposition of polysemantic neurons into monosemantic features and composing a sparse dictionary of words. However, traditional performance metrics like Mean Squared Error and L0 sparsity ignore the evaluation of the semantic representational power of SAEs -- whether they can acquire interpretable monosemantic features while preserving the semantic relationship of words. For instance, it is not obvious whether a learned sparse feature could distinguish different meanings in one word. In this paper, we propose a suite of evaluations for SAEs to analyze the quality of monosemantic features by focusing on polysemous words. Our findings reveal that SAEs developed to improve the MSE-L0 Pareto frontier may confuse interpretability, which does not necessarily enhance the extraction of monosemantic features. The analysis of SAEs with polysemous words can also figure out the internal mechanism of LLMs; deeper layers and the Attention module contribute to distinguishing polysemy in a word. Our semantics focused evaluation offers new insights into the polysemy and the existing SAE objective and contributes to the development of more practical SAEs.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/J4TPX9G3/Minegishi et al. - 2025 - Rethinking Evaluation of Sparse Autoencoders throu.pdf}
}

@article{mu_compositional_2020,
  title = {Compositional Explanations of Neurons},
  author = {Mu, Jesse and Andreas, Jacob},
  year = 2020,
  month = jun,
  journal = {NeurIPS},
  eprint = {2006.14032},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2006.14032},
  url = {http://arxiv.org/abs/2006.14032},
  urldate = {2023-09-18},
  abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/IURDBMPN/Mu and Andreas - 2020 - Compositional Explanations of Neurons.pdf}
}

@article{mudide_efficient_2024,
  title = {Efficient Dictionary Learning with Switch Sparse Autoencoders},
  author = {Mudide, Anish and Engels, Joshua and Michaud, Eric J. and Tegmark, Max and de Witt, Christian Schroeder},
  year = 2024,
  month = oct,
  journal = {CoRR},
  eprint = {2410.08201},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2410.08201},
  url = {http://arxiv.org/abs/2410.08201},
  urldate = {2025-03-03},
  abstract = {Sparse autoencoders (SAEs) are a recent technique for decomposing neural network activations into human-interpretable features. However, in order for SAEs to identify all features represented in frontier models, it will be necessary to scale them up to very high width, posing a computational challenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE architecture aimed at reducing the compute cost of training SAEs. Inspired by sparse mixture of experts models, Switch SAEs route activation vectors between smaller "expert" SAEs, enabling SAEs to efficiently scale to many more features. We present experiments comparing Switch SAEs with other SAE architectures, and find that Switch SAEs deliver a substantial Pareto improvement in the reconstruction vs. sparsity frontier for a given fixed training compute budget. We also study the geometry of features across experts, analyze features duplicated across experts, and verify that Switch SAE features are as interpretable as features found by other SAE architectures.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/69F7K2I2/Mudide et al. - 2024 - Efficient Dictionary Learning with Switch Sparse A.pdf}
}

@article{mueller_missed_2024a,
  title = {Missed Causes and Ambiguous Effects: Counterfactuals Pose Challenges for Interpreting Neural Networks},
  shorttitle = {Missed Causes and Ambiguous Effects},
  author = {Mueller, Aaron},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=pJs3ZiKBM5},
  urldate = {2024-11-07},
  abstract = {Interpretability research takes counterfactual theories of causality for granted. Most causal methods rely on counterfactual interventions to inputs or the activations of particular model components, followed by observations of the change in models' output logits or behaviors. While this yields more faithful evidence than correlational methods, counterfactuals nonetheless have key problems that bias our findings in specific and predictable ways. Specifically, (i) counterfactual theories do not effectively capture multiple independently sufficient causes of the same effect, which leads us to miss certain causes entirely; and (ii) counterfactual dependencies in neural networks are generally not transitive, which complicates methods for extracting and interpreting causal graphs from neural networks. We discuss the implications of these challenges for interpretability researchers and propose concrete suggestions for future work.},
  language = {en},
  keywords = {icml,oral},
  file = {/Users/leonardbereska/Zotero/storage/I9IGIHK8/Mueller - 2024 - Missed Causes and Ambiguous Effects Counterfactua.pdf}
}

@article{murfet_deep_2020,
  title = {Deep Learning is Singular, and That's Good},
  author = {Murfet, Daniel and Wei, Susan and Gong, Mingming and Li, Hui and Gell-Redman, Jesse and Quella, Thomas},
  year = 2020,
  month = oct,
  journal = {CoRR},
  doi = {10.48550/arXiv.2010.11560},
  url = {http://arxiv.org/abs/2010.11560},
  urldate = {2024-11-25},
  abstract = {In singular models, the optimal set of parameters forms an analytic set with singularities and classical statistical inference cannot be applied to such models. This is significant for deep learning as neural networks are singular and thus "dividing" by the determinant of the Hessian or employing the Laplace approximation are not appropriate. Despite its potential for addressing fundamental issues in deep learning, singular learning theory appears to have made little inroads into the developing canon of deep learning theory. Via a mix of theory and experiment, we present an invitation to singular learning theory as a vehicle for understanding deep learning and suggest important future work to make singular learning theory directly applicable to how deep learning is performed in practice.},
  file = {/Users/leonardbereska/Zotero/storage/AY9W8KKP/Murfet et al. - 2020 - Deep Learning is Singular, and That's Good.pdf}
}

@article{nabeshima_matryoshka_2024,
  title = {Matryoshka Sparse Autoencoders},
  author = {Nabeshima, Noa},
  year = 2024,
  month = dec,
  url = {https://www.alignmentforum.org/posts/zbebxYCqsryPALh8C/matryoshka-sparse-autoencoders},
  urldate = {2024-12-20},
  abstract = {View trees here Search through latents with a token-regex language View individual latents here See code here (github.com/noanabeshima/matryoshka-sae\dots},
  language = {en}
}

@article{nanda_200superposition_2023,
  title = {200 COP in MI: Exploring Polysemanticity and Superposition},
  shorttitle = {200 COP in MI},
  author = {Nanda, Neel},
  year = 2023,
  month = jan,
  journal = {Neel Nanda's Blog},
  url = {https://www.lesswrong.com/posts/o6ptPu7arZrqRCxyz/200-cop-in-mi-exploring-polysemanticity-and-superposition},
  urldate = {2024-03-01},
  abstract = {Important Note: Since writing this, there's been a lot of exciting work on understanding superposition via training sparse autoencoders to take featu\dots},
  language = {en},
  keywords = {understanding polysemanticity},
  file = {/Users/leonardbereska/Zotero/storage/JVT33JIQ/Nanda - 2023 - 200 COP in MI Exploring Polysemanticity and Super.html}
}

@article{nanda_progress_2023,
  title = {Progress measures for grokking via mechanistic interpretability},
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  year = 2023,
  month = jan,
  journal = {ICLR},
  eprint = {2301.05217},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2301.05217},
  url = {http://arxiv.org/abs/2301.05217},
  urldate = {2023-08-26},
  abstract = {Neural networks often exhibit emergent behavior, where qualitatively new capabilities arise from scaling up the amount of parameters, training data, or training steps. One approach to understanding emergence is to find continuous \textbackslash textit\textbraceleft progress measures\textbraceright{} that underlie the seemingly discontinuous qualitative changes. We argue that progress measures can be found via mechanistic interpretability: reverse-engineering learned behaviors into their individual components. As a case study, we investigate the recently-discovered phenomenon of ``grokking'' exhibited by small transformers trained on modular addition tasks. We fully reverse engineer the algorithm learned by these networks, which uses discrete Fourier transforms and trigonometric identities to convert addition to rotation about a circle. We confirm the algorithm by analyzing the activations and weights and by performing ablations in Fourier space. Based on this understanding, we define progress measures that allow us to study the dynamics of training and split training into three continuous phases: memorization, circuit formation, and cleanup. Our results show that grokking, rather than being a sudden shift, arises from the gradual amplification of structured mechanisms encoded in the weights, followed by the later removal of memorizing components.},
  archiveprefix = {arXiv},
  keywords = {algorithms,dynamics,empirical findings in superposition},
  file = {/Users/leonardbereska/Zotero/storage/BI63JRD9/Nanda et al. - 2023 - Progress measures for grokking via mechanistic int.pdf}
}

@book{nielsen_quantum_2011,
  title = {Quantum Computation and Quantum Information},
  author = {Nielsen, Michael A. and Chuang, Isaac L.},
  year = 2011,
  month = jan,
  publisher = {Cambridge University Press},
  url = {https://www.semanticscholar.org/paper/Quantum-Computation-and-Quantum-Information-(10th-Nielsen-Chuang/3bfb6c2d7a301ab094db12ceaa8812782aaac23b},
  urldate = {2025-05-08},
  abstract = {One of the most cited books in physics of all time, Quantum Computation and Quantum Information remains the best textbook in this exciting field of science. This 10th anniversary edition includes an introduction from the authors setting the work in context. This comprehensive textbook describes such remarkable effects as fast quantum algorithms, quantum teleportation, quantum cryptography and quantum error-correction. Quantum mechanics and computer science are introduced before moving on to describe what a quantum computer is, how it can be used to solve problems faster than 'classical' computers and its real-world implementation. It concludes with an in-depth treatment of quantum information. Containing a wealth of figures and exercises, this well-known textbook is ideal for courses on the subject, and will interest beginning graduate students and researchers in physics, computer science, mathematics, and electrical engineering.}
}

@article{obayashi_feature_2021,
  title = {Feature extraction of fields of fluid dynamics data using sparse convolutional autoencoder},
  author = {Obayashi, Wataru and Aono, Hikaru and Tatsukawa, Tomoaki and Fujii, Kozo},
  year = 2021,
  month = oct,
  journal = {AIP Advances},
  volume = {11},
  number = {10},
  pages = {105211},
  issn = {2158-3226},
  doi = {10.1063/5.0065637},
  url = {https://pubs.aip.org/adv/article/11/10/105211/661167/Feature-extraction-of-fields-of-fluid-dynamics},
  urldate = {2025-05-08},
  abstract = {A neural network technique that extracts underlying flow features from the original flow field data is newly proposed. The technique here is based on the convolutional and sparse autoencoder learning algorithms and is called sparse convolutional autoencoder. Unlike the typical convolutional neural network (CNN) that changes the size of the data itself in the intermediate layers, flow field data size is not changed in the learning process of this method and only the numbers of channels are changed in each layer. Different but the same size of the data as the input are obtained by convolution with multiple spatially overlapping flow field data under the assumption of sparsity. When data restoration is realized in this autoencoder system, the channel numbers of data in the intermediate layers turn out to contain different flow characteristics of the original flow field. The proposed method is applied to the low Reynolds number flows over a circular cylinder. The high-fidelity unsteady flow data obtained by solving two-dimensional compressible Navier--Stokes equations with a high-resolution numerical scheme are used as a test case. In the proposed method, sparsity introduced in the middle-hidden layer is essential for the successful separation of the original data. The results presented in the example seem to correspond to positive and negative magnitudes of the original data, but future studies will reveal other features of the method. The present method shows flow features different from those of proper orthogonal decomposition in each mode, which is probably due to nonlinear decomposition in the CNN process.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/5X39WDDJ/Obayashi et al. - 2021 - Feature extraction of fields of fluid dynamics dat.pdf}
}

@article{olah_distributed_2023,
  title = {Distributed Representations: Composition \& Superposition},
  author = {Olah, Chris},
  year = 2023,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/superposition-composition/index.html},
  keywords = {explaining superposition,foundational,superposition},
  file = {/Users/leonardbereska/Zotero/storage/5YMYB5SZ/Olah - 2023 - Distributed Representations Composition & Superpo.html}
}

@article{olah_mechanistic_2022,
  title = {Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases},
  author = {Olah, Christopher},
  year = 2022,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/mech-interp-essay/index.html},
  keywords = {explaining superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/ACFGP82J/Olah - 2022 - Mechanistic Interpretability, Variables, and the I.html}
}

@article{olshausen_sparse_1997,
  title = {Sparse coding with an overcomplete basis set: a strategy employed by V1?},
  shorttitle = {Sparse coding with an overcomplete basis set},
  author = {Olshausen, B. A. and Field, D. J.},
  year = 1997,
  month = dec,
  journal = {Vision Res},
  volume = {37},
  number = {23},
  pages = {3311--3325},
  issn = {0042-6989},
  doi = {10.1016/s0042-6989(97)00169-7},
  url = {https://pubmed.ncbi.nlm.nih.gov/9425546/},
  abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete--i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
  language = {eng},
  pmid = {9425546},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/NJH8JATT/Olshausen and Field - 1997 - Sparse coding with an overcomplete basis set a st.pdf}
}

@article{olsson_incontext_2022,
  title = {In-context Learning and Induction Heads},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = 2022,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
  keywords = {circuit,empirical findings in superposition,in-context},
  file = {/Users/leonardbereska/Zotero/storage/9ZXSBRBA/Olsson et al. - 2022 - In-context Learning and Induction Heads.html}
}

@article{omahony_disentangling_2023,
  title = {Disentangling Neuron Representations with Concept Vectors},
  author = {O'Mahony, Laura and Andrearczyk, Vincent and Muller, Henning and Graziani, Mara},
  year = 2023,
  month = apr,
  journal = {CVPR Workshops},
  eprint = {2304.09707},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2304.09707},
  url = {http://arxiv.org/abs/2304.09707},
  urldate = {2023-09-18},
  abstract = {Mechanistic interpretability aims to understand how models store representations by breaking down neural networks into interpretable units. However, the occurrence of polysemantic neurons, or neurons that respond to multiple unrelated features, makes interpreting individual neurons challenging. This has led to the search for meaningful vectors, known as concept vectors, in activation space instead of individual neurons. The main contribution of this paper is a method to disentangle polysemantic neurons into concept vectors encapsulating distinct features. Our method can search for fine-grained concepts according to the user's desired level of concept separation. The analysis shows that polysemantic neurons can be disentangled into directions consisting of linear combinations of neurons. Our evaluations show that the concept vectors found encode coherent, human-understandable features.},
  archiveprefix = {arXiv},
  keywords = {disentangling},
  file = {/Users/leonardbereska/Zotero/storage/VLHWMIRP/O'Mahony et al. - 2023 - Disentangling Neuron Representations with Concept .pdf}
}

@article{oneill_compute_2025,
  title = {Compute Optimal Inference and Provable Amortisation Gap in Sparse Autoencoders},
  author = {O'Neill, Charles and Gumran, Alim and Klindt, David},
  year = 2025,
  month = jan,
  journal = {CoRR},
  eprint = {2411.13117},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2411.13117},
  url = {http://arxiv.org/abs/2411.13117},
  urldate = {2025-03-05},
  abstract = {A recent line of work has shown promise in using sparse autoencoders (SAEs) to uncover interpretable features in neural network representations. However, the simple linear-nonlinear encoding mechanism in SAEs limits their ability to perform accurate sparse inference. Using compressed sensing theory, we prove that an SAE encoder is inherently insufficient for accurate sparse inference, even in solvable cases. We then decouple encoding and decoding processes to empirically explore conditions where more sophisticated sparse inference methods outperform traditional SAE encoders. Our results reveal substantial performance gains with minimal compute increases in correct inference of sparse codes. We demonstrate this generalises to SAEs applied to large language models, where more expressive encoders achieve greater interpretability. This work opens new avenues for understanding neural network representations and analysing large language model activations.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/VXMDYLMI/O'Neill et al. - 2025 - Compute Optimal Inference and Provable Amortisatio.pdf}
}

@article{oneill_sparse_2024,
  title = {Sparse Autoencoders Enable Scalable and Reliable Circuit Identification in Language Models},
  author = {O'Neill, Charles and Bui, Thang},
  year = 2024,
  month = may,
  journal = {CoRR},
  eprint = {2405.12522},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2405.12522},
  url = {http://arxiv.org/abs/2405.12522},
  urldate = {2024-08-08},
  abstract = {This paper introduces an efficient and robust method for discovering interpretable circuits in large language models using discrete sparse autoencoders. Our approach addresses key limitations of existing techniques, namely computational complexity and sensitivity to hyperparameters. We propose training sparse autoencoders on carefully designed positive and negative examples, where the model can only correctly predict the next token for the positive examples. We hypothesise that learned representations of attention head outputs will signal when a head is engaged in specific computations. By discretising the learned representations into integer codes and measuring the overlap between codes unique to positive examples for each head, we enable direct identification of attention heads involved in circuits without the need for expensive ablations or architectural modifications. On three well-studied tasks - indirect object identification, greater-than comparisons, and docstring completion - the proposed method achieves higher precision and recall in recovering ground-truth circuits compared to state-of-the-art baselines, while reducing runtime from hours to seconds. Notably, we require only 5-10 text examples for each task to learn robust representations. Our findings highlight the promise of discrete sparse autoencoders for scalable and efficient mechanistic interpretability, offering a new direction for analysing the inner workings of large language models.},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder,sparse feature circuits},
  file = {/Users/leonardbereska/Zotero/storage/B6WEMM8D/O'Neill and Bui - 2024 - Sparse Autoencoders Enable Scalable and Reliable C.pdf}
}

@article{pan_dissecting_2024,
  title = {Dissecting Query-Key Interaction in Vision Transformers},
  author = {Pan, Xu and Philip, Aaron and Xie, Ziqian and Schwartz, Odelia},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=CsF3PwBN6N},
  urldate = {2024-08-16},
  abstract = {Self-attention in vision transformers is often thought to perform perceptual grouping where tokens attend to other tokens with similar embeddings, which could correspond to semantically similar features of an object. However, attending to dissimilar tokens can be beneficial by providing contextual information. We propose to use the Singular Value Decomposition to dissect the query-key interaction (i.e. \$\textbraceleft\textbackslash textbf\textbraceleft W\textbraceright\_q\textbraceright\textasciicircum\textbackslash top\textbackslash textbf\textbraceleft W\textbraceright\_k\$). We find that early layers attend more to similar tokens, while late layers show increased attention to dissimilar tokens, providing evidence corresponding to perceptual grouping and contextualization, respectively. Many of these interactions between features represented by singular vectors are interpretable and semantic, such as attention between relevant objects, between parts of an object, or between the foreground and background. This offers a novel perspective on interpreting the attention mechanism, which contributes to understanding how transformer models utilize context and salient features when processing images.},
  language = {en}
}

@article{park_geometry_2024a,
  title = {The Geometry of Categorical and Hierarchical Concepts in Large Language Models},
  author = {Park, Kiho and Choe, Yo Joong and Jiang, Yibo and Veitch, Victor},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=KXuYjuBzKo},
  urldate = {2024-11-07},
  abstract = {Understanding how semantic meaning is encoded in the representation spaces of large language models is a fundamental problem in interpretability. In this paper, we study the two foundational questions in this area. First, how are categorical concepts, such as \textbraceleft 'mammal', 'bird', 'reptile', 'fish'\textbraceright, represented? Second, how are hierarchical relations between concepts encoded? For example, how is the fact that 'dog' is a kind of 'mammal' encoded? We show how to extend the linear representation hypothesis to answer these questions. We then find a remarkably simple structure: simple categorical concepts are represented as simplices, hierarchically related concepts are orthogonal in a sense we make precise, and (in consequence) complex concepts are represented as polytopes constructed from direct sums of simplices, reflecting the hierarchical structure. We validate the results on the Gemma large language model, estimating representations for 957 hierarchically related concepts using data from the WordNet hierarchy.},
  language = {en},
  keywords = {icml,oral},
  file = {/Users/leonardbereska/Zotero/storage/BEDVILQ5/Park et al. - 2024 - The Geometry of Categorical and Hierarchical Conce.pdf}
}

@article{park_linear_2023,
  title = {The Linear Representation Hypothesis and the Geometry of Large Language Models},
  author = {Park, Kiho and Choe, Yo Joong and Veitch, Victor},
  year = 2023,
  month = nov,
  journal = {NeurIPS Workshop on Causal Representation Learning},
  eprint = {2311.03658},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2311.03658},
  urldate = {2023-11-09},
  abstract = {Informally, the 'linear representation hypothesis' is the idea that high-level concepts are represented linearly as directions in some representation space. In this paper, we address two closely related questions: What does "linear representation" actually mean? And, how do we make sense of geometric notions (e.g., cosine similarity or projection) in the representation space? To answer these, we use the language of counterfactuals to give two formalizations of "linear representation", one in the output (word) representation space, and one in the input (sentence) space. We then prove these connect to linear probing and model steering, respectively. To make sense of geometric notions, we use the formalization to identify a particular (non-Euclidean) inner product that respects language structure in a sense we make precise. Using this causal inner product, we show how to unify all notions of linear representation. In particular, this allows the construction of probes and steering vectors using counterfactual pairs. Experiments with LLaMA-2 demonstrate the existence of linear representations of concepts, the connection to interpretation and control, and the fundamental role of the choice of inner product.},
  archiveprefix = {arXiv},
  keywords = {linearity},
  file = {/Users/leonardbereska/Zotero/storage/ZK7VCT4A/Park et al. - 2023 - The Linear Representation Hypothesis and the Geome.pdf}
}

@article{paulo_automatically_2024,
  title = {Automatically Interpreting Millions of Features in Large Language Models},
  author = {Paulo, Gonccalo and Mallen, Alex Troy and Juang, Caden and Belrose, Nora},
  year = 2024,
  month = oct,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Automatically-Interpreting-Millions-of-Features-in-Paulo-Mallen/04d3e91a32d424f8bc076ca632c444193166fb54},
  urldate = {2024-11-08},
  abstract = {While the activations of neurons in deep neural networks usually do not have a simple human-understandable interpretation, sparse autoencoders (SAEs) can be used to transform these activations into a higher-dimensional latent space which may be more easily interpretable. However, these SAEs can have millions of distinct latent features, making it infeasible for humans to manually interpret each one. In this work, we build an open-source automated pipeline to generate and evaluate natural language explanations for SAE features using LLMs. We test our framework on SAEs of varying sizes, activation functions, and losses, trained on two different open-weight LLMs. We introduce five new techniques to score the quality of explanations that are cheaper to run than the previous state of the art. One of these techniques, intervention scoring, evaluates the interpretability of the effects of intervening on a feature, which we find explains features that are not recalled by existing methods. We propose guidelines for generating better explanations that remain valid for a broader set of activating contexts, and discuss pitfalls with existing scoring techniques. We use our explanations to measure the semantic similarity of independently trained SAEs, and find that SAEs trained on nearby layers of the residual stream are highly similar. Our large-scale analysis confirms that SAE latents are indeed much more interpretable than neurons, even when neurons are sparsified using top-\$k\$ postprocessing. Our code is available at https://github.com/EleutherAI/sae-auto-interp, and our explanations are available at https://huggingface.co/datasets/EleutherAI/auto\_interp\_explanations.},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/RN524AXC/Paulo et al. - 2024 - Automatically Interpreting Millions of Features in Large Language Models.pdf}
}

@article{paulo_sparse_2025,
  title = {Sparse Autoencoders Trained on the Same Data Learn Different Features},
  author = {Paulo, Gon{\c c}alo and Belrose, Nora},
  year = 2025,
  month = jan,
  journal = {CoRR},
  eprint = {2501.16615},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2501.16615},
  url = {http://arxiv.org/abs/2501.16615},
  urldate = {2025-03-03},
  abstract = {Sparse autoencoders (SAEs) are a useful tool for uncovering human-interpretable features in the activations of large language models (LLMs). While some expect SAEs to find the true underlying features used by a model, our research shows that SAEs trained on the same model and data, differing only in the random seed used to initialize their weights, identify different sets of features. For example, in an SAE with 131K latents trained on a feedforward network in Llama 3 8B, only 30\% of the features were shared across different seeds. We observed this phenomenon across multiple layers of three different LLMs, two datasets, and several SAE architectures. While ReLU SAEs trained with the L1 sparsity loss showed greater stability across seeds, SAEs using the state-of-the-art TopK activation function were more seed-dependent, even when controlling for the level of sparsity. Our results suggest that the set of features uncovered by an SAE should be viewed as a pragmatically useful decomposition of activation space, rather than an exhaustive and universal list of features "truly used" by the model.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/CS2P3ACZ/Paulo and Belrose - 2025 - Sparse Autoencoders Trained on the Same Data Learn.pdf}
}

@article{pavlitska_relationship_2023,
  title = {Relationship between Model Compression and Adversarial Robustness: A Review of Current Evidence},
  shorttitle = {Relationship between Model Compression and Adversarial Robustness},
  author = {Pavlitska, Svetlana and Grolig, Hannes and Z{\"o}llner, J. Marius},
  year = 2023,
  month = nov,
  journal = {CoRR},
  eprint = {2311.15782},
  url = {http://arxiv.org/abs/2311.15782},
  urldate = {2024-11-06},
  abstract = {Increasing the model capacity is a known approach to enhance the adversarial robustness of deep learning networks. On the other hand, various model compression techniques, including pruning and quantization, can reduce the size of the network while preserving its accuracy. Several recent studies have addressed the relationship between model compression and adversarial robustness, while some experiments have reported contradictory results. This work summarizes available evidence and discusses possible explanations for the observed effects.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/WZYPURBW/Pavlitska et al. - 2023 - Relationship between Model Compression and Adversa.pdf}
}

@article{plenio_introduction_2006,
  title = {An introduction to entanglement measures},
  author = {Plenio, Martin B. and Virmani, S.},
  year = 2006,
  month = jun,
  journal = {CoRR},
  eprint = {quant-ph/0504163},
  doi = {10.48550/arXiv.quant-ph/0504163},
  url = {http://arxiv.org/abs/quant-ph/0504163},
  urldate = {2024-12-09},
  abstract = {We review the theory of entanglement measures, concentrating mostly on the finite dimensional two-party case. Topics covered include: single-copy and asymptotic entanglement manipulation; the entanglement of formation; the entanglement cost; the distillable entanglement; the relative entropic measures; the squashed entanglement; log-negativity; the robustness monotones; the greatest cross-norm; uniqueness and extremality theorems. Infinite dimensional systems and multi-party settings will be discussed briefly.},
  archiveprefix = {arXiv},
  keywords = {matrix effective dimensionality measures},
  file = {/Users/leonardbereska/Zotero/storage/DI3B6PBT/Plenio and Virmani - 2006 - An introduction to entanglement measures.pdf}
}

@article{power_grokking_2022,
  title = {Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets},
  shorttitle = {Grokking},
  author = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  year = 2022,
  month = jan,
  journal = {CoRR},
  eprint = {2201.02177},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2201.02177},
  url = {http://arxiv.org/abs/2201.02177},
  urldate = {2023-11-10},
  abstract = {In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of "grokking" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/HAFYPFWN/Power et al. - 2022 - Grokking Generalization Beyond Overfitting on Sma.pdf}
}

@article{rajamanoharan_improving_2024,
  title = {Improving Dictionary Learning with Gated Sparse Autoencoders},
  author = {Rajamanoharan, Senthooran and Conmy, Arthur and Smith, Lewis and Lieberum, Tom and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Shah, Rohin and Nanda, Neel},
  year = 2024,
  month = apr,
  journal = {CoRR},
  eprint = {2404.16014},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.16014},
  url = {http://arxiv.org/abs/2404.16014},
  urldate = {2025-03-03},
  abstract = {Recent work has found that sparse autoencoders (SAEs) are an effective technique for unsupervised discovery of interpretable features in language models' (LMs) activations, by finding sparse, linear reconstructions of LM activations. We introduce the Gated Sparse Autoencoder (Gated SAE), which achieves a Pareto improvement over training with prevailing methods. In SAEs, the L1 penalty used to encourage sparsity introduces many undesirable biases, such as shrinkage -- systematic underestimation of feature activations. The key insight of Gated SAEs is to separate the functionality of (a) determining which directions to use and (b) estimating the magnitudes of those directions: this enables us to apply the L1 penalty only to the former, limiting the scope of undesirable side effects. Through training SAEs on LMs of up to 7B parameters we find that, in typical hyper-parameter ranges, Gated SAEs solve shrinkage, are similarly interpretable, and require half as many firing features to achieve comparable reconstruction fidelity.},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/86LE4CK4/Rajamanoharan et al. - 2024 - Improving Dictionary Learning with Gated Sparse Au.pdf}
}

@article{rajamanoharan_jumping_2024,
  title = {Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse Autoencoders},
  shorttitle = {Jumping Ahead},
  author = {Rajamanoharan, Senthooran and Lieberum, Tom and Sonnerat, Nicolas and Conmy, Arthur and Varma, Vikrant and Kram{\'a}r, J{\'a}nos and Nanda, Neel},
  year = 2024,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2407.14435},
  url = {https://arxiv.org/abs/2407.14435},
  urldate = {2025-03-03},
  abstract = {Sparse autoencoders (SAEs) are a promising unsupervised approach for identifying causally relevant and interpretable linear features in a language model's (LM) activations. To be useful for downstream tasks, SAEs need to decompose LM activations faithfully; yet to be interpretable the decomposition must be sparse -- two objectives that are in tension. In this paper, we introduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity at a given sparsity level on Gemma 2 9B activations, compared to other recent advances such as Gated and TopK SAEs. We also show that this improvement does not come at the cost of interpretability through manual and automated interpretability studies. JumpReLU SAEs are a simple modification of vanilla (ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU activation function -- and are similarly efficient to train and run. By utilising straight-through-estimators (STEs) in a principled manner, we show how it is possible to train JumpReLU SAEs effectively despite the discontinuous JumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs to directly train L0 to be sparse, instead of training on proxies such as L1, avoiding problems like shrinkage.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/F8F3L9DQ/Rajamanoharan et al. - 2024 - Jumping Ahead Improving Reconstruction Fidelity w.pdf}
}

@article{ramirez_mdl_2012,
  title = {An MDL Framework for Sparse Coding and Dictionary Learning},
  author = {Ramirez, Ignacio and Sapiro, Guillermo},
  year = 2012,
  month = jun,
  journal = {IEEE Trans. Signal Process.},
  volume = {60},
  number = {6},
  pages = {2913--2927},
  issn = {1053-587X, 1941-0476},
  doi = {10.1109/TSP.2012.2187203},
  url = {http://ieeexplore.ieee.org/document/6148296/},
  urldate = {2025-05-07},
  abstract = {The power of sparse signal modeling with learned overcomplete dictionaries has been demonstrated in a variety of applications and fields, from signal processing to statistical inference and machine learning. However, the statistical properties of these models, such as underfitting or overfitting given sets of data, are still not well characterized in the literature. As a result, the success of sparse modeling depends on hand-tuning critical parameters for each data and application. This work aims at addressing this by providing a practical and objective characterization of sparse models by means of the minimum description length (MDL) principle-a well-established information-theoretic approach to model selection in statistical inference. The resulting framework derives a family of efficient sparse coding and dictionary learning algorithms which, by virtue of the MDL principle, are completely parameter free. Furthermore, such framework allows to incorporate additional prior information to existing models, such as Markovian dependencies, or to define completely new problem formulations, including in the matrix analysis area, in a natural way. These virtues will be demonstrated with parameter-free algorithms for the classic image denoising and classification problems, and for low-rank matrix recovery in video applications. However, the framework is not limited to this imaging data, and can be applied to a wide range of signal and data types and tasks.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/leonardbereska/Zotero/storage/H6Q239A2/Ramirez and Sapiro - 2012 - An MDL Framework for Sparse Coding and Dictionary .pdf}
}

@article{rauker_transparent_2023,
  title = {Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks},
  shorttitle = {Toward Transparent AI},
  author = {R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and Hadfield-Menell, Dylan},
  year = 2023,
  month = aug,
  journal = {TMLR},
  eprint = {2207.13243},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2207.13243},
  url = {http://arxiv.org/abs/2207.13243},
  urldate = {2023-08-27},
  abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.},
  archiveprefix = {arXiv},
  keywords = {adversarial,review},
  file = {/Users/leonardbereska/Zotero/storage/2MVPX3C4/Räuker et al. - 2023 - Toward Transparent AI A Survey on Interpreting th.pdf}
}

@article{recanatesi_scaledependent_2022,
  title = {A scale-dependent measure of system dimensionality},
  author = {Recanatesi, Stefano and Bradde, Serena and Balasubramanian, Vijay and Steinmetz, Nicholas A. and Shea-Brown, Eric},
  year = 2022,
  month = aug,
  journal = {Patterns (N Y)},
  volume = {3},
  number = {8},
  pages = {100555},
  issn = {2666-3899},
  doi = {10.1016/j.patter.2022.100555},
  abstract = {A fundamental problem in science is uncovering the effective number of degrees of freedom in a complex system: its dimensionality. A system's dimensionality depends on its spatiotemporal scale. Here, we introduce a scale-dependent generalization of a classic enumeration of latent variables, the participation ratio. We demonstrate how the scale-dependent participation ratio identifies the appropriate dimension at local, intermediate, and global scales in several systems such as the Lorenz attractor, hidden Markov models, and switching linear dynamical systems. We show analytically how, at different limiting scales, the scale-dependent participation ratio relates to well-established measures of dimensionality. This measure applied in neural population recordings across multiple brain areas and brain states shows fundamental trends in the dimensionality of neural activity-for example, in behaviorally engaged versus spontaneous states. Our novel method unifies widely used measures of dimensionality and applies broadly to multivariate data across several fields of science.},
  language = {eng},
  pmcid = {PMC9403367},
  pmid = {36033586}
}

@article{ross_improving_2017,
  title = {Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients},
  author = {Ross, Andrew Slavin and Doshi-Velez, Finale},
  year = 2017,
  month = nov,
  journal = {AAAI},
  eprint = {1711.09404},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1711.09404},
  url = {http://arxiv.org/abs/1711.09404},
  urldate = {2024-08-03},
  abstract = {Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more "legitimate," interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/PYKXG52I/Ross and Doshi-Velez - 2017 - Improving the Adversarial Robustness and Interpret.pdf}
}

@article{roy_effective_2007,
  title = {The effective rank: A measure of effective dimensionality},
  shorttitle = {The effective rank},
  author = {Roy, Olivier and Vetterli, Martin},
  year = 2007,
  month = sep,
  journal = {2007 15th European Signal Processing Conference},
  pages = {606--610},
  url = {https://ieeexplore.ieee.org/document/7098875/},
  urldate = {2024-12-05},
  abstract = {Many signal processing algorithms include numerical problems where the solution is obtained by adjusting the value of parameters such that a specific matrix exhibits rank deficiency. Since rank minimization is generally not practicable owing to its integer nature, we propose a real-valued extension that we term effective rank. After proving some of its properties, the effective rank is provided with an operational meaning using a result on the coefficient rate of a stationary random process. Finally, the proposed measure is assessed in a practical scenario and other potential applications are suggested.},
  keywords = {matrix effective dimensionality measures},
  file = {/Users/leonardbereska/Zotero/storage/QD3D5F7J/Roy and Vetterli - 2007 - The effective rank A measure of effective dimensi.html}
}

@article{rozell_sparse_2008,
  title = {Sparse Coding via Thresholding and Local Competition in Neural Circuits},
  author = {Rozell, Christopher J. and Johnson, Don H. and Baraniuk, Richard G. and Olshausen, Bruno A.},
  year = 2008,
  month = oct,
  journal = {Neural Computation},
  volume = {20},
  number = {10},
  pages = {2526--2563},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.03-07-486},
  url = {https://doi.org/10.1162/neco.2008.03-07-486},
  urldate = {2023-09-18},
  abstract = {While evidence indicates that neural systems may be employing sparse approximations to represent sensed stimuli, the mechanisms underlying this ability are not understood. We describe a locally competitive algorithm (LCA) that solves a collection of sparse coding principles minimizing a weighted combination of mean-squared error and a coefficient cost function. LCAs are designed to be implemented in a dynamical system composed of many neuron-like elements operating in parallel. These algorithms use thresholding functions to induce local (usually one-way) inhibitory competitions between nodes to produce sparse representations. LCAs produce coefficients with sparsity levels comparable to the most popular centralized sparse coding algorithms while being readily suited for neural implementation. Additionally, LCA coefficients for video sequences demonstrate inertial properties that are both qualitatively and quantitatively more regular (i.e., smoother and more predictable) than the coefficients produced by greedy algorithms.},
  file = {/Users/leonardbereska/Zotero/storage/CC6FIN65/Rozell et al. - 2008 - Sparse Coding via Thresholding and Local Competiti.pdf}
}

@article{salman_adversarially_2020,
  title = {Do Adversarially Robust ImageNet Models Transfer Better?},
  author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  year = 2020,
  month = dec,
  journal = {NeurIPS},
  eprint = {2007.08489},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2007.08489},
  url = {http://arxiv.org/abs/2007.08489},
  urldate = {2024-08-03},
  abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer .},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/APQEKH3M/Salman et al. - 2020 - Do Adversarially Robust ImageNet Models Transfer B.pdf}
}

@article{santurkar_image_2019,
  title = {Image Synthesis with a Single (Robust) Classifier},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Ilyas, Andrew and Engstrom, Logan and Madry, Aleksander},
  year = 2019,
  month = aug,
  journal = {NeurIPS},
  eprint = {1906.09453},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1906.09453},
  url = {http://arxiv.org/abs/1906.09453},
  urldate = {2024-08-04},
  abstract = {We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context. Code and models for our experiments can be found at https://git.io/robust-apps.},
  archiveprefix = {arXiv},
  keywords = {adversarial},
  file = {/Users/leonardbereska/Zotero/storage/EXJGENUU/Santurkar et al. - 2019 - Image Synthesis with a Single (Robust) Classifier.pdf}
}

@article{scherlis_polysemanticity_2023,
  title = {Polysemanticity and Capacity in Neural Networks},
  author = {Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S. and Benton, Joe and Shlegeris, Buck},
  year = 2023,
  month = jul,
  journal = {CoRR},
  eprint = {2210.01892},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.01892},
  url = {http://arxiv.org/abs/2210.01892},
  urldate = {2023-09-18},
  abstract = {Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \textbackslash emph\textbraceleft capacity\textbraceright, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model architecture on the interpretability of its neurons.},
  archiveprefix = {arXiv},
  keywords = {fundamental,polysemanticity,superposition,toy models,understanding polysemanticity},
  file = {/Users/leonardbereska/Zotero/storage/DASCWAYC/Scherlis et al. - 2023 - Polysemanticity and Capacity in Neural Networks.pdf}
}

@article{schrodinger_discussion_1935,
  title = {Discussion of Probability Relations between Separated Systems},
  author = {Schr{\"o}dinger, Erwin},
  year = 1935,
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {31},
  number = {4},
  pages = {555--563},
  issn = {1469-8064, 0305-0041},
  doi = {10.1017/S0305004100013554},
  url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/abs/discussion-of-probability-relations-between-separated-systems/C1C71E1AA5BA56EBE6588AAACB9A222D},
  urldate = {2025-05-08},
  abstract = {The probability relations which can occur between two separated physical systems are discussed, on the assumption that their state is known by a representative in common. The two families of observables, relating to the first and to the second system respectively, are linked by at least one match between two definite members, one of either family. The word match is short for stating that the values of the two observables in question determine each other uniquely and therefore (since the actual labelling is irrelevant) can be taken to be equal. In general there is but one match, but there can be more. If, in addition to the first match, there is a second one between canonical conjugates of the first mates, then there are infinitely many matches, every function of the first canonical pair matching with the same function of the second canonical pair. Thus there is a complete one-to-one correspondence between those two branches (of the two families of observables) which relate to the two degrees of freedom in question. If there are no others, the one-to-one correspondence persists as time advances, but the observables of the first system (say) change their mates in the way that the latter, i.e. the observables of the second system, undergo a certain continuous contact-transformation.},
  language = {en}
}

@article{sharkey_list_2024,
  title = {A List of 45+ Mech Interp Project Ideas from Apollo Research's Interpretability Team},
  author = {Sharkey, Lee and Bushnaq, Lucius and Braun, Dan and StefanHex and Goldowsky-Dill, Nicholas},
  year = 2024,
  month = jul,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/KfkpgXdgRheSRWDy8/a-list-of-45-mech-interp-project-ideas-from-apollo-research},
  urldate = {2025-02-01},
  abstract = {Why we made this list:~ {$\bullet$}  * The interpretability team at Apollo Research wrapped up a few projects recently[1]. In order to decide what we'd work on\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/3GYE67QB/Sharkey et al. - 2024 - A List of 45+ Mech Interp Project Ideas from Apoll.html}
}

@article{sharkey_taking_2022,
  title = {Taking features out of superposition with sparse autoencoders},
  author = {Sharkey, Lee and Braun, Dan and Millidge, Beren},
  year = 2022,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/z6QQJbtpkEAX3Aojj/interim-research-report-taking-features-out-of-superposition},
  urldate = {2023-07-31},
  abstract = {Recent results from Anthropic suggest that neural networks represent features in superposition. This motivates the search for a method that can identify those features. Here, we construct a toy dataset of neural activations and see if we can recover the known ground truth features using sparse coding. We show that, contrary to some initial expectations, it turns out that an extremely simple method -- training a single layer autoencoder to reconstruct neural activations with an L1 penalty on hidden activations -- doesn't just identify features that minimize the loss, but actually recovers the ground truth features that generated the data. We're sharing these observations quickly so that others can begin to extract the features used by neural networks as early as possible. We also share some incomplete observations of what happens when we apply this method to a small language model and our reflections on further research directions.},
  language = {en},
  keywords = {feature,sparse autoencoder,superposition,toy model of superposition},
  file = {/Users/leonardbereska/Zotero/storage/8KE3X8B9/Sharkey et al. - 2022 - [Interim research report] Taking features out of s.html}
}

@article{shi_hypothesis_2024a,
  title = {Hypothesis Testing the Circuit Hypothesis in LLMs},
  author = {Shi, Claudia and Beltran-Velez, Nicolas and Nazaret, Achille and Zheng, Carolina and Garriga-Alonso, Adri{\`a} and Jesson, Andrew and Makar, Maggie and Blei, David},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=ibSNv9cldu},
  urldate = {2024-11-07},
  abstract = {Large language models (LLMs) demonstrate surprising capabilities, but we do not understand how they are implemented. One hypothesis suggests that these capabilities are primarily executed by small subnetworks within the LLM, known as circuits. But how can we evaluate this hypothesis? In this paper, we formalize a set of criteria that a circuit is hypothesized to meet and develop a suite of hypothesis tests to evaluate how well circuits satisfy them. The criteria focus on the extent to which the LLM's behavior is preserved, the degree of localization of this behavior, and whether the circuit is minimal. We apply these tests to six circuits described in the research literature. We find that synthetic circuits -- circuits that are hard-coded in the model -- align with the idealized properties. Circuits discovered in Transformer models satisfy the criteria to varying degrees.},
  language = {en},
  keywords = {faithfulness,icml,oral},
  file = {/Users/leonardbereska/Zotero/storage/QJWXHGNE/Shi et al. - 2024 - Hypothesis Testing the Circuit Hypothesis in LLMs.pdf}
}

@article{shin_disentangling_2024,
  title = {Disentangling quantum neural networks for unified estimation of quantum entropies and distance measures},
  author = {Shin, Myeongjin and Lee, Seungwoo and Lee, Junseo and Lee, Mingyu and Ji, Donghwa and Yeo, Hyeonjun and Jeong, Kabgyun},
  year = 2024,
  month = dec,
  journal = {Phys. Rev. A},
  volume = {110},
  number = {6},
  eprint = {2401.07716},
  primaryclass = {quant-ph},
  pages = {062418},
  issn = {2469-9926, 2469-9934},
  doi = {10.1103/PhysRevA.110.062418},
  url = {http://arxiv.org/abs/2401.07716},
  urldate = {2025-06-05},
  abstract = {The estimation of quantum entropies and distance measures, such as von Neumann entropy, R\textbackslash '\textbraceleft e\textbraceright nyi entropy, Tsallis entropy, trace distance, and fidelity-induced distances such as the Bures distance, has been a key area of research in quantum information science. In our study, we introduce the disentangling quantum neural network (DEQNN), designed to efficiently estimate various physical quantities in quantum information. Estimation algorithms for these quantities are generally tied to the size of the Hilbert space of the quantum state to be estimated. Our proposed DEQNN offers a unified dimensionality reduction methodology that can significantly reduce the size of the Hilbert space while preserving the values of diverse physical quantities. We provide an in-depth discussion of the physical scenarios and limitations in which our algorithm is applicable, as well as the learnability of the proposed quantum neural network.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/FP5BXQKG/Shin et al. - 2024 - Disentangling quantum neural networks for unified .pdf}
}

@article{shwartz-ziv_opening_2017,
  title = {Opening the Black Box of Deep Neural Networks via Information},
  author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
  year = 2017,
  month = apr,
  journal = {CoRR},
  eprint = {1703.00810},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1703.00810},
  url = {http://arxiv.org/abs/1703.00810},
  urldate = {2025-06-05},
  abstract = {Despite their great success, there is still no comprehensive theoretical understanding of learning with Deep Neural Networks (DNNs) or their inner organization. Previous work proposed to analyze DNNs in the \textbackslash textit\textbraceleft Information Plane\textbraceright; i.e., the plane of the Mutual Information values that each layer preserves on the input and output variables. They suggested that the goal of the network is to optimize the Information Bottleneck (IB) tradeoff between compression and prediction, successively, for each layer. In this work we follow up on this idea and demonstrate the effectiveness of the Information-Plane visualization of DNNs. Our main results are: (i) most of the training epochs in standard DL are spent on \textbraceleft\textbackslash emph compression\textbraceright{} of the input to efficient representation and not on fitting the training labels. (ii) The representation compression phase begins when the training errors becomes small and the Stochastic Gradient Decent (SGD) epochs change from a fast drift to smaller training error into a stochastic relaxation, or random diffusion, constrained by the training error value. (iii) The converged layers lie on or very close to the Information Bottleneck (IB) theoretical bound, and the maps from the input to any hidden layer and from this hidden layer to the output satisfy the IB self-consistent equations. This generalization through noise mechanism is unique to Deep Neural Networks and absent in one layer networks. (iv) The training time is dramatically reduced when adding more hidden layers. Thus the main advantage of the hidden layers is computational. This can be explained by the reduced relaxation time, as this it scales super-linearly (exponentially for simple diffusion) with the information compression from the previous layer.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/LQR55X3L/Shwartz-Ziv and Tishby - 2017 - Opening the Black Box of Deep Neural Networks via .pdf}
}

@article{smith_strong_2024,
  title = {The `strong' feature hypothesis could be wrong},
  author = {Smith, Lewis},
  year = 2024,
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/tojtPCCRpKLSHBdpn/the-strong-feature-hypothesis-could-be-wrong},
  urldate = {2024-12-03},
  abstract = {NB. I am on the Google Deepmind language model interpretability team. But the arguments/views in this post are my own, and shouldn't be read as a tea\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/TGP6W333/Smith - 2024 - The ‘strong’ feature hypothesis could be wrong.html}
}

@article{sun_learning_2024,
  title = {Learning and Unlearning of Fabricated Knowledge in Language Models},
  author = {Sun, Chen and Miller, Nolan Andrew and Zhmoginov, Andrey and Vladymyrov, Max and Sandler, Mark},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=R5Q5lANcjY},
  urldate = {2024-08-16},
  abstract = {What happens when a new piece of knowledge is introduced into the training data and how long does it last while a large language model (LM) continues to train? We investigate this question by injecting facts into LMs from a new probing dataset, "Outlandish", which is designed to permit the testing of a spectrum of different fact types. When studying how robust these memories are, there appears to be a sweet spot in the spectrum of fact novelty between consistency with world knowledge and total randomness, where the injected memory is the most enduring. Specifically we show that facts that conflict with common knowledge are remembered for tens of thousands of training steps, while prompts not conflicting with common knowledge (mundane), as well as scrambled prompts (randomly jumbled) are both forgotten much more rapidly. Further, knowledge-conflicting facts can "prime'' how the language model hallucinates on logically unrelated prompts, showing their propensity for non-target generalization, while both mundane and randomly jumbled facts prime significantly less. Finally, we show that impacts of knowledge-conflicting facts in LMs, though they can be long lasting, can be largely erased by novel application of multi-step sparse updates, even while the training ability of the model is preserved. As such, this very simple procedure has direct implications for mitigating the effects of data poisoning in training.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/NXLH5AEG/Sun et al. - 2024 - Learning and Unlearning of Fabricated Knowledge in.pdf}
}

@article{taggart_prolu_2024,
  title = {ProLU: A Nonlinearity for Sparse Autoencoders},
  shorttitle = {ProLU},
  author = {Taggart, Glen},
  year = 2024,
  month = apr,
  url = {https://www.alignmentforum.org/posts/HEpufTdakGTTKgoYF/prolu-a-nonlinearity-for-sparse-autoencoders},
  urldate = {2025-03-03},
  abstract = {Abstract This paper presents~ProLU, an alternative to~ReLU~for the activation function in sparse autoencoders that produces a pareto improvement over\dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/KNC4BZEL/Taggart - 2024 - ProLU A Nonlinearity for Sparse Autoencoders.html}
}

@article{tamkin_codebook_2023,
  title = {Codebook Features: Sparse and Discrete Interpretability for Neural Networks},
  author = {Tamkin, Alex and Taufeeque, Mohammad and Goodman, Noah D},
  year = 2023,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2310.17230},
  abstract = {Understanding neural networks is challenging in part because of the dense, continuous nature of their hidden states. We explore whether we can train neural networks to have hidden states that are sparse, discrete, and more interpretable by quantizing their continuous features into what we call codebook features. Codebook features are produced by finetuning neural networks with vector quantization bottlenecks at each layer, producing a network whose hidden features are the sum of a small number of discrete vector codes chosen from a larger codebook. Surprisingly, we find that neural networks can operate under this extreme bottleneck with only modest degradation in performance. This sparse, discrete bottleneck also provides an intuitive way of controlling neural network behavior: first, find codes that activate when the desired behavior is present, then activate those same codes during generation to elicit that behavior. We validate our approach by training codebook Transformers on several different datasets. First, we explore a finite state machine dataset with far more hidden states than neurons. In this setting, our approach overcomes the superposition problem by assigning states to distinct codes, and we find that we can make the neural network behave as if it is in a different state by activating the code for that state. Second, we train Transformer language models with up to 410M parameters on two natural language datasets. We identify codes in these models representing diverse, disentangled concepts (ranging from negative emotions to months of the year) and find that we can guide the model to generate different topics by activating the appropriate codes during inference. Overall, codebook features appear to be a promising unit of analysis and control for neural networks and interpretability. Our codebase and models are open-sourced at this https URL.},
  keywords = {sparse autoencoder,sparse feature circuits},
  file = {/Users/leonardbereska/Zotero/storage/H4G6WIHA/Tamkin et al. - 2023 - Codebook Features Sparse and Discrete Interpretab.pdf}
}

@article{templeton_scaling_2024,
  title = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
  author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian},
  year = 2024,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
  abstract = {Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer. At the time, a major concern was that this method might not scale feasibly to state-of-the-art transformers and, as a result, be unable to practically contribute to AI safety. Since then, scaling sparse autoencoders has been a major priority of the Anthropic interpretability team, and we're pleased to report extracting high-quality features from Claude 3 Sonnet, 1 Anthropic's medium-sized production model. We find a diversity of highly abstract features. They both respond to and behaviorally cause abstract behaviors. Examples of features we find include features for famous people, features for countries and cities, and features tracking type signatures in code. Many features are multilingual (responding to the same concept across languages) and multimodal (responding to the same concept in both text and images), as well as encompassing both abstract and concrete instantiations of the same idea (such as code with security vulnerabilities, and abstract discussion of security vulnerabilities). Some of the features we find are of particular interest because they may be safety-relevant -- that is, they are plausibly connected to a range of ways in which modern AI systems may cause harm. In particular, we find features related to security vulnerabilities and backdoors in code; bias (including both overt slurs, and more subtle biases); lying, deception, and power-seeking (including treacherous turns); sycophancy; and dangerous / criminal content (e.g., producing bioweapons). However, we caution not to read too much into the mere existence of such features: there's a difference (for example) between knowing about lies, being capable of lying, and actually lying in the real world. This research is also very preliminary. Further work will be needed to understand the implications of these potentially safety-relevant features. Key Results Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders. The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references. There appears to be a systematic relationship between the frequency of concepts and the dictionary size needed to resolve features for them. Features can be used to steer large models (see e.g. Influence on Behavior). This extends prior work on steering models using other methods (see Related Work). We observe features related to a broad range of safety concerns, including deception, sycophancy, bias, and dangerous content.},
  keywords = {scaling,sparse autoencoder}
}

@article{thasarathan_universal_2025,
  title = {Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment},
  shorttitle = {Universal Sparse Autoencoders},
  author = {Thasarathan, Harrish and Forsyth, Julian and Fel, Thomas and Kowal, Matthew and Derpanis, Konstantinos},
  year = 2025,
  month = feb,
  journal = {CoRR},
  eprint = {2502.03714},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2502.03714},
  url = {http://arxiv.org/abs/2502.03714},
  urldate = {2025-03-03},
  abstract = {We present Universal Sparse Autoencoders (USAEs), a framework for uncovering and aligning interpretable concepts spanning multiple pretrained deep neural networks. Unlike existing concept-based interpretability methods, which focus on a single model, USAEs jointly learn a universal concept space that can reconstruct and interpret the internal activations of multiple models at once. Our core insight is to train a single, overcomplete sparse autoencoder (SAE) that ingests activations from any model and decodes them to approximate the activations of any other model under consideration. By optimizing a shared objective, the learned dictionary captures common factors of variation-concepts-across different tasks, architectures, and datasets. We show that USAEs discover semantically coherent and important universal concepts across vision models; ranging from low-level features (e.g., colors and textures) to higher-level structures (e.g., parts and objects). Overall, USAEs provide a powerful new method for interpretable cross-model analysis and offers novel applications, such as coordinated activation maximization, that open avenues for deeper insights in multi-model AI systems},
  archiveprefix = {arXiv}
}

@article{thurnherr_tracrbench_2024,
  title = {TracrBench: Generating Interpretability Testbeds with Large Language Models},
  shorttitle = {TracrBench},
  author = {Thurnherr, Hannes and Scheurer, J{\'e}r{\'e}my},
  year = 2024,
  journal = {ICML MI Workshop},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2409.13714},
  url = {https://arxiv.org/abs/2409.13714},
  urldate = {2024-11-07},
  abstract = {Achieving a mechanistic understanding of transformer-based language models is an open challenge, especially due to their large number of parameters. Moreover, the lack of ground truth mappings between model weights and their functional roles hinders the effective evaluation of interpretability methods, impeding overall progress. Tracr, a method for generating compiled transformers with inherent ground truth mappings in RASP, has been proposed to address this issue. However, manually creating a large number of models needed for verifying interpretability methods is labour-intensive and time-consuming. In this work, we present a novel approach for generating interpretability test beds using large language models (LLMs) and introduce TracrBench, a novel dataset consisting of 121 manually written and LLM-generated, human-validated RASP programs and their corresponding transformer weights. During this process, we evaluate the ability of frontier LLMs to autonomously generate RASP programs and find that this task poses significant challenges. GPT-4-turbo, with a 20-shot prompt and best-of-5 sampling, correctly implements only 57 out of 101 test programs, necessitating the manual implementation of the remaining programs. With its 121 samples, TracrBench aims to serve as a valuable testbed for evaluating and comparing interpretability methods.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {compiled transformers,icml,interpbench},
  file = {/Users/leonardbereska/Zotero/storage/7V6YTXXQ/Thurnherr and Scheurer - 2024 - TracrBench Generating Interpretability Testbeds w.pdf}
}

@article{tigges_language_2024,
  title = {Language Models Linearly Represent Sentiment},
  author = {Tigges, Curt and Hollinsworth, Oskar John and Geiger, Atticus and Nanda, Neel},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=Xsf6dOOMMc},
  urldate = {2024-08-16},
  abstract = {Sentiment is a pervasive feature in natural language text, yet it is an open question how sentiment is represented within Large Language Models (LLMs). In this study, we reveal that across a range of models, sentiment is represented linearly: a single direction in activation space mostly captures the feature across a range of tasks with one extreme for positive and the other for negative. In a causal analysis, we isolate this direction using interventions and show it is causally active in both toy tasks and real world datasets such as Stanford Sentiment Treebank. We analyze the mechanisms that involve this direction and discover a phenomenon which we term the summarization motif: sentiment is not solely represented on emotionally charged words, but is additionally summarized at intermediate positions without inherent sentiment, such as punctuation and names. We show that in Stanford Sentiment Treebank zero-shot classification, ablating the sentiment direction across all tokens results in a drop in accuracy from 100\% to 62\% (vs. 50\% random baseline), while ablating the summarized sentiment direction at comma positions alone produces close to half this result (reducing accuracy to 82\%).},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/8FHMNRT9/Tigges et al. - 2024 - Language Models Linearly Represent Sentiment.pdf}
}

@article{till_sparse_2024,
  title = {Do sparse autoencoders find "true features"?},
  author = {Till, Demian},
  year = 2024,
  month = feb,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/QoR8noAB3Mp2KBA4B/do-sparse-autoencoders-find-true-features},
  urldate = {2024-12-20},
  abstract = {Thanks to Joseph Bloom and James Oldfield for giving feedback on drafts which helped improve the post \dots},
  language = {en}
}

@article{tishby_information_2000,
  title = {The information bottleneck method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = 2000,
  month = apr,
  journal = {CoRR},
  eprint = {physics/0004057},
  doi = {10.48550/arXiv.physics/0004057},
  url = {http://arxiv.org/abs/physics/0004057},
  urldate = {2024-01-18},
  abstract = {We define the relevant information in a signal \$x\textbackslash in X\$ as being the information that this signal provides about another signal \$y\textbackslash in \textbackslash Y\$. Examples include the information that face images provide about the names of the people portrayed, or the information that speech sounds provide about the words spoken. Understanding the signal \$x\$ requires more than just predicting \$y\$, it also requires specifying which features of \$\textbackslash X\$ play a role in the prediction. We formalize this problem as that of finding a short code for \$\textbackslash X\$ that preserves the maximum information about \$\textbackslash Y\$. That is, we squeeze the information that \$\textbackslash X\$ provides about \$\textbackslash Y\$ through a `bottleneck' formed by a limited set of codewords \$\textbackslash tX\$. This constrained optimization problem can be seen as a generalization of rate distortion theory in which the distortion measure \$d(x,\textbackslash x)\$ emerges from the joint statistics of \$\textbackslash X\$ and \$\textbackslash Y\$. This approach yields an exact set of self consistent equations for the coding rules \$X \textbackslash to \textbackslash tX\$ and \$\textbackslash tX \textbackslash to \textbackslash Y\$. Solutions to these equations can be found by a convergent re-estimation method that generalizes the Blahut-Arimoto algorithm. Our variational principle provides a surprisingly rich framework for discussing a variety of problems in signal processing and learning, as will be described in detail elsewhere.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/4LKQN8QT/Tishby et al. - 2000 - The information bottleneck method.pdf}
}

@article{tokui_disentanglement_2021,
  title = {Disentanglement Analysis with Partial Information Decomposition},
  author = {Tokui, Seiya and Sato, Issei},
  year = 2021,
  month = aug,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/9d31879efaea84a237bd1abec678d3315818a2ef},
  urldate = {2024-12-09},
  abstract = {We propose a framework to analyze how multivariate representations disentangle ground-truth generative factors. A quantitative analysis of disentanglement has been based on metrics designed to compare how one variable explains each generative factor. Current metrics, however, may fail to detect entanglement that involves more than two variables, e.g., representations that duplicate and rotate generative factors in high dimensional spaces. In this work, we establish a framework to analyze information sharing in a multivariate representation with Partial Information Decomposition and propose a new disentanglement metric. This framework enables us to understand disentanglement in terms of uniqueness, redundancy, and synergy. We develop an experimental protocol to assess how increasingly entangled representations are evaluated with each metric and confirm that the proposed metric correctly responds to entanglement. Through experiments on variational autoencoders, we find that models with similar disentanglement scores have a variety of characteristics in entanglement, for each of which a distinct strategy may be required to obtain a disentangled representation.},
  file = {/Users/leonardbereska/Zotero/storage/L8HVS92G/Tokui and Sato - 2021 - Disentanglement Analysis with Partial Information Decomposition.pdf}
}

@article{tsiligkaridis_second_2020,
  title = {Second Order Optimization for Adversarial Robustness and Interpretability},
  author = {Tsiligkaridis, Theodoros and Roberts, Jay},
  year = 2020,
  month = sep,
  journal = {CoRR},
  eprint = {2009.04923},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2009.04923},
  url = {http://arxiv.org/abs/2009.04923},
  urldate = {2024-08-04},
  abstract = {Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique aimed at learning features robust to such attacks and is widely regarded as a very effective defense. However, the computational cost of such training can be prohibitive as the network size and input dimensions grow. Inspired by the relationship between robustness and curvature, we propose a novel regularizer which incorporates first and second order information via a quadratic approximation to the adversarial loss. The worst case quadratic loss is approximated via an iterative scheme. It is shown that using only a single iteration in our regularizer achieves stronger robustness than prior gradient and curvature regularization schemes, avoids gradient obfuscation, and, with additional iterations, achieves strong robustness with significantly lower training time than AT. Further, it retains the interesting facet of AT that networks learn features which are well-aligned with human perception. We demonstrate experimentally that our method produces higher quality human-interpretable features than other geometric regularization techniques. These robust features are then used to provide human-friendly explanations to model predictions.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/XIT5K6EP/Tsiligkaridis and Roberts - 2020 - Second Order Optimization for Adversarial Robustne.pdf}
}

@article{tsipras_robustness_2019,
  title = {Robustness May Be at Odds with Accuracy},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  year = 2019,
  month = sep,
  journal = {ICLR},
  eprint = {1805.12152},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1805.12152},
  url = {http://arxiv.org/abs/1805.12152},
  urldate = {2024-08-03},
  abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/FW8KUFLM/Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf}
}

@article{vaintrob_mathematical_2024,
  title = {Toward A Mathematical Framework for Computation in Superposition},
  author = {Vaintrob, Dmitry and Mendel, Jake and H{\"a}nni, Kaarel},
  year = 2024,
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/2roZtSr5TGmLjXMnT/toward-a-mathematical-framework-for-computation-in},
  urldate = {2024-02-22},
  abstract = {Author order randomized. Authors contributed roughly equally --- see attribution section for details. \dots},
  language = {en},
  keywords = {algorithms,computation,computation in superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/YI4CT4CC/Vaintrob et al. - 2024 - Toward A Mathematical Framework for Computation in.html}
}

@article{valeriani_geometry_2023,
  title = {The geometry of hidden representations of large transformer models},
  author = {Valeriani, Lucrezia and Doimo, Diego and Cuturello, Francesca and Laio, Alessandro and Ansuini, Alessio and Cazzaniga, Alberto},
  year = 2023,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.00294},
  url = {https://arxiv.org/abs/2302.00294},
  urldate = {2025-05-07},
  abstract = {Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be observed across many models trained on diverse datasets. Based on our findings, we point out an explicit strategy to identify, without supervision, the layers that maximize semantic content: representations at intermediate layers corresponding to a relative minimum of the ID profile are more suitable for downstream learning tasks.},
  copyright = {Creative Commons Attribution 4.0 International},
  file = {/Users/leonardbereska/Zotero/storage/B7NTDLLT/Valeriani et al. - 2023 - The geometry of hidden representations of large transformer models.pdf}
}

@article{variengien_look_2023,
  title = {Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models},
  shorttitle = {Look Before You Leap},
  author = {Variengien, Alexandre and Winsor, Eric},
  year = 2023,
  month = dec,
  journal = {ICML MI Workshop},
  eprint = {2312.10091},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2312.10091},
  url = {http://arxiv.org/abs/2312.10091},
  urldate = {2024-01-03},
  abstract = {When solving challenging problems, language models (LMs) are able to identify relevant information from long and complicated contexts. To study how LMs solve retrieval tasks in diverse situations, we introduce ORION, a collection of structured retrieval tasks spanning six domains, from text understanding to coding. Each task in ORION can be represented abstractly by a request (e.g. a question) that retrieves an attribute (e.g. the character name) from a context (e.g. a story). We apply causal analysis on 18 open-source language models with sizes ranging from 125 million to 70 billion parameters. We find that LMs internally decompose retrieval tasks in a modular way: middle layers at the last token position process the request, while late layers retrieve the correct entity from the context. After causally enforcing this decomposition, models are still able to solve the original task, preserving 70\% of the original correct token probability in 98 of the 106 studied model-task pairs. We connect our macroscopic decomposition with a microscopic description by performing a fine-grained case study of a question-answering task on Pythia-2.8b. Building on our high-level understanding, we demonstrate a proof of concept application for scalable internal oversight of LMs to mitigate prompt-injection while requiring human supervision on only a single input. Our solution improves accuracy drastically (from 15.5\% to 97.5\% on Pythia-12b). This work presents evidence of a universal emergent modular processing of tasks across varied domains and models and is a pioneering effort in applying interpretability for scalable internal oversight of LMs.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/5SQNQ5A4/Variengien and Winsor - 2023 - Look Before You Leap A Universal Emergent Decompo.pdf}
}

@article{viswanathan_geometry_2025,
  title = {The Geometry of Tokens in Internal Representations of Large Language Models},
  author = {Viswanathan, Karthik and Gardinazzi, Yuri and Panerai, Giada and Cazzaniga, Alberto and Biagetti, Matteo},
  year = 2025,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2501.10573},
  url = {https://arxiv.org/abs/2501.10573},
  urldate = {2025-05-07},
  abstract = {We investigate the relationship between the geometry of token embeddings and their role in the next token prediction within transformer models. An important aspect of this connection uses the notion of empirical measure, which encodes the distribution of token point clouds across transformer layers and drives the evolution of token representations in the mean-field interacting picture. We use metrics such as intrinsic dimension, neighborhood overlap, and cosine similarity to observationally probe these empirical measures across layers. To validate our approach, we compare these metrics to a dataset where the tokens are shuffled, which disrupts the syntactic and semantic structure. Our findings reveal a correlation between the geometric properties of token embeddings and the cross-entropy loss of next token predictions, implying that prompts with higher loss values have tokens represented in higher-dimensional spaces.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/leonardbereska/Zotero/storage/DHTGJVU8/Viswanathan et al. - 2025 - The Geometry of Tokens in Internal Representations of Large Language Models.pdf}
}

@article{wang_differentiation_2024a,
  title = {Differentiation and Specialization of Attention Heads via the Refined Local Learning Coefficient},
  author = {Wang, George and Hoogland, Jesse and van Wingerden, Stan and Furman, Zach and Murfet, Daniel},
  year = 2024,
  month = oct,
  journal = {CoRR},
  doi = {10.48550/arXiv.2410.02984},
  url = {http://arxiv.org/abs/2410.02984},
  urldate = {2024-11-15},
  abstract = {We introduce refined variants of the Local Learning Coefficient (LLC), a measure of model complexity grounded in singular learning theory, to study the development of internal structure in transformer language models during training. By applying these \textbackslash textit\textbraceleft refined LLCs\textbraceright{} (rLLCs) to individual components of a two-layer attention-only transformer, we gain novel insights into the progressive differentiation and specialization of attention heads. Our methodology reveals how attention heads differentiate into distinct functional roles over the course of training, analyzes the types of data these heads specialize to process, and discovers a previously unidentified multigram circuit. These findings demonstrate that rLLCs provide a principled, quantitative toolkit for \textbackslash textit\textbraceleft developmental interpretability\textbraceright, which aims to understand models through their evolution across the learning process. More broadly, this work takes a step towards establishing the correspondence between data distributional structure, geometric properties of the loss landscape, learning dynamics, and emergent computational structures in neural networks.},
  file = {/Users/leonardbereska/Zotero/storage/NMMD6HR6/Wang et al. - 2024 - Differentiation and Specialization of Attention He.pdf}
}

@article{wang_grokked_2024,
  title = {Grokked Transformers are Implicit Reasoners: A Mechanistic Journey to the Edge of Generalization},
  shorttitle = {Grokked Transformers are Implicit Reasoners},
  author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=ns8IH5Sn5y},
  urldate = {2024-08-16},
  abstract = {We study whether transformers can learn to *implicitly* reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers *can* learn implicit reasoning, but only through *grokking*, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
  language = {en},
  keywords = {grokking},
  file = {/Users/leonardbereska/Zotero/storage/35UMZ3NI/Wang et al. - 2024 - Grokked Transformers are Implicit Reasoners A Mec.pdf}
}

@book{watanabe_algebraic_2009,
  title = {Algebraic Geometry and Statistical Learning Theory},
  author = {Watanabe, Sumio},
  year = 2009,
  series = {Cambridge Monographs on Applied and Computational Mathematics},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511800474},
  url = {https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A},
  urldate = {2023-11-10},
  abstract = {Sure to be influential, this book lays the foundations for the use of algebraic geometry in statistical learning theory. Many widely used statistical models and learning machines applied to information science have a parameter space that is singular: mixture models, neural networks, HMMs, Bayesian networks, and stochastic context-free grammars are major examples. Algebraic geometry and singularity theory provide the necessary tools for studying such non-smooth models. Four main formulas are established: 1. the log likelihood function can be given a common standard form using resolution of singularities, even applied to more complex models; 2. the asymptotic behaviour of the marginal likelihood or 'the evidence' is derived based on zeta function theory; 3. new methods are derived to estimate the generalization errors in Bayes and Gibbs estimations from training errors; 4. the generalization errors of maximum likelihood and a posteriori methods are clarified by empirical process theory on algebraic varieties.},
  keywords = {singular learning theory,textbook}
}

@article{watanabe_almost_2007,
  title = {Almost All Learning Machines are Singular},
  author = {Watanabe, Sumio},
  year = 2007,
  month = apr,
  journal = {2007 IEEE Symposium on Foundations of Computational Intelligence},
  pages = {383--388},
  publisher = {IEEE},
  address = {Honolulu, HI, USA},
  doi = {10.1109/FOCI.2007.371500},
  url = {http://ieeexplore.ieee.org/document/4233934/},
  urldate = {2024-11-25},
  abstract = {A learning machine is called singular if its Fisher information matrix is singular. Almost all learning machines used in information processing are singular, for example, layered neural networks, normal mixtures, binomial mixtures, Bayes networks, hidden Markov models, Boltzmann machines, stochastic context-free grammars, and reduced rank regressions are singular. In singular learning machines, the likelihood function can not be approximated by any quadratic form of the parameter. Moreover, neither the distribution of the maximum likelihood estimator nor the Bayes a posteriori distribution converges to the normal distribution, even if the number of training samples tends to infinity. Therefore, the conventional statistical learning theory does not hold in singular learning machines. This paper establishes the new mathematical foundation for singular learning machines. We propose that, by using resolution of singularities, the likelihood function can be represented as the standard form, by which we can prove the asymptotic behavior of the generalization errors of the maximum likelihood method and the Bayes estimation. The result will be a base on which training algorithms of singular learning machines are devised and optimized},
  isbn = {9781424407033}
}

@book{watanabe_mathematical_2018,
  title = {Mathematical Theory of Bayesian Statistics},
  author = {Watanabe, Sumio},
  year = 2018,
  month = apr,
  edition = {1},
  publisher = {{Chapman and Hall}},
  doi = {10.1201/9781315373010},
  url = {https://www.taylorfrancis.com/books/9781482238082},
  urldate = {2023-11-10},
  abstract = {Semantic Scholar extracted view of "Mathematical Theory of Bayesian Statistics" by Sumio Watanabe},
  language = {en},
  keywords = {singular learning theory,textbook}
}

@article{wattenberg_relational_2024,
  title = {Relational Composition in Neural Networks: A Survey and Call to Action},
  shorttitle = {Relational Composition in Neural Networks},
  author = {Wattenberg, Martin and Vi{\'e}gas, Fernanda},
  year = 2024,
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=zzCEiUIPk9},
  urldate = {2024-08-16},
  abstract = {Many neural nets appear to represent data as linear combinations of ``feature vectors.'' Algorithms for discovering these vectors have seen impressive recent success. However, we argue that this success is incomplete without an understanding of relational composition: how (or whether) neural nets combine feature vectors to represent more complicated relationships. To facilitate research in this area, this paper offers a guided tour of various relational mechanisms that have been proposed, along with preliminary analysis of how such mechanisms might affect the search for interpretable features. We end with a series of promising areas for empirical research, which may help determine how neural networks represent structured data.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/WEMPRHNJ/Wattenberg and Viégas - 2024 - Relational Composition in Neural Networks A Surve.pdf}
}

@article{weiss_thinking_2021,
  title = {Thinking Like Transformers},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  year = 2021,
  month = jul,
  journal = {CoRR},
  eprint = {2106.06981},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.06981},
  url = {http://arxiv.org/abs/2106.06981},
  urldate = {2023-06-23},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  archiveprefix = {arXiv},
  keywords = {compiled transformers,computation in superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/YCKN78ZS/Weiss et al. - 2021 - Thinking Like Transformers.pdf}
}

@article{whittington_disentangling_2022,
  title = {Disentangling with Biological Constraints: A Theory of Functional Cell Types},
  shorttitle = {Disentangling with Biological Constraints},
  author = {Whittington, James C. R. and Dorrell, Will and Ganguli, Surya and Behrens, Timothy E. J.},
  year = 2022,
  month = sep,
  journal = {CoRR},
  eprint = {2210.01768},
  primaryclass = {cs, q-bio},
  url = {http://arxiv.org/abs/2210.01768},
  urldate = {2023-01-09},
  abstract = {Neurons in the brain are often finely tuned for specific task variables. Moreover, such disentangled representations are highly sought after in machine learning. Here we mathematically prove that simple biological constraints on neurons, namely nonnegativity and energy efficiency in both activity and weights, promote such sought after disentangled representations by enforcing neurons to become selective for single factors of task variation. We demonstrate these constraints lead to disentangling in a variety of tasks and architectures, including variational autoencoders. We also use this theory to explain why the brain partitions its cells into distinct cell types such as grid and object-vector cells, and also explain when the brain instead entangles representations in response to entangled task factors. Overall, this work provides a mathematical understanding of why, when, and how neurons represent factors in both brains and machines, and is a first step towards understanding of how task demands structure neural representations.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {bioinspired,disentangling,grid cells},
  file = {/Users/leonardbereska/Zotero/storage/6QD5J4UH/Whittington et al. - 2022 - Disentangling with Biological Constraints A Theor.pdf}
}

@article{wortsman_supermasks_2020,
  title = {Supermasks in superposition},
  author = {Wortsman, Mitchell and Ramanujan, Vivek and Liu, Rosanne and Kembhavi, Aniruddha and Rastegari, Mohammad and Yosinski, Jason and Farhadi, Ali},
  year = 2020,
  journal = {NeurIPS},
  url = {https://arxiv.org/abs/2006.14769},
  keywords = {empirical findings in superposition,superposition},
  file = {/Users/leonardbereska/Zotero/storage/Q629Y7YB/Wortsman et al. - 2020 - Supermasks in superposition.pdf}
}

@article{wright_addressing_2024,
  title = {Addressing Feature Suppression in SAEs},
  author = {Wright, Benjamin and Sharkey, Lee},
  year = 2024,
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes},
  urldate = {2025-02-01},
  abstract = {Produced as part of the ML Alignment Theory Scholars Program - Winter 2023-24 Cohort as part of Lee Sharkey's stream. \dots},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/QXF4H9IY/Wright and Sharkey - 2024 - Addressing Feature Suppression in SAEs.html}
}

@article{xiao_fashionmnist_2017,
  title = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  shorttitle = {Fashion-MNIST},
  author = {Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  year = 2017,
  month = sep,
  journal = {CoRR},
  eprint = {1708.07747},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1708.07747},
  url = {http://arxiv.org/abs/1708.07747},
  urldate = {2025-07-07},
  abstract = {We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at https://github.com/zalandoresearch/fashion-mnist},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/QP6KUZK2/Xiao et al. - 2017 - Fashion-MNIST a Novel Image Dataset for Benchmark.pdf;/Users/leonardbereska/Zotero/storage/CPY7BK9G/1708.html}
}

@article{yu_whitebox_2023,
  title = {White-Box Transformers via Sparse Rate Reduction},
  author = {Yu, Yaodong and Buchanan, Sam and Pai, Druv and Chu, Tianzhe and Wu, Ziyang and Tong, Shengbang and Haeffele, Benjamin D. and Ma, Yi},
  year = 2023,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2306.01129},
  url = {https://arxiv.org/abs/2306.01129},
  urldate = {2023-11-10},
  abstract = {In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at \textbackslash url\textbraceleft https://github.com/Ma-Lab-Berkeley/CRATE\textbraceright.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/leonardbereska/Zotero/storage/W67WAT78/Yu et al. - 2023 - White-Box Transformers via Sparse Rate Reduction.pdf}
}

@article{yun_transformer_2021,
  title = {Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors},
  shorttitle = {Transformer visualization via dictionary learning},
  author = {Yun, Zeyu and Chen, Yubei and Olshausen, Bruno A. and LeCun, Yann},
  year = 2021,
  journal = {NAACL Workshop DeeLIO},
  eprint = {2103.15949},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2103.15949},
  url = {http://arxiv.org/abs/2103.15949},
  urldate = {2023-11-16},
  abstract = {Transformer networks have revolutionized NLP representation learning since they were introduced. Though a great effort has been made to explain the representation in transformers, it is widely recognized that our understanding is not sufficient. One important reason is that there lack enough visualization tools for detailed analysis. In this paper, we propose to use dictionary learning to open up these "black boxes" as linear superpositions of transformer factors. Through visualization, we demonstrate the hierarchical semantic structures captured by the transformer factors, e.g., word-level polysemy disambiguation, sentence-level pattern formation, and long-range dependency. While some of these patterns confirm the conventional prior linguistic knowledge, the rest are relatively unexpected, which may provide new insights. We hope this visualization tool can bring further knowledge and a better understanding of how transformer networks work. The code is available at https://github.com/zeyuyun1/TransformerVis},
  archiveprefix = {arXiv},
  keywords = {sparse autoencoder},
  file = {/Users/leonardbereska/Zotero/storage/TDV636A2/Yun et al. - 2021 - Transformer visualization via dictionary learning.pdf}
}

@article{zou_universal_2023,
  title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author = {Zou, Andy and Wang, Zifan and Kolter, J. Zico and Fredrikson, Matt},
  year = 2023,
  month = jul,
  journal = {CoRR},
  eprint = {2307.15043},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.15043},
  url = {http://arxiv.org/abs/2307.15043},
  urldate = {2023-10-27},
  abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/7EWJYTXS/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on .pdf}
}
