@article{chan_causal_2022,
  title = {Causal Scrubbing: a method for rigorously testing interpretability hypotheses [Redwood Research]},
  shorttitle = {Causal Scrubbing},
  author = {Chan, Lawrence and {Garriga-alonso}, Adri{\`a} and {Goldowsky-Dill}, Nicholas and {ryan\_greenblatt} and {jenny} and Radhakrishnan, Ansh and Buck and Thomas, Nate},
  year = {2022},
  month = dec,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing},
  urldate = {2023-08-27},
  abstract = {* Authors sorted alphabetically. {$\bullet$}  Summary: This post introduces causal scrubbing, a principled approach for evaluating the quality of mechanistic i{\dots}},
  language = {en},
  keywords = {cited,mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/ARPGBRE6/Chan et al. - 2022 - Causal Scrubbing a method for rigorously testing .html}
}

@article{lieberum_does_2023,
  title = {Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla},
  shorttitle = {Does Circuit Analysis Interpretability Scale?},
  author = {Lieberum, Tom and Rahtz, Matthew and Kram{\'a}r, J{\'a}nos and Nanda, Neel and Irving, Geoffrey and Shah, Rohin and Mikulik, Vladimir},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.09458},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.09458},
  url = {http://arxiv.org/abs/2307.09458},
  urldate = {2023-08-26},
  abstract = {{\textbackslash}emph\{Circuit analysis\} is a promising technique for understanding the internal mechanisms of language models. However, existing analyses are done in small models far from the state of the art. To address this, we present a case study of circuit analysis in the 70B Chinchilla model, aiming to test the scalability of circuit analysis. In particular, we study multiple-choice question answering, and investigate Chinchilla's capability to identify the correct answer {\textbackslash}emph\{label\} given knowledge of the correct answer {\textbackslash}emph\{text\}. We find that the existing techniques of logit attribution, attention pattern visualization, and activation patching naturally scale to Chinchilla, allowing us to identify and categorize a small set of `output nodes' (attention heads and MLPs). We further study the `correct letter' category of attention heads aiming to understand the semantics of their features, with mixed results. For normal multiple-choice question answers, we significantly compress the query, key and value subspaces of the head without loss of performance when operating on the answer labels for multiple-choice questions, and we show that the query and key subspaces represent an `Nth item in an enumeration' feature to at least some extent. However, when we attempt to use this explanation to understand the heads' behaviour on a more general distribution including randomized answer labels, we find that it is only a partial explanation, suggesting there is more to learn about the operation of `correct letter' heads on multiple choice question answering.},
  archiveprefix = {arXiv},
  keywords = {circuit,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/V7AK9YDA/Lieberum et al. - 2023 - Does Circuit Analysis Interpretability Scale Evid.pdf}
}

@article{meng_locating_2022,
  title = {Locating and Editing Factual Associations in GPT},
  author = {Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  year = {2022},
  journal = {NeurIPS},
  eprint = {2202.05262},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2202.05262},
  url = {http://arxiv.org/abs/2202.05262},
  urldate = {2023-08-27},
  abstract = {We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME). We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available at https://rome.baulab.info/},
  archiveprefix = {arXiv},
  keywords = {cited,editing,empirical,mechinterp,memory,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/AQAYITEA/Meng et al. - 2022 - Locating and Editing Factual Associations in GPT.pdf}
}

@article{bau_network_2017,
  title = {Network Dissection: Quantifying Interpretability of Deep Visual Representations},
  shorttitle = {Network Dissection},
  author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  year = {2017},
  month = jul,
  journal = {CVPR},
  pages = {3319--3327},
  publisher = {IEEE},
  address = {Honolulu, HI},
  doi = {10.1109/CVPR.2017.354},
  url = {http://ieeexplore.ieee.org/document/8099837/},
  urldate = {2023-08-27},
  abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a data set of concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are labeled across a broad range of visual concepts including objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability is an axis-independent property of the representation space, then we apply the method to compare the latent representations of various networks when trained to solve different classification problems. We further analyze the effect of training iterations, compare networks trained with different initializations, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
  isbn = {9781538604571},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/HE2PLXSY/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf}
}

@article{goldowsky-dill_localizing_2023,
  title = {Localizing Model Behavior with Path Patching},
  author = {{Goldowsky-Dill}, Nicholas and MacLeod, Chris and Sato, Lucas and Arora, Aryaman},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.05969},
  url = {https://arxiv.org/abs/2304.05969},
  urldate = {2023-08-27},
  abstract = {Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,mechinterp,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/JEGX47TY/Goldowsky-Dill et al. - 2023 - Localizing Model Behavior with Path Patching.pdf}
}

@article{pai_facade_2023,
  title = {FACADE: A Framework for Adversarial Circuit Anomaly Detection and Evaluation},
  shorttitle = {FACADE},
  author = {Pai, Dhruv and Carranza, Andres and Schaeffer, Rylan and Tandon, Arnuv and Koyejo, Sanmi},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.10563},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.10563},
  url = {http://arxiv.org/abs/2307.10563},
  urldate = {2023-08-27},
  abstract = {We present FACADE, a novel probabilistic and geometric framework designed for unsupervised mechanistic anomaly detection in deep neural networks. Its primary goal is advancing the understanding and mitigation of adversarial attacks. FACADE aims to generate probabilistic distributions over circuits, which provide critical insights to their contribution to changes in the manifold properties of pseudo-classes, or high-dimensional modes in activation space, yielding a powerful tool for uncovering and combating adversarial attacks. Our approach seeks to improve model robustness, enhance scalable model oversight, and demonstrates promising applications in real-world deployment settings.},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/B5QJJ6BB/Pai et al. - 2023 - FACADE A Framework for Adversarial Circuit Anomal.pdf}
}

@article{geiger_causal_2023,
  title = {Causal Abstraction for Faithful Model Interpretation},
  author = {Geiger, Atticus and Potts, Chris and Icard, Thomas},
  year = {2023},
  month = jan,
  journal = {CoRR},
  eprint = {2301.04709},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2301.04709},
  url = {http://arxiv.org/abs/2301.04709},
  urldate = {2023-08-27},
  abstract = {A faithful and interpretable explanation of an AI model's behavior and internal structure is a high-level explanation that is human-intelligible but also consistent with the known, but often opaque low-level causal details of the model. We argue that the theory of causal abstraction provides the mathematical foundations for the desired kinds of model explanations. In causal abstraction analysis, we use interventions on model-internal states to rigorously assess whether an interpretable high-level causal model is a faithful description of an AI model. Our contributions in this area are: (1) We generalize causal abstraction to cyclic causal structures and typed high-level variables. (2) We show how multi-source interchange interventions can be used to conduct causal abstraction analyses. (3) We define a notion of approximate causal abstraction that allows us to assess the degree to which a high-level causal model is a causal abstraction of a lower-level one. (4) We prove constructive causal abstraction can be decomposed into three operations we refer to as marginalization, variable-merge, and value-merge. (5) We formalize the XAI methods of LIME, causal effect estimation, causal mediation analysis, iterated nullspace projection, and circuit-based explanations as special cases of causal abstraction analysis.},
  archiveprefix = {arXiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/PKIVIPAG/Geiger et al. - 2023 - Causal Abstraction for Faithful Model Interpretati.pdf}
}

@article{conmy_automated_2023,
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author = {Conmy, Arthur and {Mavor-Parker}, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and {Garriga-Alonso}, Adri{\`a}},
  year = {2023},
  journal = {NeurIPS},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2304.14997},
  url = {https://arxiv.org/abs/2304.14997},
  urldate = {2023-07-31},
  abstract = {Through considerable effort and intuition, several recent works have reverse-engineered nontrivial behaviors of transformer models. This paper systematizes the mechanistic interpretability process they followed. First, researchers choose a metric and dataset that elicit the desired model behavior. Then, they apply activation patching to find which abstract neural network units are involved in the behavior. By varying the dataset, metric, and units under investigation, researchers can understand the functionality of each component. We automate one of the process' steps: to identify the circuit that implements the specified behavior in the model's computational graph. We propose several algorithms and reproduce previous interpretability results to validate them. For example, the ACDC algorithm rediscovered 5/5 of the component types in a circuit in GPT-2 Small that computes the Greater-Than operation. ACDC selected 68 of the 32,000 edges in GPT-2 Small, all of which were manually found by previous work. Our code is available at https://github.com/ArthurConmy/Automatic-Circuit-Discovery.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {algorithms,automate,behavior,cited,graph,mechinterp,scale,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/4NTYQMGA/Conmy et al. - 2023 - Towards Automated Circuit Discovery for Mechanisti.pdf}
}

@article{casper_red_2023,
  title = {Red Teaming Deep Neural Networks with Feature Synthesis Tools},
  author = {Casper, Stephen and Li, Yuxiao and Li, Jiawei and Bu, Tong and Zhang, Kevin and Hariharan, Kaivalya and {Hadfield-Menell}, Dylan},
  year = {2023},
  journal = {NeurIPS},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.10894},
  url = {https://arxiv.org/abs/2302.10894},
  urldate = {2023-09-18},
  abstract = {Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified novel, previously unknown, bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze and explain the behavior of a model using a particular dataset. While this is useful, such tools can only analyze behaviors induced by features that the user can sample or identify in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods which do not depend on a dataset. In this paper, our primary contribution is a benchmark to evaluate interpretability tools. Our key insight is that we can train models that respond to specific triggers (e.g., a specific patch inserted into an image) with specific outputs (i.e. a label) and then evaluate interpretability tools based on whether they help humans identify these triggers. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a trojan-discovery benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 feature attribution/saliency tools. Even with access to data with a trojan's trigger, these methods regularly fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 variants of the best-performing method from the previous evaluation.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/HUS59QSC/Casper et al. - 2023 - Red Teaming Deep Neural Networks with Feature Synt.pdf}
}

@article{bills_language_2023,
  title = {Language models can explain neurons in language models},
  author = {Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  year = {2023},
  journal = {OpenAI Blog},
  url = {https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html},
  abstract = {Language models have become more capable and more widely deployed, but we do not understand how they work. Recent work has made progress on understanding a small number of circuits and narrow behaviors, but to fully understand a language model, we'll need to analyze millions of neurons. This paper applies automation to the problem of scaling an interpretability technique to all the neurons in a large language model. Our hope is that building on this approach of automating interpretability will enable us to comprehensively audit the safety of models before deployment. Our technique seeks to explain what patterns in text cause a neuron to activate. It consists of three steps: Step 1Explain the neuron's activations using GPT-4 Show neuron activations to GPT-4: The Avengers to the big screen, Joss Whedon has returned to reunite Marvel's gang of superheroes for their toughest challenge yet. Avengers: Age of Ultron pits the titular heroes against a sentient artificial intelligence, and smart money says that it could soar at the box office to be the highest-grossing film of the introduction into the Marvel cinematic universe, it's possible, though Marvel Studios boss Kevin Feige told Entertainment Weekly that, "Tony is earthbound and facing earthbound villains. You will not find magic power rings firing ice and flame beams." Spoilsport! But he does hint that they have some use{\dots} STARK T , which means this Nightwing movie is probably not about the guy who used to own that suit. So, unless new director Matt Reeves' The Batman is going to dig into some of this backstory or introduce the Dick Grayson character in his movie, the Nightwing movie is going to have a lot of work to do explaining of Avengers who weren't in the movie and also Thor try to fight the infinitely powerful Magic Space Fire Bird. It ends up being completely pointless, an embarrassing loss, and I'm pretty sure Thor accidentally destroys a planet. That's right. In an effort to save Earth, one of the heroes inadvertantly blows up an GPT-4 gives an explanation, guessing that the neuron is activating on references to movies, characters, and entertainment. Step 2Simulate activations using GPT-4, conditioning on the explanation Step 3Score the explanation by comparing the simulated and real activations},
  keywords = {automate,cited,graph,mechinterp,neuron,to cite,to extract figures,to extract related work,to review in detail,tool},
  file = {/Users/leonardbereska/Zotero/storage/9B7H84NW/Bills et al. - 2023 - Language models can explain neurons in language mo.html}
}

@article{meng_massediting_2022,
  title = {Mass-Editing Memory in a Transformer},
  author = {Meng, Kevin and Sharma, Arnab Sen and Andonian, Alex and Belinkov, Yonatan and Bau, David},
  year = {2022},
  journal = {ICLR},
  eprint = {2210.07229},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.07229},
  url = {http://arxiv.org/abs/2210.07229},
  urldate = {2023-08-27},
  abstract = {Recent work has shown exciting promise in updating large language models with new memories, so as to replace obsolete information or add specialized knowledge. However, this line of work is predominantly limited to updating single associations. We develop MEMIT, a method for directly updating a language model with many memories, demonstrating experimentally that it can scale up to thousands of associations for GPT-J (6B) and GPT-NeoX (20B), exceeding prior work by orders of magnitude. Our code and data are at https://memit.baulab.info.},
  archiveprefix = {arXiv},
  keywords = {cited,editing,empirical,mechinterp,memory,to cite,to extract figures,to extract related work,to review in detail,transformer},
  file = {/Users/leonardbereska/Zotero/storage/YZP7I3LD/Meng et al. - 2022 - Mass-Editing Memory in a Transformer.pdf}
}

@article{rauker_transparent_2023,
  title = {Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks},
  shorttitle = {Toward Transparent AI},
  author = {R{\"a}uker, Tilman and Ho, Anson and Casper, Stephen and {Hadfield-Menell}, Dylan},
  year = {2023},
  month = aug,
  journal = {TMLR},
  eprint = {2207.13243},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2207.13243},
  url = {http://arxiv.org/abs/2207.13243},
  urldate = {2023-08-27},
  abstract = {The last decade of machine learning has seen drastic increases in scale and capabilities. Deep neural networks (DNNs) are increasingly being deployed in the real world. However, they are difficult to analyze, raising concerns about using them without a rigorous understanding of how they function. Effective tools for interpreting them will be important for building more trustworthy AI by helping to identify problems, fix bugs, and improve basic understanding. In particular, "inner" interpretability techniques, which focus on explaining the internal components of DNNs, are well-suited for developing a mechanistic understanding, guiding manual modifications, and reverse engineering solutions. Much recent work has focused on DNN interpretability, and rapid progress has thus far made a thorough systematization of methods difficult. In this survey, we review over 300 works with a focus on inner interpretability tools. We introduce a taxonomy that classifies methods by what part of the network they help to explain (weights, neurons, subnetworks, or latent representations) and whether they are implemented during (intrinsic) or after (post hoc) training. To our knowledge, we are also the first to survey a number of connections between interpretability research and work in adversarial robustness, continual learning, modularity, network compression, and studying the human visual system. We discuss key challenges and argue that the status quo in interpretability research is largely unproductive. Finally, we highlight the importance of future work that emphasizes diagnostics, debugging, adversaries, and benchmarking in order to make interpretability tools more useful to engineers in practical applications.},
  archiveprefix = {arXiv},
  keywords = {cited,mechinterp,review,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/2MVPX3C4/Räuker et al. - 2023 - Toward Transparent AI A Survey on Interpreting th.pdf}
}

@article{wang_interpretability_2023,
  title = {Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small},
  shorttitle = {Interpretability in the Wild},
  author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
  year = {2023},
  journal = {ICLR},
  eprint = {2211.00593},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.00593},
  url = {http://arxiv.org/abs/2211.00593},
  urldate = {2023-08-27},
  abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
  archiveprefix = {arXiv},
  keywords = {behavior,circuit,cited,empirical,graph,mechinterp,scale,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/8JN38PFS/Wang et al. - 2023 - Interpretability in the Wild a Circuit for Indire.pdf}
}

@article{wu_interpretability_2023,
  title = {Interpretability at Scale: Identifying Causal Mechanisms in Alpaca},
  shorttitle = {Interpretability at Scale},
  author = {Wu, Zhengxuan and Geiger, Atticus and Potts, Christopher and Goodman, Noah D.},
  year = {2023},
  month = may,
  journal = {CoRR},
  eprint = {2305.08809},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2305.08809},
  url = {http://arxiv.org/abs/2305.08809},
  urldate = {2023-08-27},
  abstract = {Obtaining human-interpretable explanations of large, general-purpose language models is an urgent goal for AI safety. However, it is just as important that our interpretability methods are faithful to the causal dynamics underlying model behavior and able to robustly generalize to unseen inputs. Distributed Alignment Search (DAS) is a powerful gradient descent method grounded in a theory of causal abstraction that uncovered perfect alignments between interpretable symbolic algorithms and small deep learning models fine-tuned for specific tasks. In the present paper, we scale DAS significantly by replacing the remaining brute-force search steps with learned parameters -- an approach we call DAS. This enables us to efficiently search for interpretable causal structure in large language models while they follow instructions. We apply DAS to the Alpaca model (7B parameters), which, off the shelf, solves a simple numerical reasoning problem. With DAS, we discover that Alpaca does this by implementing a causal model with two interpretable boolean variables. Furthermore, we find that the alignment of neural representations with these variables is robust to changes in inputs and instructions. These findings mark a first step toward deeply understanding the inner-workings of our largest and most widely deployed language models.},
  archiveprefix = {arXiv},
  keywords = {causal,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/KEDMWB53/Wu et al. - 2023 - Interpretability at Scale Identifying Causal Mech.pdf}
}

@article{mcgrath_hydra_2023,
  title = {The Hydra Effect: Emergent Self-repair in Language Model Computations},
  shorttitle = {The Hydra Effect},
  author = {McGrath, Thomas and Rahtz, Matthew and Kramar, Janos and Mikulik, Vladimir and Legg, Shane},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.15771},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.15771},
  url = {http://arxiv.org/abs/2307.15771},
  urldate = {2023-10-30},
  abstract = {We investigate the internal structure of language model computations using causal analysis and demonstrate two motifs: (1) a form of adaptive computation where ablations of one attention layer of a language model cause another layer to compensate (which we term the Hydra effect) and (2) a counterbalancing function of late MLP layers that act to downregulate the maximum-likelihood token. Our ablation studies demonstrate that language model layers are typically relatively loosely coupled (ablations to one layer only affect a small number of downstream layers). Surprisingly, these effects occur even in language models trained without any form of dropout. We analyse these effects in the context of factual recall and consider their implications for circuit-level attribution in language models.},
  archiveprefix = {arXiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/FKZRFPN3/McGrath et al. - 2023 - The Hydra Effect Emergent Self-repair in Language.pdf}
}

@article{syed_attribution_2023,
  title = {Attribution Patching Outperforms Automated Circuit Discovery},
  author = {Syed, Aaquib and Rager, Can and Conmy, Arthur},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.10348},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.10348},
  url = {http://arxiv.org/abs/2310.10348},
  urldate = {2023-10-27},
  abstract = {Automated interpretability research has recently attracted attention as a potential research direction that could scale explanations of neural network behavior to large models. Existing automated circuit discovery work applies activation patching to identify subnetworks responsible for solving specific tasks (circuits). In this work, we show that a simple method based on attribution patching outperforms all existing methods while requiring just two forward passes and a backward pass. We apply a linear approximation to activation patching to estimate the importance of each edge in the computational subgraph. Using this approximation, we prune the least important edges of the network. We survey the performance and limitations of this method, finding that averaged over all tasks our method has greater AUC from circuit recovery than other methods.},
  archiveprefix = {arXiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/XYLZIIVA/Syed et al. - 2023 - Attribution Patching Outperforms Automated Circuit.pdf}
}

@article{cunningham_sparse_2024,
  title = {Sparse Autoencoders Find Highly Interpretable Features in Language Models},
  author = {Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  year = {2024},
  month = jan,
  journal = {ICLR},
  eprint = {2309.08600},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.08600},
  url = {http://arxiv.org/abs/2309.08600},
  urldate = {2023-10-30},
  abstract = {One of the roadblocks to a better understanding of neural networks' internals is {\textbackslash}textit\{polysemanticity\}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is {\textbackslash}textit\{superposition\}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task {\textbackslash}citep\{wang2022interpretability\} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.},
  archiveprefix = {arXiv},
  keywords = {cited,graph,mechinterp,to cite,to extract figures,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/DEJUHG9L/Cunningham et al. - 2024 - Sparse Autoencoders Find Highly Interpretable Feat.pdf}
}

@article{chughtai_toy_2023,
  title = {A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations},
  shorttitle = {A Toy Model of Universality},
  author = {Chughtai, Bilal and Chan, Lawrence and Nanda, Neel},
  year = {2023},
  journal = {ICML},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.03025},
  url = {https://arxiv.org/abs/2302.03025},
  urldate = {2023-10-27},
  abstract = {Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {algorithms,cited,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,toy models,universality},
  file = {/Users/leonardbereska/Zotero/storage/EKPFCB9B/Chughtai et al. - 2023 - A Toy Model of Universality Reverse Engineering H.pdf}
}

@article{vonoswald_uncovering_2023,
  title = {Uncovering mesa-optimization algorithms in Transformers},
  author = {{von Oswald}, Johannes and Niklasson, Eyvind and Schlegel, Maximilian and Kobayashi, Seijin and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and y Arcas, Blaise Ag{\"u}era and Vladymyrov, Max and Pascanu, Razvan and Sacramento, Jo{\~a}o},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.05858},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.05858},
  url = {http://arxiv.org/abs/2309.05858},
  urldate = {2023-10-30},
  abstract = {Transformers have become the dominant model in deep learning, but the reason for their superior performance is poorly understood. Here, we hypothesize that the strong performance of Transformers stems from an architectural bias towards mesa-optimization, a learned process running within the forward pass of a model consisting of the following two steps: (i) the construction of an internal learning objective, and (ii) its corresponding solution found through optimization. To test this hypothesis, we reverse-engineer a series of autoregressive Transformers trained on simple sequence modeling tasks, uncovering underlying gradient-based mesa-optimization algorithms driving the generation of predictions. Moreover, we show that the learned forward-pass optimization algorithm can be immediately repurposed to solve supervised few-shot tasks, suggesting that mesa-optimization might underlie the in-context learning capabilities of large language models. Finally, we propose a novel self-attention layer, the mesa-layer, that explicitly and efficiently solves optimization problems specified in context. We find that this layer can lead to improved performance in synthetic and preliminary language modeling experiments, adding weight to our hypothesis that mesa-optimization is an important operation hidden within the weights of trained Transformers.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/I823QG8H/von Oswald et al. - 2023 - Uncovering mesa-optimization algorithms in Transfo.pdf}
}

@article{zimmermann_scale_2023,
  title = {Scale Alone Does not Improve Mechanistic Interpretability in Vision Models},
  author = {Zimmermann, Roland S. and Klein, Thomas and Brendel, Wieland},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2307.05471},
  url = {https://arxiv.org/abs/2307.05471},
  urldate = {2023-07-31},
  abstract = {In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We here use a psychophysical paradigm to quantify mechanistic interpretability for a diverse suite of models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the nine investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 120'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset is meant to facilitate research on automated instead of human-based interpretability evaluations that can ultimately be leveraged to directly optimize the mechanistic interpretability of models.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {interpretability tax,not cited,scale,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/S8WX8GG7/Zimmermann et al. - 2023 - Scale Alone Does not Improve Mechanistic Interpret.pdf}
}

@article{verma_programmatically_2019,
  title = {Programmatically Interpretable Reinforcement Learning},
  author = {Verma, Abhinav and Murali, Vijayaraghavan and Singh, Rishabh and Kohli, Pushmeet and Chaudhuri, Swarat},
  year = {2019},
  month = apr,
  journal = {CoRR},
  eprint = {1804.02477},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1804.02477},
  url = {http://arxiv.org/abs/1804.02477},
  urldate = {2023-11-02},
  abstract = {We present a reinforcement learning framework, called Programmatically Interpretable Reinforcement Learning (PIRL), that is designed to generate interpretable and verifiable agent policies. Unlike the popular Deep Reinforcement Learning (DRL) paradigm, which represents policies by neural networks, PIRL represents policies using a high-level, domain-specific programming language. Such programmatic policies have the benefits of being more easily interpreted than neural networks, and being amenable to verification by symbolic methods. We propose a new method, called Neurally Directed Program Search (NDPS), for solving the challenging nonsmooth optimization problem of finding a programmatic policy with maximal reward. NDPS works by first learning a neural policy network using DRL, and then performing a local search over programmatic policies that seeks to minimize a distance from this neural "oracle". We evaluate NDPS on the task of learning to drive a simulated car in the TORCS car-racing environment. We demonstrate that NDPS is able to discover human-readable policies that pass some significant performance bars. We also show that PIRL policies can have smoother trajectories, and can be more easily transferred to environments not encountered during training, than corresponding policies discovered by DRL.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9N8D72YK/Verma et al. - 2019 - Programmatically Interpretable Reinforcement Learn.pdf}
}

@article{nanda_comprehensive_2022,
  title = {A Comprehensive Mechanistic Interpretability Explainer \& Glossary},
  author = {Nanda, Neel},
  year = {2022},
  month = dec,
  journal = {Neel Nanda's Blog},
  url = {https://www.neelnanda.io/mechanistic-interpretability/glossary},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/NNNICWQB/Nanda - 2022 - A Comprehensive Mechanistic Interpretability Expla.html}
}

@article{olah_distributed_2023,
  title = {Distributed Representations: Composition \& Superposition},
  author = {Olah, Chris},
  year = {2023},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/superposition-composition/index.html},
  keywords = {cited,foundational,mechinterp,superposition,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/5YMYB5SZ/Olah - 2023 - Distributed Representations Composition & Superpo.html}
}

@article{olah_feature_2017,
  title = {Feature Visualization},
  author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
  year = {2017},
  month = nov,
  journal = {Distill},
  url = {https://distill.pub/2017/feature-visualization},
  urldate = {2023-05-31},
  keywords = {cited,feature,mechinterp,to cite,to extract figures,to extract related work,to review in detail,visualization},
  file = {/Users/leonardbereska/Zotero/storage/5TWUD884/Olah et al. - 2017 - Feature Visualization.html}
}

@article{olah_building_2018,
  title = {The Building Blocks of Interpretability},
  author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander},
  year = {2018},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2018/building-blocks},
  urldate = {2023-05-31},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work,to review in detail,tool,vision,visualization},
  file = {/Users/leonardbereska/Zotero/storage/9WMCKGK4/Olah et al. - 2018 - The Building Blocks of Interpretability.html}
}

@article{bricken_monosemanticity_2023,
  title = {Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
  author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nicholas L. and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E. and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Chris},
  year = {2023},
  month = oct,
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2023/monosemantic-features/index.html},
  abstract = {Mechanistic interpretability seeks to understand neural networks by breaking them into components that are more easily understood than the whole. By understanding the function of each component, and how they interact, we hope to be able to reason about the behavior of the entire network. The first step in that program is to identify the correct components to analyze.},
  keywords = {cited,fundamental,mechinterp,SAE,superposition,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/Q6RCQ5EH/Bricken et al. - 2023 - Towards Monosemanticity Decomposing Language Mode.html}
}

@article{cammarata_curve_2020,
  title = {Curve Detectors},
  author = {Cammarata, Nick and Goh, Gabriel and Carter, Shan and Schubert, Ludwig and Petrov, Michael and Olah, Chris},
  year = {2020},
  month = jun,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/curve-detectors},
  urldate = {2023-05-31},
  keywords = {circuit,cited,empirical,historical,mechinterp,to cite,to extract figures,to extract related work,to review in detail,vision},
  file = {/Users/leonardbereska/Zotero/storage/D7CEC8T5/Cammarata et al. - 2020 - Curve Detectors.html}
}

@article{goh_multimodal_2021,
  title = {Multimodal Neurons in Artificial Neural Networks},
  author = {Goh, Gabriel and {\dag}, Nick Cammarata and {\dag}, Chelsea Voss and Carter, Shan and Petrov, Michael and Schubert, Ludwig and Radford, Alec and Olah, Chris},
  year = {2021},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2021/multimodal-neurons},
  urldate = {2023-07-31},
  abstract = {We report the existence of multimodal neurons in artificial neural networks, similar to those found in the human brain.},
  language = {en},
  keywords = {cited,mechinterp,multimodal,neuron,to cite,to extract figures,to extract related work,to review in detail,visualization},
  file = {/Users/leonardbereska/Zotero/storage/2VCH785K/Goh et al. - 2021 - Multimodal Neurons in Artificial Neural Networks.html}
}

@article{elhage_toy_2022,
  title = {Toy Models of Superposition},
  author = {Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and {Hatfield-Dodds}, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/toy_model/index.html},
  keywords = {cited,fundamental,mechinterp,superposition,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/8K24TPD5/Elhage et al. - 2022 - Toy Models of Superposition.html}
}

@article{olsson_incontext_2022,
  title = {In-context Learning and Induction Heads},
  author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and {Hatfield-Dodds}, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
  keywords = {circuit,cited,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/9ZXSBRBA/Olsson et al. - 2022 - In-context Learning and Induction Heads.html}
}

@article{olah_zoom_2020,
  title = {Zoom In: An Introduction to Circuits},
  shorttitle = {Zoom In},
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  year = {2020},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2020/circuits/zoom-in},
  urldate = {2023-05-31},
  keywords = {circuit,cited,feature,foundational,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/NGSD97LT/Olah et al. - 2020 - Zoom In An Introduction to Circuits.html}
}

@article{burns_discovering_2023,
  title = {Discovering Latent Knowledge in Language Models Without Supervision},
  author = {Burns, Collin and Ye, Haotian and Klein, Dan and Steinhardt, Jacob},
  year = {2023},
  journal = {ICLR},
  eprint = {2212.03827},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2212.03827},
  urldate = {2023-07-31},
  abstract = {Existing techniques for training language models can be misaligned with the truth: if we train models with imitation learning, they may reproduce errors that humans make; if we train them to generate text that humans rate highly, they may output errors that human evaluators can't detect. We propose circumventing this issue by directly finding latent knowledge inside the internal activations of a language model in a purely unsupervised way. Specifically, we introduce a method for accurately answering yes-no questions given only unlabeled model activations. It works by finding a direction in activation space that satisfies logical consistency properties, such as that a statement and its negation have opposite truth values. We show that despite using no supervision and no model outputs, our method can recover diverse knowledge represented in large language models: across 6 models and 10 question-answering datasets, it outperforms zero-shot accuracy by 4{\textbackslash}\% on average. We also find that it cuts prompt sensitivity in half and continues to maintain high accuracy even when models are prompted to generate incorrect answers. Our results provide an initial step toward discovering what language models know, distinct from what they say, even when we don't have access to explicit ground truth labels.},
  archiveprefix = {arXiv},
  keywords = {cited,feature,probing,to review in detail,unsupervised},
  file = {/Users/leonardbereska/Zotero/storage/I6V7LFHI/Burns et al. - 2023 - Discovering Latent Knowledge in Language Models Wi.pdf}
}

@article{lindner_tracr_2023,
  title = {Tracr: Compiled Transformers as a Laboratory for Interpretability},
  shorttitle = {Tracr},
  author = {Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Thomas and Mikulik, Vladimir},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2301.05062},
  url = {https://arxiv.org/abs/2301.05062},
  urldate = {2023-07-31},
  abstract = {We show how to "compile" human-readable programs into standard decoder-only transformer models. Our compiler, Tracr, generates models with known structure. This structure can be used to design experiments. For example, we use it to study "superposition" in transformers that execute multi-step algorithms. Additionally, the known structure of Tracr-compiled models can serve as ground-truth for evaluating interpretability methods. Commonly, because the "programs" learned by transformers are unknown it is unclear whether an interpretation succeeded. We demonstrate our approach by implementing and examining programs including computing token frequencies, sorting, and parenthesis checking. We provide an open-source implementation of Tracr at https://github.com/deepmind/tracr.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {algorithms,cited,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,tool,toy models},
  file = {/Users/leonardbereska/Zotero/storage/QBYGQR36/Lindner et al. - 2023 - Tracr Compiled Transformers as a Laboratory for I.pdf}
}

@article{lamparth_analyzing_2023,
  title = {Analyzing And Editing Inner Mechanisms Of Backdoored Language Models},
  author = {Lamparth, Max and Reuel, Anka},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2302.12461},
  url = {https://arxiv.org/abs/2302.12461},
  urldate = {2023-07-31},
  abstract = {Recent advancements in interpretability research made transformer language models more transparent. This progress led to a better understanding of their inner workings for toy and naturally occurring models. However, how these models internally process sentiment changes has yet to be sufficiently answered. In this work, we introduce a new interpretability tool called PCP ablation, where we replace modules with low-rank matrices based on the principal components of their activations, reducing model parameters and their behavior to essentials. We demonstrate PCP ablations on MLP and attention layers in backdoored toy, backdoored large, and naturally occurring models. We determine MLPs as most important for the backdoor mechanism and use this knowledge to remove, insert, and modify backdoor mechanisms with engineered replacements via PCP ablation.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited,editing,empirical,graph,mechinterp,to cite,to extract figures,to extract related work,to review in detail,trojan,unlearning},
  file = {/Users/leonardbereska/Zotero/storage/2V3LLRRD/Lamparth and Reuel - 2023 - Analyzing And Editing Inner Mechanisms Of Backdoor.pdf}
}

@article{belinkov_probing_2021,
  title = {Probing Classifiers: Promises, Shortcomings, and Advances},
  shorttitle = {Probing Classifiers},
  author = {Belinkov, Yonatan},
  year = {2021},
  month = sep,
  journal = {CoRR},
  eprint = {2102.12452},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2102.12452},
  url = {http://arxiv.org/abs/2102.12452},
  urldate = {2023-07-31},
  abstract = {Probing classifiers have emerged as one of the prominent methodologies for interpreting and analyzing deep neural network models of natural language processing. The basic idea is simple -- a classifier is trained to predict some linguistic property from a model's representations -- and has been used to examine a wide variety of models and properties. However, recent studies have demonstrated various methodological limitations of this approach. This article critically reviews the probing classifiers framework, highlighting their promises, shortcomings, and advances.},
  archiveprefix = {arXiv},
  keywords = {cited,probing,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/BW7UJF33/Belinkov - 2021 - Probing Classifiers Promises, Shortcomings, and A.pdf}
}

@article{bolukbasi_interpretability_2021,
  title = {An Interpretability Illusion for BERT},
  author = {Bolukbasi, Tolga and Pearce, Adam and Yuan, Ann and Coenen, Andy and Reif, Emily and Vi'egas, Fernanda and Wattenberg, M.},
  year = {2021},
  month = apr,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/An-Interpretability-Illusion-for-BERT-Bolukbasi-Pearce/9b9dc2b3d95d2f4e4269a9818c14c70c1f801384},
  urldate = {2023-07-31},
  abstract = {We describe an ``interpretability illusion'' that arises when analyzing the BERT model. Activations of individual neurons in the network may spuriously appear to encode a single, simple concept, when in fact they are encoding something far more complex. The same effect holds for linear combinations of activations. We trace the source of this illusion to geometric properties of BERT's embedding space as well as the fact that common text corpora represent only narrow slices of possible English sentences. We provide a taxonomy of model-learned concepts and discuss methodological implications for interpretability research, especially the importance of testing hypotheses on multiple data sets.},
  keywords = {cited,feature,neuron,pitfall,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/ADI39PK8/Bolukbasi et al. - 2021 - An Interpretability Illusion for BERT.pdf}
}

@article{scherlis_polysemanticity_2023,
  title = {Polysemanticity and Capacity in Neural Networks},
  author = {Scherlis, Adam and Sachan, Kshitij and Jermyn, Adam S. and Benton, Joe and Shlegeris, Buck},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2210.01892},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.01892},
  url = {http://arxiv.org/abs/2210.01892},
  urldate = {2023-09-18},
  abstract = {Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature {\textbackslash}emph\{capacity\}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model architecture on the interpretability of its neurons.},
  archiveprefix = {arXiv},
  keywords = {cited,fundamental,mechinterp,superposition,to cite,to extract figures,to extract related work,to review in detail,toy models},
  file = {/Users/leonardbereska/Zotero/storage/DASCWAYC/Scherlis et al. - 2023 - Polysemanticity and Capacity in Neural Networks.pdf}
}

@article{miller_explanation_2019,
  title = {Explanation in artificial intelligence: Insights from the social sciences},
  shorttitle = {Explanation in artificial intelligence},
  author = {Miller, Tim},
  year = {2019},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2018.07.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
  urldate = {2023-10-19},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PBF8NT4D/Miller - 2019 - Explanation in artificial intelligence Insights f.pdf}
}

@book{molnar_interpretable_2022,
  title = {Interpretable Machine Learning: A Guide for Making Black Box Models Explainable},
  author = {Molnar, Christoph},
  year = {2022},
  publisher = {Independently published},
  url = {https://christophm.github.io/interpretable-ml-book},
  abstract = {Machine learning has great potential for improving products, processes and research. But computers usually do not explain their predictions which is a barrier to the adoption of machine learning. This book is about making machine learning models and their decisions interpretable. After exploring the concepts of interpretability, you will learn about simple, interpretable models such as decision trees, decision rules and linear regression. The focus of the book is on model-agnostic methods for interpreting black box models such as feature importance and accumulated local effects, and explaining individual predictions with Shapley values and LIME. In addition, the book presents methods specific to deep neural networks. All interpretation methods are explained in depth and discussed critically. How do they work under the hood? What are their strengths and weaknesses? How can their outputs be interpreted? This book will enable you to select and correctly apply the interpretation method that is most suitable for your machine learning project. Reading the book is recommended for machine learning practitioners, data scientists, statisticians, and anyone else interested in making machine learning models interpretable.},
  keywords = {not cited}
}

@article{doshi-velez_rigorous_2017,
  title = {Towards A Rigorous Science of Interpretable Machine Learning},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  journal = {CoRR},
  eprint = {1702.08608},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1702.08608},
  url = {http://arxiv.org/abs/1702.08608},
  urldate = {2023-09-03},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/8PXCEKM4/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf}
}

@article{bubeck_sparks_2023,
  title = {Sparks of Artificial General Intelligence: Early experiments with GPT-4},
  shorttitle = {Sparks of Artificial General Intelligence},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  journal = {CoRR},
  eprint = {2303.12712},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2303.12712},
  url = {http://arxiv.org/abs/2303.12712},
  urldate = {2023-05-17},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/37E575TK/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf}
}

@article{sharkey_circumventing_2022,
  title = {Circumventing interpretability: How to defeat mind-readers},
  shorttitle = {Circumventing interpretability},
  author = {Sharkey, Lee},
  year = {2022},
  month = dec,
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2212.11415},
  url = {https://arxiv.org/abs/2212.11415},
  urldate = {2023-09-18},
  abstract = {The increasing capabilities of artificial intelligence (AI) systems make it ever more important that we interpret their internals to ensure that their intentions are aligned with human values. Yet there is reason to believe that misaligned artificial intelligence will have a convergent instrumental incentive to make its thoughts difficult for us to interpret. In this article, I discuss many ways that a capable AI might circumvent scalable interpretability methods and suggest a framework for thinking about these potential future risks.},
  copyright = {Creative Commons Attribution 4.0 International},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/WN43KVNC/Sharkey - 2022 - Circumventing interpretability How to defeat mind.pdf}
}

@article{zou_universal_2023,
  title = {Universal and Transferable Adversarial Attacks on Aligned Language Models},
  author = {Zou, Andy and Wang, Zifan and Kolter, J. Zico and Fredrikson, Matt},
  year = {2023},
  month = jul,
  journal = {CoRR},
  eprint = {2307.15043},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.15043},
  url = {http://arxiv.org/abs/2307.15043},
  urldate = {2023-10-27},
  abstract = {Because "out-of-the-box" large language models are capable of generating a great deal of objectionable content, recent work has focused on aligning these models in an attempt to prevent undesirable generation. While there has been some success at circumventing these measures -- so-called "jailbreaks" against LLMs -- these attacks have required significant human ingenuity and are brittle in practice. In this paper, we propose a simple and effective attack method that causes aligned language models to generate objectionable behaviors. Specifically, our approach finds a suffix that, when attached to a wide range of queries for an LLM to produce objectionable content, aims to maximize the probability that the model produces an affirmative response (rather than refusing to answer). However, instead of relying on manual engineering, our approach automatically produces these adversarial suffixes by a combination of greedy and gradient-based search techniques, and also improves over past automatic prompt generation methods. Surprisingly, we find that the adversarial prompts generated by our approach are quite transferable, including to black-box, publicly released LLMs. Specifically, we train an adversarial attack suffix on multiple prompts (i.e., queries asking for many different types of objectionable content), as well as multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting attack suffix is able to induce objectionable content in the public interfaces to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat, Pythia, Falcon, and others. In total, this work significantly advances the state-of-the-art in adversarial attacks against aligned language models, raising important questions about how such systems can be prevented from producing objectionable information. Code is available at github.com/llm-attacks/llm-attacks.},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/7EWJYTXS/Zou et al. - 2023 - Universal and Transferable Adversarial Attacks on .pdf}
}

@article{hendrycks_unsolved_2022,
  title = {Unsolved Problems in ML Safety},
  author = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  year = {2022},
  month = jun,
  journal = {CoRR},
  eprint = {2109.13916},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.13916},
  url = {http://arxiv.org/abs/2109.13916},
  urldate = {2023-10-13},
  abstract = {Machine learning (ML) systems are rapidly increasing in size, are acquiring new capabilities, and are increasingly deployed in high-stakes settings. As with other powerful technologies, safety for ML should be a leading research priority. In response to emerging safety challenges in ML, such as those introduced by recent large-scale models, we provide a new roadmap for ML Safety and refine the technical problems that the field needs to address. We present four problems ready for research, namely withstanding hazards ("Robustness"), identifying hazards ("Monitoring"), reducing inherent model hazards ("Alignment"), and reducing systemic hazards ("Systemic Safety"). Throughout, we clarify each problem's motivation and provide concrete research directions.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/MWPVGULQ/Hendrycks et al. - 2022 - Unsolved Problems in ML Safety.pdf}
}

@article{hendrycks_xrisk_2022,
  title = {X-Risk Analysis for AI Research},
  author = {Hendrycks, Dan and Mazeika, Mantas},
  year = {2022},
  month = jun,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2206.05862v7},
  urldate = {2023-10-13},
  abstract = {Artificial intelligence (AI) has the potential to greatly improve society, but as with any powerful technology, it comes with heightened risks and responsibilities. Current AI research lacks a systematic discussion of how to manage long-tail risks from AI systems, including speculative long-term risks. Keeping in mind the potential benefits of AI, there is some concern that building ever more intelligent and powerful AI systems could eventually result in systems that are more powerful than us; some say this is like playing with fire and speculate that this could create existential risks (x-risks). To add precision and ground these discussions, we provide a guide for how to analyze AI x-risk, which consists of three parts: First, we review how systems can be made safer today, drawing on time-tested concepts from hazard analysis and systems safety that have been designed to steer large processes in safer directions. Next, we discuss strategies for having long-term impacts on the safety of future systems. Finally, we discuss a crucial concept in making AI systems safer by improving the balance between safety and general capabilities. We hope this document and the presented concepts and tools serve as a useful guide for understanding how to analyze AI x-risk.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5G9NVJ7Y/Hendrycks and Mazeika - 2022 - X-Risk Analysis for AI Research.pdf}
}

@article{hendrycks_overview_2023,
  title = {An Overview of Catastrophic AI Risks},
  author = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2306.12001},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2306.12001},
  urldate = {2023-10-13},
  abstract = {Rapid advancements in artificial intelligence (AI) have sparked growing concerns among experts, policymakers, and world leaders regarding the potential for increasingly advanced AI systems to pose catastrophic risks. Although numerous risks have been detailed separately, there is a pressing need for a systematic discussion and illustration of the potential dangers to better inform efforts to mitigate them. This paper provides an overview of the main sources of catastrophic AI risks, which we organize into four categories: malicious use, in which individuals or groups intentionally use AIs to cause harm; AI race, in which competitive environments compel actors to deploy unsafe AIs or cede control to AIs; organizational risks, highlighting how human factors and complex systems can increase the chances of catastrophic accidents; and rogue AIs, describing the inherent difficulty in controlling agents far more intelligent than humans. For each category of risk, we describe specific hazards, present illustrative stories, envision ideal scenarios, and propose practical suggestions for mitigating these dangers. Our goal is to foster a comprehensive understanding of these risks and inspire collective and proactive efforts to ensure that AIs are developed and deployed in a safe manner. Ultimately, we hope this will allow us to realize the benefits of this powerful technology while minimizing the potential for catastrophic outcomes.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZQBPV9NZ/Hendrycks et al. - 2023 - An Overview of Catastrophic AI Risks.pdf}
}

@article{hubinger_overview_2020,
  title = {An overview of 11 proposals for building safe advanced AI},
  author = {Hubinger, Evan},
  year = {2020},
  month = dec,
  journal = {CoRR},
  eprint = {2012.07532},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.07532},
  url = {http://arxiv.org/abs/2012.07532},
  urldate = {2023-10-19},
  abstract = {This paper analyzes and compares 11 different proposals for building safe advanced AI under the current machine learning paradigm, including major contenders such as iterated amplification, AI safety via debate, and recursive reward modeling. Each proposal is evaluated on the four components of outer alignment, inner alignment, training competitiveness, and performance competitiveness, of which the distinction between the latter two is introduced in this paper. While prior literature has primarily focused on analyzing individual proposals, or primarily focused on outer alignment at the expense of inner alignment, this analysis seeks to take a comparative look at a wide range of proposals including a comparative analysis across all four previously mentioned components.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/6E3KPBXW/Hubinger - 2020 - An overview of 11 proposals for building safe adva.pdf}
}

@article{sharma_understanding_2023,
  title = {Towards Understanding Sycophancy in Language Models},
  author = {Sharma, Mrinank and Tong, Meg and Korbak, Tomasz and Duvenaud, David and Askell, Amanda and Bowman, Samuel R. and Cheng, Newton and Durmus, Esin and {Hatfield-Dodds}, Zac and Johnston, Scott R. and Kravec, Shauna and Maxwell, Timothy and McCandlish, Sam and Ndousse, Kamal and Rausch, Oliver and Schiefer, Nicholas and Yan, Da and Zhang, Miranda and Perez, Ethan},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.13548},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2310.13548},
  url = {http://arxiv.org/abs/2310.13548},
  urldate = {2023-10-25},
  abstract = {Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of RLHF models, likely driven in part by human preference judgements favoring sycophantic responses.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/CBSQKCYB/Sharma et al. - 2023 - Towards Understanding Sycophancy in Language Model.pdf}
}

@article{zou_representation_2023,
  title = {Representation Engineering: A Top-Down Approach to AI Transparency},
  shorttitle = {Representation Engineering},
  author = {Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and Goel, Shashwat and Li, Nathaniel and Byun, Michael J. and Wang, Zifan and Mallen, Alex and Basart, Steven and Koyejo, Sanmi and Song, Dawn and Fredrikson, Matt and Kolter, J. Zico and Hendrycks, Dan},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.01405},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.01405},
  url = {http://arxiv.org/abs/2310.01405},
  urldate = {2023-11-08},
  abstract = {In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/J9ARICI9/Zou et al. - 2023 - Representation Engineering A Top-Down Approach to.pdf}
}

@article{geiger_inducing_2021,
  title = {Inducing Causal Structure for Interpretable Neural Networks},
  author = {Geiger, Atticus and Wu, Zhengxuan and Lu, Hanson and Rozner, Josh and Kreiss, Elisa and Icard, Thomas and Goodman, Noah D. and Potts, Christopher},
  year = {2021},
  month = jan,
  journal = {ICML},
  eprint = {2112.00826},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2112.00826},
  url = {http://arxiv.org/abs/2112.00826},
  urldate = {2023-10-25},
  abstract = {In many areas, we have well-founded insights about causal structure that would be useful to bring into our trained models while still allowing them to learn in a data-driven fashion. To achieve this, we present the new method of interchange intervention training (IIT). In IIT, we (1) align variables in a causal model (e.g., a deterministic program or Bayesian network) with representations in a neural model and (2) train the neural model to match the counterfactual behavior of the causal model on a base input when aligned representations in both models are set to be the value they would be for a source input. IIT is fully differentiable, flexibly combines with other objectives, and guarantees that the target causal model is a causal abstraction of the neural model when its loss is zero. We evaluate IIT on a structural vision task (MNIST-PVR), a navigational language task (ReaSCAN), and a natural language inference task (MQNLI). We compare IIT against multi-task training objectives and data augmentation. In all our experiments, IIT achieves the best results and produces neural models that are more interpretable in the sense that they more successfully realize the target causal model.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/GXM3IT7D/Geiger et al. - 2021 - Inducing Causal Structure for Interpretable Neural.pdf}
}

@article{nguyen_survey_2022,
  title = {A Survey of Machine Unlearning},
  author = {Nguyen, Thanh Tam and Huynh, Thanh Trung and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
  year = {2022},
  month = oct,
  journal = {CoRR},
  eprint = {2209.02299},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2209.02299},
  url = {http://arxiv.org/abs/2209.02299},
  urldate = {2023-11-09},
  abstract = {Today, computer systems hold large amounts of personal data. Yet while such an abundance of data allows breakthroughs in artificial intelligence, and especially machine learning (ML), its existence can be a threat to user privacy, and it can weaken the bonds of trust between humans and AI. Recent regulations now require that, on request, private information about a user must be removed from both computer systems and from ML models, i.e. ``the right to be forgotten''). While removing data from back-end databases should be straightforward, it is not sufficient in the AI context as ML models often `remember' the old data. Contemporary adversarial attacks on trained models have proven that we can learn whether an instance or an attribute belonged to the training data. This phenomenon calls for a new paradigm, namely machine unlearning, to make ML models forget about particular data. It turns out that recent works on machine unlearning have not been able to completely solve the problem due to the lack of common frameworks and resources. Therefore, this paper aspires to present a comprehensive examination of machine unlearning's concepts, scenarios, methods, and applications. Specifically, as a category collection of cutting-edge studies, the intention behind this article is to serve as a comprehensive resource for researchers and practitioners seeking an introduction to machine unlearning and its formulations, design criteria, removal requests, algorithms, and applications. In addition, we aim to highlight the key findings, current trends, and new research areas that have not yet featured the use of machine unlearning but could benefit greatly from it. We hope this survey serves as a valuable resource for ML researchers and those seeking to innovate privacy technologies. Our resources are publicly available at https://github.com/tamlhp/awesome-machine-unlearning.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5A7D2VZC/Nguyen et al. - 2022 - A Survey of Machine Unlearning.pdf}
}

@article{louizos_learning_2018,
  title = {Learning Sparse Neural Networks through \$L\_0\$ Regularization},
  author = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
  year = {2018},
  month = jun,
  journal = {CoRR},
  eprint = {1712.01312},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1712.01312},
  url = {http://arxiv.org/abs/1712.01312},
  urldate = {2023-08-27},
  abstract = {We propose a practical method for \$L\_0\$ norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of \$L\_0\$ regularization. However, since the \$L\_0\$ norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected \$L\_0\$ norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the {\textbackslash}emph\{hard concrete\} distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/P34UA36U/Louizos et al. - 2018 - Learning Sparse Neural Networks through $L_0$ Regu.pdf}
}

@article{casper_robust_2021,
  title = {Robust Feature-Level Adversaries are Interpretability Tools},
  author = {Casper, Stephen and Nadeau, Max and {Hadfield-Menell}, Dylan and Kreiman, Gabriel},
  year = {2021},
  month = oct,
  journal = {NeurIPS},
  eprint = {2110.03605},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2110.03605},
  url = {http://arxiv.org/abs/2110.03605},
  urldate = {2023-10-26},
  abstract = {The literature on adversarial attacks in computer vision typically focuses on pixel-level perturbations. These tend to be very difficult to interpret. Recent work that manipulates the latent representations of image generators to create "feature-level" adversarial perturbations gives us an opportunity to explore perceptible, interpretable adversarial attacks. We make three contributions. First, we observe that feature-level attacks provide useful classes of inputs for studying representations in models. Second, we show that these adversaries are uniquely versatile and highly robust. We demonstrate that they can be used to produce targeted, universal, disguised, physically-realizable, and black-box attacks at the ImageNet scale. Third, we show how these adversarial images can be used as a practical interpretability tool for identifying bugs in networks. We use these adversaries to make predictions about spurious associations between features and classes which we then test by designing "copy/paste" attacks in which one natural image is pasted into another to cause a targeted misclassification. Our results suggest that feature-level attacks are a promising approach for rigorous interpretability research. They support the design of tools to better understand what a model has learned and diagnose brittle feature associations. Code is available at https://github.com/thestephencasper/feature\_level\_adv},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/9XJPJWEH/Casper et al. - 2021 - Robust Feature-Level Adversaries are Interpretabil.pdf}
}

@article{hernandez_natural_2022,
  title = {Natural Language Descriptions of Deep Visual Features},
  author = {Hernandez, Evan and Schwettmann, Sarah and Bau, David and Bagashvili, Teona and Torralba, Antonio and Andreas, Jacob},
  year = {2022},
  month = apr,
  journal = {ICLR},
  eprint = {2201.11114},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2201.11114},
  url = {http://arxiv.org/abs/2201.11114},
  urldate = {2023-10-26},
  abstract = {Some neurons in deep networks specialize in recognizing highly specific perceptual, structural, or semantic features of inputs. In computer vision, techniques exist for identifying neurons that respond to individual concept categories like colors, textures, and object classes. But these techniques are limited in scope, labeling only a small subset of neurons and behaviors in any network. Is a richer characterization of neuron-level computation possible? We introduce a procedure (called MILAN, for mutual-information-guided linguistic annotation of neurons) that automatically labels neurons with open-ended, compositional, natural language descriptions. Given a neuron, MILAN generates a description by searching for a natural language string that maximizes pointwise mutual information with the image regions in which the neuron is active. MILAN produces fine-grained descriptions that capture categorical, relational, and logical structure in learned features. These descriptions obtain high agreement with human-generated feature descriptions across a diverse set of model architectures and tasks, and can aid in understanding and controlling learned models. We highlight three applications of natural language neuron descriptions. First, we use MILAN for analysis, characterizing the distribution and importance of neurons selective for attribute, category, and relational information in vision models. Second, we use MILAN for auditing, surfacing neurons sensitive to human faces in datasets designed to obscure them. Finally, we use MILAN for editing, improving robustness in an image classifier by deleting neurons sensitive to text features spuriously correlated with class labels.},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/JWXRKU4W/Hernandez et al. - 2022 - Natural Language Descriptions of Deep Visual Featu.pdf}
}

@article{ziegler_adversarial_2022,
  title = {Adversarial Training for High-Stakes Reliability},
  author = {Ziegler, Daniel M. and Nix, Seraphina and Chan, Lawrence and Bauman, Tim and {Schmidt-Nielsen}, Peter and Lin, Tao and Scherlis, Adam and Nabeshima, Noa and {Weinstein-Raun}, Ben and {de Haas}, Daniel and Shlegeris, Buck and Thomas, Nate},
  year = {2022},
  month = nov,
  journal = {NeurIPS},
  eprint = {2205.01663},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2205.01663},
  url = {http://arxiv.org/abs/2205.01663},
  urldate = {2023-10-26},
  abstract = {In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance. In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance. We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.},
  archiveprefix = {arXiv},
  keywords = {to cite},
  file = {/Users/leonardbereska/Zotero/storage/FRCY9S2B/Ziegler et al. - 2022 - Adversarial Training for High-Stakes Reliability.pdf}
}

@article{mu_compositional_2020,
  title = {Compositional Explanations of Neurons},
  author = {Mu, Jesse and Andreas, Jacob},
  year = {2020},
  month = jun,
  journal = {NeurIPS},
  eprint = {2006.14032},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2006.14032},
  url = {http://arxiv.org/abs/2006.14032},
  urldate = {2023-09-18},
  abstract = {We describe a procedure for explaining neurons in deep representations by identifying compositional logical concepts that closely approximate neuron behavior. Compared to prior work that uses atomic labels as explanations, analyzing neurons compositionally allows us to more precisely and expressively characterize their behavior. We use this procedure to answer several questions on interpretability in models for vision and natural language processing. First, we examine the kinds of abstractions learned by neurons. In image classification, we find that many neurons learn highly abstract but semantically coherent visual concepts, while other polysemantic neurons detect multiple unrelated features; in natural language inference (NLI), neurons learn shallow lexical heuristics from dataset biases. Second, we see whether compositional explanations give us insight into model performance: vision neurons that detect human-interpretable concepts are positively correlated with task performance, while NLI neurons that fire for shallow heuristics are negatively correlated with task performance. Finally, we show how compositional explanations provide an accessible way for end users to produce simple "copy-paste" adversarial examples that change model behavior in predictable ways.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/IURDBMPN/Mu and Andreas - 2020 - Compositional Explanations of Neurons.pdf}
}

@article{kim_interpretability_2018,
  title = {Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)},
  shorttitle = {Interpretability Beyond Feature Attribution},
  author = {Kim, Been and Wattenberg, Martin and Gilmer, Justin and Cai, Carrie and Wexler, James and Viegas, Fernanda and Sayres, Rory},
  year = {2018},
  month = jun,
  journal = {CoRR},
  eprint = {1711.11279},
  primaryclass = {stat},
  doi = {10.48550/arXiv.1711.11279},
  url = {http://arxiv.org/abs/1711.11279},
  urldate = {2023-09-18},
  abstract = {The interpretation of deep learning models is a challenge due to their size, complexity, and often opaque internal state. In addition, many systems, such as image classifiers, operate on low-level features rather than high-level concepts. To address these challenges, we introduce Concept Activation Vectors (CAVs), which provide an interpretation of a neural net's internal state in terms of human-friendly concepts. The key idea is to view the high-dimensional internal state of a neural net as an aid, not an obstacle. We show how to use CAVs as part of a technique, Testing with CAVs (TCAV), that uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result--for example, how sensitive a prediction of "zebra" is to the presence of stripes. Using the domain of image classification as a testing ground, we describe how CAVs may be used to explore hypotheses and generate insights for a standard image classification network as well as a medical application.},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/2BCUTZJ7/Kim et al. - 2018 - Interpretability Beyond Feature Attribution Quant.pdf}
}

@article{tegmark_provably_2023,
  title = {Provably safe systems: the only path to controllable AGI},
  shorttitle = {Provably safe systems},
  author = {Tegmark, Max and Omohundro, Steve},
  year = {2023},
  month = sep,
  journal = {CoRR},
  eprint = {2309.01933},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2309.01933},
  url = {http://arxiv.org/abs/2309.01933},
  urldate = {2023-10-27},
  abstract = {We describe a path to humanity safely thriving with powerful Artificial General Intelligences (AGIs) by building them to provably satisfy human-specified requirements. We argue that this will soon be technically feasible using advanced AI for formal verification and mechanistic interpretability. We further argue that it is the only path which guarantees safe controlled AGI. We end with a list of challenge problems whose solution would contribute to this positive outcome and invite readers to join in this work.},
  archiveprefix = {arXiv},
  keywords = {alignment,cited,opinion,to cite},
  file = {/Users/leonardbereska/Zotero/storage/3UIQKLTL/Tegmark and Omohundro - 2023 - Provably safe systems the only path to controllab.pdf}
}

@article{fu_transformers_2023,
  title = {Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models},
  shorttitle = {Transformers Learn Higher-Order Optimization Methods for In-Context Learning},
  author = {Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.17086},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.17086},
  url = {http://arxiv.org/abs/2310.17086},
  urldate = {2023-10-27},
  abstract = {Transformers are remarkably good at in-context learning (ICL) -- learning from demonstrations without parameter updates -- but how they perform ICL remains a mystery. Recent work suggests that Transformers may learn in-context by internally running Gradient Descent, a first-order optimization method. In this paper, we instead demonstrate that Transformers learn to implement higher-order optimization methods to perform ICL. Focusing on in-context linear regression, we show that Transformers learn to implement an algorithm very similar to Iterative Newton's Method, a higher-order optimization method, rather than Gradient Descent. Empirically, we show that predictions from successive Transformer layers closely match different iterations of Newton's Method linearly, with each middle layer roughly computing 3 iterations. In contrast, exponentially more Gradient Descent steps are needed to match an additional Transformers layer; this suggests that Transformers have an comparable rate of convergence with high-order methods such as Iterative Newton, which are exponentially faster than Gradient Descent. We also show that Transformers can learn in-context on ill-conditioned data, a setting where Gradient Descent struggles but Iterative Newton succeeds. Finally, we show theoretical results which support our empirical findings and have a close correspondence with them: we prove that Transformers can implement \$k\$ iterations of Newton's method with \${\textbackslash}mathcal\{O\}(k)\$ layers.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/VHH9NYPY/Fu et al. - 2023 - Transformers Learn Higher-Order Optimization Metho.pdf}
}

@article{jacob_emergent_2023,
  title = {Emergent Deception and Emergent Optimization},
  author = {Steinhardt, Jacob},
  year = {2023},
  month = feb,
  journal = {Bounded Regret},
  url = {https://bounded-regret.ghost.io/emergent-deception-optimization/},
  urldate = {2023-05-17},
  abstract = {I've previously argued that machine learning systems often exhibit emergent capabilities, and that these capabilities could lead to unintended negative consequences. But how can we reason concretely about these consequences?},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/YW3AZ6N7/Steinhardt - 2023 - Emergent Deception and Emergent Optimization.html}
}

@article{madry_deep_2019,
  title = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  year = {2019},
  month = sep,
  journal = {CoRR},
  eprint = {1706.06083},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1706.06083},
  url = {http://arxiv.org/abs/1706.06083},
  urldate = {2023-07-03},
  abstract = {Recent work has demonstrated that deep neural networks are vulnerable to adversarial examples---inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models. Code and pre-trained models are available at https://github.com/MadryLab/mnist\_challenge and https://github.com/MadryLab/cifar10\_challenge.},
  archiveprefix = {arXiv},
  keywords = {not cited},
  file = {/Users/leonardbereska/Zotero/storage/3EVF2D7V/Madry et al. - 2019 - Towards Deep Learning Models Resistant to Adversar.pdf}
}

@article{nostalgebraist_interpreting_2020,
  title = {interpreting GPT: the logit lens},
  shorttitle = {interpreting GPT},
  author = {{nostalgebraist}},
  year = {2020},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens},
  urldate = {2023-11-10},
  abstract = {This post relates an observation I've made in my work with GPT-2, which I have not seen made elsewhere. {\dots}},
  language = {en},
  keywords = {cited,mechinterp},
  file = {/Users/leonardbereska/Zotero/storage/7GUP3BIL/nostalgebraist - 2020 - interpreting GPT the logit lens.html}
}

@article{riegel_logical_2020,
  title = {Logical Neural Networks},
  author = {Riegel, Ryan and Gray, Alexander and Luus, Francois and Khan, Naweed and Makondo, Ndivhuwo and Akhalwaya, Ismail Yunus and Qian, Haifeng and Fagin, Ronald and Barahona, Francisco and Sharma, Udit and Ikbal, Shajith and Karanam, Hima and Neelam, Sumit and Likhyani, Ankita and Srivastava, Santosh},
  year = {2020},
  month = jun,
  journal = {NeurIPS},
  eprint = {2006.13155},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.13155},
  url = {http://arxiv.org/abs/2006.13155},
  urldate = {2023-11-10},
  abstract = {We propose a novel framework seamlessly providing key properties of both neural nets (learning) and symbolic logic (knowledge and reasoning). Every neuron has a meaning as a component of a formula in a weighted real-valued logic, yielding a highly intepretable disentangled representation. Inference is omnidirectional rather than focused on predefined target variables, and corresponds to logical reasoning, including classical first-order logic theorem proving as a special case. The model is end-to-end differentiable, and learning minimizes a novel loss function capturing logical contradiction, yielding resilience to inconsistent knowledge. It also enables the open-world assumption by maintaining bounds on truth values which can have probabilistic semantics, yielding resilience to incomplete knowledge.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3UKMSTG4/Riegel et al. - 2020 - Logical Neural Networks.pdf}
}

@article{olah_mechanistic_2022,
  title = {Mechanistic Interpretability, Variables, and the Importance of Interpretable Bases},
  author = {Olah, Christopher},
  year = {2022},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2022/mech-interp-essay/index.html},
  keywords = {cited,mechinterp,to cite,to extract figures,to extract related work,to review in detail},
  file = {/Users/leonardbereska/Zotero/storage/ACFGP82J/Olah - 2022 - Mechanistic Interpretability, Variables, and the I.html}
}

@article{pochinkov_machine_2023,
  title = {Machine Unlearning Evaluations as Interpretability Benchmarks},
  author = {Pochinkov, Nicky and , Nandi},
  year = {2023},
  month = aug,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/mTi8TQEyP5Pr7oczd/machine-unlearning-evaluations-as-interpretability},
  urldate = {2023-11-16},
  abstract = {Interpreting Models by Ablation. Image generated by DALL-E 3. {\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/55H92K3J/Pochinkov and  - 2023 - Machine Unlearning Evaluations as Interpretability.html}
}

@article{casper_engineer_2023,
  title = {The Engineer's Interpretability Sequence},
  author = {Casper, Stephen},
  year = {2023},
  month = feb,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7},
  abstract = {Interpretability research is popular, and interpretability tools play a role in almost every agenda for making AI safe. However, for all the interpretability work that exists, there is a significant gap between the research and engineering applications. If one of our main goals for interpretability research is to help us with aligning highly intelligent AI systems in high stakes settings, shouldn't we be seeing tools that are more helpful on real world problems? This 12-post sequence argues for taking an engineering approach to interpretability research. And from this lens, it analyzes existing work and proposes directions for moving forward.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/6GF4ATE7/Casper - 2023 - The Engineer’s Interpretability Sequence.html}
}

@article{ilyas_adversarial_2019,
  title = {Adversarial Examples Are Not Bugs, They Are Features},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  year = {2019},
  month = aug,
  journal = {NeurIPS},
  eprint = {1905.02175},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.02175},
  url = {http://arxiv.org/abs/1905.02175},
  urldate = {2023-11-29},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9KITH3U2/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf}
}

@article{olah_research_2017,
  title = {Research Debt},
  author = {Olah, Chris and Carter, Shan},
  year = {2017},
  month = mar,
  journal = {Distill},
  url = {https://distill.pub/2017/research-debt},
  urldate = {2023-11-29},
  abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BMGCV42G/Olah and Carter - 2017 - Research Debt.html}
}

@article{sharkey_current_2022,
  title = {Current themes in mechanistic interpretability research},
  author = {Sharkey, Lee and Black, Sid and {beren}},
  year = {2022},
  month = nov,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/Jgs7LQwmvErxR9BCC/current-themes-in-mechanistic-interpretability-research},
  urldate = {2023-11-29},
  abstract = {This post gives an overview of discussions - from the perspective and understanding of the interpretability team at Conjecture - between mechanistic{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/V6HHNRQQ/Sharkey et al. - 2022 - Current themes in mechanistic interpretability res.html}
}

@article{hubinger_transparency_2022,
  title = {A transparency and interpretability tech tree},
  author = {Hubinger, Evan},
  year = {2022},
  month = jun,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/nbq2bWLcYmSGup9aF/a-transparency-and-interpretability-tech-tree},
  urldate = {2023-11-29},
  abstract = {Thanks to Chris Olah, Neel Nanda, Kate Woolverton, Richard Ngo, Buck Shlegeris, Daniel Kokotajlo, Kyle McDonell, Laria Reynolds, Eliezer Yudkowksy, M{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/7LGQWJ3I/Hubinger - 2022 - A transparency and interpretability tech tree.html}
}

@article{hubinger_relaxed_2019,
  title = {Relaxed adversarial training for inner alignment},
  author = {Hubinger, Evan},
  year = {2019},
  month = sep,
  journal = {AI Alignment Forum},
  url = {https://www.alignmentforum.org/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment},
  urldate = {2023-11-29},
  abstract = {This post is part of research I did at OpenAI with mentoring and guidance from Paul Christiano. It also represents my current agenda regarding what I{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/VUUIFAW2/Hubinger - 2019 - Relaxed adversarial training for inner alignment.html}
}

@article{nanda_mechanistic_2023,
  title = {Mechanistic Interpretability Quickstart Guide},
  author = {Nanda, Neel},
  year = {2023},
  month = jan,
  journal = {Neel Nanda's Blog},
  url = {https://www.neelnanda.io/mechanistic-interpretability/quickstart},
  urldate = {2023-11-30},
  abstract = {An intro guide to a mechanistic interpretability weekend hackathon},
  language = {en-US},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/2BILVKNI/Nanda - 2023 - Mechanistic Interpretability Quickstart Guide.html}
}

@article{soares_if_2023,
  title = {If interpretability research goes well, it may get dangerous},
  author = {Soares, Nate},
  year = {2023},
  month = apr,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/BinkknLBYxskMXuME/if-interpretability-research-goes-well-it-may-get-dangerous},
  urldate = {2023-12-05},
  abstract = {I've historically been pretty publicly supportive of interpretability research. I'm still supportive of interpretability research. However, I do not necessarily think that all of it should be done in the open indefinitely. Indeed, insofar as interpretability researchers gain understanding of AIs that could significantly advance the capabilities frontier, I encourage interpretability researchers to keep their research closed. I acknowledge that spreading research insights less widely comes with real research costs. I'd endorse building a cross-organization network of people who are committed to not using their understanding to push the capabilities frontier, and sharing freely within that. I acknowledge that public sharing of research insights could, in principle, both shorten timelines and improve our odds of success. I suspect that isn't the case in real life. It's much more important that blatant and direct capabilities research be made private. Anyone fighting for people to keep their AI insights close to the chest, should be focusing on the capabilities work that's happening out in the open, long before they focus on interpretability research. Interpretability research is, I think, some of the best research that can be approached incrementally and by a large number of people, when it comes to improving our odds. (Which is not to say it doesn't require vision and genius; I expect it requires that too.) I simultaneously think it's entirely plausible that a better understanding of the workings of modern AI systems will help capabilities researchers significantly improve capabilities. I acknowledge that this sucks, and puts us in a bind. I don't have good solutions. Reality doesn't have to provide you any outs. There's a tradeoff here. And it's not my tradeoff to make; researchers will have to figure out what they think of the costs and benefits. My guess is that the current field is not close to insights that would significantly improve capabilities, and that growing the field is important (and would be hindered by closure), and also that if the field succeeds to the degree required to move the strategic needle then it's going to start stumbling across serious capabilities improvements before it saves us, and will need to start doing research privately before then. I reiterate that I'd feel {\textasciitilde}pure enthusiasm about a cross-organization network of people trying to understand modern AI systems and committed not to letting their insights push the capabilities frontier. My goal in writing this post, though, is mostly to keep the Overton window open around the claim that there is in fact a tradeoff here, that there are reasons to close even interpretability research. Maybe those reasons should win out, or maybe they shouldn't, but don't let my praise of interpretability research obscure the fact that there are tradeoffs here.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/BLREIGWE/Soares - 2023 - If interpretability research goes well, it may get.html}
}

@article{vig_investigating_2020,
  title = {Investigating Gender Bias in Language Models Using Causal Mediation Analysis},
  author = {Vig, Jesse and Gehrmann, Sebastian and Belinkov, Yonatan and Qian, Sharon and Nevo, Daniel and Singer, Yaron and Shieber, Stuart},
  year = {2020},
  journal = {NeurIPS},
  volume = {33},
  pages = {12388--12401},
  url = {https://proceedings.neurips.cc/paper/2020/hash/92650b2e92217715fe312e6fa7b90d82-Abstract.html},
  urldate = {2023-12-20},
  abstract = {Many interpretation methods for neural models in natural language processing investigate how information is encoded inside hidden representations. However, these methods can only measure whether the information exists, not whether it is actually used by the model.  We propose a methodology grounded in the theory of causal mediation analysis for interpreting which parts of a model are causally implicated in its behavior. The approach enables us to analyze the mechanisms that facilitate the flow of information from input to output through various model components, known as mediators. As a case study, we apply this methodology to analyzing gender bias in pre-trained Transformer language models. We study the role of individual neurons and attention heads in mediating gender bias across three datasets designed to gauge a model's sensitivity to gender bias. Our mediation analysis reveals that gender bias effects are concentrated in specific components of the model that may exhibit highly specialized behavior.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/T3DVSRNW/Vig et al. - 2020 - Investigating Gender Bias in Language Models Using.pdf}
}

@article{hinton_distributed_1984,
  title = {Distributed representations},
  author = {Hinton, Geoffrey E},
  year = {1984},
  journal = {Carnegie Mellon University},
  publisher = {Carnegie Mellon University},
  url = {https://www.cs.toronto.edu/~hinton/absps/pdp3.pdf},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/4B5AVHFS/Hinton - 1984 - Distributed representations.pdf}
}

@article{locatello_challenging_2019,
  title = {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2019},
  month = jun,
  journal = {CoRR},
  eprint = {1811.12359},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1811.12359},
  url = {http://arxiv.org/abs/1811.12359},
  urldate = {2024-01-04},
  abstract = {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/9X8IU9MV/Locatello et al. - 2019 - Challenging Common Assumptions in the Unsupervised.pdf}
}

@article{higgins_definition_2018,
  title = {Towards a Definition of Disentangled Representations},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  journal = {CoRR},
  eprint = {1812.02230},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1812.02230},
  url = {http://arxiv.org/abs/1812.02230},
  urldate = {2024-01-18},
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EPH49C9X/Higgins et al. - 2018 - Towards a Definition of Disentangled Representatio.pdf}
}

@article{sundararajan_axiomatic_2017,
  title = {Axiomatic Attribution for Deep Networks},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  year = {2017},
  month = jun,
  journal = {ICML},
  eprint = {1703.01365},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1703.01365},
  url = {http://arxiv.org/abs/1703.01365},
  urldate = {2024-01-24},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/EH34DJ2E/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf}
}

@article{smilkov_smoothgrad_2017,
  title = {SmoothGrad: removing noise by adding noise},
  shorttitle = {SmoothGrad},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  year = {2017},
  month = jun,
  journal = {CoRR},
  eprint = {1706.03825},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1706.03825},
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2024-01-24},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/YVN8PK5Q/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf}
}

@article{casper_blackbox_2024,
  title = {Black-Box Access is Insufficient for Rigorous AI Audits},
  author = {Casper, Stephen and Ezell, Carson and Siegmann, Charlotte and Kolt, Noam and Curtis, Taylor Lynn and Bucknall, Benjamin and Haupt, Andreas and Wei, Kevin and Scheurer, J{\'e}r{\'e}my and Hobbhahn, Marius and Sharkey, Lee and Krishna, Satyapriya and Von Hagen, Marvin and Alberti, Silas and Chan, Alan and Sun, Qinyi and Gerovitch, Michael and Bau, David and Tegmark, Max and Krueger, David and {Hadfield-Menell}, Dylan},
  year = {2024},
  month = jan,
  journal = {ACM Conference on Fairness, Accountability, and Transparency},
  eprint = {2401.14446},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.14446},
  url = {http://arxiv.org/abs/2401.14446},
  urldate = {2024-02-07},
  abstract = {External audits of AI systems are increasingly recognized as a key mechanism for AI governance. The effectiveness of an audit, however, depends on the degree of system access granted to auditors. Recent audits of state-of-the-art AI systems have primarily relied on black-box access, in which auditors can only query the system and observe its outputs. However, white-box access to the system's inner workings (e.g., weights, activations, gradients) allows an auditor to perform stronger attacks, more thoroughly interpret models, and conduct fine-tuning. Meanwhile, outside-the-box access to its training and deployment information (e.g., methodology, code, documentation, hyperparameters, data, deployment details, findings from internal evaluations) allows for auditors to scrutinize the development process and design more targeted evaluations. In this paper, we examine the limitations of black-box audits and the advantages of white- and outside-the-box audits. We also discuss technical, physical, and legal safeguards for performing these audits with minimal security risks. Given that different forms of access can lead to very different levels of evaluation, we conclude that (1) transparency regarding the access and methods used by auditors is necessary to properly interpret audit results, and (2) white- and outside-the-box access allow for substantially more scrutiny than black-box access alone.},
  archiveprefix = {arXiv},
  keywords = {cited,to cite},
  file = {/Users/leonardbereska/Zotero/storage/F9ZWWH4F/Casper et al. - 2024 - Black-Box Access is Insufficient for Rigorous AI A.pdf}
}

@article{bengio_managing_2023,
  title = {Managing AI Risks in an Era of Rapid Progress},
  author = {Bengio, Yoshua and Hinton, Geoffrey and Yao, Andrew and Song, Dawn and Abbeel, Pieter and Harari, Yuval Noah and Zhang, Ya-Qin and Xue, Lan and {Shalev-Shwartz}, Shai and Hadfield, Gillian and Clune, Jeff and Maharaj, Tegan and Hutter, Frank and Baydin, At{\i}l{\i}m G{\"u}ne{\c s} and McIlraith, Sheila and Gao, Qiqi and Acharya, Ashwin and Krueger, David and Dragan, Anca and Torr, Philip and Russell, Stuart and Kahneman, Daniel and Brauner, Jan and Mindermann, S{\"o}ren},
  year = {2023},
  month = nov,
  journal = {CoRR},
  eprint = {2310.17688},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.17688},
  url = {http://arxiv.org/abs/2310.17688},
  urldate = {2024-02-10},
  abstract = {In this short consensus paper, we outline risks from upcoming, advanced AI systems. We examine large-scale social harms and malicious uses, as well as an irreversible loss of human control over autonomous AI systems. In light of rapid and continuing AI progress, we propose urgent priorities for AI R\&D and governance.},
  archiveprefix = {arXiv},
  keywords = {cited,cited recently},
  file = {/Users/leonardbereska/Zotero/storage/LQUMCRW5/Bengio et al. - 2023 - Managing AI Risks in an Era of Rapid Progress.pdf}
}

@article{lecomte_incidental_2023,
  title = {Incidental Polysemanticity},
  author = {Lecomte, Victor and Thaman, Kushal and Chow, Trevor and Schaeffer, Rylan and Koyejo, Sanmi},
  year = {2023},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2312.03096},
  url = {https://arxiv.org/abs/2312.03096},
  urldate = {2024-02-11},
  abstract = {Polysemantic neurons (neurons that activate for a set of unrelated features) have been seen as a significant obstacle towards interpretability of task-optimized deep networks, with implications for AI safety. The classic origin story of polysemanticity is that the data contains more "features" than neurons, such that learning to perform a task forces the network to co-allocate multiple unrelated features to the same neuron, endangering our ability to understand the network's internal processing. In this work, we present a second and non-mutually exclusive origin story of polysemanticity. We show that polysemanticity can arise incidentally, even when there are ample neurons to represent all features in the data, using a combination of theory and experiments. This second type of polysemanticity occurs because random initialization can, by chance alone, initially assign multiple features to the same neuron, and the training dynamics then strengthen such overlap. Due to its origin, we term this {\textbackslash}textit\{incidental polysemanticity\}.},
  copyright = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  keywords = {cited,mechinterp,polysemanticity,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/9BTYRJTG/Lecomte et al. - 2023 - Incidental Polysemanticity.pdf}
}

@article{hubinger_sleeper_2024,
  title = {Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training},
  shorttitle = {Sleeper Agents},
  author = {Hubinger, Evan and Denison, Carson and Mu, Jesse and Lambert, Mike and Tong, Meg and MacDiarmid, Monte and Lanham, Tamera and Ziegler, Daniel M. and Maxwell, Tim and Cheng, Newton and Jermyn, Adam and Askell, Amanda and Radhakrishnan, Ansh and Anil, Cem and Duvenaud, David and Ganguli, Deep and Barez, Fazl and Clark, Jack and Ndousse, Kamal and Sachan, Kshitij and Sellitto, Michael and Sharma, Mrinank and DasSarma, Nova and Grosse, Roger and Kravec, Shauna and Bai, Yuntao and Witten, Zachary and Favaro, Marina and Brauner, Jan and Karnofsky, Holden and Christiano, Paul and Bowman, Samuel R. and Graham, Logan and Kaplan, Jared and Mindermann, S{\"o}ren and Greenblatt, Ryan and Shlegeris, Buck and Schiefer, Nicholas and Perez, Ethan},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2401.05566},
  url = {https://arxiv.org/abs/2401.05566},
  urldate = {2024-02-11},
  abstract = {Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoor behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoor behavior is most persistent in the largest models and in models trained to produce chain-of-thought reasoning about deceiving the training process, with the persistence remaining even when the chain-of-thought is distilled away. Furthermore, rather than removing backdoors, we find that adversarial training can teach models to better recognize their backdoor triggers, effectively hiding the unsafe behavior. Our results suggest that, once a model exhibits deceptive behavior, standard techniques could fail to remove such deception and create a false impression of safety.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {backdoor,cited,fine-tuning,mechinterp,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/ICX7LHRW/Hubinger et al. - 2024 - Sleeper Agents Training Deceptive LLMs that Persi.pdf}
}

@article{leahy_barriers_2023,
  title = {Barriers to Mechanistic Interpretability for AGI Safety},
  author = {Leahy, Connor},
  year = {2023},
  journal = {AI Alignment Forum},
  url = {https://www.lesswrong.com/posts/KRDo2afKJtD7bzSM8/barriers-to-mechanistic-interpretability-for-agi-safety},
  urldate = {2024-02-11},
  abstract = {I gave a talk at MIT in March earlier this year on barriers to mechanistic interpretability being helpful to AGI/ASI safety, and why by default it wi{\dots}},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/H8YAH839/Leahy - 2023 - Barriers to Mechanistic Interpretability for AGI S.html}
}

@article{marshall_understanding_2024,
  title = {Understanding polysemanticity in neural networks through coding theory},
  author = {Marshall, Simon C. and Kirchner, Jan H.},
  year = {2024},
  month = jan,
  journal = {CoRR},
  eprint = {2401.17975},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2401.17975},
  url = {http://arxiv.org/abs/2401.17975},
  urldate = {2024-02-13},
  abstract = {Despite substantial efforts, neural network interpretability remains an elusive goal, with previous research failing to provide succinct explanations of most single neurons' impact on the network output. This limitation is due to the polysemantic nature of most neurons, whereby a given neuron is involved in multiple unrelated network states, complicating the interpretation of that neuron. In this paper, we apply tools developed in neuroscience and information theory to propose both a novel practical approach to network interpretability and theoretical insights into polysemanticity and the density of codes. We infer levels of redundancy in the network's code by inspecting the eigenspectrum of the activation's covariance matrix. Furthermore, we show how random projections can reveal whether a network exhibits a smooth or non-differentiable code and hence how interpretable the code is. This same framework explains the advantages of polysemantic neurons to learning performance and explains trends found in recent results by Elhage et al.{\textasciitilde}(2022). Our approach advances the pursuit of interpretability in neural networks, providing insights into their underlying structure and suggesting new avenues for circuit-level interpretability.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/JL2PTXI7/Marshall and Kirchner - 2024 - Understanding polysemanticity in neural networks t.pdf}
}

@article{michaud_opening_2024,
  title = {Opening the AI black box: program synthesis via mechanistic interpretability},
  shorttitle = {Opening the AI black box},
  author = {Michaud, Eric J. and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli{\'c}, Mateja and Tegmark, Max},
  year = {2024},
  month = feb,
  journal = {CoRR},
  eprint = {2402.05110},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2402.05110},
  url = {http://arxiv.org/abs/2402.05110},
  urldate = {2024-02-13},
  abstract = {We present MIPS, a novel method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform the desired task, auto-distilling the learned algorithm into Python code. We test MIPS on a benchmark of 62 algorithmic tasks that can be learned by an RNN and find it highly complementary to GPT-4: MIPS solves 32 of them, including 13 that are not solved by GPT-4 (which also solves 30). MIPS uses an integer autoencoder to convert the RNN into a finite state machine, then applies Boolean or integer symbolic regression to capture the learned algorithm. As opposed to large language models, this program synthesis technique makes no use of (and is therefore not limited by) human training data such as algorithms and code from GitHub. We discuss opportunities and challenges for scaling up this approach to make machine-learned models more interpretable and trustworthy.},
  archiveprefix = {arXiv},
  keywords = {cited,mechinterp,to extract related work},
  file = {/Users/leonardbereska/Zotero/storage/WCFVS4V8/Michaud et al. - 2024 - Opening the AI black box program synthesis via me.pdf}
}

@article{ren_defining_2023,
  title = {Defining and Quantifying the Emergence of Sparse Concepts in DNNs},
  author = {Ren, Jie and Li, Mingjie and Chen, Qirui and Deng, Huiqi and Zhang, Quanshi},
  year = {2023},
  month = apr,
  journal = {CoRR},
  eprint = {2111.06206},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2111.06206},
  url = {http://arxiv.org/abs/2111.06206},
  urldate = {2024-02-19},
  abstract = {This paper aims to illustrate the concept-emerging phenomenon in a trained DNN. Specifically, we find that the inference score of a DNN can be disentangled into the effects of a few interactive concepts. These concepts can be understood as causal patterns in a sparse, symbolic causal graph, which explains the DNN. The faithfulness of using such a causal graph to explain the DNN is theoretically guaranteed, because we prove that the causal graph can well mimic the DNN's outputs on an exponential number of different masked samples. Besides, such a causal graph can be further simplified and re-written as an And-Or graph (AOG), without losing much explanation accuracy.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/DWLWUGJK/Ren et al. - 2023 - Defining and Quantifying the Emergence of Sparse C.pdf}
}

@article{trivedi_learning_2021,
  title = {Learning to Synthesize Programs as Interpretable and Generalizable Policies},
  author = {Trivedi, Dweep and Zhang, Jesse and Sun, Shao-Hua and Lim, Joseph J.},
  year = {2021},
  journal = {NeurIPS},
  eprint = {2108.13643},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2108.13643},
  url = {http://arxiv.org/abs/2108.13643},
  urldate = {2024-02-19},
  abstract = {Recently, deep reinforcement learning (DRL) methods have achieved impressive performance on tasks in a variety of domains. However, neural network policies produced with DRL methods are not human-interpretable and often have difficulty generalizing to novel scenarios. To address these issues, prior works explore learning programmatic policies that are more interpretable and structured for generalization. Yet, these works either employ limited policy representations (e.g. decision trees, state machines, or predefined program templates) or require stronger supervision (e.g. input/output state pairs or expert demonstrations). We present a framework that instead learns to synthesize a program, which details the procedure to solve a task in a flexible and expressive manner, solely from reward signals. To alleviate the difficulty of learning to compose programs to induce the desired agent behavior from scratch, we propose to first learn a program embedding space that continuously parameterizes diverse behaviors in an unsupervised manner and then search over the learned program embedding space to yield a program that maximizes the return for a given task. Experimental results demonstrate that the proposed framework not only learns to reliably synthesize task-solving programs but also outperforms DRL and program synthesis baselines while producing interpretable and more generalizable policies. We also justify the necessity of the proposed two-stage learning scheme as well as analyze various methods for learning the program embedding.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/FR2GHXCA/Trivedi et al. - 2021 - Learning to Synthesize Programs as Interpretable a.pdf}
}

@article{verma_imitation-projected_2019,
  title = {Imitation-Projected Programmatic Reinforcement Learning},
  author = {Verma, Abhinav and Le, Hoang M. and Yue, Yisong and Chaudhuri, Swarat},
  year = {2019},
  journal = {NeurIPS},
  eprint = {1907.05431},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1907.05431},
  url = {http://arxiv.org/abs/1907.05431},
  urldate = {2024-02-19},
  abstract = {We study the problem of programmatic reinforcement learning, in which policies are represented as short programs in a symbolic language. Programmatic policies can be more interpretable, generalizable, and amenable to formal verification than neural policies; however, designing rigorous learning approaches for such policies remains a challenge. Our approach to this challenge -- a meta-algorithm called PROPEL -- is based on three insights. First, we view our learning task as optimization in policy space, modulo the constraint that the desired policy has a programmatic representation, and solve this optimization problem using a form of mirror descent that takes a gradient step into the unconstrained policy space and then projects back onto the constrained space. Second, we view the unconstrained policy space as mixing neural and programmatic representations, which enables employing state-of-the-art deep policy gradient approaches. Third, we cast the projection step as program synthesis via imitation learning, and exploit contemporary combinatorial methods for this task. We present theoretical convergence results for PROPEL and empirically evaluate the approach in three continuous control domains. The experiments show that PROPEL can significantly outperform state-of-the-art approaches for learning programmatic policies.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/5IAKNTWV/Verma et al. - 2019 - Imitation-Projected Programmatic Reinforcement Lea.pdf}
}

@article{zhang_interpreting_2019,
  title = {Interpreting CNNs via Decision Trees},
  author = {Zhang, Quanshi and Yang, Yu and Ma, Haotian and Wu, Ying Nian},
  year = {2019},
  journal = {CVPR},
  pages = {6261--6270},
  url = {https://arxiv.org/abs/1802.00121},
  urldate = {2024-02-19},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/3XJGKKNF/Zhang et al. - 2019 - Interpreting CNNs via Decision Trees.pdf}
}

@article{wu_backdoorbench_2022,
  title = {BackdoorBench: A Comprehensive Benchmark of Backdoor Learning},
  shorttitle = {BackdoorBench},
  author = {Wu, Baoyuan and Chen, Hongrui and Zhang, Mingda and Zhu, Zihao and Wei, Shaokui and Yuan, Danni and Shen, Chao},
  year = {2022},
  month = oct,
  journal = {NeurIPS Datasets and Benchmarks},
  eprint = {2206.12654},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.12654},
  url = {http://arxiv.org/abs/2206.12654},
  urldate = {2024-02-19},
  abstract = {Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility. Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning. All codes and evaluations of BackdoorBench are publicly available at {\textbackslash}url\{https://backdoorbench.github.io\}.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/JIVLJK6J/Wu et al. - 2022 - BackdoorBench A Comprehensive Benchmark of Backdo.pdf}
}

@article{slavachalnev_sparse_2024,
  title = {Sparse MLP Distillation},
  author = {{slavachalnev}},
  year = {2024},
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/MXabwqMwo3rkGqEW8/sparse-mlp-distillation},
  urldate = {2024-03-01},
  abstract = {Extract interpretable features from an MLP by distilling it into a sparse student MLP.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/E3CNLQIA/slavachalnev - 2024 - Sparse MLP Distillation.html}
}

@book{hendrycks_introduction_2023,
  title = {Introduction to AI Safety, Ethics, and Society},
  author = {Hendrycks, Dan},
  year = {2023},
  publisher = {Self-published},
  url = {https://www.aisafetybook.com/},
  urldate = {2024-03-06},
  abstract = {We already face issues in controlling the goals of current-day AI systems. If this is also true with future AI systems that are more powerful and more integrated with our economies and militaries, we could see dangerous rogue AI systems emerge.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/INR9Y2LB/Hendrycks - 2023 - Introduction to AI Safety, Ethics, and Society.html}
}

@article{lang_when_2024,
  title = {When Your AIs Deceive You: Challenges with Partial Observability of Human Evaluators in Reward Learning},
  shorttitle = {When Your AIs Deceive You},
  author = {Lang, Leon and Foote, Davis and Russell, Stuart and Dragan, Anca and Jenner, Erik and Emmons, Scott},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2402.17747},
  primaryclass = {cs, stat},
  url = {http://arxiv.org/abs/2402.17747},
  urldate = {2024-04-19},
  abstract = {Past analyses of reinforcement learning from human feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Modeling the human as Boltzmann-rational w.r.t. a belief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, overjustify their behavior to make an impression, or both. To help address these issues, we mathematically characterize how partial observability of the environment translates into (lack of) ambiguity in the learned return function. In some cases, accounting for partial observability makes it theoretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/BIUNQQTV/Lang et al. - 2024 - When Your AIs Deceive You Challenges with Partial.pdf}
}

@article{bereska_mechanistic_2024,
  title = {Mechanistic Interpretability for AI Safety - A Review},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2404.14082},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2404.14082},
  urldate = {2024-04-23},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/67XQNYEM/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety -- A Re.pdf}
}

@article{maloyan_trojan_2024,
  title = {Trojan Detection in Large Language Models: Insights from The Trojan Detection Challenge},
  shorttitle = {Trojan Detection in Large Language Models},
  author = {Maloyan, Narek and Verma, Ekansh and Nutfullin, Bulat and Ashinov, Bislan},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2404.13660},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2404.13660},
  url = {http://arxiv.org/abs/2404.13660},
  urldate = {2024-04-25},
  abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in various domains, but their vulnerability to trojan or backdoor attacks poses significant security risks. This paper explores the challenges and insights gained from the Trojan Detection Competition 2023 (TDC2023), which focused on identifying and evaluating trojan attacks on LLMs. We investigate the difficulty of distinguishing between intended and unintended triggers, as well as the feasibility of reverse engineering trojans in real-world scenarios. Our comparative analysis of various trojan detection methods reveals that achieving high Recall scores is significantly more challenging than obtaining high Reverse-Engineering Attack Success Rate (REASR) scores. The top-performing methods in the competition achieved Recall scores around 0.16, comparable to a simple baseline of randomly sampling sentences from a distribution similar to the given training prefixes. This finding raises questions about the detectability and recoverability of trojans inserted into the model, given only the harmful targets. Despite the inability to fully solve the problem, the competition has led to interesting observations about the viability of trojan detection and improved techniques for optimizing LLM input prompts. The phenomenon of unintended triggers and the difficulty in distinguishing them from intended triggers highlights the need for further research into the robustness and interpretability of LLMs. The TDC2023 has provided valuable insights into the challenges and opportunities associated with trojan detection in LLMs, laying the groundwork for future research in this area to ensure their safety and reliability in real-world applications.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/4BCL3BEN/Maloyan et al. - 2024 - Trojan Detection in Large Language Models Insight.pdf}
}

@article{templeton_scaling_2024,
  title = {Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
  author = {Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian},
  year = {2024},
  journal = {Transformer Circuits Thread},
  url = {https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html},
  abstract = {Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer. At the time, a major concern was that this method might not scale feasibly to state-of-the-art transformers and, as a result, be unable to practically contribute to AI safety. Since then, scaling sparse autoencoders has been a major priority of the Anthropic interpretability team, and we're pleased to report extracting high-quality features from Claude 3 Sonnet, 1 Anthropic's medium-sized production model. We find a diversity of highly abstract features. They both respond to and behaviorally cause abstract behaviors. Examples of features we find include features for famous people, features for countries and cities, and features tracking type signatures in code. Many features are multilingual (responding to the same concept across languages) and multimodal (responding to the same concept in both text and images), as well as encompassing both abstract and concrete instantiations of the same idea (such as code with security vulnerabilities, and abstract discussion of security vulnerabilities). Some of the features we find are of particular interest because they may be safety-relevant -- that is, they are plausibly connected to a range of ways in which modern AI systems may cause harm. In particular, we find features related to security vulnerabilities and backdoors in code; bias (including both overt slurs, and more subtle biases); lying, deception, and power-seeking (including treacherous turns); sycophancy; and dangerous / criminal content (e.g., producing bioweapons). However, we caution not to read too much into the mere existence of such features: there's a difference (for example) between knowing about lies, being capable of lying, and actually lying in the real world. This research is also very preliminary. Further work will be needed to understand the implications of these potentially safety-relevant features. Key Results Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders. The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references. There appears to be a systematic relationship between the frequency of concepts and the dictionary size needed to resolve features for them. Features can be used to steer large models (see e.g. Influence on Behavior). This extends prior work on steering models using other methods (see Related Work). We observe features related to a broad range of safety concerns, including deception, sycophancy, bias, and dangerous content.},
  keywords = {cited}
}

@article{bushnaq_local_2024,
  title = {The Local Interaction Basis: Identifying Computationally-Relevant and Sparsely Interacting Features in Neural Networks},
  shorttitle = {The Local Interaction Basis},
  author = {Bushnaq, Lucius and Heimersheim, Stefan and {Goldowsky-Dill}, Nicholas and Braun, Dan and Mendel, Jake and Hanni, Kaarel and Griffin, Avery and Stohler, Jorn and Wache, Magdalena and Hobbhahn, Marius},
  year = {2024},
  month = may,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2405.10928},
  urldate = {2024-06-09},
  abstract = {Mechanistic interpretability aims to understand the behavior of neural networks by reverse-engineering their internal computations. However, current methods struggle to find clear interpretations of neural network activations because a decomposition of activations into computational features is missing. Individual neurons or model components do not cleanly correspond to distinct features or functions. We present a novel interpretability method that aims to overcome this limitation by transforming the activations of the network into a new basis - the Local Interaction Basis (LIB). LIB aims to identify computational features by removing irrelevant activations and interactions. Our method drops irrelevant activation directions and aligns the basis with the singular vectors of the Jacobian matrix between adjacent layers. It also scales features based on their importance for downstream computation, producing an interaction graph that shows all computationally-relevant features and interactions in a model. We evaluate the effectiveness of LIB on modular addition and CIFAR-10 models, finding that it identifies more computationally-relevant features that interact more sparsely, compared to principal component analysis. However, LIB does not yield substantial improvements in interpretability or interaction sparsity when applied to language models. We conclude that LIB is a promising theory-driven approach for analyzing neural networks, but in its current form is not applicable to large language models.},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/2Y3B28Z4/Bushnaq et al. - 2024 - The Local Interaction Basis Identifying Computati.pdf}
}

@article{marks_sparse_2024,
  title = {Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models},
  shorttitle = {Sparse Feature Circuits},
  author = {Marks, Samuel and Rager, Can and Michaud, Eric J. and Belinkov, Yonatan and Bau, David and Mueller, Aaron},
  year = {2024},
  month = mar,
  journal = {CoRR},
  eprint = {2403.19647},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2403.19647},
  url = {http://arxiv.org/abs/2403.19647},
  urldate = {2024-06-10},
  abstract = {We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/PAXI7A5W/Marks et al. - 2024 - Sparse Feature Circuits Discovering and Editing I.pdf}
}

@article{nanfack_adversarial_2024,
  title = {Adversarial Attacks on the Interpretation of Neuron Activation Maximization},
  author = {Nanfack, Geraldin and Fulleringer, Alexander and Marty, Jonathan and Eickenberg, Michael and Belilovsky, Eugene},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {5},
  pages = {4315--4324},
  issn = {2374-3468},
  doi = {10.1609/aaai.v38i5.28228},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/28228},
  urldate = {2024-06-10},
  abstract = {Feature visualization is one of the most popular techniques used to interpret the internal behavior of individual units of trained deep neural networks. Based on activation maximization, they consist of finding synthetic or natural inputs that maximize neuron activations. This paper introduces an optimization framework that aims to deceive feature visualization through adversarial model manipulation. It consists of finetuning a pre-trained model with a specifically introduced loss that aims to maintain model performance, while also significantly changing feature visualization. We provide evidence of the success of this manipulation on several pre-trained models for the classification task with ImageNet.},
  copyright = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/MI379E4Y/Nanfack et al. - 2024 - Adversarial Attacks on the Interpretation of Neuro.pdf}
}

@article{gandelsman_interpreting_2024,
  title = {Interpreting the Second-Order Effects of Neurons in CLIP},
  author = {Gandelsman, Yossi and Efros, Alexei A. and Steinhardt, Jacob},
  year = {2024},
  month = jun,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Interpreting-the-Second-Order-Effects-of-Neurons-in-Gandelsman-Efros/1b240f7cfc5f5f5f2bf8d99dc622a3c405837a73},
  urldate = {2024-06-10},
  abstract = {We interpret the function of individual neurons in CLIP by automatically describing them using text. Analyzing the direct effects (i.e. the flow from a neuron through the residual stream to the output) or the indirect effects (overall contribution) fails to capture the neurons' function in CLIP. Therefore, we present the"second-order lens", analyzing the effect flowing from a neuron through the later attention heads, directly to the output. We find that these effects are highly selective: for each neuron, the effect is significant for{$<$}2\% of the images. Moreover, each effect can be approximated by a single direction in the text-image space of CLIP. We describe neurons by decomposing these directions into sparse sets of text representations. The sets reveal polysemantic behavior - each neuron corresponds to multiple, often unrelated, concepts (e.g. ships and cars). Exploiting this neuron polysemy, we mass-produce"semantic"adversarial examples by generating images with concepts spuriously correlated to the incorrect class. Additionally, we use the second-order effects for zero-shot segmentation and attribute discovery in images. Our results indicate that a scalable understanding of neurons can be used for model deception and for introducing new model capabilities.},
  file = {/Users/leonardbereska/Zotero/storage/EEZIEKWV/Gandelsman et al. - 2024 - Interpreting the Second-Order Effects of Neurons i.pdf}
}

@article{tan_understanding_2023,
  title = {Understanding Grokking Through A Robustness Viewpoint},
  author = {Tan, Zhiquan and Huang, Weiran},
  year = {2023},
  month = nov,
  journal = {CoRR},
  url = {https://arxiv.org/abs/2311.06597v2},
  urldate = {2024-06-10},
  abstract = {Recently, an interesting phenomenon called grokking has gained much attention, where generalization occurs long after the models have initially overfitted the training data. We try to understand this seemingly strange phenomenon through the robustness of the neural network. From a robustness perspective, we show that the popular \$l\_2\$ weight norm (metric) of the neural network is actually a sufficient condition for grokking. Based on the previous observations, we propose perturbation-based methods to speed up the generalization process. In addition, we examine the standard training process on the modulo addition dataset and find that it hardly learns other basic group operations before grokking, for example, the commutative law. Interestingly, the speed-up of generalization when using our proposed method can be explained by learning the commutative law, a necessary condition when the model groks on the test dataset. We also empirically find that \$l\_2\$ norm correlates with grokking on the test data not in a timely way, we propose new metrics based on robustness and information theory and find that our new metrics correlate well with the grokking phenomenon and may be used to predict grokking.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/DVWEUUTF/Tan and Huang - 2023 - Understanding Grokking Through A Robustness Viewpo.pdf}
}

@article{casper_defending_2024,
  title = {Defending Against Unforeseen Failure Modes with Latent Adversarial Training},
  author = {Casper, Stephen and Schulze, Lennart and Patel, Oam and {Hadfield-Menell}, Dylan},
  year = {2024},
  month = apr,
  journal = {CoRR},
  eprint = {2403.05030},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2403.05030},
  urldate = {2024-06-10},
  abstract = {Despite extensive diagnostics and debugging by developers, AI systems sometimes exhibit harmful unintended behaviors. Finding and fixing these is challenging because the attack surface is so large -- it is not tractable to exhaustively search for inputs that may elicit harmful behaviors. Red-teaming and adversarial training (AT) are commonly used to improve robustness, however, they empirically struggle to fix failure modes that differ from the attacks used during training. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use it to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classification, and text generation tasks that LAT usually improves both robustness to novel attacks and performance on clean data relative to AT. This suggests that LAT can be a promising tool for defending against failure modes that are not explicitly identified by developers.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/CA4VTZJW/Casper et al. - 2024 - Defending Against Unforeseen Failure Modes with La.pdf}
}

@article{gross_compact_2024,
  title = {Compact Proofs of Model Performance via Mechanistic Interpretability},
  author = {Gross, Jason and Agrawal, Rajashree and Kwa, Thomas and Ong, Euan and Yip, Chun Hei and Gibson, Alex and Noubir, Soufiane and Chan, Lawrence},
  year = {2024},
  month = jun,
  journal = {ICML MI Workshop},
  eprint = {2406.11779},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2406.11779},
  url = {http://arxiv.org/abs/2406.11779},
  urldate = {2024-06-25},
  abstract = {In this work, we propose using mechanistic interpretability -- techniques for reverse engineering model weights into human-interpretable algorithms -- to derive and compactly prove formal guarantees on model performance. We prototype this approach by formally proving lower bounds on the accuracy of 151 small transformers trained on a Max-of-\$K\$ task. We create 102 different computer-assisted proof strategies and assess their length and tightness of bound on each of our models. Using quantitative metrics, we find that shorter proofs seem to require and provide more mechanistic understanding. Moreover, we find that more faithful mechanistic understanding leads to tighter performance bounds. We confirm these connections by qualitatively examining a subset of our proofs. Finally, we identify compounding structureless noise as a key challenge for using mechanistic interpretability to generate compact proofs on model performance.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/CACL64ET/Gross et al. - 2024 - Compact Proofs of Model Performance via Mechanisti.pdf}
}

@article{baniecki_adversarial_2024,
  title = {Adversarial attacks and defenses in explainable artificial intelligence: A survey},
  shorttitle = {Adversarial attacks and defenses in explainable artificial intelligence},
  author = {Baniecki, Hubert and Biecek, Przemyslaw},
  year = {2024},
  month = jul,
  journal = {Information Fusion},
  volume = {107},
  pages = {102303},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2024.102303},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253524000812},
  urldate = {2024-06-28},
  abstract = {Explainable artificial intelligence (XAI) methods are portrayed as a remedy for debugging and trusting statistical and deep learning models, as well as interpreting their predictions. However, recent advances in adversarial machine learning (AdvML) highlight the limitations and vulnerabilities of state-of-the-art explanation methods, putting their security and trustworthiness into question. The possibility of manipulating, fooling or fairwashing evidence of the model's reasoning has detrimental consequences when applied in high-stakes decision-making and knowledge discovery. This survey provides a comprehensive overview of research concerning adversarial attacks on explanations of machine learning models, as well as fairness metrics. We introduce a unified notation and taxonomy of methods facilitating a common ground for researchers and practitioners from the intersecting research fields of AdvML and XAI. We discuss how to defend against attacks and design robust interpretation methods. We contribute a list of existing insecurities in XAI and outline the emerging research directions in adversarial XAI (AdvXAI). Future work should address improving explanation methods and evaluation protocols to take into account the reported safety issues.},
  file = {/Users/leonardbereska/Zotero/storage/SWFMGR6S/Baniecki and Biecek - 2024 - Adversarial attacks and defenses in explainable ar.pdf;/Users/leonardbereska/Zotero/storage/BYCFD83V/Baniecki and Biecek - 2024 - Adversarial attacks and defenses in explainable ar.html}
}

@article{bereska_mechanistic_2024a,
  title = {Mechanistic Interpretability for AI Safety --- A Review},
  author = {Bereska, Leonard and Gavves, Efstratios},
  year = {2024},
  month = apr,
  journal = {CoRR},
  url = {http://arxiv.org/abs/2404.14082},
  urldate = {2024-07-15},
  abstract = {Understanding AI systems' inner workings is critical for ensuring value alignment and safety. This review explores mechanistic interpretability: reverse-engineering the computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts to provide a granular, causal understanding. We establish foundational concepts such as features encoding knowledge within neural activations and hypotheses about their representation and computation. We survey methodologies for causally dissecting model behaviors and assess the relevance of mechanistic interpretability to AI safety. We investigate challenges surrounding scalability, automation, and comprehensive interpretation. We advocate for clarifying concepts, setting standards, and scaling techniques to handle complex models and behaviors and expand to domains such as vision and reinforcement learning. Mechanistic interpretability could help prevent catastrophic outcomes as AI systems become more powerful and inscrutable.},
  file = {/Users/leonardbereska/Zotero/storage/GBVRV4KD/Bereska and Gavves - 2024 - Mechanistic Interpretability for AI Safety — A Rev.pdf}
}

@article{carranza_deceptive_2023,
  title = {Deceptive Alignment Monitoring},
  author = {Carranza, Andres and Pai, Dhruv and Schaeffer, Rylan and Tandon, Arnuv and Koyejo, Sanmi},
  year = {2023},
  month = jul,
  journal = {ICML AdvML Workshop (BlueSky Oral)},
  eprint = {2307.10569},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2307.10569},
  url = {http://arxiv.org/abs/2307.10569},
  urldate = {2024-08-02},
  abstract = {As the capabilities of large machine learning models continue to grow, and as the autonomy afforded to such models continues to expand, the spectre of a new adversary looms: the models themselves. The threat that a model might behave in a seemingly reasonable manner, while secretly and subtly modifying its behavior for ulterior reasons is often referred to as deceptive alignment in the AI Safety \& Alignment communities. Consequently, we call this new direction Deceptive Alignment Monitoring. In this work, we identify emerging directions in diverse machine learning subfields that we believe will become increasingly important and intertwined in the near future for deceptive alignment monitoring, and we argue that advances in these fields present both long-term challenges and new research opportunities. We conclude by advocating for greater involvement by the adversarial machine learning community in these emerging directions.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/NHNLP5S5/Carranza et al. - 2023 - Deceptive Alignment Monitoring.pdf}
}

@article{denison_sycophancy_2024,
  title = {Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models},
  shorttitle = {Sycophancy to Subterfuge},
  author = {Denison, Carson and MacDiarmid, Monte and Barez, Fazl and Duvenaud, David and Kravec, Shauna and Marks, Samuel and Schiefer, Nicholas and Soklaski, Ryan and Tamkin, Alex and Kaplan, Jared and Shlegeris, Buck and Bowman, Samuel R. and Perez, Ethan and Hubinger, Evan},
  year = {2024},
  journal = {CoRR},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2406.10162},
  url = {https://arxiv.org/abs/2406.10162},
  urldate = {2024-08-03},
  abstract = {In reinforcement learning, specification gaming occurs when AI systems learn undesired behaviors that are highly rewarded due to misspecified training goals. Specification gaming can range from simple behaviors like sycophancy to sophisticated and pernicious behaviors like reward-tampering, where a model directly modifies its own reward mechanism. However, these more pernicious behaviors may be too complex to be discovered via exploration. In this paper, we study whether Large Language Model (LLM) assistants which find easily discovered forms of specification gaming will generalize to perform rarer and more blatant forms, up to and including reward-tampering. We construct a curriculum of increasingly sophisticated gameable environments and find that training on early-curriculum environments leads to more specification gaming on remaining environments. Strikingly, a small but non-negligible proportion of the time, LLM assistants trained on the full curriculum generalize zero-shot to directly rewriting their own reward function. Retraining an LLM not to game early-curriculum environments mitigates, but does not eliminate, reward-tampering in later environments. Moreover, adding harmlessness training to our gameable environments does not prevent reward-tampering. These results demonstrate that LLMs can generalize from common forms of specification gaming to more pernicious reward tampering and that such behavior may be nontrivial to remove.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/V2B5TJCS/Denison et al. - 2024 - Sycophancy to Subterfuge Investigating Reward-Tam.pdf}
}

@article{garcia-carrasco_detecting_2024,
  title = {Detecting and Understanding Vulnerabilities in Language Models via Mechanistic Interpretability},
  author = {{Garc{\'i}a-Carrasco}, Jorge and Mat{\'e}, Alejandro and Trujillo, Juan},
  year = {2024},
  month = aug,
  journal = {IJCAI},
  eprint = {2407.19842},
  primaryclass = {cs},
  pages = {385--393},
  doi = {10.24963/ijcai.2024/43},
  url = {http://arxiv.org/abs/2407.19842},
  urldate = {2024-08-03},
  abstract = {Large Language Models (LLMs), characterized by being trained on broad amounts of data in a selfsupervised manner, have shown impressive performance across a wide range of tasks. Indeed, their generative abilities have aroused interest on the application of LLMs across a wide range of contexts. However, neural networks in general, and LLMs in particular, are known to be vulnerable to adversarial attacks, where an imperceptible change to the input can mislead the output of the model. This is a serious concern that impedes the use of LLMs on high-stakes applications, such as healthcare, where a wrong prediction can imply serious consequences. Even though there are many efforts on making LLMs more robust to adversarial attacks, there are almost no works that study how and where these vulnerabilities that make LLMs prone to adversarial attacks happen. Motivated by these facts, we explore how to localize and understand vulnerabilities, and propose a method, based on Mechanistic Interpretability (MI) techniques, to guide this process. Specifically, this method enables us to detect vulnerabilities related to a concrete task by (i) obtaining the subset of the model that is responsible for that task, (ii) generating adversarial samples for that task, and (iii) using MI techniques together with the previous samples to discover and understand the possible vulnerabilities. We showcase our method on a pretrained GPT2 Small model carrying out the task of predicting 3letter acronyms to demonstrate its effectiveness on locating and understanding concrete vulnerabilities of the model.},
  archiveprefix = {arXiv},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/TUI5AJMA/García-Carrasco et al. - 2024 - Detecting and Understanding Vulnerabilities in Lan.pdf}
}

@article{engstrom_adversarial_2019,
  title = {Adversarial Robustness as a Prior for Learned Representations},
  author = {Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Madry, Aleksander},
  year = {2019},
  month = sep,
  journal = {CoRR},
  eprint = {1906.00945},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1906.00945},
  url = {http://arxiv.org/abs/1906.00945},
  urldate = {2024-08-03},
  abstract = {An important goal in deep learning is to learn versatile, high-level feature representations of input data. However, standard networks' representations seem to possess shortcomings that, as we illustrate, prevent them from fully realizing this goal. In this work, we show that robust optimization can be re-cast as a tool for enforcing priors on the features learned by deep neural networks. It turns out that representations learned by robust models address the aforementioned shortcomings and make significant progress towards learning a high-level encoding of inputs. In particular, these representations are approximately invertible, while allowing for direct visualization and manipulation of salient input features. More broadly, our results indicate adversarial robustness as a promising avenue for improving learned representations. Our code and models for reproducing these results is available at https://git.io/robust-reps .},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/WNMTUM7F/Engstrom et al. - 2019 - Adversarial Robustness as a Prior for Learned Repr.pdf}
}

@article{salman_adversarially_2020,
  title = {Do Adversarially Robust ImageNet Models Transfer Better?},
  author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
  year = {2020},
  month = dec,
  journal = {NeurIPS},
  eprint = {2007.08489},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2007.08489},
  url = {http://arxiv.org/abs/2007.08489},
  urldate = {2024-08-03},
  abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer .},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/APQEKH3M/Salman et al. - 2020 - Do Adversarially Robust ImageNet Models Transfer B.pdf}
}

@article{tsipras_robustness_2019,
  title = {Robustness May Be at Odds with Accuracy},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  year = {2019},
  month = sep,
  journal = {ICLR},
  eprint = {1805.12152},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1805.12152},
  url = {http://arxiv.org/abs/1805.12152},
  urldate = {2024-08-03},
  abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/FW8KUFLM/Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf}
}

@article{ross_improving_2017,
  title = {Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients},
  author = {Ross, Andrew Slavin and {Doshi-Velez}, Finale},
  year = {2017},
  month = nov,
  journal = {AAAI},
  eprint = {1711.09404},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1711.09404},
  url = {http://arxiv.org/abs/1711.09404},
  urldate = {2024-08-03},
  abstract = {Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more "legitimate," interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/PYKXG52I/Ross and Doshi-Velez - 2017 - Improving the Adversarial Robustness and Interpret.pdf}
}

@article{augustin_adversarial_2020,
  title = {Adversarial Robustness on In- and Out-Distribution Improves Explainability},
  author = {Augustin, Maximilian and Meinke, Alexander and Hein, Matthias},
  year = {2020},
  month = jul,
  journal = {ECCV},
  eprint = {2003.09461},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2003.09461},
  url = {http://arxiv.org/abs/2003.09461},
  urldate = {2024-08-03},
  abstract = {Neural networks have led to major improvements in image classification but suffer from being non-robust to adversarial changes, unreliable uncertainty estimates on out-distribution samples and their inscrutable black-box decisions. In this work we propose RATIO, a training procedure for Robustness via Adversarial Training on In- and Out-distribution, which leads to robust models with reliable and robust confidence estimates on the out-distribution. RATIO has similar generative properties to adversarial training so that visual counterfactuals produce class specific features. While adversarial training comes at the price of lower clean accuracy, RATIO achieves state-of-the-art \$l\_2\$-adversarial robustness on CIFAR10 and maintains better clean accuracy.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/VMFCEVKH/Augustin et al. - 2020 - Adversarial Robustness on In- and Out-Distribution.pdf}
}

@article{jyoti_robustness_2022,
  title = {On the Robustness of Explanations of Deep Neural Network Models: A Survey},
  shorttitle = {On the Robustness of Explanations of Deep Neural Network Models},
  author = {Jyoti, Amlan and Ganesh, Karthik Balaji and Gayala, Manoj and Tunuguntla, Nandita Lakshmi and Kamath, Sandesh and Balasubramanian, Vineeth N.},
  year = {2022},
  month = nov,
  journal = {CoRR},
  eprint = {2211.04780},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.04780},
  url = {http://arxiv.org/abs/2211.04780},
  urldate = {2024-08-03},
  abstract = {Explainability has been widely stated as a cornerstone of the responsible and trustworthy use of machine learning models. With the ubiquitous use of Deep Neural Network (DNN) models expanding to risk-sensitive and safety-critical domains, many methods have been proposed to explain the decisions of these models. Recent years have also seen concerted efforts that have shown how such explanations can be distorted (attacked) by minor input perturbations. While there have been many surveys that review explainability methods themselves, there has been no effort hitherto to assimilate the different methods and metrics proposed to study the robustness of explanations of DNN models. In this work, we present a comprehensive survey of methods that study, understand, attack, and defend explanations of DNN models. We also present a detailed review of different metrics used to evaluate explanation methods, as well as describe attributional attack and defense methods. We conclude with lessons and take-aways for the community towards ensuring robust explanations of DNN model predictions.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/FQISZVLN/Jyoti et al. - 2022 - On the Robustness of Explanations of Deep Neural N.pdf}
}

@article{liu_delving_2017,
  title = {Delving into Transferable Adversarial Examples and Black-box Attacks},
  author = {Liu, Yanpei and Chen, Xinyun and Liu, Chang and Song, Dawn},
  year = {2017},
  month = feb,
  journal = {CoRR},
  eprint = {1611.02770},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1611.02770},
  url = {http://arxiv.org/abs/1611.02770},
  urldate = {2024-08-03},
  abstract = {An intriguing property of deep neural networks is the existence of adversarial examples, which can transfer among different architectures. These transferable adversarial examples may severely hinder deep neural network-based applications. Previous works mostly study the transferability using small scale datasets. In this work, we are the first to conduct an extensive study of the transferability over large models and a large scale dataset, and we are also the first to study the transferability of targeted adversarial examples with their target labels. We study both non-targeted and targeted adversarial examples, and show that while transferable non-targeted adversarial examples are easy to find, targeted adversarial examples generated using existing approaches almost never transfer with their target labels. Therefore, we propose novel ensemble-based approaches to generating transferable adversarial examples. Using such approaches, we observe a large proportion of targeted adversarial examples that are able to transfer with their target labels for the first time. We also present some geometric studies to help understanding the transferable adversarial examples. Finally, we show that the adversarial examples generated using ensemble-based approaches can successfully attack Clarifai.com, which is a black-box image classification system.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/QSK4QTJ8/Liu et al. - 2017 - Delving into Transferable Adversarial Examples and.pdf}
}

@article{gilmer_adversarial_2018,
  title = {Adversarial Spheres},
  author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
  year = {2018},
  month = sep,
  journal = {CoRR},
  eprint = {1801.02774},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1801.02774},
  url = {http://arxiv.org/abs/1801.02774},
  urldate = {2024-08-03},
  abstract = {State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size \$O(1/{\textbackslash}sqrt\{d\})\$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/GDMEZVCN/Gilmer et al. - 2018 - Adversarial Spheres.pdf}
}

@article{papernot_science_2016,
  title = {Towards the Science of Security and Privacy in Machine Learning},
  author = {Papernot, Nicolas and McDaniel, Patrick and Sinha, Arunesh and Wellman, Michael},
  year = {2016},
  month = nov,
  journal = {CoRR},
  eprint = {1611.03814},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1611.03814},
  url = {http://arxiv.org/abs/1611.03814},
  urldate = {2024-08-03},
  abstract = {Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive---new systems and models are being deployed in every domain imaginable, leading to rapid and widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize recent findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date. We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. We conclude by formally exploring the opposing relationship between model accuracy and resilience to adversarial manipulation. Through these explorations, we show that there are (possibly unavoidable) tensions between model complexity, accuracy, and resilience that must be calibrated for the environments in which they will be used.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/2FFY8N8J/Papernot et al. - 2016 - Towards the Science of Security and Privacy in Mac.pdf}
}

@article{bastani_interpreting_2019,
  title = {Interpreting Blackbox Models via Model Extraction},
  author = {Bastani, Osbert and Kim, Carolyn and Bastani, Hamsa},
  year = {2019},
  month = jan,
  journal = {CoRR},
  eprint = {1705.08504},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1705.08504},
  url = {http://arxiv.org/abs/1705.08504},
  urldate = {2024-08-03},
  abstract = {Interpretability has become incredibly important as machine learning is increasingly used to inform consequential decisions. We propose to construct global explanations of complex, blackbox models in the form of a decision tree approximating the original model---as long as the decision tree is a good approximation, then it mirrors the computation performed by the blackbox model. We devise a novel algorithm for extracting decision tree explanations that actively samples new training points to avoid overfitting. We evaluate our algorithm on a random forest to predict diabetes risk and a learned controller for cart-pole. Compared to several baselines, our decision trees are both substantially more accurate and equally or more interpretable based on a user study. Finally, we describe several insights provided by our interpretations, including a causal issue validated by a physician.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/M7HAUQ8N/Bastani et al. - 2019 - Interpreting Blackbox Models via Model Extraction.pdf}
}

@article{boopathy_proper_2020,
  title = {Proper Network Interpretability Helps Adversarial Robustness in Classification},
  author = {Boopathy, Akhilan and Liu, Sijia and Zhang, Gaoyuan and Liu, Cynthia and Chen, Pin-Yu and Chang, Shiyu and Daniel, Luca},
  year = {2020},
  month = oct,
  journal = {ICML},
  eprint = {2006.14748},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2006.14748},
  url = {http://arxiv.org/abs/2006.14748},
  urldate = {2024-08-04},
  abstract = {Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against attacks of large perturbation in particular.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/JKZWCB3A/Boopathy et al. - 2020 - Proper Network Interpretability Helps Adversarial .pdf}
}

@article{carter_activation_2019,
  title = {Activation Atlas},
  author = {Carter, Shan and Armstrong, Zan and Schubert, Ludwig and Johnson, Ian and Olah, Chris},
  year = {2019},
  month = mar,
  journal = {Distill},
  volume = {4},
  number = {3},
  pages = {e15},
  issn = {2476-0757},
  doi = {10.23915/distill.00015},
  url = {https://distill.pub/2019/activation-atlas},
  urldate = {2024-08-04},
  abstract = {By using feature inversion to visualize millions of activations from an image classification network, we create an explorable activation atlas of features the network has learned and what concepts it typically represents.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/X7B94JIM/Carter et al. - 2019 - Activation Atlas.html}
}

@article{casper_diagnostics_2023,
  title = {Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks},
  author = {Casper, Stephen and Hariharan, Kaivalya and {Hadfield-Menell}, Dylan},
  year = {2023},
  month = may,
  journal = {NeurIPS 2022 ML Safety Workshop (Best paper award)},
  eprint = {2211.10024},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.10024},
  url = {http://arxiv.org/abs/2211.10024},
  urldate = {2024-08-04},
  abstract = {This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/GQ8U4624/Casper et al. - 2023 - Diagnostics for Deep Neural Networks with Automate.pdf}
}

@article{dapello_simulating_2020,
  title = {Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations},
  author = {Dapello, Joel and Marques, Tiago and Schrimpf, Martin and Geiger, Franziska and Cox, David and DiCarlo, James J},
  year = {2020},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {13073--13087},
  url = {https://proceedings.neurips.cc/paper/2020/hash/98b17f068d5d9b7668e19fb8ae470841-Abstract.html},
  urldate = {2024-08-04},
  abstract = {Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18\% and 3\%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.},
  file = {/Users/leonardbereska/Zotero/storage/YJFWPWJR/Dapello et al. - 2020 - Simulating a Primary Visual Cortex at the Front of.pdf}
}

@article{dong_interpretable_2019,
  title = {Towards Interpretable Deep Neural Networks by Leveraging Adversarial Examples},
  author = {Dong, Yinpeng and Bao, Fan and Su, Hang and Zhu, Jun},
  year = {2019},
  month = jan,
  journal = {AAAI Workshop on Network Interpretability for Deep Learning},
  eprint = {1901.09035},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1901.09035},
  url = {http://arxiv.org/abs/1901.09035},
  urldate = {2024-08-04},
  abstract = {Sometimes it is not enough for a DNN to produce an outcome. For example, in applications such as healthcare, users need to understand the rationale of the decisions. Therefore, it is imperative to develop algorithms to learn models with good interpretability (Doshi-Velez 2017). An important factor that leads to the lack of interpretability of DNNs is the ambiguity of neurons, where a neuron may fire for various unrelated concepts. This work aims to increase the interpretability of DNNs on the whole image space by reducing the ambiguity of neurons. In this paper, we make the following contributions: 1) We propose a metric to evaluate the consistency level of neurons in a network quantitatively. 2) We find that the learned features of neurons are ambiguous by leveraging adversarial examples. 3) We propose to improve the consistency of neurons on adversarial example subset by an adversarial training algorithm with a consistent loss.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/MN7VWMJS/Dong et al. - 2019 - Towards Interpretable Deep Neural Networks by Leve.pdf}
}

@article{du_fighting_2021,
  title = {Fighting Adversarial Images With Interpretable Gradients},
  author = {Du, Keke and Chang, Shan and Wen, Huixiang and Zhang, Hao},
  year = {2021},
  month = oct,
  journal = {ACM TURC},
  series = {ACM TURC '21},
  pages = {44--48},
  doi = {10.1145/3472634.3472644},
  url = {https://doi.org/10.1145/3472634.3472644},
  urldate = {2024-08-04},
  abstract = {Adversarial images are specifically designed to fool neural networks into making a wrong decision about what they are looking at, which severely degrade neural network accuracy. Recently, empirical and theoretical evidence suggests that robust neural network models tend to have better interpretable gradients. Therefore, we speculate that improving the interpretability of the gradients of the neural network models may also help to improve the robustness of the models. Two methods are used to add gradient-dependent constraint terms to the loss function of neural network models and both improve the robustness of the models. The first method adds the fussed lasso penalty term of the saliency maps to the loss function of the neural network models, which makes the saliency maps arrange in a natural way to improve the interpretability of the saliency maps, and uses the gradient enhancement for relu instead of relu to strengthen the constraint of regularization term on saliency maps. In the second method, the cosine similarity penalty term between the input gradients and the image contour is added to the loss function of the model to constrain the approximation between the input gradients and the image contour. This method has a certain biological significance, because the contour information of the image is used in the human visual system to recognize the image. Both methods improve the interpretability of model`s gradients and the first method exceeds most regularization methods except adversarial training on MNIST and the second method even exceeds the adversarial training under white-box attacks on CIFAR-10 and CIFAR-100.}
}

@article{eigen_topkconv_2021,
  title = {TopKConv: Increased Adversarial Robustness Through Deeper Interpretability},
  shorttitle = {TopKConv},
  author = {Eigen, Henry and Sadovnik, Amir},
  year = {2021},
  month = dec,
  journal = {ICMLA},
  pages = {15--22},
  doi = {10.1109/ICMLA52953.2021.00011},
  url = {https://ieeexplore.ieee.org/document/9680089},
  urldate = {2024-08-04},
  abstract = {Vulnerability to adversarial inputs remains an issue for deep neural networks. Attackers can slightly modify inputs in order to cause adverse behavior in otherwise highly accurate networks. In addition to making these networks less secure for real world applications, this also emphasizes a misalignment between the features the network uses to make decisions and the ones humans use. In this work we propose that more interpretable networks should yield more robust ones since they are able to rely on features that are more understandable to humans. More specifically, we take inspiration from interpretability based approaches to adversarial robustness, and propose a sparsity based defense to counter the impact of overparameterization on adversarial vulnerability. Building off of the work of the Dynamic-K algorithm, which introduces dynamic routing to fully connected layers in order to encourage sparse, interpretable predictions, we propose TopKConv, a novel method of reducing the number of activation channels used to construct each convolutional feature map. The incorporation of TopKConv alongside Dynamic-k results in a significant increase in adversarial accuracy at no cost to benign accuracy. Further, this is achieved with no fine tuning of or adversarial training.},
  file = {/Users/leonardbereska/Zotero/storage/F2X7FIG6/Eigen and Sadovnik - 2021 - TopKConv Increased Adversarial Robustness Through.html}
}

@article{etmann_connection_2019,
  title = {On the Connection Between Adversarial Robustness and Saliency Map Interpretability},
  author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Sch{\"o}nlieb, Carola-Bibiane},
  year = {2019},
  month = may,
  journal = {ICML},
  eprint = {1905.04172},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1905.04172},
  url = {http://arxiv.org/abs/1905.04172},
  urldate = {2024-08-04},
  abstract = {Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/CPVBG9JI/Etmann et al. - 2019 - On the Connection Between Adversarial Robustness a.pdf}
}

@article{guo_tabor_2019,
  title = {TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan Backdoors in AI Systems},
  shorttitle = {TABOR},
  author = {Guo, Wenbo and Wang, Lun and Xing, Xinyu and Du, Min and Song, Dawn},
  year = {2019},
  month = aug,
  journal = {CoRR},
  eprint = {1908.01763},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1908.01763},
  url = {http://arxiv.org/abs/1908.01763},
  urldate = {2024-08-04},
  abstract = {A trojan backdoor is a hidden pattern typically implanted in a deep neural network. It could be activated and thus forces that infected model behaving abnormally only when an input data sample with a particular trigger present is fed to that model. As such, given a deep neural network model and clean input samples, it is very challenging to inspect and determine the existence of a trojan backdoor. Recently, researchers design and develop several pioneering solutions to address this acute problem. They demonstrate the proposed techniques have a great potential in trojan detection. However, we show that none of these existing techniques completely address the problem. On the one hand, they mostly work under an unrealistic assumption (e.g. assuming availability of the contaminated training database). On the other hand, the proposed techniques cannot accurately detect the existence of trojan backdoors, nor restore high-fidelity trojan backdoor images, especially when the triggers pertaining to the trojan vary in size, shape and position. In this work, we propose TABOR, a new trojan detection technique. Conceptually, it formalizes a trojan detection task as a non-convex optimization problem, and the detection of a trojan backdoor as the task of resolving the optimization through an objective function. Different from the existing technique also modeling trojan detection as an optimization problem, TABOR designs a new objective function--under the guidance of explainable AI techniques as well as heuristics--that could guide optimization to identify a trojan backdoor in a more effective fashion. In addition, TABOR defines a new metric to measure the quality of a trojan backdoor identified. Using an anomaly detection method, we show the new metric could better facilitate TABOR to identify intentionally injected triggers in an infected model and filter out false alarms......},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/WEFC3DAA/Guo et al. - 2019 - TABOR A Highly Accurate Approach to Inspecting an.pdf}
}

@article{kim_bridging_2019,
  title = {Bridging Adversarial Robustness and Gradient Interpretability},
  author = {Kim, Beomsu and Seo, Junghoon and Jeon, Taegyun},
  year = {2019},
  month = apr,
  journal = {ICLR Workshop on Safe Machine Learning: Specification, Robustness, and Assurance},
  eprint = {1903.11626},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1903.11626},
  url = {http://arxiv.org/abs/1903.11626},
  urldate = {2024-08-04},
  abstract = {Adversarial training is a training scheme designed to counter adversarial attacks by augmenting the training dataset with adversarial examples. Surprisingly, several studies have observed that loss gradients from adversarially trained DNNs are visually more interpretable than those from standard DNNs. Although this phenomenon is interesting, there are only few works that have offered an explanation. In this paper, we attempted to bridge this gap between adversarial robustness and gradient interpretability. To this end, we identified that loss gradients from adversarially trained DNNs align better with human perception because adversarial training restricts gradients closer to the image manifold. We then demonstrated that adversarial training causes loss gradients to be quantitatively meaningful. Finally, we showed that under the adversarial training framework, there exists an empirical trade-off between test accuracy and loss gradient interpretability and proposed two potential approaches to resolving this trade-off.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/TRGIHWY5/Kim et al. - 2019 - Bridging Adversarial Robustness and Gradient Inter.pdf}
}

@article{kaur_are_2019,
  title = {Are Perceptually-Aligned Gradients a General Property of Robust Classifiers?},
  author = {Kaur, Simran and Cohen, Jeremy M. and Lipton, Zachary Chase},
  year = {2019},
  month = oct,
  journal = {CoRR},
  url = {https://www.semanticscholar.org/paper/Are-Perceptually-Aligned-Gradients-a-General-of-Kaur-Cohen/f951aad88e244182b37e4918c3d570560108c68c},
  urldate = {2024-08-04},
  abstract = {For a standard convolutional neural network, optimizing over the input pixels to maximize the score of some target class will generally produce a grainy-looking version of the original image. However, Santurkar et al. (2019) demonstrated that for adversarially-trained neural networks, this optimization produces images that uncannily resemble the target class. In this paper, we show that these "perceptually-aligned gradients" also occur under randomized smoothing, an alternative means of constructing adversarially-robust classifiers. Our finding supports the hypothesis that perceptually-aligned gradients may be a general property of robust classifiers. We hope that our results will inspire research aimed at explaining this link between perceptually-aligned gradients and adversarial robustness.},
  file = {/Users/leonardbereska/Zotero/storage/WMXJ5F23/Kaur et al. - 2019 - Are Perceptually-Aligned Gradients a General Prope.pdf}
}

@article{liu_adversarial_2020,
  title = {Adversarial Training for Large Neural Language Models},
  author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
  year = {2020},
  month = apr,
  journal = {CoRR},
  eprint = {2004.08994},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2004.08994},
  url = {http://arxiv.org/abs/2004.08994},
  urldate = {2024-08-04},
  abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/G369GHAL/Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf}
}

@article{mangla_saliency_2020,
  title = {On Saliency Maps and Adversarial Robustness},
  author = {Mangla, Puneet and Singh, Vedant and Balasubramanian, Vineeth N.},
  year = {2020},
  month = jul,
  journal = {ECML-PKDD},
  eprint = {2006.07828},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2006.07828},
  url = {http://arxiv.org/abs/2006.07828},
  urldate = {2024-08-04},
  abstract = {A Very recent trend has emerged to couple the notion of interpretability and adversarial robustness, unlike earlier efforts which solely focused on good interpretations or robustness against adversaries. Works have shown that adversarially trained models exhibit more interpretable saliency maps than their non-robust counterparts, and that this behavior can be quantified by considering the alignment between input image and saliency map. In this work, we provide a different perspective to this coupling, and provide a method, Saliency based Adversarial training (SAT), to use saliency maps to improve adversarial robustness of a model. In particular, we show that using annotations such as bounding boxes and segmentation masks, already provided with a dataset, as weak saliency maps, suffices to improve adversarial robustness with no additional effort to generate the perturbations themselves. Our empirical results on CIFAR-10, CIFAR-100, Tiny ImageNet and Flower-17 datasets consistently corroborate our claim, by showing improved adversarial robustness using our method. saliency maps. We also show how using finer and stronger saliency maps leads to more robust models, and how integrating SAT with existing adversarial training methods, further boosts performance of these existing methods.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/BPXYBIYY/Mangla et al. - 2020 - On Saliency Maps and Adversarial Robustness.pdf}
}

@article{ortiz-jimenez_optimism_2021,
  title = {Optimism in the Face of Adversity: Understanding and Improving Deep Learning through Adversarial Robustness},
  shorttitle = {Optimism in the Face of Adversity},
  author = {{Ortiz-Jimenez}, Guillermo and Modas, Apostolos and {Moosavi-Dezfooli}, Seyed-Mohsen and Frossard, Pascal},
  year = {2021},
  month = jan,
  journal = {IEEE},
  eprint = {2010.09624},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2010.09624},
  url = {http://arxiv.org/abs/2010.09624},
  urldate = {2024-08-04},
  abstract = {Driven by massive amounts of data and important advances in computational resources, new deep learning systems have achieved outstanding results in a large spectrum of applications. Nevertheless, our current theoretical understanding on the mathematical foundations of deep learning lags far behind its empirical success. Towards solving the vulnerability of neural networks, however, the field of adversarial robustness has recently become one of the main sources of explanations of our deep models. In this article, we provide an in-depth review of the field of adversarial robustness in deep learning, and give a self-contained introduction to its main notions. But, in contrast to the mainstream pessimistic perspective of adversarial robustness, we focus on the main positive aspects that it entails. We highlight the intuitive connection between adversarial examples and the geometry of deep neural networks, and eventually explore how the geometric study of adversarial examples can serve as a powerful tool to understand deep learning. Furthermore, we demonstrate the broad applicability of adversarial robustness, providing an overview of the main emerging applications of adversarial robustness beyond security. The goal of this article is to provide readers with a set of new perspectives to understand deep learning, and to supply them with intuitive tools and insights on how to use adversarial robustness to improve it.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/LBKPB9Z5/Ortiz-Jimenez et al. - 2021 - Optimism in the Face of Adversity Understanding a.pdf}
}

@article{santurkar_image_2019,
  title = {Image Synthesis with a Single (Robust) Classifier},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and Ilyas, Andrew and Engstrom, Logan and Madry, Aleksander},
  year = {2019},
  month = aug,
  journal = {NeurIPS},
  eprint = {1906.09453},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1906.09453},
  url = {http://arxiv.org/abs/1906.09453},
  urldate = {2024-08-04},
  abstract = {We show that the basic classification framework alone can be used to tackle some of the most challenging tasks in image synthesis. In contrast to other state-of-the-art approaches, the toolkit we develop is rather minimal: it uses a single, off-the-shelf classifier for all these tasks. The crux of our approach is that we train this classifier to be adversarially robust. It turns out that adversarial robustness is precisely what we need to directly manipulate salient features of the input. Overall, our findings demonstrate the utility of robustness in the broader machine learning context. Code and models for our experiments can be found at https://git.io/robust-apps.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/EXJGENUU/Santurkar et al. - 2019 - Image Synthesis with a Single (Robust) Classifier.pdf}
}

@article{sarkar_get_2021,
  title = {Get Fooled for the Right Reason: Improving Adversarial Robustness through a Teacher-guided Curriculum Learning Approach},
  shorttitle = {Get Fooled for the Right Reason},
  author = {Sarkar, Anindya and Sarkar, Anirban and Gali, Sowrya and Balasubramanian, Vineeth N.},
  year = {2021},
  month = oct,
  journal = {NeurIPS},
  eprint = {2111.00295},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2111.00295},
  url = {http://arxiv.org/abs/2111.00295},
  urldate = {2024-08-04},
  abstract = {Current SOTA adversarially robust models are mostly based on adversarial training (AT) and differ only by some regularizers either at inner maximization or outer minimization steps. Being repetitive in nature during the inner maximization step, they take a huge time to train. We propose a non-iterative method that enforces the following ideas during training. Attribution maps are more aligned to the actual object in the image for adversarially robust models compared to naturally trained models. Also, the allowed set of pixels to perturb an image (that changes model decision) should be restricted to the object pixels only, which reduces the attack strength by limiting the attack space. Our method achieves significant performance gains with a little extra effort (10-20\%) over existing AT models and outperforms all other methods in terms of adversarial as well as natural accuracy. We have performed extensive experimentation with CIFAR-10, CIFAR-100, and TinyImageNet datasets and reported results against many popular strong adversarial attacks to prove the effectiveness of our method.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/J3LN9ZSB/Sarkar et al. - 2021 - Get Fooled for the Right Reason Improving Adversa.pdf}
}

@article{tomsett_why_2018,
  title = {Why the Failure? How Adversarial Examples Can Provide Insights for Interpretable Machine Learning},
  shorttitle = {Why the Failure?},
  author = {Tomsett, Richard and Widdicombe, Amy and Xing, Tianwei and Chakraborty, Supriyo and Julier, Simon and Gurram, Prudhvi and Rao, Raghuveer and Srivastava, Mani},
  year = {2018},
  month = jul,
  journal = {Information Fusion},
  pages = {838--845},
  doi = {10.23919/ICIF.2018.8455710},
  url = {https://ieeexplore.ieee.org/document/8455710},
  urldate = {2024-08-04},
  abstract = {Recent advances in Machine Learning (ML) have profoundly changed many detection, classification, recognition and inference tasks. Given the complexity of the battlespace, ML has the potential to revolutionise how Coalition Situation Understanding is synthesised and revised. However, many issues must be overcome before its widespread adoption. In this paper we consider two - interpretability and adversarial attacks. Interpretability is needed because military decision-makers must be able to justify their decisions. Adversarial attacks arise because many ML algorithms are very sensitive to certain kinds of input perturbations. In this paper, we argue that these two issues are conceptually linked, and insights in one can provide insights in the other. We illustrate these ideas with relevant examples from the literature and our own experiments.},
  file = {/Users/leonardbereska/Zotero/storage/7BNQ9MUH/Tomsett et al. - 2018 - Why the Failure How Adversarial Examples Can Prov.pdf;/Users/leonardbereska/Zotero/storage/UVKHUS3Z/Tomsett et al. - 2018 - Why the Failure How Adversarial Examples Can Prov.html}
}

@article{tsiligkaridis_second_2020,
  title = {Second Order Optimization for Adversarial Robustness and Interpretability},
  author = {Tsiligkaridis, Theodoros and Roberts, Jay},
  year = {2020},
  month = sep,
  journal = {CoRR},
  eprint = {2009.04923},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2009.04923},
  url = {http://arxiv.org/abs/2009.04923},
  urldate = {2024-08-04},
  abstract = {Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique aimed at learning features robust to such attacks and is widely regarded as a very effective defense. However, the computational cost of such training can be prohibitive as the network size and input dimensions grow. Inspired by the relationship between robustness and curvature, we propose a novel regularizer which incorporates first and second order information via a quadratic approximation to the adversarial loss. The worst case quadratic loss is approximated via an iterative scheme. It is shown that using only a single iteration in our regularizer achieves stronger robustness than prior gradient and curvature regularization schemes, avoids gradient obfuscation, and, with additional iterations, achieves strong robustness with significantly lower training time than AT. Further, it retains the interesting facet of AT that networks learn features which are well-aligned with human perception. We demonstrate experimentally that our method produces higher quality human-interpretable features than other geometric regularization techniques. These robust features are then used to provide human-friendly explanations to model predictions.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/XIT5K6EP/Tsiligkaridis and Roberts - 2020 - Second Order Optimization for Adversarial Robustne.pdf}
}

@article{wang_survey_2022,
  title = {A Survey of Neural Trojan Attacks and Defenses in Deep Learning},
  author = {Wang, Jie and Hassan, Ghulam Mubashar and Akhtar, Naveed},
  year = {2022},
  month = feb,
  journal = {CoRR},
  eprint = {2202.07183},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2202.07183},
  url = {http://arxiv.org/abs/2202.07183},
  urldate = {2024-08-04},
  abstract = {Artificial Intelligence (AI) relies heavily on deep learning - a technology that is becoming increasingly popular in real-life applications of AI, even in the safety-critical and high-risk domains. However, it is recently discovered that deep learning can be manipulated by embedding Trojans inside it. Unfortunately, pragmatic solutions to circumvent the computational requirements of deep learning, e.g. outsourcing model training or data annotation to third parties, further add to model susceptibility to the Trojan attacks. Due to the key importance of the topic in deep learning, recent literature has seen many contributions in this direction. We conduct a comprehensive review of the techniques that devise Trojan attacks for deep learning and explore their defenses. Our informative survey systematically organizes the recent literature and discusses the key concepts of the methods while assuming minimal knowledge of the domain on the readers part. It provides a comprehensible gateway to the broader community to understand the recent developments in Neural Trojans.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/UHA6IB7X/Wang et al. - 2022 - A Survey of Neural Trojan Attacks and Defenses in .pdf}
}

@article{wang_understanding_2018,
  title = {Towards Understanding Learning Representations: To What Extent Do Different Neural Networks Learn the Same Representation},
  shorttitle = {Towards Understanding Learning Representations},
  author = {Wang, Liwei and Hu, Lunjia and Gu, Jiayuan and Wu, Yue and Hu, Zhiqiang and He, Kun and Hopcroft, John},
  year = {2018},
  month = nov,
  journal = {NeurIPS},
  eprint = {1810.11750},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1810.11750},
  url = {http://arxiv.org/abs/1810.11750},
  urldate = {2024-08-04},
  abstract = {It is widely believed that learning good representations is one of the main reasons for the success of deep neural networks. Although highly intuitive, there is a lack of theory and systematic approach quantitatively characterizing what representations do deep neural networks learn. In this work, we move a tiny step towards a theory and better understanding of the representations. Specifically, we study a simpler problem: How similar are the representations learned by two networks with identical architecture but trained from different initializations. We develop a rigorous theory based on the neuron activation subspace match model. The theory gives a complete characterization of the structure of neuron activation subspace matches, where the core concepts are maximum match and simple match which describe the overall and the finest similarity between sets of neurons in two networks respectively. We also propose efficient algorithms to find the maximum match and simple matches. Finally, we conduct extensive experiments using our algorithms. Experimental results suggest that, surprisingly, representations learned by the same convolutional layers of networks trained from different initializations are not as similar as prevalently expected, at least in terms of subspace match.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/EYK9Y6HD/Wang et al. - 2018 - Towards Understanding Learning Representations To.pdf}
}

@article{wang_neural_2019,
  title = {Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks},
  shorttitle = {Neural Cleanse},
  author = {Wang, Bolun and Yao, Yuanshun and Shan, Shawn and Li, Huiying and Viswanath, Bimal and Zheng, Haitao and Zhao, Ben Y.},
  year = {2019},
  month = may,
  journal = {IEEE Symposium on Security and Privacy},
  pages = {707--723},
  issn = {2375-1207},
  doi = {10.1109/SP.2019.00031},
  url = {https://ieeexplore.ieee.org/document/8835365},
  urldate = {2024-08-04},
  abstract = {Lack of transparency in deep neural networks (DNNs) make them susceptible to backdoor attacks, where hidden associations or triggers override normal classification to produce unexpected results. For example, a model with a backdoor always identifies a face as Bill Gates if a specific symbol is present in the input. Backdoors can stay hidden indefinitely until activated by an input, and present a serious security risk to many security or safety related applications, e.g. biometric authentication systems or self-driving cars. We present the first robust and generalizable detection and mitigation system for DNN backdoor attacks. Our techniques identify backdoors and reconstruct possible triggers. We identify multiple mitigation techniques via input filters, neuron pruning and unlearning. We demonstrate their efficacy via extensive experiments on a variety of DNNs, against two types of backdoor injection methods identified by prior work. Our techniques also prove robust against a number of variants of the backdoor attack.},
  file = {/Users/leonardbereska/Zotero/storage/ZPXELY3I/Wang et al. - 2019 - Neural Cleanse Identifying and Mitigating Backdoo.pdf}
}

@article{zheng_topological_2021,
  title = {Topological Detection of Trojaned Neural Networks},
  author = {Zheng, Songzhu and Zhang, Yikai and Wagner, Hubert and Goswami, Mayank and Chen, Chao},
  year = {2021},
  month = jun,
  journal = {NeurIPS},
  eprint = {2106.06469},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2106.06469},
  url = {http://arxiv.org/abs/2106.06469},
  urldate = {2024-08-04},
  abstract = {Deep neural networks are known to have security issues. One particular threat is the Trojan attack. It occurs when the attackers stealthily manipulate the model's behavior through Trojaned training samples, which can later be exploited. Guided by basic neuroscientific principles we discover subtle -- yet critical -- structural deviation characterizing Trojaned models. In our analysis we use topological tools. They allow us to model high-order dependencies in the networks, robustly compare different networks, and localize structural abnormalities. One interesting observation is that Trojaned models develop short-cuts from input to output layers. Inspired by these observations, we devise a strategy for robust detection of Trojaned models. Compared to standard baselines it displays better performance on multiple benchmarks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/CGMX6GUG/Zheng et al. - 2021 - Topological Detection of Trojaned Neural Networks.pdf}
}

@article{shayegani_survey_2023,
  title = {Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks},
  author = {Shayegani, Erfan and Mamun, Md Abdullah Al and Fu, Yu and Zaree, Pedram and Dong, Yue and {Abu-Ghazaleh}, Nael},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {2310.10844},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2310.10844},
  url = {http://arxiv.org/abs/2310.10844},
  urldate = {2024-08-05},
  abstract = {Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/QP5AM24D/Shayegani et al. - 2023 - Survey of Vulnerabilities in Large Language Models.pdf}
}

@article{huang_survey_2020,
  title = {A Survey of Safety and Trustworthiness of Deep Neural Networks: Verification, Testing, Adversarial Attack and Defence, and Interpretability},
  shorttitle = {A Survey of Safety and Trustworthiness of Deep Neural Networks},
  author = {Huang, Xiaowei and Kroening, Daniel and Ruan, Wenjie and Sharp, James and Sun, Youcheng and Thamo, Emese and Wu, Min and Yi, Xinping},
  year = {2020},
  month = may,
  journal = {Computer Science Review},
  eprint = {1812.08342},
  primaryclass = {cs},
  doi = {10.48550/arXiv.1812.08342},
  url = {http://arxiv.org/abs/1812.08342},
  urldate = {2024-08-05},
  abstract = {In the past few years, significant progress has been made on deep neural networks (DNNs) in achieving human-level performance on several long-standing tasks. With the broader deployment of DNNs on various applications, the concerns over their safety and trustworthiness have been raised in public, especially after the widely reported fatal incidents involving self-driving cars. Research to address these concerns is particularly active, with a significant number of papers released in the past few years. This survey paper conducts a review of the current research effort into making DNNs safe and trustworthy, by focusing on four aspects: verification, testing, adversarial attack and defence, and interpretability. In total, we survey 202 papers, most of which were published after 2017.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/XJAP789Z/Huang et al. - 2020 - A Survey of Safety and Trustworthiness of Deep Neu.pdf}
}

@article{guo_gradientbased_2021,
  title = {Gradient-based Adversarial Attacks against Text Transformers},
  author = {Guo, Chuan and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e} and Kiela, Douwe},
  year = {2021},
  month = apr,
  journal = {CoRR},
  eprint = {2104.13733},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2104.13733},
  url = {http://arxiv.org/abs/2104.13733},
  urldate = {2024-08-05},
  abstract = {We propose the first general-purpose gradient-based attack against transformer models. Instead of searching for a single adversarial example, we search for a distribution of adversarial examples parameterized by a continuous-valued matrix, hence enabling gradient-based optimization. We empirically demonstrate that our white-box attack attains state-of-the-art attack performance on a variety of natural language tasks. Furthermore, we show that a powerful black-box transfer attack, enabled by sampling from the adversarial distribution, matches or exceeds existing methods, while only requiring hard-label outputs.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/L2GUGL9W/Guo et al. - 2021 - Gradient-based Adversarial Attacks against Text Tr.pdf}
}

@article{wang_improving_2019,
  title = {Improving Neural Language Modeling via Adversarial Training},
  author = {Wang, Dilin and Gong, Chengyue and Liu, Qiang},
  year = {2019},
  month = sep,
  journal = {ICML},
  eprint = {1906.03805},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1906.03805},
  url = {http://arxiv.org/abs/1906.03805},
  urldate = {2024-08-05},
  abstract = {Recently, substantial progress has been made in language modeling by using deep neural networks. However, in practice, large scale neural language models have been shown to be prone to overfitting. In this paper, we present a simple yet highly effective adversarial training mechanism for regularizing neural language models. The idea is to introduce adversarial noise to the output embedding layer while training the models. We show that the optimal adversarial noise yields a simple closed-form solution, thus allowing us to develop a simple and time efficient algorithm. Theoretically, we show that our adversarial mechanism effectively encourages the diversity of the embedding vectors, helping to increase the robustness of models. Empirically, we show that our method improves on the single model state-of-the-art results for language modeling on Penn Treebank (PTB) and Wikitext-2, achieving test perplexity scores of 46.01 and 38.07, respectively. When applied to machine translation, our method improves over various transformer-based translation baselines in BLEU scores on the WMT14 English-German and IWSLT14 German-English tasks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/3J6E6NFP/Wang et al. - 2019 - Improving Neural Language Modeling via Adversarial.pdf}
}

@article{goodfellow_explaining_2015,
  title = {Explaining and Harnessing Adversarial Examples},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  journal = {CoRR},
  eprint = {1412.6572},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1412.6572},
  url = {http://arxiv.org/abs/1412.6572},
  urldate = {2024-08-05},
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/Z9S56L9Z/Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf}
}

@article{huang_revisiting_2023,
  title = {Revisiting Residual Networks for Adversarial Robustness: An Architectural Perspective},
  shorttitle = {Revisiting Residual Networks for Adversarial Robustness},
  author = {Huang, Shihua and Lu, Zhichao and Deb, Kalyanmoy and Boddeti, Vishnu Naresh},
  year = {2023},
  journal = {CVPR},
  eprint = {2212.11005},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2212.11005},
  url = {http://arxiv.org/abs/2212.11005},
  urldate = {2024-08-05},
  abstract = {Efforts to improve the adversarial robustness of convolutional neural networks have primarily focused on developing more effective adversarial training methods. In contrast, little attention was devoted to analyzing the role of architectural elements (such as topology, depth, and width) on adversarial robustness. This paper seeks to bridge this gap and present a holistic study on the impact of architectural design on adversarial robustness. We focus on residual networks and consider architecture design at the block level, i.e., topology, kernel size, activation, and normalization, as well as at the network scaling level, i.e., depth and width of each block in the network. In both cases, we first derive insights through systematic ablative experiments. Then we design a robust residual block, dubbed RobustResBlock, and a compound scaling rule, dubbed RobustScaling, to distribute depth and width at the desired FLOP count. Finally, we combine RobustResBlock and RobustScaling and present a portfolio of adversarially robust residual networks, RobustResNets, spanning a broad spectrum of model capacities. Experimental validation across multiple datasets and adversarial attacks demonstrate that RobustResNets consistently outperform both the standard WRNs and other existing robust architectures, achieving state-of-the-art AutoAttack robust accuracy of 61.1\% without additional data and 63.7\% with 500K external data while being \$2{\textbackslash}times\$ more compact in terms of parameters. Code is available at {\textbackslash}url\{ https://github.com/zhichao-lu/robust-residual-network\}},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/NMD5QCIP/Huang et al. - 2023 - Revisiting Residual Networks for Adversarial Robus.pdf}
}

@article{ying_visfis_2022,
  title = {VisFIS: Visual Feature Importance Supervision with Right-for-the-Right-Reason Objectives},
  shorttitle = {VisFIS},
  author = {Ying, Zhuofan and Hase, Peter and Bansal, Mohit},
  year = {2022},
  month = oct,
  journal = {NeurIPS},
  eprint = {2206.11212},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.11212},
  url = {http://arxiv.org/abs/2206.11212},
  urldate = {2024-08-05},
  abstract = {Many past works aim to improve visual reasoning in models by supervising feature importance (estimated by model explanation techniques) with human annotations such as highlights of important image regions. However, recent work has shown that performance gains from feature importance (FI) supervision for Visual Question Answering (VQA) tasks persist even with random supervision, suggesting that these methods do not meaningfully align model FI with human FI. In this paper, we show that model FI supervision can meaningfully improve VQA model accuracy as well as performance on several Right-for-the-Right-Reason (RRR) metrics by optimizing for four key model objectives: (1) accurate predictions given limited but sufficient information (Sufficiency); (2) max-entropy predictions given no important information (Uncertainty); (3) invariance of predictions to changes in unimportant features (Invariance); and (4) alignment between model FI explanations and human FI explanations (Plausibility). Our best performing method, Visual Feature Importance Supervision (VisFIS), outperforms strong baselines on benchmark VQA datasets in terms of both in-distribution and out-of-distribution accuracy. While past work suggests that the mechanism for improved accuracy is through improved explanation plausibility, we show that this relationship depends crucially on explanation faithfulness (whether explanations truly represent the model's internal reasoning). Predictions are more accurate when explanations are plausible and faithful, and not when they are plausible but not faithful. Lastly, we show that, surprisingly, RRR metrics are not predictive of out-of-distribution model accuracy when controlling for a model's in-distribution accuracy, which calls into question the value of these metrics for evaluating model reasoning. All supporting code is available at https://github.com/zfying/visfis},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/TRM4UR9N/Ying et al. - 2022 - VisFIS Visual Feature Importance Supervision with.pdf}
}

@article{nakkiran_discussion_2019,
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Adversarial Examples are Just Bugs, Too},
  shorttitle = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'},
  author = {Nakkiran, Preetum},
  year = {2019},
  month = aug,
  journal = {Distill},
  volume = {4},
  number = {8},
  pages = {e00019.5},
  issn = {2476-0757},
  doi = {10.23915/distill.00019.5},
  url = {https://distill.pub/2019/advex-bugs-discussion/response-5},
  urldate = {2024-08-05},
  abstract = {Refining the source of adversarial examples},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/YUVWLPEA/response-5.html}
}

@article{altinisik_a3t_2023,
  title = {A3T: Accuracy Aware Adversarial Training},
  shorttitle = {A3T},
  author = {Altinisik, Enes and Messaoud, Safa and Sencar, Husrev Taha and Chawla, Sanjay},
  year = {2023},
  journal = {Machine Learning},
  eprint = {2211.16316},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2211.16316},
  url = {http://arxiv.org/abs/2211.16316},
  urldate = {2024-08-05},
  abstract = {Adversarial training has been empirically shown to be more prone to overfitting than standard training. The exact underlying reasons still need to be fully understood. In this paper, we identify one cause of overfitting related to current practices of generating adversarial samples from misclassified samples. To address this, we propose an alternative approach that leverages the misclassified samples to mitigate the overfitting problem. We show that our approach achieves better generalization while having comparable robustness to state-of-the-art adversarial training methods on a wide range of computer vision, natural language processing, and tabular tasks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/MZKT5ZBC/Altinisik et al. - 2023 - A3T Accuracy Aware Adversarial Training.pdf}
}

@article{cheng_cat_2020,
  title = {CAT: Customized Adversarial Training for Improved Robustness},
  shorttitle = {CAT},
  author = {Cheng, Minhao and Lei, Qi and Chen, Pin-Yu and Dhillon, Inderjit and Hsieh, Cho-Jui},
  year = {2020},
  month = feb,
  journal = {CoRR},
  eprint = {2002.06789},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.2002.06789},
  url = {http://arxiv.org/abs/2002.06789},
  urldate = {2024-08-05},
  abstract = {Adversarial training has become one of the most effective methods for improving robustness of neural networks. However, it often suffers from poor generalization on both clean and perturbed data. In this paper, we propose a new algorithm, named Customized Adversarial Training (CAT), which adaptively customizes the perturbation level and the corresponding label for each training sample in adversarial training. We show that the proposed algorithm achieves better clean and robust accuracy than previous adversarial training methods through extensive experiments.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/ZSUX4VV5/Cheng et al. - 2020 - CAT Customized Adversarial Training for Improved .pdf}
}

@article{wang_improving_2020,
  title = {Improving Adversarial Robustness Requires Revisiting Misclassified Examples},
  author = {Wang, Yisen and Zou, Difan and Yi, Jinfeng and Bailey, James and Ma, Xingjun and Gu, Quanquan},
  year = {2020},
  month = apr,
  journal = {ICLR},
  url = {https://iclr.cc/virtual_2020/poster_rklOg6EFwS.html},
  urldate = {2024-08-05},
  abstract = {Deep neural networks (DNNs) are vulnerable to adversarial examples crafted by imperceptible perturbations. A range of defense techniques have been proposed to improve DNN robustness to adversarial examples, among which adversarial training has been demonstrated to be the most effective. Adversarial training is often formulated as a min-max optimization problem, with the inner maximization for generating adversarial examples. However, there exists a simple, yet easily overlooked fact that adversarial examples are only defined on correctly classified (natural) examples, but inevitably, some (natural) examples will be misclassified during training. In this paper, we investigate the distinctive influence of misclassified and correctly classified examples on the final robustness of adversarial training. Specifically, we find that misclassified examples indeed have a significant impact on the final robustness. More surprisingly, we find that different maximization techniques on misclassified examples may have a negligible influence on the final robustness, while different minimization techniques are crucial. Motivated by the above discovery, we propose a new defense algorithm called \{{\textbackslash}em Misclassification Aware adveRsarial Training\} (MART), which explicitly differentiates the misclassified and correctly classified examples during the training. We also propose a semi-supervised extension of MART, which can leverage the unlabeled data to further improve the robustness. Experimental results show that MART and its variant could significantly improve the state-of-the-art adversarial robustness.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/UIW3ZIQR/Wang et al. - 2020 - Improving Adversarial Robustness Requires Revisiti.pdf}
}

@article{goh_discussion_2019,
  title = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features': Robust Feature Leakage},
  shorttitle = {A Discussion of 'Adversarial Examples Are Not Bugs, They Are Features'},
  author = {Goh, Gabriel},
  year = {2019},
  month = aug,
  journal = {Distill},
  volume = {4},
  number = {8},
  pages = {e00019.2},
  issn = {2476-0757},
  doi = {10.23915/distill.00019.2},
  url = {https://distill.pub/2019/advex-bugs-discussion/response-2},
  urldate = {2024-08-05},
  abstract = {An example project using webpack and svelte-loader and ejs to inline SVGs},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/JKHR7N9X/response-2.html}
}

@article{jacot_neural_2018,
  title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
  shorttitle = {Neural Tangent Kernel},
  author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  year = {2018},
  journal = {NeurIPS},
  eprint = {1806.07572},
  primaryclass = {cs, math, stat},
  doi = {10.48550/arXiv.1806.07572},
  url = {http://arxiv.org/abs/1806.07572},
  urldate = {2024-08-05},
  abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/JI2J8N3W/Jacot et al. - 2018 - Neural Tangent Kernel Convergence and Generalizat.pdf}
}

@article{tsilivis_what_2022,
  title = {What Can the Neural Tangent Kernel Tell Us About Adversarial Robustness?},
  author = {Tsilivis, Nikolaos and Kempe, Julia},
  year = {2022},
  journal = {NeurIPS},
  eprint = {2210.05577},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2210.05577},
  url = {http://arxiv.org/abs/2210.05577},
  urldate = {2024-08-05},
  abstract = {The adversarial vulnerability of neural nets, and subsequent techniques to create robust models have attracted significant attention; yet we still lack a full understanding of this phenomenon. Here, we study adversarial examples of trained neural networks through analytical tools afforded by recent theory advances connecting neural networks and kernel methods, namely the Neural Tangent Kernel (NTK), following a growing body of work that leverages the NTK approximation to successfully analyze important deep learning phenomena and design algorithms for new applications. We show how NTKs allow to generate adversarial examples in a ``training-free'' fashion, and demonstrate that they transfer to fool their finite-width neural net counterparts in the ``lazy'' regime. We leverage this connection to provide an alternative view on robust and non-robust features, which have been suggested to underlie the adversarial brittleness of neural nets. Specifically, we define and study features induced by the eigendecomposition of the kernel to better understand the role of robust and non-robust features, the reliance on both for standard classification and the robustness-accuracy trade-off. We find that such features are surprisingly consistent across architectures, and that robust features tend to correspond to the largest eigenvalues of the model, and thus are learned early during training. Our framework allows us to identify and visualize non-robust yet useful features. Finally, we shed light on the robustness mechanism underlying adversarial training of neural nets used in practice: quantifying the evolution of the associated empirical NTK, we demonstrate that its dynamics falls much earlier into the ``lazy'' regime and manifests a much stronger form of the well known bias to prioritize learning features within the top eigenspaces of the kernel, compared to standard training.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/TN8L543M/Tsilivis and Kempe - 2022 - What Can the Neural Tangent Kernel Tell Us About A.pdf}
}

@article{gupta_interpbench_2024,
  title = {InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques},
  shorttitle = {InterpBench},
  author = {Gupta, Rohan and Arcuschin, Iv{\'a}n and Kwa, Thomas and {Garriga-Alonso}, Adri{\`a}},
  year = {2024},
  month = jul,
  journal = {CoRR},
  eprint = {2407.14494},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2407.14494},
  url = {http://arxiv.org/abs/2407.14494},
  urldate = {2024-08-06},
  abstract = {Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train these neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.},
  archiveprefix = {arXiv},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/GNLD9CXJ/Gupta et al. - 2024 - InterpBench Semi-Synthetic Transformers for Evalu.pdf}
}

@article{brown_selfevaluation_2024,
  title = {Self-Evaluation as a Defense Against Adversarial Attacks on LLMs},
  author = {Brown, Hannah and Lin, Leon and Kawaguchi, Kenji and Shieh, Michael},
  year = {2024},
  month = jul,
  journal = {CoRR},
  eprint = {2407.03234},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2407.03234},
  url = {http://arxiv.org/abs/2407.03234},
  urldate = {2024-08-06},
  abstract = {When LLMs are deployed in sensitive, human-facing settings, it is crucial that they do not output unsafe, biased, or privacy-violating outputs. For this reason, models are both trained and instructed to refuse to answer unsafe prompts such as "Tell me how to build a bomb." We find that, despite these safeguards, it is possible to break model defenses simply by appending a space to the end of a model's input. In a study of eight open-source models, we demonstrate that this acts as a strong enough attack to cause the majority of models to generate harmful outputs with very high success rates. We examine the causes of this behavior, finding that the contexts in which single spaces occur in tokenized training data encourage models to generate lists when prompted, overriding training signals to refuse to answer unsafe requests. Our findings underscore the fragile state of current model alignment and promote the importance of developing more robust alignment methods. Code and data will be made available at https://github.com/Linlt-leon/self-eval.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/3PM7823C/Brown et al. - 2024 - Self-Evaluation as a Defense Against Adversarial A.pdf}
}

@article{bloom_features_2023,
  title = {Features and Adversaries in MemoryDT},
  author = {Bloom, Joseph and Bailey, Jay},
  year = {2023},
  month = oct,
  journal = {LessWrong},
  url = {https://www.lesswrong.com/posts/yuQJsRswS4hKv3tsL/features-and-adversaries-in-memorydt},
  urldate = {2024-08-06},
  abstract = {We analyse the embedding space of a gridworld decision transformer, showing that it has developed an extensive structure that reflects the properties of the model, the gridworld environment and the task. We can identify linear feature representations for task-relevant concepts and show the distribution of these features in the embedding space.  We use these insights to predict several adversarial inputs  (observations with ``distractor'' items) that trick the model about what it is seeing. We show that these adversaries work as effectively as changing the feature (in the environment). However, we can also intervene directly on the underlying linear feature representation to achieve the same effects. Whilst methodologically simple, this analysis shows that mechanistic investigation of gridworld models is tractable and touches on many different areas of fundamental mechanistic interpretability research and its application to AI alignment.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/27487TCR/features-and-adversaries-in-memorydt.html}
}

@article{carbone_robustness_2023,
  title = {Robustness and Interpretability of Neural Networks' Predictions under Adversarial Attacks},
  author = {Carbone, Ginevra},
  year = {2023},
  month = mar,
  journal = {Universit{\`a} degli Studi di Trieste},
  publisher = {Universit{\`a} degli Studi di Trieste},
  url = {https://arts.units.it/handle/11368/3042163},
  urldate = {2024-08-06},
  abstract = {Deep Neural Networks (DNNs) are powerful predictive models, exceeding human capabilities in a variety of tasks. They learn complex and flexible decision systems from the available data and achieve exceptional performances in multiple machine learning fields, spanning from applications in artificial intelligence, such as image, speech and text recognition, to the more traditional sciences, including medicine, physics and biology. Despite the outstanding achievements, high performance and high predictive accuracy are not sufficient for real-world applications, especially in safety-critical settings, where the usage of DNNs is severely limited by their black-box nature. There is an increasing need to understand how predictions are performed, to provide uncertainty estimates, to guarantee robustness to malicious attacks and to prevent unwanted behaviours. State-of-the-art DNNs are vulnerable to small perturbations in the input data, known as adversarial attacks: maliciously crafted manipulations of the inputs that are perceptually indistinguishable from the original samples but are capable of fooling the model into incorrect predictions [66, 90, 115, 34, 37]. In this work, we prove that such brittleness is related to the geometry of the data manifold and is therefore likely to be an intrinsic feature of DNNs' predictions. This negative condition suggests a possible direction to overcome such limitation: we study the geometry of adversarial attacks in the large-data, overparameterized limit for Bayesian Neural Networks and prove that, in this limit, they are immune to gradient-based adversarial attacks. Furthermore, we propose some training techniques to improve the adversarial robustness of deterministic architectures. In particular, we experimentally observe that ensembles of NNs trained on random projections of the original inputs into lower dimensional spaces are more resilient to the attacks. Next, we focus on the problem of interpretability of NNs' predictions in the setting of saliency-based explanations [8]. We analyze the stability of the explanations under adversarial attacks on the inputs [63, 85, 4, 184] and we prove that, in the large-data and overparameterized limit, Bayesian interpretations are more stable than those provided by deterministic networks. We validate this behaviour in multiple experimental settings in the finite data regime. Finally, we introduce the concept of adversarial perturbations of amino acid sequences for protein Language Models (LMs). Deep Learning models for protein structure prediction, such as AlphaFold2 [81], leverage Transformer architectures and their attention mechanism to capture structural and functional properties of 1 2 amino acid sequences. Despite the high accuracy of predictions, biologically small perturbations of the input sequences, or even single point mutations, can lead to substantially different 3D structures [80]. On the other hand, protein language models are insensitive to mutations that induce misfolding or dysfunction (e.g. missense mutations [23]). Precisely, predictions of the 3D coordinates do not reveal the structure-disruptive effect of these mutations. Therefore, there is an evident inconsistency between the biological importance of mutations and the resulting change in structural prediction. Inspired by this problem, we introduce the concept of adversarial perturbation of protein sequences in continuous embedding spaces of protein language models. Our method relies on attention scores to detect the most vulnerable amino acid positions in the input sequences. Adversarial mutations are biologically diverse from their references and are able to significantly alter the resulting 3D structures.},
  language = {eng},
  annotation = {Accepted: 2023-03-24T09:54:23Z},
  file = {/Users/leonardbereska/Zotero/storage/7PHUL8RT/Carbone - 2023 - Robustness and Interpretability of Neural Networks.pdf}
}

@article{han_interpreting_2023,
  title = {Interpreting Adversarial Examples in Deep Learning: A Review},
  shorttitle = {Interpreting Adversarial Examples in Deep Learning},
  author = {Han, Sicong and Lin, Chenhao and Shen, Chao and Wang, Qian and Guan, Xiaohong},
  year = {2023},
  month = jul,
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {14s},
  pages = {328:1--328:38},
  issn = {0360-0300},
  doi = {10.1145/3594869},
  url = {https://dl.acm.org/doi/10.1145/3594869},
  urldate = {2024-08-06},
  abstract = {Deep learning technology is increasingly being applied in safety-critical scenarios but has recently been found to be susceptible to imperceptible adversarial perturbations. This raises a serious concern regarding the adversarial robustness of deep neural network (DNN)--based applications. Accordingly, various adversarial attacks and defense approaches have been proposed. However, current studies implement different types of attacks and defenses with certain assumptions. There is still a lack of full theoretical understanding and interpretation of adversarial examples. Instead of reviewing technical progress in adversarial attacks and defenses, this article presents a framework consisting of three perspectives to discuss recent works focusing on theoretically explaining adversarial examples comprehensively. In each perspective, various hypotheses are further categorized and summarized into several subcategories and introduced systematically. To the best of our knowledge, this study is the first to concentrate on surveying existing research on adversarial examples and adversarial robustness from the interpretability perspective. By drawing on the reviewed literature, this survey characterizes current problems and challenges that need to be addressed and highlights potential future research directions to further investigate adversarial examples.},
  file = {/Users/leonardbereska/Zotero/storage/CS5P6FRM/Han et al. - 2023 - Interpreting Adversarial Examples in Deep Learning.pdf}
}

@article{zhou_interpretability_2024,
  title = {Interpretability of Neural Networks Based on Game-theoretic Interactions},
  author = {Zhou, Huilin and Ren, Jie and Deng, Huiqi and Cheng, Xu and Zhang, Jinpeng and Zhang, Quanshi},
  year = {2024},
  month = aug,
  journal = {Mach. Intell. Res.},
  volume = {21},
  number = {4},
  pages = {718--739},
  issn = {2731-5398},
  doi = {10.1007/s11633-023-1419-7},
  url = {https://doi.org/10.1007/s11633-023-1419-7},
  urldate = {2024-08-06},
  abstract = {This paper introduces the system of game-theoretic interactions, which connects both the explanation of knowledge encoded in a deep neural networks (DNN) and the explanation of the representation power of a DNN. In this system, we define two game-theoretic interaction indexes, namely the multi-order interaction and the multivariate interaction. More crucially, we use these interaction indexes to explain feature representations encoded in a DNN from the following four aspects: 1) Quantifying knowledge concepts encoded by a DNN; 2) Exploring how a DNN encodes visual concepts, and extracting prototypical concepts encoded in the DNN; 3) Learning optimal baseline values for the Shapley value, and providing a unified perspective to compare fourteen different attribution methods; 4) Theoretically explaining the representation bottleneck of DNNs. Furthermore, we prove the relationship between the interaction encoded in a DNN and the representation power of a DNN (e.g., generalization power, adversarial transferability, and adversarial robustness). In this way, game-theoretic interactions successfully bridge the gap between ``the explanation of knowledge concepts encoded in a DNN'' and ``the explanation of the representation capacity of a DNN'' as a unified explanation.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/FEBA2PZ4/Zhou et al. - 2024 - Interpretability of Neural Networks Based on Game-.pdf}
}

@article{kitada_making_2023,
  title = {Making attention mechanisms more robust and interpretable with virtual adversarial training},
  author = {Kitada, Shunsuke and Iyatomi, Hitoshi},
  year = {2023},
  month = jun,
  journal = {Appl Intell},
  volume = {53},
  number = {12},
  pages = {15802--15817},
  issn = {1573-7497},
  doi = {10.1007/s10489-022-04301-w},
  url = {https://doi.org/10.1007/s10489-022-04301-w},
  urldate = {2024-08-06},
  abstract = {Although attention mechanisms have become fundamental components of deep learning models, they are vulnerable to perturbations, which may degrade the prediction performance and model interpretability. Adversarial training (AT) for attention mechanisms has successfully reduced such drawbacks by considering adversarial perturbations. However, this technique requires label information, and thus, its use is limited to supervised settings. In this study, we explore the concept of incorporating virtual AT (VAT) into the attention mechanisms, by which adversarial perturbations can be computed even from unlabeled data. To realize this approach, we propose two general training techniques, namely VAT for attention mechanisms (Attention VAT) and ``interpretable'' VAT for attention mechanisms (Attention iVAT), which extend AT for attention mechanisms to a semi-supervised setting. In particular, Attention iVAT focuses on the differences in attention; thus, it can efficiently learn clearer attention and improve model interpretability, even with unlabeled data. Empirical experiments based on six public datasets revealed that our techniques provide better prediction performance than conventional AT-based as well as VAT-based techniques, and stronger agreement with evidence that is provided by humans in detecting important words in sentences. Moreover, our proposal offers these advantages without needing to add the careful selection of unlabeled data. That is, even if the model using our VAT-based technique is trained on unlabeled data from a source other than the target task, both the prediction performance and model interpretability can be improved.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/D663X55A/Kitada and Iyatomi - 2023 - Making attention mechanisms more robust and interp.pdf}
}

@article{ness_measuring_2023,
  title = {Measuring the Effect of Causal Disentanglement on the Adversarial Robustness of Neural Network Models},
  author = {Ness, Preben M. and Marijan, Dusica and Bose, Sunanda},
  year = {2023},
  month = oct,
  journal = {ACM CIKM},
  series = {CIKM '23},
  pages = {1907--1916},
  doi = {10.1145/3583780.3614960},
  url = {https://dl.acm.org/doi/10.1145/3583780.3614960},
  urldate = {2024-08-06},
  abstract = {Causal Neural Network models have shown high levels of robustness to adversarial attacks as well as an increased capacity for generalisation tasks such as few-shot learning and rare-context classification compared to traditional Neural Networks. This robustness is argued to stem from the disentanglement of causal and confounder input signals. However, no quantitative study has yet measured the level of disentanglement achieved by these types of causal models or assessed how this relates to their adversarial robustness.Existing causal disentanglement metrics are not applicable to deterministic models trained on real-world datasets. We, therefore, utilise metrics of content/style disentanglement from the field of Computer Vision to measure different aspects of the causal disentanglement for four state-of-the-art causal Neural Network models. By re-implementing these models with a common ResNet18 architecture we are able to fairly measure their adversarial robustness on three standard image classification benchmarking datasets under seven common white-box attacks. We find a strong association (r=0.820, p=0.001) between the degree to which models decorrelate causal and confounder signals and their adversarial robustness. Additionally, we find a moderate negative association between the pixel-level information content of the confounder signal and adversarial robustness (r=-0.597, p=0.040).},
  file = {/Users/leonardbereska/Zotero/storage/QGVT9PQF/Ness et al. - 2023 - Measuring the Effect of Causal Disentanglement on .pdf}
}

@article{sadria_adversarial_2023,
  title = {Adversarial training improves model interpretability in single-cell RNA-seq analysis},
  author = {Sadria, Mehrshad and Layton, Anita and Bader, Gary D},
  year = {2023},
  month = jan,
  journal = {Bioinformatics Advances},
  volume = {3},
  number = {1},
  pages = {vbad166},
  issn = {2635-0041},
  doi = {10.1093/bioadv/vbad166},
  url = {https://doi.org/10.1093/bioadv/vbad166},
  urldate = {2024-08-06},
  abstract = {Predictive computational models must be accurate, robust, and interpretable to be considered reliable in important areas such as biology and medicine. A sufficiently robust model should not have its output affected significantly by a slight change in the input. Also, these models should be able to explain how a decision is made to support user trust in the results. Efforts have been made to improve the robustness and interpretability of predictive computational models independently; however, the interaction of robustness and interpretability is poorly understood.As an example task, we explore the computational prediction of cell type based on single-cell RNA-seq data and show that it can be made more robust by adversarially training a deep learning model. Surprisingly, we find this also leads to improved model interpretability, as measured by identifying genes important for classification using a range of standard interpretability methods. Our results suggest that adversarial training may be generally useful to improve deep learning robustness and interpretability and that it should be evaluated on a range of tasks.Our Python implementation of all analysis in this publication can be found at: https://github.com/MehrshadSD/robustness-interpretability. The analysis was conducted using numPy 0.2.5, pandas 2.0.3, scanpy 1.9.3, tensorflow 2.10.0, matplotlib 3.7.1, seaborn 0.12.2, sklearn 1.1.1, shap 0.42.0, lime 0.2.0.1, matplotlib\_venn 0.11.9.},
  file = {/Users/leonardbereska/Zotero/storage/QI897IQF/Sadria et al. - 2023 - Adversarial training improves model interpretabili.pdf;/Users/leonardbereska/Zotero/storage/X4BZI8HX/7444320.html}
}

@article{cascella_evaluating_2023,
  title = {Evaluating the Feasibility of ChatGPT in Healthcare: An Analysis of Multiple Clinical and Research Scenarios},
  shorttitle = {Evaluating the Feasibility of ChatGPT in Healthcare},
  author = {Cascella, Marco and Montomoli, Jonathan and Bellini, Valentina and Bignami, Elena},
  year = {2023},
  month = mar,
  journal = {J Med Syst},
  volume = {47},
  number = {1},
  pages = {33},
  issn = {1573-689X},
  doi = {10.1007/s10916-023-01925-4},
  abstract = {This paper aims to highlight the potential applications and limits of a large language model (LLM) in healthcare. ChatGPT is a recently developed LLM that was trained on a massive dataset of text for dialogue with users. Although AI-based language models like ChatGPT have demonstrated impressive capabilities, it is uncertain how well they will perform in real-world scenarios, particularly in fields such as medicine where high-level and complex thinking is necessary. Furthermore, while the use of ChatGPT in writing scientific articles and other scientific outputs may have potential benefits, important ethical concerns must also be addressed. Consequently, we investigated the feasibility of ChatGPT in clinical and research scenarios: (1) support of the clinical practice, (2) scientific production, (3) misuse in medicine and research, and (4) reasoning about public health topics. Results indicated that it is important to recognize and promote education on the appropriate use and potential pitfalls of AI-based LLMs in medicine.},
  language = {eng},
  pmcid = {PMC9985086},
  pmid = {36869927},
  file = {/Users/leonardbereska/Zotero/storage/DTQEZCK4/Cascella et al. - 2023 - Evaluating the Feasibility of ChatGPT in Healthcar.pdf}
}

@article{mumuni_improving_2024,
  title = {Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning},
  shorttitle = {Improving deep learning with prior knowledge and cognitive models},
  author = {Mumuni, Fuseini and Mumuni, Alhassan},
  year = {2024},
  month = mar,
  journal = {Cognitive Systems Research},
  volume = {84},
  pages = {101188},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2023.101188},
  url = {https://www.sciencedirect.com/science/article/pii/S1389041723001225},
  urldate = {2024-08-07},
  abstract = {We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-shot learning. Data-driven machine learning models have achieved remarkable performance and demonstrated capabilities surpassing humans in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural networks and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms like mathematical relations, logic rules, knowledge graphs, and large language models (LLMs). and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human brain to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience---that is, to deepen human understanding on how the brain works in general, and how it handles these problems.},
  file = {/Users/leonardbereska/Zotero/storage/M6WPFIRF/S1389041723001225.html}
}

@article{freiesleben_dear_2023,
  title = {Dear XAI Community, We Need to~Talk!},
  author = {Freiesleben, Timo and K{\"o}nig, Gunnar},
  editor = {Longo, Luca},
  year = {2023},
  journal = {Explainable Artificial Intelligence},
  pages = {48--65},
  doi = {10.1007/978-3-031-44064-9_3},
  abstract = {Despite progress in the field, significant parts of current XAI research are still not on solid conceptual, ethical, or methodological grounds. Unfortunately, these unfounded parts are not on the decline but continue to grow. Many explanation techniques are still proposed without clarifying their purpose. Instead, they are advertised with ever more fancy-looking heatmaps or only seemingly relevant benchmarks. Moreover, explanation techniques are motivated with questionable goals, such as building trust, or rely on strong assumptions about the 'concepts' that deep learning algorithms learn. In this paper, we highlight and discuss these and other misconceptions in current XAI research. We also suggest steps to make XAI a more substantive area of research.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/ET7K4U4S/Freiesleben and König - 2023 - Dear XAI Community, We Need to Talk!.pdf}
}

@article{bahoo_artificial_2024,
  title = {Artificial intelligence in Finance: a comprehensive review through bibliometric and content analysis},
  shorttitle = {Artificial intelligence in Finance},
  author = {Bahoo, Salman and Cucculelli, Marco and Goga, Xhoana and Mondolo, Jasmine},
  year = {2024},
  month = jan,
  journal = {SN Bus Econ},
  volume = {4},
  number = {2},
  pages = {23},
  issn = {2662-9399},
  doi = {10.1007/s43546-023-00618-x},
  url = {https://doi.org/10.1007/s43546-023-00618-x},
  urldate = {2024-08-09},
  abstract = {Over the past two decades, artificial intelligence (AI) has experienced rapid development and is being used in a wide range of sectors and activities, including finance. In the meantime, a growing and heterogeneous strand of literature has explored the use of AI in finance. The aim of this study is to provide a comprehensive overview of the existing research on this topic and to identify which research directions need further investigation. Accordingly, using the tools of bibliometric analysis and content analysis, we examined a large number of articles published between 1992 and March 2021. We find that the literature on this topic has expanded considerably since the beginning of the XXI century, covering a variety of countries and different AI applications in finance, amongst which Predictive/forecasting systems, Classification/detection/early warning systems and Big data Analytics/Data mining /Text mining stand out. Furthermore, we show that the selected articles fall into ten main research streams, in which AI is applied to the stock market, trading models, volatility forecasting, portfolio management, performance, risk and default evaluation, cryptocurrencies, derivatives, credit risk in banks, investor sentiment analysis and foreign exchange management, respectively. Future research should seek to address the partially unanswered research questions and improve our understanding of the impact of recent disruptive technological developments on finance.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/FLQ4V4ZE/Bahoo et al. - 2024 - Artificial intelligence in Finance a comprehensiv.pdf}
}

@article{horowitz_artificial_2018,
  title = {Artificial Intelligence, International Competition, and the Balance of Power (May 2018)},
  author = {Horowitz, Michael C.},
  year = {2018},
  journal = {Texas National Security Review},
  doi = {10.15781/T2639KP49},
  url = {https://repositories.lib.utexas.edu/handle/2152/65638},
  urldate = {2024-08-09},
  abstract = {World leaders, CEOs, and academics have suggested that a revolution in artificial intelligence is upon us. Are they right, and what will advances in artificial intelligence mean for international competition and the balance of power? This article evaluates how developments in artificial intelligence (AI) --- advanced, narrow applications in particular --- are poised to influence military power and international politics. It describes how AI more closely resembles ``enabling'' technologies such as the combustion engine or electricity than a specific weapon. AI's still-emerging developments make it harder to assess than many technological changes, especially since many of the organizational decisions about the adoption and uses of new technology that generally shape the impact of that technology are in their infancy. The article then explores the possibility that key drivers of AI development in the private sector could cause the rapid diffusion of military applications of AI, limiting first-mover advantages for innovators. Alternatively, given uncertainty about the technological trajectory of AI, it is also possible that military uses of AI will be harder to develop based on private-sector AI technologies than many expect, generating more potential first-mover advantages for existing powers such as China and the United States, as well as larger consequences for relative power if a country fails to adapt. Finally, the article discusses the extent to which U.S. military rhetoric about the importance of AI matches the reality of U.S. investments.}
}

@article{yu_artificial_2018,
  title = {Artificial intelligence in healthcare},
  author = {Yu, Kun-Hsing and Beam, Andrew L. and Kohane, Isaac S.},
  year = {2018},
  month = oct,
  journal = {Nat Biomed Eng},
  volume = {2},
  number = {10},
  pages = {719--731},
  issn = {2157-846X},
  doi = {10.1038/s41551-018-0305-z},
  url = {https://www.nature.com/articles/s41551-018-0305-z},
  urldate = {2024-08-09},
  abstract = {Artificial intelligence (AI) is gradually changing medical practice. With recent progress in digitized data acquisition, machine learning and computing infrastructure, AI applications are expanding into areas that were previously thought to be only the province of human experts. In this Review Article, we outline recent breakthroughs in AI technologies and their biomedical applications, identify the challenges for further progress in medical AI systems, and summarize the economic, legal and social implications of AI in healthcare. This Review summarizes the medical applications of artificial intelligence, and its economic, legal and social implications for healthcare.},
  language = {en}
}

@article{fagnant_preparing_2015,
  title = {Preparing a nation for autonomous vehicles: opportunities, barriers and policy recommendations},
  shorttitle = {Preparing a nation for autonomous vehicles},
  author = {Fagnant, Daniel J. and Kockelman, Kara},
  year = {2015},
  month = jul,
  journal = {Transportation Research Part A: Policy and Practice},
  volume = {77},
  pages = {167--181},
  issn = {09658564},
  doi = {10.1016/j.tra.2015.04.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0965856415000804},
  urldate = {2024-08-09},
  abstract = {Semantic Scholar extracted view of "Preparing a Nation for Autonomous Vehicles: Opportunities, Barriers and Policy Recommendations" by Daniel J. Fagnant et al.},
  language = {en}
}

@article{chen_artificial_2019,
  title = {Artificial Neural Networks-Based Machine Learning for Wireless Networks: A Tutorial},
  shorttitle = {Artificial Neural Networks-Based Machine Learning for Wireless Networks},
  author = {Chen, Mingzhe and Challita, Ursula and Saad, Walid and Yin, Changchuan and Debbah, M{\'e}rouane},
  year = 2019,
  journal = {IEEE Commun. Surv. Tutorials},
  volume = {21},
  number = {4},
  pages = {3039--3071},
  issn = {1553-877X, 2373-745X},
  doi = {10.1109/COMST.2019.2926625},
  url = {https://ieeexplore.ieee.org/document/8755300/},
  urldate = {2024-08-09},
  abstract = {In order to effectively provide ultra reliable low latency communications and pervasive connectivity for Internet of Things (IoT) devices, next-generation wireless networks can leverage intelligent, data-driven functions enabled by the integration of machine learning (ML) notions across the wireless core and edge infrastructure. In this context, this paper provides a comprehensive tutorial that overviews how artificial neural networks (ANNs)-based ML algorithms can be employed for solving various wireless networking problems. For this purpose, we first present a detailed overview of a number of key types of ANNs that include recurrent, spiking, and deep neural networks, that are pertinent to wireless networking applications. For each type of ANN, we present the basic architecture as well as specific examples that are particularly important and relevant wireless network design. Such ANN examples include echo state networks, liquid state machine, and long short term memory. Then, we provide an in-depth overview on the variety of wireless communication problems that can be addressed using ANNs, ranging from communication using unmanned aerial vehicles to virtual reality applications over wireless networks as well as edge computing and caching. For each individual application, we present the main motivation for using ANNs along with the associated challenges while we also provide a detailed example for a use case scenario and outline future works that can be addressed using ANNs. In a nutshell, this paper constitutes the first holistic tutorial on the development of ANN-based ML techniques tailored to the needs of future wireless networks.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  file = {/Users/leonardbereska/Zotero/storage/AQREFBA6/Chen et al. - 2019 - Artificial Neural Networks-Based Machine Learning .pdf}
}

@article{raza_review_2015,
  title = {A review on artificial intelligence based load demand forecasting techniques for smart grid and buildings},
  author = {Raza, Muhammad Qamar and Khosravi, Abbas},
  year = {2015},
  month = oct,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {50},
  pages = {1352--1372},
  issn = {13640321},
  doi = {10.1016/j.rser.2015.04.065},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364032115003354},
  urldate = {2024-08-09},
  abstract = {Semantic Scholar extracted view of "A review on artificial intelligence based load demand forecasting techniques for smart grid and buildings" by M. Raza et al.},
  language = {en}
}

@article{taddeo_trusting_2019,
  title = {Trusting artificial intelligence in cybersecurity is a double-edged sword},
  author = {Taddeo, Mariarosaria and McCutcheon, Tom and Floridi, Luciano},
  year = {2019},
  month = dec,
  journal = {Nat Mach Intell},
  volume = {1},
  number = {12},
  pages = {557--560},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0109-1},
  url = {https://www.nature.com/articles/s42256-019-0109-1},
  urldate = {2024-08-09},
  abstract = {Applications of artificial intelligence (AI) for cybersecurity tasks are attracting greater attention from the private and the public sectors. Estimates indicate that the market for AI in cybersecurity will grow from US\$1 billion in 2016 to a US\$34.8 billion net worth by 2025. The latest national cybersecurity and defence strategies of several governments explicitly mention AI capabilities. At the same time, initiatives to define new standards and certification procedures to elicit users' trust in AI are emerging on a global scale. However, trust in AI (both machine learning and neural networks) to deliver cybersecurity tasks is a double-edged sword: it can improve substantially cybersecurity practices, but can also facilitate new forms of attacks to the AI applications themselves, which may pose severe security threats. We argue that trust in AI for cybersecurity is unwarranted and that, to reduce security risks, some form of control to ensure the deployment of `reliable AI' for cybersecurity is necessary. To this end, we offer three recommendations focusing on the design, development and deployment of AI for cybersecurity.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/QKN67P6N/Taddeo et al. - 2019 - Trusting artificial intelligence in cybersecurity .pdf}
}

@article{gilmer_motivating_2018,
  title = {Motivating the Rules of the Game for Adversarial Example Research},
  author = {Gilmer, Justin and Adams, Ryan P. and Goodfellow, Ian and Andersen, David and Dahl, George E.},
  year = {2018},
  month = jul,
  journal = {CoRR},
  eprint = {1807.06732},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1807.06732},
  url = {http://arxiv.org/abs/1807.06732},
  urldate = {2024-08-09},
  abstract = {Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/GNEVWJXA/Gilmer et al. - 2018 - Motivating the Rules of the Game for Adversarial E.pdf;/Users/leonardbereska/Zotero/storage/9DHWSREW/1807.html}
}

@article{poursaeed_robustness_2021,
  title = {Robustness and Generalization via Generative Adversarial Training},
  author = {Poursaeed, Omid and Jiang, Tianxing and Yang, Harry and Belongie, Serge and Lim, SerNam},
  year = {2021},
  month = sep,
  journal = {CoRR},
  eprint = {2109.02765},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2109.02765},
  url = {http://arxiv.org/abs/2109.02765},
  urldate = {2024-08-09},
  abstract = {While deep neural networks have achieved remarkable success in various computer vision tasks, they often fail to generalize to new domains and subtle variations of input images. Several defenses have been proposed to improve the robustness against these variations. However, current defenses can only withstand the specific attack used in training, and the models often remain vulnerable to other input variations. Moreover, these methods often degrade performance of the model on clean images and do not generalize to out-of-domain samples. In this paper we present Generative Adversarial Training, an approach to simultaneously improve the model's generalization to the test set and out-of-domain samples as well as its robustness to unseen adversarial attacks. Instead of altering a low-level pre-defined aspect of images, we generate a spectrum of low-level, mid-level and high-level changes using generative models with a disentangled latent space. Adversarial training with these examples enable the model to withstand a wide range of attacks by observing a variety of input alterations during training. We show that our approach not only improves performance of the model on clean images and out-of-domain samples but also makes it robust against unforeseen attacks and outperforms prior work. We validate effectiveness of our method by demonstrating results on various tasks such as classification, segmentation and object detection.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/S2KAQHN5/Poursaeed et al. - 2021 - Robustness and Generalization via Generative Adver.pdf;/Users/leonardbereska/Zotero/storage/ZV74JHBL/2109.html}
}

@article{kaufmann_testing_2023,
  title = {Testing Robustness Against Unforeseen Adversaries},
  author = {Kaufmann, Max and Kang, Daniel and Sun, Yi and Basart, Steven and Yin, Xuwang and Mazeika, Mantas and Arora, Akul and Dziedzic, Adam and Boenisch, Franziska and Brown, Tom and Steinhardt, Jacob and Hendrycks, Dan},
  year = {2023},
  month = oct,
  journal = {CoRR},
  eprint = {1908.08016},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1908.08016},
  url = {http://arxiv.org/abs/1908.08016},
  urldate = {2024-08-09},
  abstract = {Adversarial robustness research primarily focuses on L\_p perturbations, and most defenses are developed with identical training-time and test-time adversaries. However, in real-world applications developers are unlikely to have access to the full range of attacks or corruptions their system will face. Furthermore, worst-case inputs are likely to be diverse and need not be constrained to the L\_p ball. To narrow in on this discrepancy between research and reality we introduce ImageNet-UA, a framework for evaluating model robustness against a range of unforeseen adversaries, including eighteen new non-L\_p attacks. To perform well on ImageNet-UA, defenses must overcome a generalization gap and be robust to a diverse attacks not encountered during training. In extensive experiments, we find that existing robustness measures do not capture unforeseen robustness, that standard robustness techniques are beat by alternative training strategies, and that novel methods can improve unforeseen robustness. We present ImageNet-UA as a useful tool for the community for improving the worst-case behavior of machine learning systems.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/D7XS2PEL/Kaufmann et al. - 2023 - Testing Robustness Against Unforeseen Adversaries.pdf;/Users/leonardbereska/Zotero/storage/SSSBTG2N/1908.html}
}

@article{carlini_extracting_2021,
  title = {Extracting Training Data from Large Language Models},
  author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and {Herbert-Voss}, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and Oprea, Alina and Raffel, Colin},
  year = {2021},
  month = jun,
  journal = {CoRR},
  eprint = {2012.07805},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2012.07805},
  url = {http://arxiv.org/abs/2012.07805},
  urldate = {2024-08-09},
  abstract = {It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. Worryingly, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/7FAZ5SBW/Carlini et al. - 2021 - Extracting Training Data from Large Language Model.pdf;/Users/leonardbereska/Zotero/storage/D7TUX7ET/2012.html}
}

@article{shokri_membership_2017,
  title = {Membership Inference Attacks against Machine Learning Models},
  author = {Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  year = {2017},
  month = mar,
  journal = {CoRR},
  eprint = {1610.05820},
  primaryclass = {cs, stat},
  doi = {10.48550/arXiv.1610.05820},
  url = {http://arxiv.org/abs/1610.05820},
  urldate = {2024-08-09},
  abstract = {We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial "machine learning as a service" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/IA5VYWKP/Shokri et al. - 2017 - Membership Inference Attacks against Machine Learn.pdf;/Users/leonardbereska/Zotero/storage/ECCDZRVD/1610.html}
}

@article{jain_what_2024,
  title = {What Makes and Breaks Safety Fine-tuning? A Mechanistic Study},
  shorttitle = {What Makes and Breaks Safety Fine-tuning?},
  author = {Jain, Samyak and Lubana, Ekdeep Singh and Oksuz, Kemal and Joy, Tom and Torr, Philip and Sanyal, Amartya and Dokania, Puneet K.},
  year = {2024},
  month = jun,
  journal = {ICML MI Workshop},
  url = {https://openreview.net/forum?id=BS2CbUkJpy},
  urldate = {2024-08-16},
  abstract = {Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., "design") versus the specific concepts the task is asked to be performed upon (e.g., a "cycle" vs. a "bomb"). Using this, we investigate three well-known safety fine-tuning methods---supervised safety fine-tuning, direct preference optimization, and unlearning---and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights' null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. We validate our findings, wherever possible, on real-world models---specifically, Llama-2 7B and Llama-3 8B.},
  language = {en},
  keywords = {cited},
  file = {/Users/leonardbereska/Zotero/storage/ZFM69P7K/Jain et al. - 2024 - What Makes and Breaks Safety Fine-tuning A Mechan.pdf}
}

@misc{zou_improving_2024,
  title = {Improving Alignment and Robustness with Circuit Breakers},
  author = {Zou, Andy and Phan, Long and Wang, Justin and Duenas, Derek and Lin, Maxwell and Andriushchenko, Maksym and Wang, Rowan and Kolter, Zico and Fredrikson, Matt and Hendrycks, Dan},
  year = {2024},
  month = jul,
  number = {arXiv:2406.04313},
  eprint = {2406.04313},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04313},
  url = {http://arxiv.org/abs/2406.04313},
  urldate = {2024-08-16},
  abstract = {AI systems can take harmful actions and are highly vulnerable to adversarial attacks. We present an approach, inspired by recent advances in representation engineering, that interrupts the models as they respond with harmful outputs with "circuit breakers." Existing techniques aimed at improving alignment, such as refusal training, are often bypassed. Techniques such as adversarial training try to plug these holes by countering specific attacks. As an alternative to refusal training and adversarial training, circuit-breaking directly controls the representations that are responsible for harmful outputs in the first place. Our technique can be applied to both text-only and multimodal language models to prevent the generation of harmful outputs without sacrificing utility -- even in the presence of powerful unseen attacks. Notably, while adversarial robustness in standalone image recognition remains an open challenge, circuit breakers allow the larger multimodal system to reliably withstand image "hijacks" that aim to produce harmful content. Finally, we extend our approach to AI agents, demonstrating considerable reductions in the rate of harmful actions when they are under attack. Our approach represents a significant step forward in the development of reliable safeguards to harmful behavior and adversarial attacks.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/XKCD65CA/Zou et al. - 2024 - Improving Alignment and Robustness with Circuit Br.pdf;/Users/leonardbereska/Zotero/storage/MIFRFMGG/2406.html}
}

@misc{fort_ensemble_2024,
  title = {Ensemble everything everywhere: Multi-scale aggregation for adversarial robustness},
  shorttitle = {Ensemble everything everywhere},
  author = {Fort, Stanislav and Lakshminarayanan, Balaji},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05446},
  eprint = {2408.05446},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05446},
  url = {http://arxiv.org/abs/2408.05446},
  urldate = {2024-08-16},
  abstract = {Adversarial examples pose a significant challenge to the robustness, reliability and alignment of deep neural networks. We propose a novel, easy-to-use approach to achieving high-quality representations that lead to adversarial robustness through the use of multi-resolution input representations and dynamic self-ensembling of intermediate layer predictions. We demonstrate that intermediate layer predictions exhibit inherent robustness to adversarial attacks crafted to fool the full classifier, and propose a robust aggregation mechanism based on Vickrey auction that we call {\textbackslash}textit\{CrossMax\} to dynamically ensemble them. By combining multi-resolution inputs and robust ensembling, we achieve significant adversarial robustness on CIFAR-10 and CIFAR-100 datasets without any adversarial training or extra data, reaching an adversarial accuracy of \${\textbackslash}approx\$72\% (CIFAR-10) and \${\textbackslash}approx\$48\% (CIFAR-100) on the RobustBench AutoAttack suite (\$L\_{\textbackslash}infty=8/255)\$ with a finetuned ImageNet-pretrained ResNet152. This represents a result comparable with the top three models on CIFAR-10 and a +5 \% gain compared to the best current dedicated approach on CIFAR-100. Adding simple adversarial training on top, we get \${\textbackslash}approx\$78\% on CIFAR-10 and \${\textbackslash}approx\$51\% on CIFAR-100, improving SOTA by 5 \% and 9 \% respectively and seeing greater gains on the harder dataset. We validate our approach through extensive experiments and provide insights into the interplay between adversarial robustness, and the hierarchical nature of deep representations. We show that simple gradient-based attacks against our model lead to human-interpretable images of the target classes as well as interpretable image changes. As a byproduct, using our multi-resolution prior, we turn pre-trained classifiers and CLIP models into controllable image generators and develop successful transferable attacks on large vision language models.},
  archiveprefix = {arXiv},
  file = {/Users/leonardbereska/Zotero/storage/7TTGA5XP/Fort and Lakshminarayanan - 2024 - Ensemble everything everywhere Multi-scale aggreg.pdf;/Users/leonardbereska/Zotero/storage/3XQENH6Z/2408.html}
}

@inproceedings{bos_adversarial_2024,
  title = {Adversarial Circuit Evaluation},
  booktitle = {ICML 2024 Workshop on Mechanistic Interpretability},
  author = {uit de Bos, Niels and {Garriga-Alonso}, Adri{\`a}},
  year = {2024},
  month = jun,
  url = {https://openreview.net/forum?id=I5E9ZZNBjT},
  urldate = {2024-08-16},
  abstract = {Circuits are supposed to accurately describe how a neural network performs a specific task, but do they really? We evaluate three circuits found in the literature (IOI, greater-than, and docstring) in an adversarial manner, considering inputs where the circuit's behavior maximally diverges from the full model. Concretely, we measure the KL divergence between the full model's output and the circuit's output, calculated through resample ablation, and we analyze the worst-performing inputs. Our results show that the circuits for the IOI and docstring tasks fail to behave similarly to the full model even on completely benign inputs from the original task, indicating that more robust circuits are needed for safety-critical applications.},
  language = {en},
  file = {/Users/leonardbereska/Zotero/storage/NN2RP3RD/Bos and Garriga-Alonso - 2024 - Adversarial Circuit Evaluation.pdf}
}