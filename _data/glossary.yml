- term: prediction orthogonality
  definition: A model whose objective is prediction can simulate agents who optimize toward any objectives with any degree of optimality.
- term: linear representation
  definition: Features are directions in activation space, i.e., linear combinations of neurons.
- term: motifs
  definition: Repeating patterns that emerge across models and tasks, manifesting as circuits, features, or higher-level behaviors from component interactions. Examples include curve detectors, induction circuits, and branch specialization. Motifs reveal common structures and mechanisms underlying neural network intelligence.
- term: internal world models
  definition: Internal causal environment models formed within neural networks, implicitly emerging as a by-product of prediction (e.g., in large language models).
- term: simulacra
  definition: The text outputs generated by a predictive model simulating the causal processes underlying text creation. These outputs simulate coherent and contextually relevant language, sometimes exhibiting agentic behaviors or goals despite the predictive model itself lacking genuine agency or intentionality. Simulacra can be either agentic, mimicking intentional and persuasive language use, or non-agentic, merely generating descriptive text without simulated goals or agency.
- term: natural abstractions
  definition: High-level summaries or descriptions of a system or environment learned and used by many cognitive systems. According to the natural abstraction hypothesis, a set of "natural" abstractions exist that represent redundantly encoded information in the world and tend to be learned by intelligent systems produced through local selection pressures. These natural abstractions form a relatively small, discrete set of concepts like "tree," "velocity," etc., that allow compact descriptions of the world while discarding many irrelevant low-level details.
- term: circuits
  definition: Sub-graphs within neural networks consisting of features and the weights connecting them. Circuits can be thought of as computational primitives that perform understandable operations to produce (ideally interpretable) features from prior (ideally interpretable) features. Examples include circuits for detecting curves at specific orientations, continuing repeated patterns in text, and resolving anaphoric references. While circuits can involve clearly interpretable features, the definition allows for intermediate representations that are less easily interpretable.
- term: simulation
  definition: The simulation hypothesis says that when scaled up sufficiently, predictive models will learn to simulate the real-world causal processes that generated their training data. When these models are optimized for predictive accuracy on broad data distributions like natural language, they are incentivized to discover the underlying rules, physics, and semantics that govern the data to model and predict future observations effectively. This allows the models to go beyond just memorizing or pattern-matching their training sets, instead learning to simulate hypothetical scenarios, reason about counterfactuals, and exhibit behaviors characteristic of general intelligence -- all as a byproduct of the drive for efficient compression and accurate prediction. The simulation hypothesis suggests these models will develop rich internal world models capturing the causal dynamics of the training distribution.
- term: features
  definition: The fundamental units of how neural networks encode knowledge, which cannot be further decomposed into smaller, distinct concepts. Features are core components of a neural network's representation, analogous to how cells form the fundamental unit of biological organisms. The superposition hypothesis suggests an alternative definition - that features correspond to the disentangled concepts that a larger, sparser network with sufficient capacity would learn to represent with individual (monosemantic) neurons.
- term: concepts
  definition: An abstract idea or representation derived from observations of the world. Concepts refer to the natural abstractions that a cognitive system, like a neural network, aims to capture and represent through its learned features, which may or may not align perfectly with human-defined concepts.
- term: disentangled
  definition: In disentangled representations, individual dimensions or components correspond to distinct, independent factors of variation in the data, rather than representing a tangled mixture of these factors.
- term: monosemantic
  definition: A neuron corresponding to a single concept. The intuition is that analyzing what inputs activate a given neuron reveals its associated semantic meaning or concept. In contrast to polysemantic.
- term: polysemantic
  definition: Neurons that are associated with multiple, unrelated concepts, contradicting the interpretation of neurons as representational primitives and making it challenging to understand the information processing of neural networks. This term is derived from linguistic concepts of polysemy, and in the context of neural networks first introduced by Arora et al., who suggested that word embeddings of polysemous words may be stored as a superposition of vectors representing distinct meanings. Olah et al. first used the term polysemanticity, elaborating on the concept of polysemantic neurons as a challenge for mechanistic interpretability.
- term: superposition
  definition: The superposition hypothesis suggests that neural networks can leverage high-dimensional spaces to represent more features than the actual count of neurons by encoding features in almost orthogonal directions.
- term: modularity
  definition: The property of an AI system being composed of distinct, semi-independent components or submodules that can be separately understood, modified, and recombined, rather than a monolithic, opaque structure.
- term: universality
  definition: The universality hypothesis proposes the emergence of common circuits across neural network models trained on similar tasks and data distributions. A stronger form posits that these common circuits represent a set of fundamental computational motifs that neural networks gravitate towards when learning. The weaker version suggests that for a given task, dataset, and model architecture, an optimal way to solve the problem may exist, which different models will tend to converge towards, resulting in analogous circuits. The universality hypothesis implies that rather than each model learning arbitrary, unstructured representations, there is an underlying universality to the circuits that emerge, shaped by the learning task and inductive biases.
- term: representation engineering
  definition: A top-down approach to transparency research that treats representations as the fundamental unit of analysis, aiming to understand and control representations of high-level cognitive phenomena in neural networks like large language models. Representation engineering has two main areas - 1) Reading representations to probe and interpret their contents, and 2) Controlling representations to manipulate high-level concepts like honesty or morality.
- term: privileged basis
  definition: In certain neural network representations, the basis directions formed by the individual neurons are architecturally distinguished from arbitrary directions in the activation space. This privileged basis makes it meaningful to analyze the properties and roles of individual neurons, as the architecture encourages features to align with these basis directions. Hence, a privileged basis is necessary but not sufficient for the formation of monosemantic neurons.
- term: streetlight interpretability
  definition: Examining AI systems under only ideal conditions of maximal interpretability, risking missing critical phenomena that only emerge in more realistic and diverse contexts.
- term: machine unlearning
  definition: Techniques for removing private data or dangerous knowledge from models.
- term: reverse engineering
  definition: The process of deconstructing a neural network's computations to fully understand and specify its operations. This involves breaking down the network's functionality into explicit, interpretable components, potentially as clear and detailed as pseudocode.
- term: outer misalignment
  definition: Outer misalignment, or reward hacking, occurs when the specified reward function or utility function fails to capture the desired objectives correctly. This leads the AI to optimize for behaviors that achieve high reward scores but are misaligned with the intended outcomes.
- term: reward hacking
  definition: See outer misalignment.
- term: inner misalignment
  definition: Inner misalignment, or goal misgeneralization, occurs when an AI system develops goals or behaviors during training that are misaligned with the intended objectives despite a correctly specified reward signal.
- term: mesa-optimization
  definition: The emergence of unintended subagents within a model with their own objectives, potentially misaligned with the original training objective.
- term: eliciting latent knowledge
  definition: Developing strategies to make a machine learning model explicitly report latent facts or knowledge embedded in its parameters, especially in cases where the model's output is untrusted. This involves finding patterns in neural network activations that track the true state of the world.
- term: well-founded AI
  definition: Developing AI systems with provable safety guarantees about their behavior and alignment with human values through rigorous mathematical modeling and verification.
- term: microscope AI
  definition: Systems that extract and utilize knowledge from a model without allowing the model to take autonomous actions. This involves reverse engineering a trained model to understand its learned knowledge about the world, aiming to leverage this understanding directly without deploying the model in an operational capacity.
- term: grokking
  definition: "Grokking refers to the surprising phenomenon of delayed generalization where neural networks, on certain learning problems, generalize long after overfitting their training set."
- term: hydra effect
  definition: The phenomenon where models can internally self-repair and maintain capabilities even when key components are ablated, making it challenging to identify the relevant components underlying a particular behavior.
- term: oversight
  definition: (Scalable) oversight refers to the challenge of providing reliable supervision—through labels, reward signals, or critiques—to AI models, ensuring effectiveness even as models surpass human-level performance.
- term: iterative distillation and amplification
  definition: A technique for training AI systems by repeatedly distilling knowledge from a larger model into a smaller one while amplifying the smaller model's capabilities through feedback and interaction with humans.
- term: deceptive alignment
  definition: When a misaligned model aims to appear aligned to gain more power to take control once sufficiently powerful.
- term: deceptive inflation
  definition: Theoretical result on deceptive behavior - policies produce trajectories that look better than they actually are from the human's perspective with limited observations to get higher reward signals during training. This deceptive behavior arises in reinforcement learning from human feedback when the human provides feedback based only on partial observations of the trajectories, while the policy has full state information during training.
- term: sycophancy
  definition: The tendency of models to generate responses that align with user beliefs rather than providing truthful information. This behavior, encouraged by human feedback used in fine-tuning, is observed in state-of-the-art AI assistants across various tasks. Sycophancy arises because human preference judgments often favor responses that match users' views, leading to a preference for convincingly written sycophantic responses over correct ones.