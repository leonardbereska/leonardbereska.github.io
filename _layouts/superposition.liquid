<!doctype html>
<html>
  <head>
    {% include head.liquid %}
    {% if site.enable_medium_zoom %}
      <!-- Medium Zoom JS -->
      <script
        defer
        src="{{ site.third_party_libraries.medium_zoom.url.js }}"
        integrity="{{ site.third_party_libraries.medium_zoom.integrity.js }}"
        crossorigin="anonymous"
      ></script>
      <script defer src="{{ '/assets/js/zoom.js' | relative_url | bust_file_cache }}"></script>
    {% endif %}
    {% include scripts/jquery.liquid %}
    {% include scripts/mathjax.liquid %}
    <!-- Distill js -->
    <script src="{{ '/assets/js/distillpub/template.v2.js' | relative_url }}"></script>
    <script src="{{ '/assets/js/distillpub/transforms.v2.js' | relative_url }}"></script>
    <script src="{{ '/assets/js/distillpub/overrides.js' | relative_url }}"></script>
    {% if page._styles %}
      <!-- Page/Post style -->
      <style type="text/css">
        {{ page._styles }}
      </style>
    {% endif %}
    <script src="https://unpkg.com/@popperjs/core@2"></script>
    <script src="https://unpkg.com/tippy.js@6"></script>
    <script src="{{ '/assets/js/distillpub/glossary-tooltips.js' | relative_url }}"></script>
    <link rel="stylesheet" href="https://unpkg.com/tippy.js@6/themes/light.css"/>

    <!-- Dublin Core metadata -->
    <meta name="dc.title" content="Superposition as Lossy Compression — Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability">
    <meta name="dc.creator" content="Bereska, Leonard">
    <meta name="dc.creator" content="Tzifa-Kratira, Zoe">
    <meta name="dc.creator" content="Samavi, Reza">
    <meta name="dc.creator" content="Gavves, Efstratios">
    <meta name="dc.date" content="2025-12">
    <meta name="dc.type" content="article">
    <meta name="dc.identifier" content="https://openreview.net/forum?id=qaNP6o5qvJ">
    <meta name="dc.description" content="Neural networks compress information through superposition: representing more features than neurons by encoding them as overlapping directions in activation space. This phenomenon creates polysemantic neurons that respond to multiple unrelated concepts, fundamentally challenging mechanistic interpretability. We present an information-theoretic framework quantifying superposition as lossy compression by measuring effective degrees of freedom through Shannon entropy applied to sparse autoencoder (SAE) activations. Our metric reveals each layer's compression ratio—the number of virtual neurons simulated relative to physical dimension—enabling principled measurement without ground truth features. Validation on toy models achieves strong correlation (r=0.94) with observable interference patterns. Applications reveal systematic organizational principles: dropout reduces features through redundant encoding under capacity constraints, algorithmic tasks resist superposition despite compression (operating at the lossless F≈N boundary), grokking exhibits sharp feature consolidation during phase transitions, and Pythia-70M shows non-monotonic layer-wise patterns mirroring intrinsic dimensionality studies. Contrary to the superposition-vulnerability hypothesis, adversarial training does not universally reduce superposition. Instead, its effect bifurcates based on task complexity relative to network capacity: abundance regimes (simple tasks with ample capacity) enable defensive feature expansion, while scarcity regimes (complex tasks under constraints) force reduction. This bifurcation holds across architectures (MLPs, CNNs, ResNet-18) and datasets (MNIST, Fashion-MNIST, CIFAR-10), revealing a nuanced relationship between representational compression and adversarial robustness. By grounding superposition measurement in rate-distortion theory and dictionary learning, this work enables quantitative investigation of how neural networks organize information under computational constraints, connecting mechanistic interpretability to adversarial robustness through the lens of lossy compression.">

    <!-- Highwire Press tags -->
    <meta name="citation_title" content="Superposition as Lossy Compression — Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability">
    <meta name="citation_author" content="Bereska, Leonard">
    <meta name="citation_author" content="Tzifa-Kratira, Zoe">
    <meta name="citation_author" content="Samavi, Reza">
    <meta name="citation_author" content="Gavves, Efstratios">
    <meta name="citation_publication_date" content="2025/12">
    <meta name="citation_journal_title" content="Transactions on Machine Learning Research">
    <meta name="citation_abstract_html_url" content="https://openreview.net/forum?id=qaNP6o5qvJ">
    <meta name="citation_fulltext_html_url" content="https://leonardbereska.github.io/blog/2025/superposition">
  </head>

  <d-front-matter>
    <script async type="text/json">
      {
            "title": "{{ page.title }}",
            "description": "{{ page.description }}",
            "published": "{{ page.date | date: '%B %d, %Y' }}",
            "authors": [
              {% for author in page.authors %}
              {
                "author": "{{ author.name }}",
                "authorURL": "{{ author.url }}",
                "affiliations": [
                  {
                    "name": "{{ author.affiliations.name }}",
                    "url": "{{ author.affiliations.url }}"
                  }
                ]
              }{% if forloop.last == false %},{% endif %}
              {% endfor %}
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script>
  </d-front-matter>

  <body class="{% if site.navbar_fixed %}fixed-top-nav{% endif %} {% unless site.footer_fixed %}sticky-bottom-footer{% endunless %}">
    <!-- Header -->
    {% include header.liquid %}

    <!-- Content -->
    <div class="post distill">
      <d-title>
        <h1>{{ page.title }}</h1>
        <p>{{ page.description }}</p>
      </d-title>
      {% if page.authors %}
        <d-byline></d-byline>
      {% endif %}

      <d-article>
        {% if page.toc %}
          <d-contents>
            <nav class="l-text figcaption">
              <h3>Contents</h3>
              {% for section in page.toc %}
                <div>
                  <a href="#{{ section.name | slugify }}">{{ section.name }}</a>
                </div>
                {% if section.subsections %}
                  <ul>
                    {% for subsection in section.subsections %}
                      <li>
                        <a href="#{{ subsection.name | slugify }}">{{ subsection.name }}</a>
                        {% if subsection.subsubsections %}
                          <ul>
                            {% for subsubsection in subsection.subsubsections %}
                              <li>
                                <a href="#{{ subsubsection.name | slugify }}">{{ subsubsection.name }}</a>
                              </li>
                            {% endfor %}
                          </ul>
                        {% endif %}
                      </li>
                    {% endfor %}
                  </ul>
                {% endif %}
              {% endfor %}
            </nav>
          </d-contents>
        {% endif %}
        {{ content }}
      </d-article>

      <d-appendix>
        <h3>Citation Information</h3>
        <p>Please cite as:</p>
        <pre class=citation>Bereska, L., Tzifa-Kratira, Z., Samavi, R. & Gavves, E. Superposition as Lossy Compression — Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability. TMLR (2025). https://openreview.net/forum?id=qaNP6o5qvJ</pre>
        <p>BibTeX Citation:</p>
        <div class="language-bibtex">
          <div class="highlight">
            <div class="code-display-wrapper">
              <pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">bereska2025superposition</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Superposition as Lossy Compression — Measure with Sparse Autoencoders and Connect to Adversarial Vulnerability}</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Bereska, Leonard and Tzifa-Kratira, Zoe and Samavi, Reza and Gavves, Efstratios}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span>   <span class="p">=</span> <span class="s">{Dec}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Transactions on Machine Learning Research}</span><span class="p">,</span>
  <span class="na">url</span>     <span class="p">=</span> <span class="s">{https://openreview.net/forum?id=qaNP6o5qvJ}</span>
<span class="p">}</span>
</code></pre>
              <button class="copy" type="button" aria-label="Copy code to clipboard">
                <i class="fa-solid fa-clipboard"></i>
              </button>
            </div>
          </div>
	</div>
	<h3>Acknowledgments</h3>
	<p>We thank Hamed Karimi for detailed feedback on the manuscript that improved clarity and presentation and discussions together with Daniel Sadig on adversarial training mechanisms. We are grateful to <a href="https://jackiebereska.github.io/">Jacqueline Bereska</a> for valuable suggestions on manuscript organization and prioritization. We thank the anonymous TMLR reviewers for their rigorous feedback, particularly on statistical methodology and theoretical foundations, which substantially strengthened this work.</p>
	<p>Part of this research was conducted during L.B.'s visit to the <a href="https://tailab.org">Trustworthy AI Lab (TAILab)</a> at <a href="https://www.torontomu.ca">Toronto Metropolitan University</a>, directed by <a href="https://www.ee.torontomu.ca/~samavi/">Reza Samavi</a>. We are grateful for the stimulating research environment that facilitated the development of the core conceptual framework.</p>
	<d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="{{ page.bibliography | prepend: '/assets/bibliography/' | relative_url }}"></d-bibliography>

      {% if site.disqus_shortname and page.disqus_comments %}{% include disqus.liquid %}{% endif %}
      {% if site.giscus.repo and page.giscus_comments %}
        {% include giscus.liquid %}
      {% endif %}
    </div>

    <!-- Footer -->
    {% include footer.liquid %}
    {% include scripts/bootstrap.liquid %}
    {% include scripts/analytics.liquid %}
    {% include scripts/progressBar.liquid %}
    {% include scripts/back_to_top.liquid %}
  </body>
</html>
